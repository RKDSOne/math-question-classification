"<p>Suppose I have an linear transformation described by a square matrix $A$.
Then, $A$ takes a vector $x\in\mathbb{R}^n$ to a vector $y\in\mathbb{R}^n$ related to $x$ by the equation $y=Ax$.</p>

<p>Grant Sanderson defined the determinant of $A$ on his youtube series on linear algebra in the following way:</p>

<p>Two vectors $x$ and $y$ determine a unique paralelogram. On the same way $Ax$ and $Ay$ determine another paralelogram. The ratio between its areas is defined to be the determinant of $A$. That is $|\det(A)|=\text{Area}(Ax,Ay)/\text{Area}(x,y)$. (The sign of the determinant is given by the orientation of the vectors. If $A$ preserves orientation, its determinant is positive.)</p>

<p>Can't we define a determinant to non-square matrices by the same formula?</p>

<p>Link to Grant's video: <a href=""https://www.youtube.com/watch?v=v8VSDg_WQlA"" rel=""nofollow"">https://www.youtube.com/watch?v=v8VSDg_WQlA</a></p>

<p>EDIT: (Example)
Take 
$$A=\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}$$
and
$$x=\begin{bmatrix} 1 \\ 0\end{bmatrix}, \quad y=\begin{bmatrix} 0 \\ 1\end{bmatrix}.$$
Using the notation defined above, we clearly have $\text{Area}(x,y)=1$.
We have that $$Ax=\begin{bmatrix} 1 \\ 3\end{bmatrix}, \quad Ay=\begin{bmatrix} 2 \\ 4\end{bmatrix}$$ too. So, $\text{Area}(Ax,Ay)=2$. Then the ratio between the areas is $2$, which coincides with the modulus of the determinant of $A$. (Of course $A$ changes the orientation of the vectors, so it's determinant is negative.)</p>

<p>EDIT 2: Here's an explicit definition of the determinant.
$$|\det(A)| =
  \begin{cases}
    \text{Area}(Ax,Ay)/\text{Area}(x,y)       &amp; \quad \text{if } \text{kernel}(A) \text{ is zero}\\
    0  &amp; \quad \text{if } \text{kernel}(A) \text{ is non-zero}\\
  \end{cases},$$
for any linearly independent vectors $x$ and $y$.</p>

<p>If $A$ preserves orientation, then $\det(A)&gt;0$. $\det(A)\leq 0$ otherwise.</p>
",<linear-algebra>
"<p>Let $T$ be a linear operator on a vector space $V$ such that its matrix representation with respect to the basis $(v_1, v_2, v_3, v_4, v_5, v_6)$ is</p>

<p>\begin{bmatrix}
    1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 \\
\end{bmatrix}</p>

<p>Find a basis for $V$ such that the matrix associated to $T$ is</p>

<p>\begin{bmatrix}
    2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
\end{bmatrix}</p>

<p>From the first matrix, I can tell the following information:
$$T(v_1)=v_1,$$
$$T(v_2)=v_1+v_2,$$
$$T(v_3)=v_3,$$
$$T(v_4)=2v_4+v_5,$$
$$T(v_5)=2v_5+v_6,$$
$$T(v_6)=2v_6.$$</p>

<p>Does that information help me attain any information on the basis associated to the second matrix? These matrices look like they are in Jordan form (almost), but I don't know what to do with that information. Please help me! Thank you.</p>
",<linear-algebra>
"<p>Let $\left\{ q_1, q_2, q_3, q_4 \right\}$ be an orthonormal basis of an inner product space $H$, and let</p>

<p>$$H_1=\text{span} [q_1, q_2], \quad H_2=\text{span} [q_3, q_4]. $$</p>

<p>Let $T$ be an operator on $H$ that satisfies</p>

<p>$$ Tw= \alpha w \quad \forall w\in H_1, \quad \quad Tw'=I\alpha w' \quad\forall w'\in H_2, $$</p>

<p>where $\alpha \neq 0$ is a complex number and $I$ is the imaginary unit. Show that $T^4=\alpha^4 I$. Use this to give a closed form for $T^{-1}$.</p>

<p>Here is what I have done:</p>

<p>Since $\left\{ q_1, q_2, q_3, q_4 \right\}$ is an orthonormal basis of $H$, then $H=H_1 \oplus H_2$ $\,$ ($H$ is the direct sum of $H_1$ and $H_2$), since every vector in $H_1$ is orthogonal to every vector in $H_2$, and vice versa. </p>

<p>Let $v\in H$. Then we can uniquely express $v=w+w'$, where $w \in H_1$ and $w'\in H_2$. Then
$$Tv=Tw+Tw'=\alpha(w+iw'),$$
$$T^2v= \alpha T(w+iw')=\alpha^2(w-w'),$$
$$T^3v=\alpha^2T(w-w')=\alpha^3(w-iw'),$$
$$T^4v=\alpha^3T(w-iw')=\alpha^4(w+w')=\alpha^4v=\alpha^4Iv.$$</p>

<p>Hence, $T^4=\alpha^4 I$.</p>

<p>How can I use this to find a closed form for $T^{-1}$? Since $T^4=\alpha^4 I$ I know that $T^4$ is invertible, but I am not sure where to go from there. I appreciate any help.</p>
",<linear-algebra>
"<p>Define a series of matrices$$H_N=
\begin{bmatrix}
1&amp;1/N&amp;1/N&amp;\cdots&amp;1/N\\
1/N&amp;2&amp;1/N&amp;\cdots&amp;1/N\\
1/N&amp;1/N&amp;3&amp;\cdots&amp;1/N\\
\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\
1/N&amp;1/N&amp;1/N&amp;\cdots&amp;N
\end{bmatrix}$$
My question is, when $N\to+\infty$, would the eigenvalues of $H$ be different from $\{1,\ldots,N\}$ ?
The answer is not obvious, as,  for the matrix series
$$G_N=\begin{bmatrix}
1&amp;1/N&amp;1/N&amp;\cdots&amp;1/N\\
1/N&amp;1&amp;1/N&amp;\cdots&amp;1/N\\
1/N&amp;1/N&amp;1&amp;\cdots&amp;1/N\\
\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\
1/N&amp;1/N&amp;1/N&amp;\cdots&amp;1
\end{bmatrix}$$
You can verify that $G_N$ has an eigenvalue of $2$.</p>
",<linear-algebra>
"<p>Number of 2x2 matrix over $z_3$ with determinant 1 
I know number of elements in $z_3$ are {0,1,2} now possibly determinant can be 1 in this case </p>

<p>$$
\begin
{bmatrix}
1&amp;2\\
0&amp;1\\
\end{bmatrix}
$$
and there can be many more but how to find the exact number of such matrices? ""the answer is 24""</p>
",<linear-algebra>
"<p>If there are two subspaces of Euclidean space of dimension $n$, denoted as $R_1$ and $R_2$. Suppose that $M$ is $n$ by $n$ matrix, and suppose that $R_1$, $R_2$ and their orthogonal complement are invariant with left multiplication of $M$.</p>

<p>The problem is to prove that the orthogonal complement of intersection of $R_1$ and $R_2$ are invariant with $M$ as well.</p>
",<linear-algebra>
"<p>For each of the following linear operators:</p>

<ul>
<li>Choose a basis in each of L, M</li>
<li>Find the corresponding matrix of given linear operator D : L → M</li>
</ul>

<p>a) L = {f(x); f(x) is a polynomial, degree of f is ≤ 3}, </p>

<p>M = {f(x); f(x) is a polynomial, degree of f is ≤3 and f(1)=f′(1)=0},</p>

<p>D(f)=(x−1)2f′′</p>

<p>This is what work is done thus far, any suggestions after this?</p>

<p>f: ax<sup>3</sup> + bx<sup>2</sup> + cx + d</p>

<p>f ': 3ax<sup>2</sup> 2bx + c</p>

<p>f '': 6ax + 2b</p>

<p>f<sub>new</sub>: 6ab(x-1)<sup>2</sup> + 2b = 6a(x<sup>2</sup>-2x+1)+2b = 6ax<sup>2</sup>-12ax+6a+2b</p>
",<linear-algebra>
"<blockquote>
  <p>Consider a linear map $L:\mathbb{R}^3 \to \mathbb{R}^3$ with
  $L\left( {\bf x} \right) = \left[ {\begin{array}{*{20}{c}}
1&amp;0&amp;1\\
1&amp;1&amp;2\\
2&amp;1&amp;3
\end{array}} \right]{\bf x}$. Show that this $L$ is not one-to-one.</p>
</blockquote>

<p>I know this is not one-to-one because the kernel of $L$ is not $\{{\bf 0}\}$. However, if I want to stick with the one-to-one definition, I got some trouble there. Here is my thinking: Assume
$L({\bf x}) = L({\bf y})$, then I must show ${\bf x} = {\bf y}$. So pick 
${\bf x} = [x_1, x_2, x_3]^T$ and ${\bf y} = [y_1, y_2, y_3]^T$ both lives in $\mathbb{R}^3$.
Then from the hypothesis of $L({\bf x}) = L({\bf y})$, I got
$$\left[ \begin{array}{l}
{x_1} + {x_3}\\
{x_1} + {x_2} + 2{x_3}\\
2{x_1} + {x_2} + 3{x_3}
\end{array} \right] = \left[ \begin{array}{l}
{y_1} + {y_3}\\
{y_1} + {y_2} + 2{y_3}\\
2{y_1} + {y_2} + 3{y_3}
\end{array} \right]$$
I think this should gives me $x_i = y_i$ for $i=1,2,3.$ which means ${\bf x} = {\bf y}$. and I got $L$ one-to-one but this contradicts to the kernel statement...</p>

<p>Can anyone help to pointing out which part I went wrong. Thank you.</p>
",<linear-algebra>
"<p>In the last 2 lectures of linear algebra we have talked about linear mappings and other stuff, but I missed actually the last one and I am quite in bad situation.</p>

<p>What matrix transforms $\left(\begin{matrix} 1 \\ 0\end{matrix}\right)$ into $\left(\begin{matrix} 2 \\ 6\end{matrix}\right)$ and tranforms $\left(\begin{matrix} 0 \\ 1\end{matrix}\right)$ into $\left(\begin{matrix} 4 \\ 8\end{matrix}\right)$?</p>

<p>I think I understood what I need to find: a matrix that multiplies our initial matrix formed by our initial vectors $$\left(\begin{matrix} 1 &amp; 0 \\ 0 &amp; 1\end{matrix}\right)$$ </p>

<p>and the resulting matrix is:
$$\left(\begin{matrix} 2 &amp; 6 \\ 4 &amp; 8\end{matrix}\right)$$</p>

<p>Am I right?</p>

<p>Is there a way to automate this process? </p>
",<linear-algebra>
"<p>Like the title says, ""If an $n\times n$ matrix $A$ is diagonalizable and has only one eigenvalue $\lambda$ with multiplicity $n$, then $A = \lambda I$. True or False?"" </p>

<p>My gut is telling me that this is true, but I'm having a little difficulty proving it formally. There's probably a very obvious proof, but thus far it has alluded me. </p>

<p>In any case, this is what I've done:</p>

<p>Since $A$ is diagonalizable, there exists a factorization such that $A = S^{-1}\Lambda S$. Since we know the eigenvalues are $\lambda$ with multiplicity $n$, $\Lambda = \text{diag}(\lambda, \lambda, ...)$. </p>

<p>If I'm given $A = \lambda I$, it's easy to show $\lambda I = S^{-1}\Lambda S$ where $S = I$ and $\Lambda = \text{diag}(\lambda, \lambda, ...)$. (Having normalized the eigenvectors). The trouble I'm having is showing $A = \lambda I$ under the given conditions. </p>

<p>I can simply let $S = \lambda I$, and thus $A = \lambda I$, but something about this approach feels off. (This is just my line of thought right now).</p>

<p>Any input would be appreciated.</p>
",<linear-algebra>
"<p>I have just started auditing Linear Algebra, and the first thing we learned was how to solve a system of linear equations by reducing it to row echelon form (or reduced row echelon form). I know how to solve a system of linear equations using determinants, and so the using row echelon form seems to be very inefficient and an easy way to make mistakes.</p>

<p>The lecturer seemed to indicate that this will be useful in the future for something else, but a friend who has a PhD in mathematics said it is totally useless.</p>

<p>Is this true? Is this technique totally useless or is there some ""higher mathematics"" field/ technique where row echelon form will be useful?</p>
",<linear-algebra>
"<p>I am working a bit on a collection of Linear Algebra examples, 
as well as some examples on induction. This is what is taught freshman year at our university.</p>

<p>I intend to release this to the public, either by selling printed copies or releasing it online. </p>

<p>Since I do not have experience using such material myself, there are some questions I would like some opinions on:</p>

<ul>
<li>How much theory should I include? Is references to course litterature enough?</li>
<li>Is there a format preference? Small text, so that the collection is more enviromental-friendly, or with big marginals for notes?</li>
<li>Best way to deal with misprints?</li>
<li>Should induction and Linear algeba be separate pieces? </li>
</ul>

<p>Please share your experience if you have done something similar.</p>

<p>EDIT:
An answer I seek is something along the lines of:
""I am a ""something"" stident, and I prefer ""something"", and would like to see more of ""something"".</p>
",<linear-algebra>
"<p>I have a 2x2 matrix A with rows (1 0) and (1 1).  How can I find their matrix exponential, e^A ?</p>

<p>I understand I need to plug it into the Taylor series but I'm lost at how to solve the series here.</p>
",<linear-algebra>
"<p>A tutorial sheet has the following problem.</p>

<blockquote>
  <p>Find a unit normal vector and a basis for the tangent space of the
  following smooth manifold $M \subseteq \mathbb{R}^2$ at a point $(a,b) \in M$. $$M=\{(x,y) \in \mathbb{R}^2 : x^2+y^2=1\}$$</p>
</blockquote>

<p>The idea is to do it without finding an explicit parametrization for $M$ (although of course, that is quite easy.)</p>

<p>The solution sheet says this:</p>

<blockquote>
  <p>Have $f(x,y) = x^2+y^2 = 1$ so a normal is $\nabla f = (2x,2y) = (2a,2b)$ at $(x,y) = (a,b)$.</p>
  
  <p>Tangent space = $\mathrm{ker}(Df) = (\nabla f)^\perp = \{v \in \mathbb{R}^2 : v \cdot (2a,2b) = 0\}$ has basis $\{(-b,a)\}$.</p>
</blockquote>

<p>The stuff about normals makes sense, but I don't get the line about tangent spaces. In particular:</p>

<ol>
<li>What is the difference between $Df$ and $\nabla f$? Aren't they the same?</li>
<li>How do we get from $\mathrm{ker}(Df)$ to $(\nabla f)^\perp$? What is the general principle here?</li>
</ol>
",<linear-algebra>
"<p>Hi I am trying to work on a proof question that I took note of because the answer did not come to mind immediately. I also found it was a good question to illustrate something I do not understand very well.</p>

<p>The question asks to prove, or disprove that if the system $AX=0$ has infinite solutions, then so to does the system $AX=B$ for any choice of $B$.</p>

<p>First off, I get confused by ""For any choice of $B$"". I thought $B$ represented the column of constants? How could you choose any $B$? Is that not implying that $B$ can be $(a,b,c,,,n)$ for any $a,b,c,,n$, in the real numbers?</p>

<p>Anyways, in regards to the proof. I know that if $AX=0$ has infinite solutions, then there must be atleast one parameter, thus $(n-r)\geq 1$ where $n$ is the number of variables and $r$ the rank of the matrix. I am not sure where to go from here. </p>

<p>Hopefully you guys can help me with this and get a better understanding!</p>

<p>Thank you,</p>
",<linear-algebra>
"<p>Denote: $[A]$ as the span of $A$. <br></p>

<p>Theorem: Every linearly independent subset of a vector space is a subset of a basis of a space. <br></p>

<p><em>Proof:</em> Let $A$ be a linearly independent subset of a vector space $V$ and let $\,B\,$ be a basis of $\,V$. <br></p>

<p>Case 1. $B\subseteq [A]\,$. Then $\,[B]\subseteq [A]\,$. But $\,[B]=V\,$ , hence, $\,A=B\,$. <br></p>

<p>Case 2. $B\not\subseteq [A]$. Then there exists $\alpha \in B$ such that $\alpha \not\in [A]$.
Thus $A\cup \{\alpha\}$ is linearly independent. Repeat the argument until an enlarged set is produced that spans $V$.</p>

<blockquote>
  <blockquote>
    <p>My question is: In case 2, how is it that $A$ is contained in a basis $B$? I just can't get the idea in the <em>repeating the argument</em> part. Thanks for your help.</p>
  </blockquote>
</blockquote>
",<linear-algebra>
"<p>In my class we're learning vector spaces, and in the text book there's an example with no solution and it goes like this:</p>

<blockquote>
  <p>If the domain of functions $f$ and $g$ is $[-1,1]$ and if they are defined $f(x) = \arcsin\left(\displaystyle\frac{2x}{1+x^2}\right)$ and $g(x) = \arctan(x)$, then $(f,g)$ is linearly independent?</p>
</blockquote>

<p>I don't know how to prove this, if I can make a linear combination of one of them using the other it's dependent, but how should I go about doing that?</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>Let $P_3(R)$ be the vectorspace of all real polynomials $\le 3$, such that the polynomial $p(x)=a_0+a_1x+a_2x^2+a_3x^3$ and let T be the linear operator on $P_3(R)$ that we get by defining $T$ as $T(p(x))=(x^3+x)p''(x)-2x^2p'(x)$ for all polynomials $p(x)\in P_3(R)$. Now decide a basis for the nullspace and the columnvector-space(Is this the same as $Im\,T$ in  english?) for $T$ or show that is only contains the zero-polynomial. Furthermore, find all eigenvalues for $T$, and examine if $T$ is diagonalizable(correct translation?)?</p>

<p>$p'(x)=a_1+2a_2x+3a_3x^2$, $p''(x)=2a_2+6a_3x$ </p>

<p>$Attempt:$
$T(a_0+a_1x+a_2x^2+a_3x^3)=(x^3+x)(2a_2+6a_3x)-2x^2(a_1+2a_2x+3a_3x^2) = x(2a_2)+x^2(-2a_1-4a_2+6a_3)+x^3(2a_2)=a_0(0)+a_1(-2x^2)+a_2(x-4x^2+2x^3)+a_3(6x^2)$</p>

<p>For the nullspace, let us find the solutions to $T(a_0+a_1x+a_2x^2+a_3x^3)=0$
Which means that $2a_2=-2a_1-4a_2+6a_3=2a_2=0 \to a_1=a_2=a_3=0$ If this is true then $a_0=0$. And the nullspace only contains the zero-polynomial. Im confused here, is this correct?   </p>

<p><em>Edit</em>: After correcting the computation mistake - I tried to convert to matrix-format, and got that the nullspace is spanned by $[1,3x+x^3]$. If this is correct, I should be able to continue.</p>

<p>For the $Im(a)$(?) we have $a_0(0)+a_1(-2x^2)+a_2(x-4x^2+2x^3)+a_3(6x^2)$ so we get the basis $[-2x^2, x-4x^2+2x^3,6x^2]$. I dont really understand this, and it´s probably wrong.</p>

<p>To find the eigenvalues we need to get this in matrix-form, right? How do we translate it into a matrix? If I could do that it would probably be easier for me to uderstand the basis aswell...</p>
",<linear-algebra>
"<p>Let $K/k$ be an extension of fields and let $v_1,\ldots,v_r,u_1,\ldots,u_r\in k^n$.  If the span of the $v$'s over $K$ equals the span of the $u$'s over $K$, must the two spans also be equal over $k$?</p>

<p>$$_K\langle v_1,\ldots,v_r\rangle=_K\langle u_1,\ldots,u_r\rangle\qquad\overset{?}\Longrightarrow\qquad_k\langle v_1,\ldots,v_r\rangle=_k\langle u_1,\ldots,u_r\rangle$$</p>

<p>Here is another way to look at this question: Given a subspace $U\subset K^n$ with some basis in $k^n$, is the procedure of restricting scalars to $k$ well-defined, or does it depend on the choice of basis of $U$?</p>
",<linear-algebra>
"<p>The information I have is for a matrix transformation from R^3 to R^3 (denoted by L()), L(a_1) = 3(a_1) and L(2(a_1))= (5,-3,6). Find L(3a_1-22a_1), L(-4a_1), L(0), L(4a_1).</p>

<p>What I tried to do was first solve for a_1. I factored out the two of L(2a_1) and then replaced L(a_1) with 3a_1. Resulting in the equation 2*3a_1 = (5,-3,6) so a_1 = (5/6,-1/2,1). </p>

<p>Now I tried to reduce L(3a_1 - 22a_1) to -19L(a_1) but that was not the correct answer, tried the same process with L(-4a_1)  = -2L(2a_1) which was also not the right result. </p>

<p>Not sure about L(0), I am assuming the zero represents the zero vector for R^3, but I would think that transforms to the zero vector but apparently not. </p>

<p>I'm sure I am misunderstanding the theory somewhere but I am not sure where. One thought I had was that I was using the properties of linear transformations but the problem only specified that it was a ""matrix transformation"". Any help is greatly appreciated. </p>
",<linear-algebra>
"<p>What is the derivative of Hadamard product of two matrices with respect to one of them?
I.e. what is $D(AB)$ with respect to $A$?</p>
",<linear-algebra>
"<p>Let $\ell^\infty$ be the bounded sequence space over the complex numbers and let $c_0$ the subspace of all sequences converging to $0$. </p>

<p>I am attempting to show that $\ell^\infty/c_0$ has infinite dimension. </p>

<p>I have looked at several different ways to show this but I think perhaps the easiest is to show that there exists an infinite linearly independent set in the quotient. It is easy to find an infinite countable set in $\ell^\infty$ (just take the sequences of the form $(0,\dots,0,1,0,\dots)$, but of course this set is identified in the quotient). I also was thinking of doing a proof by contradiction, and show that no matter what finite linearly independent set in $\ell^\infty/c_0$ you give, we can construct a sequence not in the span. But the precise way to construct such a sequence is not evident to me at this point (some sort of diagonal argument, perhaps).  </p>

<p>Will a cardinality argument be necessary?</p>
",<linear-algebra>
"<p>For the dynamic system $\dot x = Ax + Bu$ </p>

<p>There's a saying that this system is controllable when $Ker(B) \in Ker(A)$, which means that $u$ have the control in every dimension of $x$.</p>

<p>I have no problem with this theorem when you have single input $u$, which means $u$ is just a number and $B$ is with $n\times1$ dimension.</p>

<p>But I cannot completely understand when $u$ is a vector. Look at this example:</p>

<p>$A = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 
                     0 &amp;0&amp;0&amp;0\\0 &amp;0&amp;0&amp;0\\0 &amp;0&amp;0&amp;0\\ \end{bmatrix}$ , 
$B = \begin{bmatrix} 0&amp;0\\0&amp;0\\0&amp;0\\0&amp;0\\\end{bmatrix}$</p>

<p>This is obvious uncontrollable. However, look at the null space:</p>

<p>$Ker(A)=span\{\begin{bmatrix} 0\\1\\0\\0\\ \end{bmatrix},
            \begin{bmatrix} 0\\0\\1\\0\\ \end{bmatrix},
            \begin{bmatrix} 0\\0\\0\\1\\ \end{bmatrix} 
            \}$</p>

<p>$Ker(B)=span\{\begin{bmatrix} 1\\0 \end{bmatrix},
              \begin{bmatrix} 0\\1 \end{bmatrix}
            \}
$</p>

<p>Thus we have $Ker(B) \notin Ker(A)$, but this system is uncontrollable.</p>

<p>Can anyone help me with this contradictory?</p>

<p>Thank you very much and I appreciate all of your replies!</p>
",<linear-algebra>
"<p>Let $A \in \mathbb{C}^{n \times n}$ be an upper triangular matrix that satisfies $A^{*}A=AA^{*}$. Prove that $A$ must be diagonal.</p>

<p>My attempt is to partition $A$ as follows:</p>

<p>$$
A = \left[\begin{array}{cc} a_{11} &amp; \alpha\\
0 &amp; \hat{A}\end{array}\right]
$$</p>

<p>where $\alpha = (a_{12}, a_{13}, \cdots , a_{1n})$ and $\hat{A}$ is $A$ with the first row and column removed. Using this partitioning, we have:</p>

<p>$$
A^{*}A = \left[\begin{array}{cc} a_{11}^{2} &amp; a_{11}\alpha\\
a_{11}\alpha^{*} &amp; \alpha^{*}\alpha + \hat{A}^{*}\hat{A}\end{array}\right]
$$
$$
AA^{*} = \left[\begin{array}{cc} a_{11}^{2} + \alpha\alpha^{*} &amp; \alpha\hat{A}^{*}\\
\hat{A}\alpha^{*} &amp; \hat{A}\hat{A}^{*}\end{array}\right]
$$</p>

<p>Examining entry (1,1) of each of these matrix products, we see that $\alpha\alpha^{*} = 0$. From this, I would like to conclude that $\alpha = 0$ and thus, the first row of $A$ has non-zero entry only at (1,1). Then repeat this process continuously on $\hat{A}$.</p>

<p>However, I can see a flaw in my argument. If the entries of A were real, then this argument seems like it would work. But since the entries can be complex, this means that $\alpha\alpha^{*} = 0$ even with $\alpha \ne 0$. For example, $\alpha = (1, i, 1, i)$ gives $\alpha\alpha^{*} = 0$.</p>

<p>Any ideas how to proceed here? I know that with this partitioning, I must have $\alpha = 0$ since the problem statement is true (i.e. $A$ is diagonal).</p>
",<linear-algebra>
"<blockquote>
  <p>Let $\mathbb{R}^3$ be a vector space with canonical product, and $\alpha \in \mathbb{R}$. For each $\alpha$ let $F_{\alpha}=\{(x,y,z) \in \mathbb{R}^3: x=\alpha y \wedge \alpha y=\alpha z\}$ be a subspace.</p>
  
  <p>Find a basis for $F_{\alpha}$, based on $\alpha$.</p>
</blockquote>

<p>My work was:</p>

<p>$$
\begin{cases}
x=\alpha y \\
\alpha y=\alpha z
\end{cases} \Leftrightarrow\begin{cases}
x=\alpha y \\
\alpha y-\alpha z=0
\end{cases} \Leftrightarrow\begin{cases}
x=0 \\
\alpha=0 
\end{cases} \vee \begin{cases}x= \alpha z \\y=z \end{cases} $$</p>

<p>If $\alpha=0$ then $F_{0}=\{(0,y,z): y,z \in \mathbb{R}\}$. So $F_{0}=\langle (0,1,0),(0,0,1)\rangle$.</p>

<p>If $\alpha \neq0$ then $F_{\alpha}=\{(\alpha z,z,z): \alpha, z \in \mathbb{R}\}$. So $F_{\alpha}=\langle (\alpha,1,1)\rangle$.</p>

<p>But now which will be the basis of $F_{\alpha}$ based on $\alpha$ ? Thanks.</p>
",<linear-algebra>
"<p>I have determined a tangent plane to be
$$z = a(-b+y) + x(b-1)$$
$$ab = x(b-1) + ay - z$$
At the point (a,b)</p>

<p>I want to determine the normal to this plane as a function of a and b. I am not entirely sure as to what this means ""As a function of a and b"".</p>

<p>So I assume I will find the normal using the dot product of plane to normal = 0.
$$(b-1, a,-1) * (\zeta,\beta,\gamma) = 0$$
$$\zeta(b-1) + \beta*a - \gamma = 0$$
$$-\frac{\zeta(b-1) + \gamma,}\beta = a$$
$$\frac{a\beta - \gamma,}\zeta + 1 = b$$</p>

<p>I am not sure if this is what is wanted, or if there is a better way to solve this. If anyone can shed some light, that would be appreciated. This is from a past exam 2012S2 UQ, for course 'Multivariate Calculus and Ordinary Differential Equations'.</p>
",<linear-algebra>
"<p>Is it true that If the 2-norm of a symmetric real matrix is small, then the trace of the matrix is also small? I played around with some matrices in MATLAB and discovered this phenomenon. Does there exist any theorem that relates the 2-norm of a symmetric real matrix to the magnitude of its trace? </p>

<p>Thanks.</p>
",<linear-algebra>
"<p>Consider 
$$ S = \left\{(x,y) \in \mathbb{R}^2; -N-\frac{1}2 \le x \le N + \frac{1}2, |\alpha x-y| \le \frac{1}N \right\}$$</p>

<p>where $N \in \mathbb{N}, \alpha \in \mathbb{R}$.</p>

<p>I'm having a hard time beliveing that $S$ is convex and Area $S &gt; 4$. Can someone please elaborate why? </p>

<p>Thanks.</p>
",<linear-algebra>
"<p>So, I used Gaussian elimination on this matrix</p>

<p>$$\left( \begin{array}{c} -1 &amp; 3 &amp; 5 &amp; 13 \\ 3 &amp; -2 &amp; 2 &amp; 16 \end{array}\right)$$</p>

<p>to turn it to this:</p>

<p>$$\left( \begin{array}{c} -1 &amp; 3 &amp; 5 &amp; 13 \\ 0 &amp; 7 &amp; 17 &amp; 55 \end{array} \right)$$</p>

<p>I don't think this could be eliminated any further. </p>

<p>Which gives me these two equations:</p>

<p>$$-1a + 3b + 5c = 13$$</p>

<p>$$          7b + 17c = 55$$</p>

<p>Is there another method after this to simplify finding what the variables are, or is the only way to guess and check?</p>

<p>Thank you.</p>
",<linear-algebra>
"<p>If I have a set $S$ of vectors, say,
$$S=\{[4,6,2],[8,3,8],[2,4,5] \}.$$</p>

<p>How can I get all linear combinations of these vectors such that the linear combination will equal $[180,180,90]$ ?</p>

<p>edit:</p>

<p>I should be more specific, I am concerned with a set $S$ of arbitrary size. if the size of $S$ is greater than $3$, I will have free variables in my solution, right? Doesn't that mean that there are infinitely many solutions to the equality</p>

<p>suppose I am interested in all of those such that all constant multiples of the vectors in $S$ are greater than $0$. How would I go about finding the existence, and the exact values of such constants.</p>
",<linear-algebra>
"<p>Assume $A,B \in M_{n\times m}(\Bbb{R})$,and $A^TA=B^TB$,show that there exists an orthogonal matrix $P$, such that $A=PB$. </p>
",<linear-algebra>
"<p>Let $A$ be a linear operator which acts on the vector space $V=\langle x_1,x_2, \ldots,x_n\rangle$  by permutation of the basis vectors. Suppose we know its eigenvalues ( some roots of unity  ): $\lambda_1, \lambda_2, \ldots, \lambda_n.$</p>

<p>Now consider the vector space $V^{(2)} \subset {\rm Sym}^2 V$ generated by elements $x_i x_j, i&lt;j,$ $\dim V^{(2)}=\binom{n}{2}.$ Let us expand the operator $A$ on $V^{(2)}$    by linearity and by $A(x_i x_j)=A(x_i)A(x_j)$. Denote the extension by $A^{(2)}$. It is clear that  $A^{(2)}$ permutes the basis vectors of   $V^{(2)}$ so $A^{(2)}$   is an endomorphism of $V^{(2)}$.</p>

<p><strong>Question.</strong> What is the trace  of the $A^{(2)}?$ </p>

<p>By  method of trial and error I have found a formula for the trace
$$
{\rm Tr}(A^{(2)})=\sum_{i=1}^n\lambda_i^2+\sum_{i&lt;j}\lambda_i \lambda_j-\sum_{i=1}^n \lambda_i
$$
but I can't prove it. Any ideas?</p>
",<linear-algebra>
"<p>We need to determine values of k for which these have zero, one and infinite solutions.
$$x-2y=1,x-y+kz=-2,ky+4z=6$$
Now what I did:
$$A=\begin{pmatrix}1&amp;-2&amp;0\\1&amp;-1&amp;k\\0&amp;k&amp;4\end{pmatrix}$$ and $|A|=4-k^2$. When it has zero solution or when it is inconsistent we have $k^2=4\implies k=\pm 2$</p>

<p>When it is consistent or has one or infinite solutions then $|A|\ne0\implies k\ne\pm2$. How to differentiate between one and infinite solutions case of k.</p>

<p>I have also made rref:
$$M\sim\left[\begin{array}{ccc|c}1&amp;-2&amp;0&amp;1\\1&amp;-1&amp;k&amp;-2\\0&amp;0&amp;4-k^2&amp;6+k\end{array}\right]\sim\left[\begin{array}{ccc|c}1&amp;-2&amp;0&amp;1\\0&amp;1&amp;k&amp;-3\\0&amp;0&amp;4-k^2&amp;6+k\end{array}\right]\sim\left[\begin{array}{ccc|c}1&amp;-2&amp;0&amp;1\\0&amp;1&amp;k&amp;-3\\0&amp;0&amp;1&amp;\frac{6+k}{4-k^2}\end{array}\right]$$</p>
",<linear-algebra>
"<p>Given a $n \times n$ matrix $A = \begin{bmatrix}
a_1 &amp; a_2 &amp; \dots &amp; a_n
\end{bmatrix}$, where each $a_i$ are columns of $A$ for $i = 1 \dots n$.</p>

<p>Column mean is calculated by $\bar{a} =(a_1 + a_2 + \dots + a_n)/n$.</p>

<p>Then, define:</p>

<p>$B = \begin{bmatrix}
a_1 - \bar{a} &amp; a_2 - \bar{a}&amp; \dots &amp; a_n - \bar{a}
\end{bmatrix}$.</p>

<p>May I know why the rank$(B)$ is $n-1$?</p>

<p>I did try many times with computer program to prove but I want a proof of this.</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>I have the following 2 equations:</p>

<p>${6x + 9y = 3}$</p>

<p>${6x -3y = -2}$</p>

<p>The textbook asks to use the substitution method so I would appreciate if we stuck to this method, I could use the addition method but the book asks for this method.</p>

<p>So my steps of doing this are to isolate x in the first equation:</p>

<p>${6x + 9y = 3}$</p>

<p>I then divide both sides by 3 to give</p>

<p>${2x + 3y = 1}$</p>

<p>I then get:</p>

<p>${x = {-3y + 1\over 2}}$</p>

<p>I'm not entirely sure you to take this further using the substitute method.</p>
",<linear-algebra>
"<p>What is the generalization of the Pauli matrices and Dirac matrices in higher dimensions? I am actually looking for $\sqrt{\mathbb{I}}$ but I can't use the principal root which is just $\mathbb{I}$. For $n=3$, I found that the Gell-Mann matrices don't work. I'm thinking that these actually are the $SU(n)$ generators. Any ideas?</p>
",<linear-algebra>
"<p>I'm wondering about this problem on bilinear forms :</p>

<p>We have $\phi : \mathbb{M_{n}(R)}*\mathbb{M_{n}(R)} \rightarrow \mathbb{R}$ $$(A,B) \rightarrow trace(AB)$$</p>

<p>I've proved $\phi$ is a bilinear form and symetric, but how could we do to prove it is a non generate form ? </p>

<p>Besides if we have $\mathbb{S_{n}(R)}$ the subspace of $\mathbb{M_{n}(R)}$ formed by symmetric matrix, prove the restriction of $\phi$ to $\mathbb{S_{n}(R)}*\mathbb{S_{n}(R)}$ is also a non degenerate form, and then determine the orthogonal of $\mathbb{S_{n}(R)}$ for $\phi$</p>

<p>Thanks</p>
",<linear-algebra>
"<p>First of all, i don't know if the correct word is normalise or not, but I'll try to explain my issue.</p>

<p>I have a relationship between an object <code>A</code> and an object <code>B</code> equals 0.5 (max is 1)</p>

<p>you can consider this relationship as a vector that its length is 0.5</p>

<p>I have another vector between <code>A</code> and <code>C</code> and its value is 0.3</p>

<p>I have another vector between <code>A</code> and <code>D</code> and its value is 0.2</p>

<p>so these are the values that I have:</p>

<pre><code>A -&gt; B = 0.5
A -&gt; C = 0.3
A -&gt; D = 0.2
</code></pre>

<p>Now I want to <strong>give more weight to the relationships</strong>. In other words, I really care about <code>A -&gt; B</code> a lot, but I don't care so much about <code>A -&gt; C</code></p>

<p>so I want to give weight to those numbers, the weights are as this:</p>

<pre><code>the value of A -&gt; B should be multiply by 4
the value of A -&gt; C should be multiply by 1 (so no change)
the value of A -&gt; D should be multiply by 2
</code></pre>

<p>my problem is that if i multiply <code>A -&gt; B</code> by 4, then the result is 2, which is bigger than 1. all the values must be between 0 and 1.</p>

<p>my question is how (and what is the abstract equation) to normalise all these values in a correct way?</p>

<p>Regards</p>

<h3>update 1</h3>

<p>In the example I gave to you, it was co incidence that the sum of the values is 1, that's not necessary at all</p>
",<linear-algebra>
"<blockquote>
  <p>Let $N\in \text{Mat}(10 \times 10,\mathbb{C})$ be nilpotent. Furthermore let $\text{dim} \ker N =3 $, $\text{dim} \ker N^2=6$ and $\text{dim} \ker N^3=7$. What is the Jordan Normal Form?</p>
</blockquote>

<p>The only thing I know is that there have to be three blocks, since $\text{dim} \ker N = 3$.</p>

<p>Thank you very much in advance for your help.</p>
",<linear-algebra>
"<p>Let $A\in M_n$ and $\operatorname{rank}A=k$. Is the following true?</p>

<blockquote>
  <p>There are $A_i\in M_n$ ($i=1,...,k$), such that $\operatorname{rank}A_i=1$ and $A=A_1+....+A_k$.</p>
</blockquote>
",<linear-algebra>
"<p>There is an array which contains points as shown below;</p>

<pre><code>[ -0.0249795, -0.00442094, -0.00397789, -0.00390947, -0.00384182, -0.0037756, -0.00371057, 0.00180882, 0.00251853, 0.00239539, 0.00244367, 0.00249255, 0.00254166, 0.00259185, 0.0116467, 0.0155782, 0.016471 ]
</code></pre>

<p>First of all, honestly, i don't know whether there is a measurement of nonlinearity or not. If there is, i would like to know what that's name is.</p>

<p>So how can i calculate the linearity or nonlinearity of this points distribution. I mean, after you draw a line from these points, how much will the line be linear and non-linear?</p>

<p>e.g. some line points, <code>p1= [1,-0.0249795], p2= [2, -0.00442094] ...</code></p>
",<linear-algebra>
"<p>Given a basis $U$, what conditions are needed for an orthogonal basis for it?</p>

<p>For example, in the following vector space $U$, if $U =sp\{(1,1,1),(1,3,7)\}$ then what conditions are needed for an orthogonal basis for it? </p>

<p>Is it enough to have a basis of dimension $2$ that's orthogonal? or are there more conditions? </p>

<p>EDIT: If for example I find an orthogonal span of dimension 2, say $V=sp\{(1,1,1), v_2\} $ such that $v_2$ is orthogonal to $(1,1,1)$, is any vector that's orthogonal to $(1,1,1)$ fine for it to be an orthogonal span for $U$?</p>

<p>PS: I know there's GS algorithm, but I'm asking if other bases that we get in other ways are also fine.</p>
",<linear-algebra>
"<p>$Tr(XY) = 1$ and $Tr(Y) = 1$ implies that $Tr(X) = 1$.</p>

<p>I tried to prove by contradiction and switch the dummy variable of $X$ and $Y$. But I don't think my approach is right and if there is any much easier proof.</p>
",<linear-algebra>
"<p>Question 3: (6 pts) A company makes 3 kinds of snacks, using almonds and raisins. The Fruity snack
contains 100g of almonds and 300g of raisins. The Nutty snack contains 300g of almonds and 100g of
raisins. The Variety snack contains 200g of almonds and 200g of raisins. There are currently 900g of
almonds and 700g of raisins available, and we want to determine how many of each kind of snacks can be
made so that all the ingredients are completely used.
a) Define variables and set up a linear system in order to solve the problem.
b) Find all the realistic solutions to the problem.</p>

<p>I answered A)</p>

<pre><code>100x + 300y + 200z = 900

300x + 100y + 200z = 700
</code></pre>

<p>How do I do B)?</p>
",<linear-algebra>
"<p>Let $T :R2[X]→R2[X]$be linear and such that$T(1)=1+X$,$T(X)=X+X^2$ and
$T (X^2) = 1 − X^2$. Is $T$ an isomorphism?</p>

<p>ok so Ive noticed that $T(1)-T(X)= 1- X^2$ which is $T(X^2)$, but that disproves that its linear, which is already given, also how is this related to it being an isomorphism, am I missing something here or?</p>
",<linear-algebra>
"<p>Given two $n \times n$ symmetric matrices $A$ and $B$, is there a generic way to construct a larger block matrix $M$ such that $\det(M) = \det(A) - \det(B)$?</p>

<p>A simple block expression is desired, in the sense that the block components of $M$ are constant matrices or obtained by solving matrix equations involving $A$ and $B$.  Constructions of $M$ involving short algebraic expressions for its components in terms of the components of $A$ and $B$ would also be interesting, but not something that expands to an exponential number of terms in the components of $A$ and $B$ like just sticking $\det(A)$ in as a term of $M$.</p>

<p>If this is not possible in the general case, what restrictions can be placed on $A$ and $B$ to make this possible?</p>

<p>The only case I know of is when the difference of $B$ and $A$ can be written as a product of a column vector $C$ and its transpose: $B-A = CC^T$. This allows us to construct a matrix $M$ such that:</p>

<p>$$ M = \begin{bmatrix} A &amp; C \\ C^T &amp; 0 \end{bmatrix} $$
$$ \det(M) = \det(A) - \det(A + CC^T) = \det(A) - \det(B) $$</p>

<p>I'm curious if there is some way to construct an appropriate block matrix to make this possible for arbitrary symmetric matrices $A$ and $B$.</p>

<p>The first comment to this question<br>
<a href=""http://math.stackexchange.com/questions/1834227/find-a-matrix-with-determinant-equals-to-deta-detd-detb-detc"">Find a matrix with determinant equals to $\det{(A)}\det{(D)}-\det{(B)}\det{(C)}$</a><br>
suggests the answer is trivial by choosing
$$M = \begin{bmatrix} A &amp; B \\ I &amp; I \end{bmatrix}$$
but that doesn't appear to work when I tried some numerical examples.</p>
",<linear-algebra>
"<p>I need help in formulating an optimization problem. I have a system of equations as follows:<br>
    $c_1x_1+c_2x_2+c_3x_3=1$<br>
    $b_1x_1+b_2x_2+b_3x_3=1$<br>
    $a_1x_1+a_2x_2+a_3x_3=1$<br>
In my case the system is not necessarily in three variables (the number of variables can increase).</p>

<p>The chosen $x_1$, $x_2$, $x_3$ should also satisfy the following system of equations:<br>
    $c_1'x_1+c_2'x_2+c_3'x_3&lt;1$<br>
    $b_1'x_1+b_2'x_2+b_3'x_3&lt;1$<br>
    $a_1'x_1+a_2'x_2+a_3'x_3&lt;1$<br>
where<br>
$0&lt;c_1'&lt;c_1$, $0&lt;c_2'&lt;c_2$, $0&lt;c_3'&lt;c_3$<br>
$0&lt;b_1'&lt;b_1$, $0&lt;b_2'&lt;b_2$, $0&lt;b_3'&lt;b_3$<br>
$0&lt;a_1'&lt;a_1$, $0&lt;a_2'&lt;a_2$, $0&lt;a_3'&lt;a_3$  </p>

<p>I have no idea where should I look for a solution. Any pointers in the right direction would also be appreciated.</p>

<p><strong>UPDATE</strong>: I understand that the constraints $0&lt;c_1'&lt;c_1$, $0&lt;c_2'&lt;c_2$, $0&lt;c_3'&lt;c_3$ would form a rectangular area and the value of $x_1$, $x_2$, $x_3$ should be chosen so that over this rectangular area the inequalities should be met. But I don't know how to formulate the constraints into an equation.</p>
",<linear-algebra>
"<p>The non-homogenous system is as follows:
$$3x+2y+5z=10\\
3x-2y=7\\
6x+4y-10z=k$$</p>

<p>I have determined that:
$$z=1-\frac{k}{20}\\
y=\frac{k}{16}-\frac{1}{2}\\
x=2+\frac{k}{24}$$</p>

<p>What are the values of $k$ to form a matrix which is inconsistent and then consistent?</p>
",<linear-algebra>
"<p>T:V→V ; prove that if T*T = T ⇒ ImT ∩ NullT = 0v and V = ImT ⊕ NullT.</p>

<p>let v∈V.</p>

<p>T(T(v)) = T(v) ⇒ T(v) = v</p>

<p>⇒ T is the identity transformation.</p>

<p>⇒ DimNullT = 0 ⇒ ImT ∩ NullT = 0v ⇒ V = ImT ⊕ NullT</p>

<p>Is the answer correct?</p>

<p>Also does an example exist where the ImT ∩ NullT ≠ 0v? Isn't the intersection empty by definition?</p>
",<linear-algebra>
"<p>Let $x=(a,b)$, where $a,b$ are in $N$</p>

<p>Now we have the transformations:
$$T_1(x) = (ka, b+1)$$ 
$$T_2(x) = (b,a)$$ where $k$ is in $N$.
Where the order of choosing a transformation is not fixed. 
(E.g. you can first apply 3 times $$T_1(x)$$, then $$T_2(x)$$</p>

<p>Will we for all $(a,b)$ produce $a=b$ by only being allowed to use these two transformations? If so, is this true for every $k$? I'm specifically interested in the case $k=2$ though.</p>
",<linear-algebra>
"<p>I know that the following statement shouldn't be true :</p>

<blockquote>
  <p>Let $A$ be a real $n\times n$ matrix which satisfies $f_A(x)=(x+3)^2(x-1)^2$ and $m_A(x) \ne f_A(x)$ then $A$ is diagonalizable over $\mathbb R$. </p>
</blockquote>

<p>Notice that $f_A(x)$ is the characteristic polynomial of $A$ and $m_A(x)$ is the minimal polynomial of $A$.</p>

<p>I'm trying to look for a counter-example but I can't find any. I tried to pick some diagonal matrices but I found out it cannot be an appropriate counter-example because a diagonal matrix is always diagonalizable. Besides that, all I tried is some other guessing where the diagonal contains $-3,-3,1,1$ with different combinations of $1$'s on the upper diagonal and the lower diagonal.</p>

<p>Is there any way to construct such a matrix?</p>

<p>$\underline {\mbox {Note:}}$</p>

<p>I know that the opposite statement is correct (that is, if we knew that $A$ is diagonalizable over $\mathbb R$ then $m_A(x) \ne f_A(x)$ because $m_A(x)$ must be simple in that case and we know $f_A(x)$ is not simple).</p>
",<linear-algebra>
"<p>A study buddy and I were going through this question and while we made some progress, we were ultimately unsure if what we were doing was correct or not.</p>

<p>The question is:</p>

<p>Let <em>A</em> be a diagonalizable matrix and let <em>X</em> be the diagonalizing matrix. Show that the column vectors of <em>X</em> correspond to the nonzero eigenvalues of <em>R</em>(<em>A</em>).</p>

<p>What we did was this:</p>

<p>Say that <em>A</em> is <em>n</em> x <em>n</em>. Therefore <em>X</em> must be <em>n</em> x <em>n</em> and a matrix <em>D</em> must be <em>n</em> x <em>n</em> since if A is diagonalizable and X is the diagonalizing matrix:</p>

<p><em>A</em> = <em>X</em> <em>D</em> <em>X</em> $^{-1}$</p>

<p>Since <em>X</em> is invertible, its eigenvectors must be linearly independent. Therefore, because <em>X</em> is <em>n</em> x <em>n</em>, there are <em>n</em> linearly independent eigenvectors. These eigenvectors span $\mathbb{R}$$^n$.</p>

<p>Because of this, <em>R</em>(<em>A</em>) $\subset$ $\mathbb{R}$$^n$, we've shown that the the column vectors of <em>X</em> correspond to the nonzero eigenvalues of <em>R</em>(<em>A</em>)... except we haven't. My buddy and I got stuck here and we weren't sure what to do since we haven't accounted for the eigenvalues that are equal to 0.</p>

<p>Thanks for any help in advance. </p>
",<linear-algebra>
"<p>Suppose that $V$ is an n-dimensional vector space over a field $F$ and $\{\vec{v_1}, ...,\vec{v_m}\}$ is a linearly independent set in $V$. How do I show that $m \leq n$ and $\exists \{\vec{v_{m+1}},...\vec{v_n}\} $ such that $\{\vec{v_1}, ...,\vec{v_m},\vec{v_{m+1}},...\vec{v_n}\}$ is a basis for $V$? </p>

<p>The book I'm using gives a hint involving using the basis for $V$ and creating the set of vectors composed of $\{\vec{v_1}, ...,\vec{v_m}\}$ and the basis of $V$, but i don't really understand what it is asking with that.</p>
",<linear-algebra>
"<p>My intuition says no. But what if W is a zero vector?</p>
",<linear-algebra>
"<p>Suppose $A$ is a $4×4$ matrix over $C$ s.t. $Rank(A)=2$ and $A^3=A^2\neq0$. If $A$ is not diagonalizable then how to prove that:</p>

<p>There exists a vector $v$ s.t. $Av\neq 0$ and $A^2v=0$.</p>

<p>I know it is to be proved that $Imsp(A)$ is contained in $Nullsp(A)$, but really got no clue how to approach.</p>

<p><strong>My work:</strong>
$x^2(x-1)$ is  the annihilating polynomial for $A$, but I am stuck in finding the characteristic polynomial. The only two possibilities are $x^3(x-1)$ and $x^2(x-1)^2$, but how to reject the later one?</p>

<p>Thanks for any hint.</p>
",<linear-algebra>
"<p>I know that the component of <code>x</code> along <code>u</code> is $\frac{u.x}{|u|}.$ ($x$,$u$ are both vectors) but my teacher said that this is equal to $u^T x u$ (||u|| = 1). I understand the first formula, But I cant understand the second one well.  </p>
",<linear-algebra>
"<p>Both matrix multiplication and quaternion multiplication are non-commutative; hence the use of terms like ""premultiplication"" and ""postmultiplication"". After encountering the concept of ""quaternion matrices"", I am a bit puzzled as to how one may multiply two of these things, since there are at least four ways to do this.</p>

<p>Some searching has netted <a href=""http://en.cnki.com.cn/Article_en/CJFDTOTAL-LXXB198402006.htm"">this paper</a>, but not having any access to it, I have no way towards enlightenment except to ask this question here.</p>

<p>If there are indeed these four ways to multiply quaternion matrices, how does one figure out which one to use in a situation, and what shorthand might be used to talk about a particular version of a multiplication?</p>
",<linear-algebra>
"<p>I need to find eigenvalues/eigenvectors of different kinds of $n \times n$ matrices. For example, how would I determine these for the matrices listed below? What is the typical process? Should I always go by the route of finding eigenvalues by finding roots of characteristic polynomial and then getting eigenvectors by solving $(\mathbf{A} - \lambda \mathbf{I})\mathbf{x} = 0$?<br><br></p>

<p>$\begin{bmatrix}
2&amp;0&amp;0\\ 1&amp;2&amp;0\\ 
0&amp; 1 &amp; 2
\end{bmatrix}
$
 <br><br>
$\begin{bmatrix}
4 &amp;1  &amp;1  &amp;1 \\ 
 1&amp;4  &amp;1  &amp;1 \\ 
 1&amp;1  &amp;4  &amp;1 \\ 
 1&amp;  1&amp;  1&amp; 4
\end{bmatrix}$ <br><br>
These are just examples. Typically I want to find eigenvectors of $n \times n$ matrices. If you can show me the process of finding solution of one of these matrices, that would be helpful.</p>
",<linear-algebra>
"<p>How would you find eigenvalues/eigenvectors of a $n\times n$ matrix where each diagonal entry is scalar $d$ and all other entries are $1$ ? I am looking for a decomposition but cannot find anything for this. <br>For example:</p>

<p>$\begin{pmatrix}2&amp;1&amp;1&amp;1\\1&amp;2&amp;1&amp;1\\1&amp;1&amp;2&amp;1\\1&amp;1&amp;1&amp;2\end{pmatrix}$</p>
",<linear-algebra>
"<p>Consider $n$ real, symmetric, and positive semi-definite matrices as: $A_1,A_2,\cdots,A_n$. These matrices are convertible to each other under appropriate permutation ($A_i(p_i,p_i)=A_j$). Moreover, we have $A_i=Q_iV_iQ^{-1}_i$ because of symmetry. Can we say that $\rho(A_1+A_2+\cdots+A_n )= \rho(V_1+V_2+\cdots+V_n)$ where $\rho(X)$ represents spectral radius of $X$? </p>
",<linear-algebra>
"<p>I am reading <a href=""http://biomet.oxfordjournals.org/content/61/2/383.full.pdf"" rel=""nofollow"">an old paper dated back in 70'</a>, where I encounter this
$$\mid\text{det}(A,G)\mid=(\text{det}\{(A,G)'(A,G)\})^{\frac{1}{2}}.$$</p>

<p>We compute the determinant of a single matrix, don't we? What doest it mean by $\mid\text{det}(A,G)\mid$?</p>
",<linear-algebra>
"<p>The question is the following: Given a matrix $A$ with rank $k$, we are looking for a matrix $B$ of rank $j$, where $j&lt;k$ such that $\|A-B\|_2$ is minimal.</p>

<p>My idea was to choose, if $A=P \operatorname{diag}(\sigma_1,\ldots,\sigma_k,0,\ldots) Q^H$ then $B=P \operatorname{diag}(\sigma_1,\ldots,\sigma_j,0,\ldots) Q^H$.</p>

<p>Is this approach correct? if so, then i would try to proove that this is actually the best approximation.</p>
",<linear-algebra>
"<p>Would someone please explain the proof strategy at <a href=""http://math.stackexchange.com/q/462982/53259"">Need verification - Prove a Hermitian matrix $(\textbf{A}^\ast = \textbf{A})$ has only real eigenvalues</a>? I brook the algebra so I'm not asking about formal arguments or proofs. For example, $1.$ How would you determine/divine/previse to take the Hermitian conjugate and to right-multiply by $\color{orangered}{\vec{v}}$?</p>

<p>$2.$ Since we are given that $A$ is Hermitian and has eigenvalues, why not start the proof with $A^*\mathbf{v} = \lambda^*\mathbf{v}$? Here, $\mathbf{v}$ is an eigenvector and so by definition $\neq \mathbf{0}$.</p>

<p>Then $\begin{align} LHS = Av &amp; = \\ \lambda v &amp; = \end{align}$</p>

<p>$\iff \lambda \mathbf{v} = \lambda^*\mathbf{v} \iff \mathbf{0} = (\lambda^* - \lambda )\mathbf{v} \iff \mathbf{v}  \neq \mathbf{0}, so \, (\lambda^* - \lambda )=0. $</p>
",<linear-algebra>
"<p>Suppose I am going to locate an emergency point on a field having 100 houses.</p>

<p>If we see this as a scatter graph with 100 points on it, I need to find a point (or let's call it an optimal point) which is the optimal choice for the emergency point. In other words, the SUM of travelling from the emergency point to all the houses (i.e. Emgncy point to Hous.No1 + Emgncy point to Hous.No2 + ... Emgncy point to Hous.No100) is the lowest possible value.</p>
",<linear-algebra>
"<p>When does the circulant matrix have only integral roots? <br>
For example: adjacency matrix for $K_n$ has all the roots integral which is circulant, but in case of Cycle on $n&gt;3$ it is circulant but it may not have an integral roots. </p>
",<linear-algebra>
"<p>If A and B are n dimensional vector spaces </p>

<p>1) Is A+B a vector space?</p>

<p>2) Is A and B a vector space?</p>
",<linear-algebra>
"<p>Question is as follows:</p>

<p>Suppose $A^2 = I$ (the identity matrix) and F = Q, R or C. Eigenvalues of A are then $\lambda=1$ or $\lambda=-1$. Show that $\ker(L (I+A))=E(-1)(A)$ and that $im(L (I+A))=E(1)(A)$.</p>

<p>$E(-1)(A)$ means eigenspace of eigenvalue $-1$.</p>

<p>I have managed to do the part with kernel, but I'm struggling with image and eigenspace.</p>

<p>I know how to prove that image is included in eigenspace $(E)$, but I don't know how to show the other way round.</p>

<p>Definition of $E$ I'm using: $E(1)(A) = \{x : Ax = x\}$.</p>

<p>Definition of image: $im(In + A) = \{y : y=(I + A)x\}$.</p>

<p>This is how I proved image is in eigenspace:
need to be shown: $Ay = y$</p>

<p>LHS: $Ay=A(I+A)x=A(Ix + Ax)=Ax + AAx= Ax + Ix=(A + I)x = y =$ RHS</p>
",<linear-algebra>
"<p><img src=""http://i.stack.imgur.com/SJ6MW.jpg"" alt=""enter image description here""></p>

<p>This is a very basic definition of orientable and very basic example 20.5 however ı could not understand definition in an good way so ı want you to explain my green writing please :) and my example please help me ı want to learn orientation on manifold if ı could not understand in a good way this ı will not understand in a good way rest of the subject please help me </p>
",<linear-algebra>
"<p>The eigenspace corresponding with the eigenvalue <strong>zero</strong> is the same as the null space of the original matrix. All vectors in the null space are linearly independent so the eigenvectors of <strong>zero</strong> are also independent.</p>

<p>Is this conclusion right?</p>
",<linear-algebra>
"<p>Hi could you help me with following</p>

<p>$$
\begin{pmatrix}
1 &amp; a &amp; a \\
a &amp; 1 &amp; a \\
a &amp; a &amp; 1
\end{pmatrix}
$$</p>

<p>is a $3 \times 3$ matrix.</p>

<p>Find the largest interval for a such that this matrix is positive definite.</p>

<p>How to do this??</p>

<p>Thanks a lot!</p>
",<linear-algebra>
"<p>Let $d_1$, $d_2$, ..., $d_n$ be positive integers. Let $B$ be the $n \times n$ matrix
$$\begin{pmatrix}
d_1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 \\
1 &amp; d_2 &amp; 1 &amp; \cdots &amp; 1 \\
1 &amp; 1 &amp; d_3 &amp; \cdots &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 1 &amp; 1 &amp; \cdots &amp; d_n \end{pmatrix}.$$
When does $B$ have a square root in $\mathrm{Mat}_n(\mathbb{Z})$?</p>

<p>Motivation: The <a href=""http://en.wikipedia.org/wiki/Friendship_graph"" rel=""nofollow"">Friendship Theorem</a> states that the only graph in which every pair of vertices is joined by a path of length $2$ is the ""Friendship Graph"", which you can see at the linked article. If $A$ is the adjacency matrix of such a graph, with degree sequence $(d_1, d_2, \ldots, d_n)$, then $A^2=B$. So this contributes the solution $(d_1, d_2, \ldots, d_n) = (2,2,2,\ldots,2,2m)$, with $n=2m+1$.</p>

<p>I was preparing notes on the friendship theorem and got distracted by trying to figure out when this matrix has an integer square root at all. It seemed like it might make a nice challenge for here.</p>
",<linear-algebra>
"<p>I am currently going through ""Log-gases and random matrices"" by PJ Forrester. I'm coming from a totally different academic background, and I cannot understand a point of his notation. More precisely, he defines an <em>antinunitary</em> time reversal <strong>operator</strong> $T=\mathbf{Z}_{2N}K$ where $\mathbf{Z}_{2N}$ is the tensor product of the $N\times N$ identity matrix and of the $2\times2$ matrix $\begin{bmatrix}0 &amp;-1\\ 1 &amp; 0 \end{bmatrix}$, and where $K$ is the complex conjugate operator. So if I understand correctly, $T$ acts on a matrix $\mathbf{A}$ like so
\begin{equation}
T\mathbf{A} =\mathbf{Z}_{2N}\bar{\mathbf{A}},
\end{equation}
right?
Where I stop following is how one should deal with $K$ in ""mixed operations"", i.e. since it is not presented as a martix, how does one invert $K$, apply it ""from the right"", etc. </p>

<p>The property i'm trying to understand is the following: commutation relations between Hermitian matrices $\mathbf{A}$ and $T$ lead to
\begin{equation}
\mathbf{A} = T \mathbf{A} T^{-1} = \mathbf{Z}_{2N}K\mathbf{A} K^{-1} \mathbf{Z}_{2N}^{-1}   = \mathbf{Z}_{2N}K\mathbf{A} K \mathbf{Z}_{2N}^{-1}  = \mathbf{Z}_{2N}\bar{\mathbf{A}}\mathbf{Z}_{2N}^{-1}
\end{equation}
From what I gather, $T$ is more or less treated as a matrix (at least with regard to inversion operation), and $K$ is it's own inverse (that, i understard). The puzzling point for me is the last equality. Does The second $K$ disappears because it is applied (with no effect) to the real matrix $\mathbf{Z}_{2N}^{-1}$? I cannot seem to find anything on the subject easily... I just need a confirmation of my understanding, and possibly further reading material! </p>
",<linear-algebra>
"<p>I have a formula I use to determine how opaque some validation text should be based upon the length of a user's input compared to the maximum lenth allowed.  I want to modify it so that the ""ramping up"" of the opacity percentage only starts when they are at 80% of max, and then scales up proportionally from there.</p>

<p>Here is my current function:</p>

<p><strong>OpacityPercentage = CharactersEntered / MaxCharacters</strong></p>

<p>Therefore, if I have MaxCharacters of 50, then the opacity is as follows:</p>

<ul>
<li>30 chars = 60%</li>
<li>40 chars = 80%</li>
<li>41 chars = 82%</li>
<li>45 chars = 90%</li>
<li>50 chars = 100%</li>
</ul>

<p>What I want is for the opacity to be 0% until I get to 80% of max, then scale up from there.  So I would want the table to look as follows:</p>

<ul>
<li>30 chars = 0%</li>
<li>40 chars = 0%</li>
<li>41 chars = 10%</li>
<li>45 chars = 50%</li>
<li>50 chars = 100%</li>
</ul>

<p>I thought this would be simple, but I can't seem to figure out what I need to change in my existing formula.  Any advise is appreciated!</p>
",<linear-algebra>
"<p><em>Problem</em></p>

<p>Determine whether the indicated subset is a subspace of the given euclidean space:</p>

<p>$ \{[x,y,z]\ |\ x,y,z \in \mathbb{R} $ and $z=3x+2\}$ in $\mathbb{R}^{3}$</p>

<p><em>Solution</em></p>

<p>By definition, in order for a subset to be a subspace 3 conditions must be occur:</p>

<ol>
<li><strike>To pass by the origin</strike> To contain the origin.</li>
<li>To be closed under addition. </li>
<li>To be closed under scalar multiplication.</li>
</ol>

<p>So I try to solve the exercise by this way:</p>

<p>$1.$ The origin $(0,0,0) \in \mathbb{W} $</p>

<p>$2.$ Let $\vec u$ and $\vec v \in \mathbb{W} $. We have</p>

<p>$$ 
\begin{cases}
3u_1 + 2 - u_3 = 0 \\
3v_1 + 2 - v_3 = 0 \\
\end{cases}
$$
The sum is $ 6(u_1 + v_1) + 4 - (u_3+v_3) = 0 $ (which $\in \mathbb{W} $)</p>

<p>$3.$ Let $\vec u$ $ \in \mathbb{W} $ and  $\ r$ $ \in \mathbb{R} $. We have</p>

<p>$r(3u_1) + r(2) - r(u_3) = 0 \\$</p>

<p>Which, also, $ \in \mathbb{W} $</p>

<p>So, why is the book's answer: It <strong>isn't</strong> a subspace? </p>
",<linear-algebra>
"<p>I'm (reasonably) familiar with <a href=""http://en.wikipedia.org/wiki/Cholesky_decomposition"" rel=""nofollow"">factoring</a> a positive definite matrix $\mathbf{P} = \mathbf{L} \mathbf{L}^T =  \mathbf{R}^T \mathbf{R}$, and is supported by  <a href=""http://www.mathworks.com.au/help/techdoc/ref/chol.html"" rel=""nofollow"">MATLAB</a> and <a href=""http://eigen.tuxfamily.org/dox-devel/TopicLinearAlgebraDecompositions.html"" rel=""nofollow"">Eigen</a>.</p>

<p>However, I have also seen a factorization of the (same)  $\mathbf{P} = \mathbf{U} \mathbf{U}^T =  \mathbf{L&#39;}^T \mathbf{L&#39;}$</p>

<p>The following illustrates:</p>

<pre><code>&gt;&gt; A = rand(3, 4)

A =

    0.2785    0.9649    0.9572    0.1419
    0.5469    0.1576    0.4854    0.4218
    0.9575    0.9706    0.8003    0.9157

&gt;&gt; P = A * A.'

P =

    1.9449    0.8288    2.0991
    0.8288    0.7374    1.4513
    2.0991    1.4513    3.3379

&gt;&gt; R = chol(P)

R =

    1.3946    0.5943    1.5052
         0    0.6198    0.8982
         0         0    0.5153

% This function computes such that U * U.' = A * A.'
% Part of: http://www.iau.dtu.dk/research/control/kalmtool2.html 
&gt;&gt; U = triag(A)

U =

   -0.7475    0.2571   -1.1489
         0   -0.3262   -0.7944
         0         0   -1.8270

&gt;&gt; P2 = R.' * R

P2 =

    1.9449    0.8288    2.0991
    0.8288    0.7374    1.4513
    2.0991    1.4513    3.3379

&gt;&gt; P3 = U * U.'

P3 =

    1.9449    0.8288    2.0991
    0.8288    0.7374    1.4513
    2.0991    1.4513    3.3379
</code></pre>

<p>I haven't seen this particular factorization  $\mathbf{P} = \mathbf{U} \mathbf{U}^T$ before. I have a couple of questions:</p>

<ul>
<li>Is it still, by definition, Cholesky factoriation? If not, what is it called? </li>
<li>Is the simple means to compute this particular variant (e.g. a MATLAB command)</li>
<li>Is there a specific relationship between $\mathbf{U}$ and $\mathbf{R}$?</li>
</ul>
",<linear-algebra>
"<p>Choose a possible $a$ such that the linear equations have a root</p>

<p>$$\begin{matrix} x+2y+3z=a \\
                 4x+5y+6z=a^2 \\
                 7x+8y+9z=a^3 \end{matrix}$$</p>

<p>Do I begin by finding the possible values of $a$ such that the system is consistent?</p>
",<linear-algebra>
"<p>I am stuck trying to solve the following problem:</p>

<p>In diagonalizing a symmetric matrix $S$, we find that two of the eigenvalues ($\lambda_1$ and $\lambda_2$) are equal but the third ($\lambda_3$) is different. Show that <em>any</em> vector which is normal to $\hat{n}_3$ (which is the eigenvector corresponding to $\lambda_3$) is then an eigenvector of $S$ with eigenvalue equal to $\lambda_1$</p>

<p>Can anyone offer hints on, or an outline of, the solution?</p>

<p>Thank you. </p>
",<linear-algebra>
"<p>Vectors $\vec{b}$ and $\vec{c}$ are given. ∠(b,c)=2pi/3. Find vector $\vec{a}$, coplanar with $\vec{b}$ and $\vec{c}$, length $|\vec{a}|=4$ and ∠(a,b)=pi/6</p>

<p>I know it's something with triple product. Not sure where that gets me.</p>

<p>EDIT: $|\vec{b}|$=$|\vec{c}|$=1</p>
",<linear-algebra>
"<p>Given a set of linear equations $AX=B$, say $A$ is an ill posed matrix (has a few singular values equal or very close to zero), which numerical algorithm (conjugate gradient, least squares or steepest decent etc ) should be used to obtain the best solution? More specifically, is there a concrete comparison between these methods?</p>
",<linear-algebra>
"<p>Are these sets of equations linear? What is the number of variables and equations in each system? Please correct me if my answer is wrong:</p>

<p><strong>a)</strong> $Ax = b, x \in R^n$ - <strong>yes</strong>, classic system of linear equations, $var = n, eq = m$ where $A \in R^{m \times n}$</p>

<p><strong>b)</strong> $x^TAx = 1, x \in R^n$ - <strong>no</strong>, its a quadratic form, $var = n, eq = 1$</p>

<p><strong>c)</strong> $a^TXb = 0, X \in R^{m \times n}$ - <strong>yes</strong>, $var = m*n, eq = 1$</p>

<p><strong>d)</strong> $AX + XA^T = C,X \in R^{m \times n}$ - <strong>yes</strong>, not sure</p>

<p>Thanks for any help..</p>

<p>EDIT: are the first 3 solutions correct now?</p>
",<linear-algebra>
"<p>I have a few proofs I need some help with.</p>

<p><strong>a)</strong> Prove that $AB-BA = I$ does not have any solutions for any $A,B$. All matrices are regular.</p>

<p>I based my proof on matrix traces. $tr(AB) = tr(BA)$. Since $tr(X+Y) = tr(X) + tr(Y)$, it holds that $tr(AB - BA) = tr(AB) - tr(BA) = 0$ and $tr(I) = m$ so the diagonal numbers cant be ""önes"". Is this proof correct or do I have to use some other method?</p>

<p><strong>b)</strong> Prove that $(AB)^{-1} = B^{-1}A^{-1}$. I believe that I can prove this by simply writing all the matrices products down.. is there some simplier and more ""elegant"" way how to prove this?</p>

<p><strong>c)</strong> Prove that $A + A^T$ is symetric for a square $A$. Not sure abotu this one...</p>

<p>Thanks for any help in advance!</p>

<p>EDIT: $A$ in c) is square, not rectangular!</p>
",<linear-algebra>
"<p>In the vector space of $f:\mathbb R \to \mathbb R$, how do I prove that functions $\sin(x)$ and $\cos(x)$ are linearly independent. By def., two elements of a vector space are linearly independent if $0 = a\cos(x) + b\sin(x)$ implies that $a=b=0$, but how can I formalize that? Giving $x$ different values? Thanks in advance.</p>
",<linear-algebra>
"<p>I have a $t \times l$-polynomial matrix $A$ over $\mathbb{F}_q[x]$. The entries of $A$ are of degree $\le m$. I want to reduce $A$ to upper-triangular form by Gaussian elimination in case of using the Euclidean division instead of the exact one. What the worst-case computational (time) complexity it will take?</p>
",<linear-algebra>
"<p>I have several subspaces where I have to determine their dimension and whether they are affine or linear? These are my answers- are they correct? Thanks for help!</p>

<p><strong>a)</strong> $X = \{ x \in R^n | a^Tx = 0 \}, a \in R^n$ is given</p>

<ul>
<li>linear since any linear combination $\alpha x + \beta y, x, y \in X$ is in X. I believe its affine, too, since actual coefficients $\alpha, \beta$ doesnt really matter in this case because $\alpha(a_1x_1 + \dots + a_nx_n) + \beta(a_1y_1 + a_ny_n) = 0$ every time...</li>
<li>dimension? Im guessing its max $n-1$ but Im not sure</li>
</ul>

<p><strong>b)</strong> $X = \{ x \in R^n | a^Tx = c \}, a \in R^n, c \in R$ are given</p>

<ul>
<li>affine because in order for the linear combination to be in $X$, $\alpha_i$ must sum to 1</li>
<li>dimension again max $n-1$?</li>
</ul>

<p><strong>c)</strong> $X = \{ x \in R^n | x^Tx = 1 \}$</p>

<ul>
<li>affine (same justification as in b)) </li>
</ul>

<p><strong>d)</strong> $X = \{ x \in R^n | a^Tx = I \}, a \in R^n$ is given</p>

<ul>
<li>not sure at all
Are my assumptions correct</li>
</ul>
",<linear-algebra>
"<p>Let $A$ and $B$ be isomorphic unitary rings. Suppose that both of them admit a structure of (maybe finite dimensional) vector space over some field $k$. I would like to know if then $A$ and $B$ are isomorphic as vector spaces over $k$ (if they are forced to have the same dimension). Notice that in general I am not requiring $A$ and $B$ to be $k$-algebras, i.e. I am not requiring any kind of compatibility between the multiplicative structure and the product with scalars from the field. My guess is that in this generality the answer is no, but I can't provide nor find any example.</p>

<p>Here I gather some things I can prove:</p>

<p>1) if the field is $k=\mathbb{Q}$ and $A$ and $B$ are $k$-algebras, then the answer is yes.</p>

<p>2) if the field is $k=\mathbb{R}$, $A$ and $B$ are $k$-algebras and they are fields, then the answer is yes again.</p>

<p>3) if $A$ and $B$ are finite $k$-algebras (and so $k$ is finite too) the answer is yes again.</p>

<p>Unfortunately these rule out most of the examples from a first course in ring theory, so I suspect the answer would be more exotic than this, but I can't find anything. Maybe the answer is yes even in the general setting (or maybe just for $k$-algebras), and in this case I'd like to see a proof.</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>For example, I want to this to happen:
$$\begin{bmatrix}1&amp; 2&amp; 3\end{bmatrix}\times\begin{bmatrix}2&amp; 3&amp; 4\end{bmatrix} = \begin{bmatrix}2&amp; 6&amp; 12\end{bmatrix}$$
It's not exactly matrix multiplication, but I hope you can see what I'm getting at. Is there some notation in linear algebra that allows this function to be valid?</p>
",<linear-algebra>
"<p>I want a rotation matrix $R$ that transforms +x axis to +y, +y to +z, and +z to +x. One way of doing it is by a rotation about +x by 90 deg anti-clockwise, followed by a rotation about +y by 90 deg anti-clockwise. The matrices respectively are:</p>

<p>$$R_1 = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; -1 \\
0 &amp; 1 &amp; 0 \\
\end{bmatrix}$$</p>

<p>$$R_2 = \begin{bmatrix}
0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
-1 &amp; 0 &amp; 0 \\
\end{bmatrix}$$</p>

<p>Since R1 first operates first followed by R2, the resultant, I think, is R=(R2)(R1). But when I cross-check by applying R to an arbitrary point, say (1,2,3), I don't get the right transform of (3,1,2). Instead, I get it right when I do R=(R1)(R2)  which doesn't make sense to me. Please help</p>
",<linear-algebra>
"<blockquote>
  <p>Let $c_0=\{ (x_n) : x_n\in \Bbb{R}, x_n \to 0\}$ and $M=\{(x_n)\in c_0 : x_1+x_2+\cdots + x_{10}=0\}$. Then, what is dim($c_0/M$) ?</p>
</blockquote>
",<linear-algebra>
"<p>In the textbook that I'm using the standard inner product is defined as </p>

<p>$$\langle x,y\rangle = \sum_{i=1}^{n}a_{i}\overline{b_{i}}$$</p>

<p>where $x=(a_{1}, a_{2}, {...}, a_{n})$ and $y=(b_{1}, b_{2}, {...}, b_{n})$ and the bar denotes complex conjugation. </p>

<p>In one the practice problems I was doing they used the fact that </p>

<p>$\langle x,y\rangle = y^{*}x$</p>

<p>where $y^{*}$ denotes the conjugate transpose of y. I'm having trouble understanding why the definition for the standard inner product is the same as $y^{*}x$. </p>

<p>I tried doing an example where $x = (i,i)$ and $y = (-i,-i)$ </p>

<p>For $$\langle x,y\rangle = i*i+i*i=-2$$  </p>

<p>On the other hand </p>

<p>$y^{*}x=\begin{pmatrix} i\\i\\ \end{pmatrix} \begin{pmatrix} i &amp; i\\ \end{pmatrix} = \begin{pmatrix} -1 &amp; -1\\ -1 &amp; -1\\ \end{pmatrix} $</p>

<p>I was wondering why I am getting $\langle x,y\rangle \ne y^{*}x$</p>
",<linear-algebra>
"<p>If the characteristic polynomial $f_A(x)$ has multiples of the same product, for example $f_A(x)= (x+2)^2(x-1)$ so  $(x+2)$ has a multiple of $2$, then is there a condition on $A$ such that we know for sure that $m_A(x)= (x+2)(x-1)$ or $(x-1)$ or $(x+2)$ ? i.e, no multiples. </p>
",<linear-algebra>
"<p>Is there a simple way to show that the least square solution of an overdetermined linear system is equal to the right singular vector of the coefficient matrix corresponding to the smallest singular value?</p>
",<linear-algebra>
"<p>I was reading up on the Fibonacci Sequence when I've noticed some were able to calculate specific numbers. So far I've only figured out creating an array and counting to the value, which is incredibly simple, but I reckon I can't find any formula for calculating a Fibonacci number based on it's position.</p>

<p>Is there a way to do this? If so, how are we able to apply these formulas to arrays?</p>
",<linear-algebra>
"<p>By matrix-defined, I mean</p>

<p>$$\left&lt;a,b,c\right&gt;\times\left&lt;d,e,f\right&gt; = \left| 

\begin{array}{ccc}
i &amp; j &amp; k\\
a &amp; b &amp; c\\
d &amp; e &amp; f
\end{array}

\right|$$</p>

<p>...instead of the definition of the product of the magnitudes multiplied by the sign of their angle, in the direction orthogonal)</p>

<p>If I try cross producting two vectors with no $k$ component, I get one with only $k$, which is expected. But why?</p>

<p>As has been pointed out, I am asking why the algebraic definition lines up with the geometric definition.</p>
",<linear-algebra>
"<p>As I've understood it, what I've learned is that the dot product is just one of many possible <strong>inner product spaces</strong>. Can someone explain this concept? When is it useful to define it as something other than the <strong>dot product</strong>?</p>
",<linear-algebra>
"<p>Given two points around an origin $(0,0)$ in $2D$ space, how would you calculate an angle from $p_1$ to $p_2$?</p>

<p>How would this change in $3D$ space?</p>
",<linear-algebra>
"<p>I've recently started reading about Quaternions, and I keep reading that for example they're used in computer graphics and mechanics calculations to calculate movement and rotation, but without real explanations of the benefits of using them.</p>

<p>I'm wondering what exactly can be done with Quaternions that can't be done as easily (or easier) using more tradition approaches, such as with Vectors?</p>
",<linear-algebra>
"<p>What purposes do the Dot and Cross products serve? </p>

<p>Do you have any clear examples of when you would use them?</p>
",<linear-algebra>
"<p>I wonder how can I find the largest delta in which if initial condition is inside circle (How do I specify radius of that circle) bounded by delta then the solution is always within the dashed circle in the figure?
How do you approach this solely based on geometry of phase portrait since no equation is given?</p>

<p><img src=""http://i.stack.imgur.com/dovTu.jpg"" alt=""enter image description here""></p>
",<linear-algebra>
"<p>A simple question. Why is the sum of the singular values of a matrix called its <a href=""http://en.wikipedia.org/wiki/Matrix_norm#Schatten_norms"" rel=""nofollow"">nuclear norm</a>? What is the origin of, and motivation for, this term?</p>

<p>Apparently the term <em>nucleus</em> is sometimes used to refer to the kernel of a linear transformation, but that doesn't seem to have anything to do with singular values.</p>

<p>To save you the effort, neither <em>nucleus</em> nor <em>nuclear</em> have entries in <a href=""http://jeff560.tripod.com/n.html"" rel=""nofollow""><em>Earliest Known Uses of Some of the Words of Mathematics</em></a>.</p>
",<linear-algebra>
"<ol>
<li><p>In the process of proving some theorem, I have assumed that if $A B$
is invertible, then the matrices $A$ and $B$ are invertible as
well. However I'm not sure about it. I know that if $A$ and $B$ are
invertible then $AB$ is also invertible. But does it hold in the
opposite direction?</p></li>
<li><p>Is $AB=I$ enough to deduce that $A$ is invertible? Or must I prove that $BA=I$ also holds? </p></li>
</ol>

<p>(I'm dealing only with square matrices)</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>I have a set of vectors:</p>

<p>$$V=\lbrace x\in\Bbb{R}^5:x_1+x_2+x_3+x_4+x_5=0\rbrace$$</p>

<p>I need to find basis for this set of vectors. What is the ""algorithm"" for solving such problems? Where should I start?</p>

<p>EDIT:</p>

<p>Is this a valid basis:
$\lbrace[1,0,0,0,-1]^T,[0,1,0,0,-1]^T,[0,0,1,0,-1]^T,[0,0,0,1,-1]^T\rbrace$?</p>
",<linear-algebra>
"<p>Can someone explain how Cramer's rule works. I understand the mechanics of it, and it's fairly straightforward to show algebraically that it's equivalent to GJ and substitution, but what's happening under the hood? I'm guessing it has to do with properties of determinants but...</p>
",<linear-algebra>
"<p>My linear algebra book (<a href=""http://www.cin.ufpe.br/~jrsl/Books/Linear%20Algebra%20Done%20Right%20-%20Sheldon%20Axler.pdf"" rel=""nofollow"">Linear Algebra Done Right</a> by Sheldon Axler) has the following problem as exercise 1.6:</p>

<blockquote>
  <p>Give an example of a nonempty subset $U$ of $\mathbb{R}^2$ such that $U$ is closed under addition and under taking additive inverses (meaning $-u \in U$ whenever $u \in U$), but $U$ is not a subspace of $\mathbb{R}^2$.</p>
</blockquote>

<p>It seems to me that such a set cannot exist, since the only subspace condition it's not mandated to fulfill is containing $0$, and for any $u \in U$, I can negate it to get $-u$ and then get $u + (-u) = 0$.</p>

<p>What is going on?</p>
",<linear-algebra>
"<p>I have problem with this:
$$\| BC\| \leq \| B\|\| C\|$$
where $\|\cdot\|$ means spectral norm defined as $$\|A\|=\text{max}\lbrace\|Ax\|:x\in\mathbb{C^n, \|x\|=1}\rbrace$$ 
where the norms on the are according to standard scalar product. </p>

<p>For my homework I had to solve $\|A+B\|\leq\|A\|+\|B\|$ which I managed to do by myself but this second inequality seems to be (at least for me) much harder. Any ideas? </p>
",<linear-algebra>
"<p>I need some help with this problem please: </p>

<p>Let $V$ be a vector space finitely generated over $\mathbb Q$ and
let $α, β ∈ \operatorname{ End}(V )$ satisfy $3α^3 + 7α^2 − 2αβ + 4α − σ_1 = σ_0$. Show that $αβ = βα$.</p>

<p>Thanks.</p>

<p>This is an exercise from the book: The Linear Algebra a Beginning Graduate Student Should Know by Golan.</p>
",<linear-algebra>
"<p>I need some help with this problem please:</p>

<p>Let $V$ be as vector space over a field $F$ and let $α, β, γ ∈
\operatorname{End}(V )$ satisfy $αβ = σ_1 = αγ$. Show that $βγ \neq γβ$.</p>

<p>$σ_1$ is the identity function and $\operatorname{End}(V )$ the endomorphism of $V$.</p>

<p>I think this problem is wrong for if we take $β=γ$ that is not true, but if someone have an idea it will be appreciated.
Thanks.</p>

<p>This is an exercise from the book: The Linear Algebra a Beginning Graduate Student Should Know by Golan.</p>
",<linear-algebra>
"<p>Apologies for the poor title.</p>

<p>What I'm wondering is:
Say that we have a non-surjective operator $A:X\rightarrow X$ where $X$ is a Banach space, and the operator is defined in terms of the basis vectors $ê_i$:
$$
A(x)=\sum\limits_{i,k=1}^\infty\alpha_{i,k}\cdot x_{k}\cdotê_i=\sum\limits_{k=1}^\infty{y_k\cdotê_k}
$$
edit: where there are countably many $i$ for which $\alpha_{i,k}$ is non-zero for some $k$.</p>

<p>A <strike>restriction</strike> projection of the operator is:
$$
A'(x)=\sum\limits_{i\in I,k=1}^\infty\alpha_{i,k}\cdot x_{k}\cdotê_i=\sum\limits_{k\in I}{y_k\cdotê_k}
$$
Where $I$ is a finite index set.</p>

<p>Given $y=\sum\limits_{k=1}^\infty{y_k\cdotê_k}$ that is not in the image of $A$, is it always possible to take such a projection in a way so that the finite set of linear equations you get don't have a solution?</p>

<p>If one of the basis vector isn't in the image it's easy to realise that you can, for example the right shift operator:$$(x_1,x_2,x_3,x_4,...) \rightarrow (0,x_1,x_2,x_3,x_4,...)$$</p>

<p>My next thought was that there has to be a basis vector not in the image, because otherwise the image would span $X$ but I don't think that's enough.</p>

<p>This isn't a homework problem, it's just something that's been distracting me.</p>
",<linear-algebra>
"<p>Let's assume that $V$ and $W$ are vector spaces over a field $\mathbb{K}$, $\lambda\in\mathbb{K}$, $\lambda\neq0$.</p>

<p>$S: V\rightarrow W$ and $T: W\rightarrow V$ are linear maps. Prove, that</p>

<p>$\lambda$ is an eigenvalue of $TS\iff\lambda$ is an eigenvalue of $ST$</p>

<p>What can be stated about the eigenvalues of the maps $TS$ and $ST$?
Would it also be correct if $\lambda=0$?</p>

<p>That's how far I've come:
I have to prove, that </p>

<ol>
<li>$\lambda$ is an eigenvalue of $TS\Rightarrow\lambda$ is an eigenvalue of $ST$ </li>
<li>$\lambda$ is an eigenvalue of $ST\Rightarrow\lambda$ is an eigenvalue of $TS$</li>
</ol>

<p>Assuming $V=\mathbb{K}^n$ and $W=\mathbb{K}^m$, such that $S:\mathbb{K}^n\rightarrow\mathbb{K}^m$ and $T:\mathbb{K}^m\rightarrow\mathbb{K}^n$. Hence, $TS: \mathbb{K}^n\rightarrow\mathbb{K}^n$ and $TS: \mathbb{K}^m\rightarrow\mathbb{K}^m$. $TS$ and $ST$ are both endomorphisms. Since the eigenvalue is not zero, the matrices must be invertible and the determinant of both matrices is not zero. Let's say $A$ is the transformation matrix of $TS$ and $B$ the transformation matrix of $ST$</p>

<p>That's where I'm stuck right now. How do I go on from here?</p>

<p>Do I have to prove, that $det(A-2*I_3)=0=det(B-2*I_2)$?</p>
",<linear-algebra>
"<p>Is it true that if $A$ is unitary diagonalizable (i.e. $A$ is normal) and $B$ is similar to $A$ then $B$ is also unitary diagonalizable (i.e. $B$ is normal) ?</p>

<p>Here are my thoughts:</p>

<p>If $A$ is unitary diagonalizable matrix then there exist unitary matrix $P$ and diagonal matrix $D$ such that $P^*AP=D$.  $(*)$</p>

<p>We also know from the Spectral theorem that $A$ is normal ($\because$ $A$ is unitary diagonalizable $ \iff A$ is normal).</p>

<p>From the fact that $B$ is similar to $A$ we can derive that there exists invertible matrix $Q$ such that $Q^{-1}BQ=A$.</p>

<p>Finally, by replacing $A$ in $(*)$ with $Q^{-1}BQ$ we get:</p>

<p>$$ P^*Q^{-1}BQP=D \ \ (**)$$</p>

<p>The only thing I can derive from this equation $(**)$ is that $B$ is diagonalizable, because :</p>

<p>$$ P^*Q^{-1}BQP=D \implies P^{-1}Q^{-1}BQP=D \implies (QP)^{-1}B(QP)=D \implies C^{-1}BC=D$$ </p>

<p>Am I missing something? Is it possible that I can also derive that $B$ is <strong>unitary</strong> diagonalizable ?</p>
",<linear-algebra>
"<p>I'm a little bit confused by this sentence:</p>

<p>""The matrix A is the sum of two matrices that can be diagonalised independently,
and hence A is trivially reducible.""</p>

<p>How can I show that A is reducible? </p>
",<linear-algebra>
"<p>I'm able to prove $44$, but how would one deduce $43$ from it without further industry, forthwith?<br>
 $43$ seems like a reduced, 2D version of $44$? I'm not enquiring about individual proofs.</p>

<blockquote>
  <p>$44.$ Let $P$ be a point not on the plane that passes through the
  points $Q, R, S$.<br>
  Show that the distance $h$ from $P$ to the plane $ = \dfrac {\left| \left( \mathbb{a} \times \mathbb{b} \right) \cdot \mathbb{p}\right| } {\left| \mathbb{a} \times \mathbb{b}\right| }$. 
  <img src=""http://i.stack.imgur.com/IB6o9.png"" alt=""enter image description here""></p>
  
  <p>$43 = 39$ (5th edition). Let $P$ be a point not on the line $L$ that passes through the
  points $Q,R$.<br>
  Show that the distance $d$ from the point P to the line $L = \dfrac {\left| \mathbb{a} \times \mathbb{b} \right| } {\left| \mathbb{a}\right| }$. 
  <img src=""http://i.stack.imgur.com/gMcvB.png"" alt=""enter image description here""></p>
</blockquote>
",<linear-algebra>
"<p>$37.$ Find an equation of the plane that passes through the point $(1, -2, 1)$<br>
and contains
the line of intersection of the planes $x + y - z = 2$ and $2x - y + 3z = 1$. 
<img src=""http://i.stack.imgur.com/EdLNj.png"" alt=""enter image description here"">
$\bbox[3px,border:2px solid grey]{\text{ Official solution : }}$My modified <a href=""http://www.mathematics-online.org/kurse/kurs8/seite41.html"" rel=""nofollow"">picture</a> illustrates that the red line of intersection belongs to both planes. A normal vector of each plane $\perp$ to this red line of intersection. Thus, the direction vector of the red line = $\mathbb{n_1} \times \mathbb{n_2} = (1, 1, -1) \times (2, -1, 3) = \color{#C154C1}{(2, -5, -3)}$</p>

<p>For want of a normal vector to the requested plane, require another direction vector on this requested plane. Thus need the vector containing any point on the red line of intersection to the given point $(-1, 2, 1)$ in the plane. WLOG, set $\color{#FF4F00}{x = 0}$:
$\begin{cases} x + y - z = 2 \\ 2x - y + 3z = 1 \end{cases} \implies \begin{cases} y - z = 2 \\ -y + 3z = 1 \end{cases}$ $\implies (\color{#FF4F00}{x}, y, z) = (\color{#FF4F00}{0}, 7/2, 3/2).$<br>
Thus another vector parallel to the plane is $(-1, 2, 1) - (\color{#FF4F00}{0}, 7/2, 3/2) = \color{#C154C1}{(-1, -3/2, -1/2)}$. </p>

<p>In toto, a normal vector to the plane $= \color{#C154C1}{(2, -5, -3) \times (-1, -3/2, -1/2)} = 2(-1, 2, -4).$  $... \blacksquare$</p>

<p>$\bbox[3px,border:2px solid grey]{\text{ My solution : }}$
♦ Any vector on the line of intersection of the two planes produces one vector $\parallel$ to the requested plane.</p>

<p>♠ The other vector $\parallel$  the requested plane is any vector connecting any point on this line of intersection with the given point $(-1, 2, 1)$ in the plane.</p>

<p>♦ Subtract the two equations of the plane to produce their line of intersection: $3x + 2z = 3$. For a direction vector of this line, subtract any two vectors on it. WLOG, put $z = 0 \implies \color{brown}{(1, 0, 0)}$ and put $z = 3 \implies \color{brown}{(-1, 0, 3)}$. Thus direction vector $= (-1, 0, 3) - (1, 0, 0) =  (-2, 0, 3)$. </p>

<p>♠ Observe the given point $(-1, 2, 1)$ isn't on this line (but is given to be on the plane). Thus either $\color{brown}{(1, 0, 0)} - (-1, 2, 1) = (2, -2, -1)$ or $\color{brown}{(-1, 0, 3)} - (-1, 2, 1)$ are on the plane. </p>

<p>In toto, a normal vector to the plane = $(-2, 0, 3) \times (2, -2, -1) = 2(3, 2, 2) \neq k(-1, 2, -4)... \blacksquare$</p>

<p>Since the solution's and my normal vectors differ already, what's wrong with my solution? </p>
",<linear-algebra>
"<p>So far I have:</p>

<p>$\boldsymbol{f^{-1}} \circ \boldsymbol{f}(\boldsymbol{a}) = \boldsymbol{a}
\implies [\boldsymbol{D}(\boldsymbol{f^{-1}}(\boldsymbol{a}) \circ \boldsymbol{f}(\boldsymbol{a}))] = I_n
\implies [\boldsymbol{D}\boldsymbol{f^{-1}}(\boldsymbol{f}(\boldsymbol{a}))][\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})] = I_n $ by the chain rule.</p>

<p>Interpreting $[\boldsymbol{D}\boldsymbol{f^{-1}}(\boldsymbol{f}(\boldsymbol{a}))]$ as the matrix composed of row-reduction operations, $[\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})]$ row-reduces to $I_n$. Now $[\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})]$ is a $m \times n$ matrix, therefore we have $m \le n$: Is this convincing? How do I make it rigorous?</p>

<p>This looks like a similar question to <a href=""http://math.stackexchange.com/questions/257599/the-existence-of-an-inverse-to-a-differentiable-function"">Existence of an inverse to differentiable function</a></p>
",<linear-algebra>
"<blockquote>
  <p>Describe all $m$ by $n$ matrices $A$ and $B$ such that $ref(A) + ref(B) = ref(A + B)$.<br>
  Is it true that $ref(A) = A$ and $ref(B) = B$? Does $ref(A - B) = rref(A - B)$?<br>
  Here, ref = Row Echelon Form, rref = Reduced Row Echelon Form.</p>
  
  <p><strong>Terse Answer:</strong> I think $ref(A) = A$ and $ref(B) = B$ are true.<br>
  But $REF(A) - REF(B)$ may have $-1$ in some pivots.</p>
</blockquote>

<p>Would someone please explain and uncloak how to start, still less solve, this question?</p>
",<linear-algebra>
"<p>Let $V$ be finite dimensional real vector space and let $f$ and $g$ be non zero linear functionals on $V$.  Assume that $\ker(f) \subset \ker(g).$ Which of the following are true??</p>

<p>a. $\ker(f)=\ker(g)$</p>

<p>b. $f=\lambda g$ for some real number $\lambda \ne 0$.</p>

<p>c. The linear map $A\colon V\to \mathbb{R}^2$ defined by $Ax=(f(x),g(x))$ for all $x \in V$, is onto.</p>

<p>Since $\ker(f)\subset \ker(g)$ we get $\ker(f)=\ker(g)$ and hence (a) and (b) are true. Now the linear map will look like $Ax=(\lambda g(x),g(x))$ . I guess (c) will be false. Not sure though.</p>
",<linear-algebra>
"<p>Let $T_1$ and $T_2$ be diagonalisable operators on a real vector space $V$. Does it follow that $T_1+T_2$ is a diagonalisable operator?</p>

<p>My intution says no. But I can't find any counterexample.</p>
",<linear-algebra>
"<p>A cashier has a total of 30 bills, made up of ones, fives, and twenties. The number of twenties is 9 more than the number of ones. The total value of the money is $351. How many of each denomination of bills are there? (Hint: Let x = the number of ones, y = the number of fives, and z = the number of twenties)</p>

<p>I'm having a hard time figuring out the three equations for this problem.</p>
",<linear-algebra>
"<p>For $S-N$ Decomposition of a linear operator $T$ i.e $T=S+N$(Unique Expression) where $S$ is a semi-simple opearator and $N$ is a nilpotent operator and $SN=NS$ then prove that semi-simple part and nilpotent part of $T$ commutes with $A$ if $T$ commutes with $A$, where $A$ is a linear operator.</p>
",<linear-algebra>
"<p>Do you know is there any way to compute SVD or Cholesky decomposition of a matrix such as $$\begin{pmatrix}1&amp;-\infty \\ 0 &amp; 2 \end{pmatrix}$$ or not?</p>
",<linear-algebra>
"<p>I think that I should to rewrite the matrix in a appropriate form, but I can't find it. For a $2\times2$ matrix I get the characteristics polynomial $x^2-1$ and the eigenvalues $-1,1$. For $3\times3$ matrix I get $-x^3+3x+2$ and the eigenvalues are $2,-1,-1$.</p>
",<linear-algebra>
"<p>Let $A,B \in M(n,F)$ be such that $AB=BA$. Then show that $A$ and $B$ has same characteristic polynomial.</p>

<p>How can I proceed to this? If I take $|B||\lambda I-A|=|\lambda B- BA|= |\lambda B- AB|$ But this will not help me..</p>
",<linear-algebra>
"<p>Is matrix $Svv^\top S$ positive semidefinite, given $S$ is symmetric positive semidefinite? Where $v\in\mathbb{R}^n$,  $S\in \mathbb{R}^{n\times n}$</p>
",<linear-algebra>
"<p>Let $T(θ) : \Bbb R^2 → \Bbb R^2$ be the transformation that rotates each vector counterclockwise by angle $θ$.</p>

<p>(a) Write the standard matrix for $T(θ)$.</p>

<p>(b) Explain in words or pictures why $T(θ_1+θ_2) = T(θ_1) ◦ T(θ_2)$.</p>

<p>(c) Derive the angle sum formula for cosine and sine by finding the standard matrix for $T(θ_1+θ_2) = T(θ_1) ◦ T(θ_2)$; that is, prove that $\cos(θ_1+θ_2) = \cos(θ_1)\cos(θ_2)−\sin(θ_1) \sin(θ_2)$ and $\sin(θ_1+θ_2) = \sin(θ_1)\cos(θ_2) + \sin(θ_2)\cos(θ_1)$.</p>

<p>For part a - I'm not sure how to find the standard matrix when I don't know the initial values of the matrix before the vectors rotate.</p>

<p>Part b - I know $T(θ_1) ◦ T(θ_2)$ is the Hadamard product of $T(θ_1)$ and $T(θ_2)$, but I was never taught any properties to prove part b.</p>

<p>Part c - this problem completely confuses me.</p>

<p>Any help would be appreciated.</p>
",<linear-algebra>
"<p>Lyapunov's equation says: given any $Q &gt; 0$ ($Q$ positive definite) there is $P &gt; 0$ such that $A^T P + P A + Q = 0$ if and only if for $\frac{dx(t)}{dt}=A x(t)$ it is the case that the real part of each eigenvalue of $A$ is negative. Then, the ellipsoid $x^T P x \leq 1$ is an invariant of $\frac{dx(t)}{dt}=A x(t)$.</p>

<p>The geometric interpretation of $P$ is such that the eigenvectors of $P$ form the principal axes of the ellipsoid and each eigenvalue is related to the length of the ellipsoid along the axis represented by the corresponding eigenvector.</p>

<p>What is the geometric interpretation of $Q$ resp. how does the choice of $Q$ affect $P$?</p>
",<linear-algebra>
"<p>Let A =</p>

<p>\begin{bmatrix}1/4&amp;\sqrt3/4\\\sqrt3/4&amp;3/4\end{bmatrix}</p>

<p>(a) Show that for each x ∈ R, Ax is the projection of x onto the line passing through the origin making an angle of 60 degrees with the positive x-axis.</p>

<p>(b) Show that null(A) is the line passing through the origin making an angle of 30 degrees with the negative x-axis.</p>

<p>(c) Show that null(A) = row(I − A).</p>

<p>For part a - I created an Ax augmented matrix, but after reducing the matrix I'm not sure how to determine that the lines formed make an angle of 60 degrees.</p>

<p>Part b - How would I find the null space of the matrix when the reduced form of A doesn't have any free variables?</p>

<p>Part c - I can't really prove part c since I'm not sure how to find the null space or row space of I-A.</p>
",<linear-algebra>
"<p>Let $F$ be a field and let $n$ be a positive integer. Let $A,B ∈ M_{n×n}(F)$ be matrices
satisfying $A^2 + B^2 = I$ and $AB + BA = O$. Show that $tr(A) = tr(B) = 0$.</p>

<p>I know that $tr(A^2)+tr(B^2)=n$ and that $(A+B)^2=I$. I'm having a hard time showing this. I've been playing around for a bit with no success. Any solutions/hints are greatly appreciated.</p>
",<linear-algebra>
"<p>Let $\mathbf C$ be a positive-definite $k\times k$ matrix. For all vectors $\mathbf u\in \mathbb R^k$ of length $\|\mathbf u\|=1$, consider vectors $\mathbf {uu}^\top\mathbf{Cu}$; they form a surface in $\mathbb R^k$. What is this surface? In particular, what is it in case of $k=2$?</p>

<p>Here is an example for $\mathbf C = \left(\begin{array}{cc}4&amp;2\\2&amp;2\end{array}\right)$:</p>

<p><a href=""http://i.stack.imgur.com/fztne.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fztne.png"" alt=""Weird shape""></a></p>

<p>I do realize that the ""main axes"" (the longest and the shortest cuts) of this curve are given by the eigenvectors of $\mathbf C$ scaled by the respective eigenvalues. But the whole shape looks weird.</p>
",<linear-algebra>
"<p>the time averaged total energy, $\bar E$,
has the following $\varepsilon$ expansion in $D$ dimension:
\begin{equation}
\bar{E}=\varepsilon^{2-D}\frac{E_0}{2\lambda}+ \varepsilon^{4-D}E_1
\end{equation}
 from the above equation how can we write 
\begin{equation}
\omega_{\rm m}^2=
1-\frac{1}{2\lambda}\frac{(D-2)E_0}{(4-D)E_1}\,.
\end{equation}
where $$\omega_{\rm m}^2=1-\varepsilon_{\rm m}^2,$$  and $\lambda$ is a constant factor.
m denotes that angular frequency $\omega $ is minimum.</p>

<blockquote>
  <p>Can someone explain me how equation (2) can be written from equation
  (1)
  If you feel problem or need more information then please comment. 
  thanks in advance.
  for more info see <a href=""http://arxiv.org/abs/0802.3525"" rel=""nofollow"">here in equation (39 and 40)</a></p>
</blockquote>
",<linear-algebra>
"<p>Let's say I have the following equalities</p>

<p>$a_1x_1 + a_2x_2 + a_3x_3 + a_4x_4 = b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 = c_1x_1 + c_2x_2 + c_3x_3 + c_4x_4$</p>

<p>Where the $a$'s, $b$'s, and $c$'s are known, non-negative integers.</p>

<p>Is there an efficient way to check if a solution exists (the $x$'s) such that they are non-negative real numbers (except for the trivial case of all $x$'s being 0)? I don't need to actually calculate them, just need some way to see if a solution even exists.</p>
",<linear-algebra>
"<p>I'm trying to figure out what the dimension of the set of self-adjoint operators on V would be, or in more concrete terms:</p>

<p>Let $dim V =n$. Let $S(V)$ denote the set of self-adjoint linear operators on V. What is its dimension?</p>

<p>The only thing I know that somewhat resembles this might be that $dim L(V) = (dim V)^2$ but I'm not sure how I might be able to apply that to this problem.</p>

<p>Any tips or assistance would be greatly appreciated. Thanks!</p>

<p>EDIT: Also, V is a finite dimensional inner product (or Hermitian) space.</p>
",<linear-algebra>
"<p>Find the absolute extreme values taken by $f(x,y) = x^2 + 4y^2 + x - 2y$ on the closed region enclosed by the ellipse $1\over4$$x^2 + y^2 = 1$. </p>

<p>I know this might be a basic question but could someone please explain how to solve this problem?</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>What does it say about the eigenvectors of a matrix $A$ if the row-reduced form of the characteristic polynomal in coefficient matrix form has a row of 0's?</p>

<p>I know that it indicates something about the eigenvectors of $A$ but I can't remember what exactly... </p>
",<linear-algebra>
"<p>I just wanted to ask whether my proof is correct:</p>

<p>Suppose instead that $\mathbb{R}$ had a countable $\mathbb{Q}$-basis, say $v_1,v_2,v_3,\ldots$ (possibly finite).</p>

<p>Since $\mathbb{Q}$ is countable, $\,\text{span}(v_1,\ldots,v_k)$ is countable for each $k$ (possibly finitely many).</p>

<p>We have $\mathbb{R}=\bigcup_{k}\text{span}(v_1,\ldots,v_k)$ which is a countable union of countable sets.</p>

<p>It follows that $\mathbb{R}$ is countable. Contradiction.</p>

<p>I would be very grateful for any feedback.</p>

<p>Best wishes!</p>
",<linear-algebra>
"<p>Please help me in solving the recursion   $F(n)=K_0\frac{F(n-1)}{n-1}+K_1\frac{F(n-2)}{n-2}$, preferably using power series for the values of $F(n)$ in terms of $n$. Here $K_1$ and $K_2$  are constants. We are given  $F(0)=0,F(1)=3,F(2)=3/2$. Any methods of solution is welcome, even partial answers. We can discuss further. Thanks for taking time to read my question.</p>
",<linear-algebra>
"<p>Suppose the inner product on $P(R)$ is defined by $\langle p,q\rangle = \int^1_0 p(x)q(x)dx$.</p>

<p>Let $\phi$ be the linear functional on $P(R)$ defined by $\phi (p) = p(0)$ for each polynomial $p \in P(R)$. Prove that there does not exist $q \in P(R)$ such that $\phi (p) = \langle p, q\rangle$ for every $p \in P(R)$.</p>

<p>This is what I have so far:</p>

<p>Let $e_1,...,e_n$ be an orthonormal basis for $P(R)$.</p>

<p>$\phi (p) = \phi(\langle p, e_1\rangle e_1 + \dots + \langle p, e_n\rangle e_n) = \langle p, e_1\rangle \phi(e_1) + \dots + \langle p, e_n\rangle \phi(e_n)$</p>

<p>Thus, $\phi(e_1)e_1 + \dots + \phi(e_n) e_n = p(0)e_1 + \dots p(0) e_n$</p>

<p>I'm not sure where to go from here, help? Thank you!</p>
",<linear-algebra>
"<p>This came up in a practical problem involving a state change in a digital filter system.</p>

<p>Find $X$ given $A$ and $K$, where $A,X,K$ are all $n$ x $n$ square matrices:</p>

<p>$$
   X - A X A = K
$$
I couldn't find a direct way to do this, eventually I realized that it's just $n^2$ linear equations in $n^2$ unknowns, and it can be solved as follows:</p>

<ul>
<li>Restate so $X$ and $K$ are column vectors $\vec x$ and $\vec k$, each with $n^2$ elements</li>
<li>The term $A X A$ becomes $A_L A_R \vec x$, where $A_L, A_R$ are $n^2$ x $n^2$ matrices which perform the equivalent of multiplying by $A$ on the left and right in the original form</li>
<li>Find $\vec x = \left(I-A_L A_R \right)^{-1} \vec k$ and reorder to get $X$</li>
</ul>

<p>For $n=3$, for instance, if
$$
 A = \begin{bmatrix}
     a &amp; b &amp; c \\ d &amp; e &amp; f \\ g &amp; h &amp; i \end{bmatrix}
$$
then
$$
A_L = \begin{bmatrix}
  a &amp; 0 &amp; 0 &amp; b &amp; 0 &amp; 0 &amp; c &amp; 0 &amp; 0 \\
  0 &amp; a &amp; 0 &amp; 0 &amp; b &amp; 0 &amp; 0 &amp; c &amp; 0  \\
  0 &amp; 0 &amp; a &amp; 0 &amp; 0 &amp; b &amp; 0 &amp; 0 &amp; c  \\
  d &amp; 0 &amp; 0 &amp; e &amp; 0 &amp; 0 &amp; f &amp; 0 &amp; 0 \\
  0 &amp; d &amp; 0 &amp; 0 &amp; e &amp; 0 &amp; 0 &amp; f &amp; 0 \\
  0 &amp; 0 &amp; d &amp; 0 &amp; 0 &amp; e &amp; 0 &amp; 0 &amp; f \\
  g &amp; 0 &amp; 0 &amp; h &amp; 0 &amp; 0 &amp; i &amp; 0 &amp; 0 \\
  0 &amp; g &amp; 0 &amp; 0 &amp; h &amp; 0 &amp; 0 &amp; i &amp; 0 \\
  0 &amp; 0 &amp; g &amp; 0 &amp; 0 &amp; h &amp; 0 &amp; 0 &amp; i \end{bmatrix}
$$
and (in block form):
$$
A_R = \begin{bmatrix}
  A^T &amp; 0 &amp; 0 \\
   0 &amp; A^T &amp; 0 \\
   0 &amp;  0 &amp; A^T \end{bmatrix}
$$</p>

<p>Question is: is there any easier way to solve this? I'm now thinking that the relationship between the $n^2$ variables is such that you can't solve it without this kind of rewriting, but I'd be happy to be proven wrong. Is there a name for the procedure of restating the problem as above? Or a name for the type of the original equation? Perhaps it would be more natural with tensor notation - I'm not that familiar with that area.</p>

<p>One other note: the original equation can be be rewritten as the following two forms:</p>

<p>$$ 
X = A X A + K
$$
and (assuming $A$ is not singular)
$$
X = A^{-1} \left( X -  K \right) A^{-1}
$$
... and it seems to me that one of these (depending on the properties of $A$) may be usable as a iterator that will converge to the correct value of $X$. In some applications this may easier to work with than the full solution.</p>
",<linear-algebra>
"<p>I have a very stupid and simple question, which I do not have a clear idea on. One article that I read said, </p>

<blockquote>
  <p>""As a basic relationship in linear algebra states,
  the scalar product of vectors $x$ and $y$ in a base space of $A$ is
  $xAy$."" </p>
</blockquote>

<p>What does the 'base space' specifically represent? Is there any other term that indicates the same concept? What is the difference between ordinary scalar or inner product $xy$ and $xAy$? What role base space A does play? What are the impacts by converting $xy$ into $xAy$? Thank you so much in advance!</p>

<p>(I have uploaded the corresponding page below or you can have an access to the article via <a href=""http://www.sciencedirect.com/science/article/pii/S0378873310000031"" rel=""nofollow"">http://www.sciencedirect.com/science/article/pii/S0378873310000031</a> , in page 199, third paragraph.)</p>

<p><a href=""http://i.stack.imgur.com/tJMI5.jpg"" rel=""nofollow"">enter image description here</a></p>
",<linear-algebra>
"<ul>
<li>I want to show the following: For any square  matrix $A$ n $\times$ n of the form: $A=\begin{pmatrix} 
0 &amp; a_{1,2} &amp; . &amp; . &amp; . &amp; a_{1,n} \\ 
0 &amp; 0 &amp; a_{2,3} &amp; . &amp; . &amp; a_{2,n} \\ 
0 &amp; 0 &amp; 0 &amp; . &amp; .  &amp; .\\
. &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; a_{n-1,n} \\
. &amp; . &amp; . &amp; . &amp; . &amp; 0  
\end{pmatrix}$,</li>
</ul>

<p>$A^n=0$</p>

<p>In order to do that, I want to show that if you multiply such a matrix b times. All the coefficients $(a)_{ij}$ such that $i&lt;i+b-1$is equal to 0</p>

<ul>
<li><strong>My attempt</strong></li>
</ul>

<p>I am trying to show that $\forall b, \leq n$ $(A^b)_{i,i+b-1}= 0$</p>

<p>I will try to show this by induction</p>

<ul>
<li><strong>basis</strong> Let's show that $(A^2)_{i,i+1}=0$
By definition, $(A^2)_{i,i+1} = \Sigma^n_{k=0}a_{i,k}a_{k,i+1}$</li>
</ul>

<p>For $i \geq k$, $a_{i,k}=à \implies a_{i,k}a_{k,i+1} = 0$</p>

<p>For $i=k-1$, $i+1=k \implies i+1 \geq k \implies a_{k,i+1} = 0 \implies a_{i,k}a_{k,i+1} = 0$ And therefore, for $i &lt; k, a_{i,k}a_{k,i+1}=0$</p>

<p>Therefore, $(A^2)_{i,i+}=0$</p>

<p>We now have a matrix of the form: $A^2 = \begin{pmatrix} 
0 &amp; 0 &amp; a_{1,3} &amp; . &amp; . &amp; a_{1,n} \\ 
0 &amp; 0 &amp; 0 &amp; a_{2,4} &amp; . &amp; a_{2,n} \\ 
0 &amp; 0 &amp; 0 &amp; . &amp; .  &amp; .\\
. &amp; . &amp; . &amp; . &amp; . &amp; a_{n,n-2} \\
. &amp; . &amp; . &amp; . &amp; . &amp; 0 \\
. &amp; . &amp; . &amp; . &amp; . &amp; 0  
\end{pmatrix}$</p>

<ul>
<li><strong>Inductive step</strong> And here is where the struggle starts (I cannot finish this step).</li>
</ul>

<p>Let $(A^b)_{i,i+b-1}$ be TRUE, let's show that it implies that $(A^{b+1})_{i,i+b}$ is TRUE:</p>

<p>$(A^b)_{i,i+b-1}=0$</p>

<p>$\implies$ $(A)_{i,i+b-1}(A^b)_{i,i+b-1}=0$</p>

<p>$\implies \Sigma^n_{k=0} a_{i,i+b-1}(a^b)_{i,i+b-1}= 0$.</p>

<p><strong>EDIT: I think I have solved my problem, can you guys confirm</strong></p>

<p>We now have a multiplication of the form: $A \times A^b =\begin{pmatrix} 
0 &amp; a_{1,2} &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; a_{1,n} \\ 
0 &amp; 0 &amp; a_{2,3} &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; a_{2,n} \\ 
0 &amp; 0 &amp; 0 &amp; . &amp; .  &amp; . &amp; . &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; a_{n-1,n} \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; 0 \\
\end{pmatrix} \times  \begin{pmatrix} 
0 &amp; 0 &amp; . &amp; 0 &amp; a_{1,b+1} &amp; a_{1,b+2} &amp; . &amp; . &amp; . &amp; a_{1,n} \\ 
0 &amp; 0 &amp; 0 &amp; . &amp; 0 &amp; a_{2,b+2} &amp; a_{2,b+3} &amp; . &amp; . &amp; a_{2,n} \\ 
0 &amp; 0 &amp; 0 &amp; . &amp; .  &amp; 0 &amp; . &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; a_{n-b,n} \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; 0 \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
0 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; 0 \\ 
\end{pmatrix}$</p>

<p>Let's call the first matrix $N$ and the second matrix $M$.</p>

<p>Let's show that $(NM)_{i,i+b} = 0$:</p>

<p>$(NM)_{i,i+b} = \Sigma_{k=0}^n n_{i,k}m_{k,i+b} = (0 \times \Sigma_{k=0}^i n_{k,i+b}) + (\Sigma_{k=n-i}^n a_{i,k} \times 0) = 0$</p>

<p>Therefore $(NM)_{i,i+b} = (AA^b)_{i,i+b} = (A^{b+1})_{i,i+b} = 0$</p>

<p><strong>I think I have succeeded to prove by induction that $(A^b)_{i,i+b-1} = 0$ But I am not certain</strong></p>

<p>Can someone confirm that my proof by induction is right?</p>
",<linear-algebra>
"<p>Let $T$ is a linear operator on a vector space V such that $dim(RangeT)=k$ , then how to prove  $T$ can have $at$ $most$ $k+1$ distinct eigen values ??</p>
",<linear-algebra>
"<p>How do you show that two matrices are similar? I know that if they are similar, then B=(v^-1)AV, but how do you find V^-1 and V? For example, the matrices {[1,2]T, [3,2]T} and {[0,4]T, [2,2]T}, how would you find the P and P^-1 (if they are similar)? Or would you have to show they are similar in a different way?</p>

<p>Thank you!</p>
",<linear-algebra>
"<p>Suppose we have a $(n+m)\times(n+m)$ matrix
$$A=\begin{pmatrix}
  0 &amp; B \\
  B^T &amp; C \\
 \end{pmatrix},$$
where $B$ is a $m\times n$ matrix and $C$ is a symmetric $n\times n$ matrix. Now suppose $B\ll C$ (in terms of eigenvalues), then matrix $A$ has two distinct sets of eigenvalues. $n$ of them are of order of the eigenvalues of $C$, the remaining three are suppressed by two powers of 
$$\theta=BC^{-1}.$$
One can block-diagonalize $A$ by expanding $\theta$. On one hand one obtains $m\times m$ matrix 
$$D=-\theta C\theta^T$$. On the other hand there is the $n\times n$ matrix 
$$E=C+\frac{1}{2}(\theta^\dagger\theta C+C^T\theta^T\theta^*).$$</p>

<p>Could anyone tell me how to do the block-diagonalization by expanding $\theta$?</p>
",<linear-algebra>
"<p>Let $A\in M_n({\Bbb F}_p )$. Suppose that $A^p = I_n$. Show that$(A-I_n)^p = 0$, and $A$ has an eigenvector $v\in {\Bbb F}^n_p$ with eigenvalue 1.</p>

<p>I know that $p$ divides the binomial coefficient $(^p_i)$ for $1\leq i\leq p-1$.
But what is the next?</p>

<p>Thnaks!</p>
",<linear-algebra>
"<p>Every basis of $\mathbb R^6$ can not be reduced to a basis of $5$-dimensional  subspace of $\mathbb R^6$ by removing one vector . Can anyone give an example for that?</p>
",<linear-algebra>
"<p>For any values of parameter $a$ solve the following system of linear equations:
$$\begin{cases} x+y+2z=1 \\ 2x+ay-z=4 \\ 3x+y+3z=1 \end{cases} $$</p>

<p>Calculating the value of determinant I found out, after I equalled it with zero, that $a$ has the value $-4\over 3$. So I thought that if it's required to solve this equation for any $a$ , then first of all I had to suppose that $D=0$, this means that $a={-4\over 3}$. For this value I found out that there is an infinite number of solutions for this system. But now my question is: what do I have to do with the case when $D\neq 0$? </p>

<p>Thank you!</p>
",<linear-algebra>
"<p>Let $X$ be a normed vector space and $Y \subset X$ a closed subspace. We consider the quotient $X / Y$ and equip it with the quotient norm. Then we may form the completion $\overline{X / Y}$.</p>

<p>We compare $\overline{X / Y}$ to the following space: denote by $\overline{X}$ the completion of $X$ and by $\overline{Y} \subset \overline{X}$ the closure of $Y \subset \overline{X}$. Now we form the quotient $\overline{X} / \overline{Y}$ with the quotient norm.</p>

<blockquote>
  <p>Are $\overline{X / Y}$ and $\overline{X} / \overline{Y}$ (naturally) isomorphic, i.e., is there a linear, bijective isometry between them?</p>
</blockquote>
",<linear-algebra>
"<p>I am asked to write a Matlab program to find the coefficients of the resulting polynomial which is the product of two other polynomials. However, I need someone to clarify the underlying concepts for me. In this post, I will use $P_a(x)=1+2x+3x^2$ and $P_b=1+2x+3x^2+4x^3$ as our examples for the two polynomials. Using matrix formulation (which is the requirement):
$$
a = \left(\begin{array}{c}
1\\
2\\
3\\
0\\
0\\
0\end{array} \right)
\qquad b = \left(\begin{array}{c}
1\\
2\\
3\\
4\\
0\\
0\\\end{array} \right)
$$
And we are looking for the vector $c$ that is the coefficients of our product polynomial. Since the degree of the resulting polynomial is $5$, we will have $6$ coefficients including the constant term. And I know that $c$ is a product of the circulant matrix of $a$ and the vector $b$:
$$
c = \left(\begin{array}{c}
c_0\\
c_1\\
c_2\\
c_3\\
c_4\\
c_5\end{array} \right)=
\left(\begin{array}{cccccc}
1&amp;0&amp;0&amp;0&amp;3&amp;2\\
2&amp;1&amp;0&amp;0&amp;0&amp;3\\
3&amp;2&amp;1&amp;0&amp;0&amp;0\\
0&amp;3&amp;2&amp;1&amp;0&amp;0\\
0&amp;0&amp;3&amp;2&amp;1&amp;0\\
0&amp;0&amp;0&amp;3&amp;2&amp;1\\
\end{array} \right)
\&gt;\dot\
\&gt;\left(\begin{array}{c}
1\\
2\\
3\\
4\\
0\\
0\\
\end{array} \right)
$$
and we denote the circulant matrix $A$. I know I can construct $A$ by using $FFT$: $$
A = F^{-1}\&gt;diag(F\,a)\&gt;F
$$
And here's what confuses me. From this point on, do I just multiply the matrix A by the vector b <strong>directly</strong>? I am required to implement the algorithm in $O(n\&gt;logn)$ time. And Let me repeat the requirement: <strong>1. Matrix formulation of the problem 2. Using FFT 3. Overall run time being $O(n\&gt;logn)$</strong> Any input is greatly appreciated. Thanks!</p>
",<linear-algebra>
"<p>Given two square matrices $A, B$, when is
$$\det(A+tB) = 0$$
for all $t\in \mathbb{R}$?</p>

<p>An easy sufficient condition is that $A$ and $B$'s kernels have nontrivial intersection. Per Henning's comment below, this is not also necessary.  Does there exist a nice necessary and sufficient characterization?</p>
",<linear-algebra>
"<p>Let $f \in \textrm{O}(n, \mathbb R)$. (O is the orthogonal group) Show:</p>

<p>i) If $f(W) \subset W$ for a subspace $W \subset \mathbb R^n$, then $f(W^\perp) \subset W^\perp$.</p>

<p>ii) If $f(\langle v \rangle ) \subset \langle v \rangle$ for a vector $0 \neq v \in \mathbb R$, then $v$ is an eigenvector of $f$ with value $1$ or $-1$.</p>
",<linear-algebra>
"<p>Find a projection $E$ wich projects $\mathbb{R}^2$ onto the subspace spanned by $(1,-1)$ along the subspace spanned by $(1,2)$.</p>

<p>What is the way to approach this problem? Almost to start! </p>

<p>Any suggestion is welcome, thanks.</p>
",<linear-algebra>
"<p>let $P_5 = \{ a_0 + a_1x + a_2x^2 + a_3x^3 + a_4x^4 + a_5x^5 \}$ be the vector space of polynomials of degree $\leq 5$ over $\mathbb{Q}$. Denote $D: P_5 \to P_5$ as the differentiation linear map, i.e. $D(\alpha) = \dfrac{d\alpha}{dx}$</p>

<p>1)Find the inverse of $D^4 + D^2 + Id$,</p>

<p>my answer: $Id - D^2$ as $D^6 - Id = (D^2-Id)(D^4 + D^2 + Id)$</p>

<p>2) find a unique solution to $\alpha \in P_5$ to the differential equation $\dfrac{d^4\alpha}{dx} + \dfrac{d^2\alpha}{dx^2} + \alpha = x^5 + 2x^3$</p>

<p>Could someone explain how I can use (1) to solve part (2)?</p>
",<linear-algebra>
"<p>Find the equation of the plane that passes through the line of intersection of the planes $4x - 2y + z - 3 = 0$ and $2x - y + 3z + 1 = 0$, and that is perpendicular to the plane $3x + y - z + 7 = 0$.</p>

<p>I have attached a picture and that is what I got.</p>

<p>Can someone please tell me if my answer is right because the answer at the back of the textbook is different but I am pretty confident with my solution. Thanks!</p>

<p><img src=""http://i.stack.imgur.com/5yp08.jpg"" alt=""enter image description here""></p>
",<linear-algebra>
"<p>Let the projector be the $N \times N$ matrix $A$. Let its rank be $r$. Let the dimension of the space for which it is the projector be $m$. Is $m==r$?</p>
",<linear-algebra>
"<p>Let $0$ denote the function ${T}$ that takes each element of some vector space to the additive identity of another vector space. Prove that $T$ is linear.</p>

<p>I just want to make sure I understand the basic properties of a linear map with this very simple function.</p>

<p>$ 0 \in\mathcal{L}(V,W)$ defined by $0v = 0$.</p>

<p>Additivity: $0(v+w) = 0v + 0w$ for all $v,w \in V$.</p>

<p>Homogeneity: $0(av) = a(0v)$ for all $a \in \mathbb{F}$ and all $v \in V$.</p>

<p>Therefore $T$ is linear.</p>

<p>Is this verification correct?</p>
",<linear-algebra>
"<p>$$\vec{v}^{t}\textbf{A}\vec{v} &gt; \textbf{0}\text{ and }\textbf{A}\vec{v} = \lambda\vec{v}\quad \Rightarrow \lambda&gt;\textbf{0}\quad(\mathbb{F}=\mathbb{R}) $$</p>

<p>proof:
$$\vec{v}^{t}\textbf{A}\vec{v} &gt; \textbf{0} \text{ and } \textbf{A}\vec{v}=\lambda\vec{v} $$
$$\Rightarrow\vec{v}^{t}\lambda\vec{v} &gt; \textbf{0} $$
$$\Rightarrow\lambda\vec{v}^{t}\vec{v} &gt; \textbf{0} $$
$$\Rightarrow\lambda\langle v, v\rangle&gt; \textbf{0} $$
$$\vec{v} \neq \vec{0}$$
$$\Rightarrow\langle v, v\rangle &gt; \textbf{0} $$
$$\Rightarrow\lambda &gt; \textbf{0}$$</p>
",<linear-algebra>
"<p>This question is somewhat simple: </p>

<p>If we write the transposition of a matrix like this:</p>

<pre><code>A_transposed = T.A
</code></pre>

<p>where T is the operator performing the transposition, can I find a matrix form for this operator, that is independent of the matrix A ? (In other words that would transpose any matrix the size of A? </p>
",<linear-algebra>
"<p>Find the equation of the plane tangent to the surface:
$$x^{\frac{1}{3}}+y^{\frac{1}{3}}+z^{\frac{1}{3}}=1$$ at the point:
$$P=\left(1,-1,1\right)$$
How to find it? I know i have to calculate a gradient which is:
$$\left(\frac{1}{3}x^{-\frac{2}{3}}, \frac{1}{3}y^{-\frac{2}{3}}, \frac{1}{3}z^{-\frac{2}{3}} \right)$$ but what should i do next? I think i need to substitute point into the gradient but how to substitute this point if i have $-1$ under the root?</p>
",<linear-algebra>
"<p>So, why do eigenvalues exclusively form the main diagonal in a diagonalizable matrix?</p>

<p>If we have $n\times n$ matrix ($n$ being a natural number) that is diagonalizable, why is it eigenvalues (exclusively eigenvalues) that make up the main diagonal? </p>
",<linear-algebra>
"<p>In the beginning of linear algebra courses, there are vectors in $\mathbb R^n$ and the dot product is introduced. We learn that if the dot product of two vectors is zero, then these vectors are called orthogonal and there is a right angle between them. Therefore, we understand perpendicularity from the definition of orthogonality. </p>

<p>In the last chapters, all previous notions are generalized as vector spaces and inner products are introduced. Then we learn vectors $\vec u$ and $\vec v$ are orthogonal if $\langle \vec u, \vec v \rangle=0$ in inner product spaces. Our previous knowledge guides us to seek a right angle between them. </p>

<p>Let $P$ be the vector space of first degree polynomials. Let $p(x)=ax+b$ and $q(x)=cx+d$. Define $\langle p(x), q(x) \rangle$ as</p>

<p>$$\langle p(x), q(x) \rangle=ac+bd$$</p>

<p>Then if we consider $p(x)=x-2$ and $q(x)=4x+2$, we find that $\langle p(x), q(x) \rangle=0$. But if we draw them, we see that there is $30.96$ degrees between them. So orthogonality in inner product spaces does not necessarily mean perpendicularity. If not perpendicularity, what should we understand from orthogonality in inner product spaces?</p>
",<linear-algebra>
"<p>$A$  is  an $n\times n$ matrix. Now  if  the  row-reduced  echelon  form  for  this  $A$  is   $E$  then  after  all  the  row  operations  we  have $\det(A)=M\det(E)$   where  $M$  is  a non-zero  scalar  from  the   field. 
 If  $E$  is  the  $n\times n$ identity  matrix  then  $\det(A) \neq 0$.
For  the  converse, $\det(A)$   is  non-zero, so  is  $M$. Then obviously $\det(E)\neq0$. How to  reach the  conclusion  that  $E$  is  the  identity matrix,from  here  just  need  a  little  help  with  that.</p>
",<linear-algebra>
"<p>The problem is to minimize the largest eigenvalue of a function of $x$.</p>

<p>objective:
$$ \min\{\lambda_{\max}(A(x))\}$$
where
$$A(x) = A_0+x_1A_1+x_2A_2+...x_nA_n$$ and all $A$ is positive semidefinite.</p>

<p>This problem can be solved by equivalent  SDP:
$$\text{minimize} \ \ t \\ \text{subject to} \ \ \ A(x)\leq tI$$
since
$$\lambda_{\max}(A(x)) \leq t \ \ \\ \text{iff} \ \ \ A(x)-tI \leq0$$
My question is why is that?</p>

<p>I can't find special property of eigenvalue for  positive semidefinite matrix where the above inequality holds</p>

<p>Can someone give me a brief proof? or point out where I can find the proof.</p>
",<linear-algebra>
"<p>Let $A\in \Bbb R^{n\times n}$ be a matrix such that $\mathrm{rank}(A) = n-1$ and consider the equation 
$$
  Ax = 0.
$$
Clearly, its solutions span a $1$-dimensional space, thus an additional assumption may lead to a unique solution. Let $a\in \mathbb R^n$ be a vector and consider a system of equations
$$ \tag{1}
\begin{cases}
  Ax &amp; = 0,
\\
 a\cdot x &amp; = 0
\end{cases}
$$
where $a\cdot x = \sum_i a_ix_i$ is the inner product. I have two questions: </p>

<ol>
<li><p>What are necessary and sufficient conditions on $a$ for $(1)$ to have the unique solution?</p></li>
<li><p>Can we rewrite $(1)$ in an equivalent matrix form, e.g. $(A+C)x = 0$ for some $C$.</p></li>
</ol>
",<linear-algebra>
"<p>Is the following true always for a matrix norm </p>

<p>$$\lVert AB\rVert \leqslant \lVert A\rVert \cdot \lVert B\rVert \text{ ?}$$</p>

<p>Related to this
 given $r$ is positive constant, $H$ is symmetric positive definite
 is the following true :</p>

<p>$$\lVert (rI - H)(rI + H)^{-1}\rVert &lt; 1 $$</p>

<p>or</p>

<p>$(rI - H)(rI + H)^{-1}$  has the spectral radius less than $1$ certainly?</p>

<p>Thank you.</p>
",<linear-algebra>
"<p>Let $A$ be any $n \times n$ matrix and $\| \cdot \|$ be the matrix norm induced by vector norm  on $\mathbb{R}^n$
(Euclidean
n-dimensional space). </p>

<p>If $\|I - A\| &lt; 1$, then show that $A$ is invertible 
and
derive the estimate</p>

<p>$\|A^{-1}\| &lt; \frac{1}{ 1 - \| I - A \|}$.</p>

<p>Similarly 
when can we expand $\frac{1}{ 1 - \| I - A \|}$ as a power series only is it if and only if the norm is less than $1$?</p>

<p>Thanks a lot!</p>
",<linear-algebra>
"<p>Line <em>m</em> goes through a point D(2, -4). Line <em>m</em> is parallel to line <em>l:</em> $5x+3y=-17$. Describe line <em>m</em> with an equation of type $ax+by=c$.</p>

<p>The solution should be $c=5*2+3*-4=-2$ so $\text{m: }5x+3y=-2$</p>

<p>The lines are parallel so $a=5$, $x$ and $y$ are known so: $5*2+b(-4)=c$. At this point I lack some information I should know to push that equation further. I suppose I should be able to fill either $b$ or $c$, but which one and why?</p>
",<linear-algebra>
"<p>Let $A_1, A_2, A_3, \ldots , A_m$ be positive semi-definite Hermitian matrices and then consider the polynomial $p(z,z_1,z_2,\ldots,z_m) = \det(z+z_1A_1 + z_2A_2 + \cdots+z_mA_m)$</p>

<p>Now Tao argues that if $z,z_1,z_2,\ldots,z_m$ have a positive imaginary part then the ""skew-adjoint part"" (what is this?) of $z+z_1A_1 + z_2A_2 + \cdots+z_mA_m $ is strictly positive definite and hence the quadratic form $\operatorname{Im} [ \langle (z+z_1A_1 + z_2A_2 + \cdots+z_mA_m)v, v \rangle   ]$ is non-degenerate and hence it follows that $z+z_1A_1 + z_2A_2 + \cdots+z_mA_m$ is non-singular.</p>

<p>Can someone kindly help understand what happened here? </p>
",<linear-algebra>
"<p>I'm losing my mind over this question.
For $H$ a Hilbert space, $A,B$ closed subspaces, and $B$ is of dimension $1$, I want to prove that $A+B$ is also closed.</p>

<p>I'm looking for a straightforward proof, with minimum use of high theorems (without Hahn-Banach, etc).</p>

<p>What I did so far was taking a converging sequence in A+B, and mark it as:
$x_k=a_k+\lambda_kv$, for $v$ the base of $B$.
I figured out that I could write the limit, $x$, as $x=P_Ax+P_{A^\perp}$ and do the same on $B$, and maybe work something from there.</p>

<p>I also thought that somehow it would be good to take advantage of the sequence $\lambda_k$ which is in $\mathbb{R}$ resp. $\mathbb{C}$. Maybe prove that it converges somehow..</p>

<p>Thanks so much</p>
",<linear-algebra>
"<p>I have a question on my assignment asking me to prove that if $A$ is an $n\times n$ orthogonal matrix, then $\det(A) = \pm1$. What I did so far is:</p>

<p>We know that $A\cdot A^t = I$ (since it is orthogonal), and that would mean that if $A$ is a matrix with elements $a,b,c,d$ then :</p>

<p>\begin{cases}a^2 + b^2 = 1\\
ac + bd = 0\\
c^2 + d^2 = 1\end{cases}</p>

<p>which implies that \begin{align}\det(A)&amp;= ad - bc\\
&amp;= -\frac{bd^2}{c} - cb\\
&amp;= -\frac{bd^2}{c} - \frac{c^2b}{c}\\
&amp;= -\frac{bd^2 + bc^2}{c}\\
&amp;= -\frac{b(d^2 + c^2)}{c}\\
&amp;= -\frac{b(1)}{c}\\
&amp;= -b/c\end{align}</p>

<p>I might be doing something wrong here, could someone please help me out understand this?</p>
",<linear-algebra>
"<p>Finding whether a given set is subspace of</p>

<p>$$ P_n$$</p>

<p>which is for $n\ge 0$ the set of all polynials of degree at most consist of all polynomial of the form</p>

<p>$$p(t)=a_0+a_1t^1+\cdots+a_nt^n$$</p>

<p>where coefficient and t are real numbers.</p>

<p>is </p>

<p>$$p(t)=a+t^2$$</p>

<p>where $a$ is a real number.</p>

<p>a subspace of $P_n$</p>

<p>I think no because the zero Vector of $P_n$ $0$ is not in</p>

<p>$$p(t)=a+t^2$$</p>

<p>if $t$ is zero then $a+0$ does not equal $0$?</p>
",<linear-algebra>
"<p>It's clear that for any field $\mathbb{F}$ any finite group $G$ can be embedded into $GL_{n}(\mathbb{F})$ for some $n$.</p>

<p>My question is about one modification of this result.
Let's fix positive integer $N$. Is it true that for any finite group $G$ there exist a field $\mathbb{F}$ such that $G$ is embeddable into $GL_N(\mathbb{F})$?</p>

<p><strong>UPD:</strong> For $N=1$ it's false. Let's consider $N&gt;1$.  </p>
",<linear-algebra>
"<p>What part of the definition of a vector space (see <a href=""https://en.wikipedia.org/wiki/Vector_space#Definition"" rel=""nofollow"">here</a>) requires it to be <strong>closed</strong> under addition and multiplication by a scalar in the field? I would understand if we defined a vector space as a group of vectors rather then a set but we don't, also non of the axioms require this to be a condition?</p>
",<linear-algebra>
"<p>Question goes: Law enforcement would like to know the time at which a person died. The investigator arrived on the scene at 8:15pm, which we will call $t$ hours after death. At 8:15 (i.e $t$ hours after death), the temp of the body was found to be $27.4°C$ (Degrees). One hour later, $t+ 1$ hours after death, the body was found to be $26.1°C$. Known constants are $T_s=21°C$, $T_o=36.8°C$.</p>

<p>At what time did the victim die?</p>

<p><strong>MY WORKING</strong></p>

<p>Formula: $T(t)=T_s+(T_o-T_s)e^{-kt}$</p>

<p><strong>1</strong>. $T(t)=T_s+(T_o-T_s)e^{-kt} \quad \rightarrow \quad  
27.4=21+15.8e^{-kt}$</p>

<p><strong>2</strong>.$T(t)=T_s+(T_o-T_s)e^{-kt} \quad \rightarrow \quad 26.1=21+15.8e^{-kt}$</p>

<ol>
<li><p>$27.4=21+15.8e^{-kt}\rightarrow  
6.4=15.8e^{-kt}\rightarrow  
\ln(6.4/15.8)=-kt\rightarrow  
-0.903=-kt$</p></li>
<li><p>$26.1=21+15.8e^{-kt} \rightarrow  
5.1=15.8e^{-k(t+1)} \rightarrow 
ln(5.1/15.8)=-k(t+1)  \rightarrow $</p></li>
</ol>

<p>$-1.131=-k(t+1)$</p>

<p>This is as far as I have got and I believe I should be doing simultaneous equations but am totally unsure if thats correct. Any help would be appreciated.</p>
",<linear-algebra>
"<p>How do I prove the statement: if there exist unique $u$ and $w$ such that for any $v$, $v=u+w$, then $V$ is the direct sum of $U$ and $W$? ($U,W,V$ are vector spaces, $u \in U, w \in W, v \in V$)</p>

<p>I have this vague feeling that I should negate the conclusion and show a contradiction occurs, but it's not so easy for me. How do I expand the logic? </p>
",<linear-algebra>
"<p>It may be that the title of my question is wrong but i am writing this question because i am struck while reading this paper  <a href=""https://projecteuclid.org/download/pdf_1/euclid.kjm/1250776060"" rel=""nofollow"">Brownian motion on rotational group</a> Where $^*\mathscr{f} $ is transpose of $\mathscr{f}$.</p>

<p>If $\mathcal{\gamma_1(=|\mathscr{f}|^2)\ge \gamma _2\ge\gamma _3} $  are the eigenvalues of  $\mathscr{f^*f}$ ,if the $p_1,p_2,p_3$ are the corresponding projections $(\gamma_1 p_1+\gamma_2p_2+\gamma_3p_3=\mathscr{f^*f})$,and if </p>

<p>$$\int_{S^2}do$$ </p>

<p>is the arithmetic average over the spherical surface $S^2$ ,then the </p>

<p>$$\int_{S^2}|p_1o|^2do=\int_{S^2}|p_2o|^2do=\int_{S^2}|p_3o|^2do=\frac{1}{3}$$and $$\int_{S^2 }of \ ^*fodo $$</p>

<p>where $o\ f\ ^*f\ o$ is the inner product of $o\in S^2 $ and $\mathcal{f}\in R^3$.
1. are f the skew hermitian matrix ?and where are these projections taken on?
2.what does it mean to have  inner product on two different basis  ?
any help will be appreciated</p>
",<linear-algebra>
"<blockquote>
  <p>The set of all polynomials in a single variable $x$ forms a vector space $P$ of infinite dimension. Differentiation is a
  linear transformation on this vector space: </p>
  
  <p>$\frac{d}{dx}: P → P, p(x) → p'(x)$.</p>
  
  <p>(a) What is the dimension of the kernel of $\frac{d}{dx}$
  as a linear transformation on $P$ ? </p>
  
  <p>(b) The linear transformation $\frac{d}{dx}
+ 2x$ acts on $P$ as $p(x) → p'(x) + 2xp(x)$. What is the dimension of its kernel?</p>
</blockquote>

<p>I do know what dimensions and kernels of matrices are but this question is confusing me and I don't really understand it. Would really appreciate some help.</p>
",<linear-algebra>
"<p>Show that reflecting $R_2$ across the line $y = x$ and then reflecting it across the $y$−axis is the same as rotating it counterclockwise by $90$ degrees.</p>

<p>I know how to prove this statement geometrically, but I'm assuming the question is asking me to prove this through rotational vectors, which I'm not sure how to prove.</p>
",<linear-algebra>
"<p>Let T : R2 → R2 be the linear transformation which rotates the plane clockwise by 45 degrees, then expands the plane by a factor of 2 in the direction of the x-axis, then finally rotates the plane counterclockwise by 45 degrees. </p>

<p>Find a standard matrix for T. </p>

<p>What does T do to the square whose vertices are (0, 0),(1, 1),(0, 2),(−1, 1)?</p>

<p>I'm struggling with this question at the moment. How would I find the standard matrix of T when I don't know the initial values of R2, before transformation?</p>

<p>For the transformation of the square, I'm not sure what the question is asking. Wouldn't the transformation of the square just be all four vectors rotated by 45 degrees, expanded towards the x-axis by a factor of 2 and then rotated counterclockwise by 45? How would I show that in terms of matrix T.</p>

<p>Help would be appreciated.</p>
",<linear-algebra>
"<p>Let $V$ be a matrix. </p>

<p>What conditions should we require so that we can find a random vector $X = (X_1, \dots, X_n)$ so that $V = Var(X)$? </p>

<p>Of course necessary conditions are:</p>

<ul>
<li>All the elements on the diagonal should be positive</li>
<li>The matrix has to be symmetric</li>
<li>$v_{ij} \le \sqrt{v_{ii}v_{jj}}$ (Because of $Cov(X_i, X_j) \le \sqrt{Var(X_i) Var(X_j)})$</li>
</ul>

<p>But I am sure these are not sufficient as I have a counterexample.</p>

<p>So what other properties we should require on a matrix so that it can be considered a covariance matrix? </p>
",<linear-algebra>
"<p>I have a collection of 3D points in the standard $x$, $y$, $z$ vector space. Now I pick one of the points $p$ as a new origin and two other points $a$ and $b$ such that $a - p$ and $b - p$ form two vectors of a new vector space. The third vector of the space I will call $x$ and calculate that as the cross product of the first two vectors.</p>

<p>Now I would like to recast or reevaluate each of the points in my collection in terms of the new vector space. How do I do that?</p>

<p>(Also, if 'recasting' not the right term here, please correct me.)</p>
",<linear-algebra>
"<p>Let $F:= \mathbb{F}_7[x]/(x^2+3x+1)$</p>

<ol>
<li>Is it a field?</li>
<li>Find all the roots in F of the polynom  $f (Y) := Y^2+[3]_{F}Y +[1]_{F} \in F[Y]$.</li>
</ol>

<p><strong>Attempt:</strong></p>

<ol>
<li>It is a field, because $x^2+3x+1$ is irreducible $\in \mathbb{F}_7[x]$. In fact it has no roots $\in \mathbb{F}_7$.</li>
<li>I suppose I can't just replace numbers from $0$ to $6$ in the place of the $Y$. What should you do to solve this problem?</li>
</ol>
",<linear-algebra>
"<p>My idea of proving every real symmetric matrix can be diagonalized is that, first prove two eigenvectors with different eigenvalues must be orthogonal, then I failed to prove that all the eigenvectors span the whole vector space.</p>

<p>To be specific, my question is, if $A$ is a real symmetric $n\times n$ matrix, let $p(t)=\det(tI-A)$ be the characteristic polynomial of $A$, and $\lambda$ be some eigenvalue of $A$, and $\lambda$ is a root of $p(t)$ of order $k$, then how to prove $\dim (\ker(\lambda I-A))=k$?</p>
",<linear-algebra>
"<blockquote>
  <p>\begin{align*}
  x - \alpha y &amp;= 1\\
  \alpha x - y &amp;= 1
  \end{align*}</p>
</blockquote>

<p>For which values of alpha does the system have an infinite number of solutions, no solutions and one solution.</p>

<p>Find the solution when it is unique.</p>

<p>My attempt:</p>

<p>$-\alpha \cdot \mathrm{eqn}_1 + \mathrm{eqn}_2$ resulting in $(\alpha^2 - 1)y = 1-\alpha$.</p>

<p>then we get $y = (1-\alpha)/(\alpha^2-1)$</p>

<p>so, $y = -1/(1+\alpha)$, but I am trying to proceed </p>
",<linear-algebra>
"<p>I'm having some trouble calculating the angle of an human joint in 3D using the Microsoft Kinect.</p>

<p>Here's an example of the angle of the elbow (using the shoulder and wrist joint):</p>

<p><a href=""http://i.stack.imgur.com/9tkQp.jpg"" rel=""nofollow"" title=""Example"">Image of example</a></p>

<p>Calculating angles between 0° and 180° is no problem, but when the person <strong>hyper</strong>extends his elbow my calculation returns 170° instead of 190°.</p>

<p>The calculation I'm using is as follows:</p>

<ol>
<li>$d = b - a$</li>
<li>$e = b - c$</li>
</ol>

<p>Where a, b and c are 3D-points and d and e are 3D-vectors.</p>

<p>My question is: <strong>How can I calculate the angle between $d$ en $e$ where the angle is between 0° and 360°?</strong></p>

<p>Thanks in advance!</p>
",<linear-algebra>
"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://math.stackexchange.com/questions/119904/units-and-nilpotents"">Units and Nilpotents</a>  </p>
</blockquote>



<p>Given $A^{2012}=0$ prove that $A+I$ is invertible and find an expression for $(A+I)^{-1}$ in terms of $A$. ($I$ is the identity matrix).</p>
",<linear-algebra>
"<p>From <a href=""http://en.wikipedia.org/wiki/Basis_%28linear_algebra%29#Definition"" rel=""nofollow"">Wikipedia</a>:</p>

<p>""A basis $B$ of a vector space $V$ over a field $K$ is a linearly independent subset of $V$ that spans (or generates) $V$.(1)</p>

<p>$B$ is a minimal generating set of $V$, i.e., it is a generating set and no proper subset of B is also a generating set.(2)</p>

<p>$B$ is a maximal set of linearly independent vectors, i.e., it is a linearly independent set but no other linearly independent set contains it as a proper subset.""(3)</p>

<p>I tried to prove (1) => (3) => (2), to see that these are equivalent definitions, can you tell me if my proof is correct:</p>

<p>(1) => (3):
Let $B$ be linearly independent and spanning. Then $B$ is maximal: Let $v$ be any vector in $V$. Then since $B$ is spanning, $\exists b_i \in B, k_i \in K: \sum_{i=1}^n b_i k_i = v$. Hence $v - \sum_{i=1}^n b_i k_i  = 0$ and hence $B \cup \{v\}$ is linearly dependent. So $B$ is maximal since $v$ was arbitrary. </p>

<p>(3) => (2):
Let $B$ be maximal and linearly independent. Then $B$ is minimal and spanning:</p>

<p>spanning: Let $v \in V$ be arbitrary. $B$ is maximal hence $B \cup \{v\}$ is linearly dependent. i.e. $\exists b_i \in B , k_i \in K : \sum_i b_i k_i = v$, i.e. $B$ is spanning. </p>

<p>minimal: $B$ is linearly independent. Let $b \in B$. Then $b \notin span( B \setminus \{b\})$ hence $B$ is minimal.</p>

<p>(2) => (1):
Let $B$ be minimal and spanning. Then $B$ is linearly independent:
Assume $B$ not linearly independent then $\exists b_i \in B, k_i \in K: b = \sum_i b_i k_i$. Then $B \setminus \{b\}$ is spanning which contradicts that $B$ is minimal.</p>
",<linear-algebra>
"<p>I have seen the statement ""Every finite dimensional vector space has a basis."" (<a href=""http://www.cs.xu.edu/math/math240/07s/06_Bases.pdf"" rel=""nofollow"">Here</a> on page 5)</p>

<p>I'm confused about what this tells me. It seems to tell me nothing: by definition, the dimension of a vector space is the number of elements in a basis of it. Then saying a vector space is finite dimensional is the same as saying that it has a basis.</p>

<p>Or are there any other definitions of dimension than the number of basis elements?</p>
",<linear-algebra>
"<p>Let $w_1, \dots, w_m \in \mathbb{C}^d$.</p>

<p>Condition (1) is:</p>

<p>$\sum_i |\langle v, w_i \rangle |^2 = \eta$ whenever $\|v\| = 1$.</p>

<p>Condition (2) is:</p>

<p>$\sum_i u_i u_i^* = I^d$, where $u_i = w_i / \sqrt{\eta}$</p>

<p><a href=""http://arxiv.org/pdf/1306.3969v3.pdf"" rel=""nofollow"">This paper</a> claims that the two conditions are equivalent (top of page 3, just beneath the statement of Corollary 1.3).  I can't figure out why that would be.  Can you help point me in the right direction?</p>
",<linear-algebra>
"<p>Show that the rank of $ n\times n$ symmetric tridiagonal matrix is at least $n-1$, and prove that it has $n$ distinct eigenvalues.</p>
",<linear-algebra>
"<p>For each $m \in \{0,1,2,3\} $  find a subspace of {0,1,2,3} of dimension $m$ and verify answers. I'm not sure what is meant by this or how to begin solving? </p>
",<linear-algebra>
"<p>Hey so I got a question about Vector spaces </p>

<blockquote>
  <p>Let $V=(8,\infty)$. For $u,v$ in $V$ and $a$ in $\mathbb R$ define vector addition by $u\boxplus v:= uv-8(u+v)+72$ and scalar multiplication by $a\boxdot u :=(u-8)^a +8$. It can be shown that $(V,\boxplus,\boxdot)$ is a vector space over the scalar field $\mathbb R$. Find the additive inverse of 16. </p>
</blockquote>

<p>So what I did was find the zero vector which is 9, and set u to 16, but that was the incorrect answer. So I'm confused as to how you find the additive inverse.</p>

<p>Thanks!  </p>
",<linear-algebra>
"<p>Is there a formal proper way of finding the line between two points?</p>

<p>By that I don't mean the line connecting the two points, I mean a line that runs the same distance away from point 1 and point 2.</p>

<p>To phrase it another way, I want to find the equation of a line that divides the plane into two equal parts, where each of the two points are the same distance from the line.</p>

<p>I drew a picture. In this picture, how do I find the purple line?</p>

<p><img src=""http://i.stack.imgur.com/gDe5v.jpg"" alt=""Line equidistant from two points""></p>

<p>It may or not be relevant, but I'm asking because I am trying to learn about Support Vector Machines.</p>
",<linear-algebra>
"<p><strong>Question:</strong> </p>

<blockquote>
  <p>Using the Lagrange's Multipliers method, find the points on the ellipse $x^2+2y^2=1$, that are situated in the longest and shortest distance from the line $x+y=2$. </p>
</blockquote>

<p>I know how to use Lagrange's Method but I do not know how to restrict this question to: 'find a functions max and min on a given set', thus you need to help me solely with that.</p>
",<linear-algebra>
"<p>I'm trying to figure this out: we're dealing with real numbers only here, with the standard scalar product defined as $&lt;\mathbf{v},\mathbf{w}&gt; = \mathbf{v}^T\mathbf{w}$, and we are told, unusually perhaps, that orthogonal matrices are defined as those for which $&lt;R\mathbf{v},R\mathbf{w}&gt; = &lt;\mathbf{v},\mathbf{w}&gt;$, and I need to show from this that the matrix R can be characterised by $R^TR=\mathbf{1}$ (and also that $det(R)=(+/-)1$ but this is fine).</p>

<p>Using the definition of the scalar product, it is not hard to get to $\mathbf{v}^TR^TR\mathbf{w}=\mathbf{v}^T\mathbf{w}$, which I feel is a good starting point, and seems to be close; whilst $R^TR=\mathbf{1}$ would make this work, it isn't the only matrix it can be. I also know $\mathbf{v}^TR^TR\mathbf{w}=\mathbf{w}^TR^TR\mathbf{v}$ but I'm not sure if that helps.</p>

<p>Thanks</p>
",<linear-algebra>
"<p>Find the equation of a plane tangent to the surface given by $$xyz+x^2-3y^2+z^3=14$$ at $$P=\left( 5,-2,3 \right)$$
In my opinion answer is: $$4x+27y+25z-41=0$$ If not please tell me what am i doing wrong.</p>
",<linear-algebra>
"<p>Is $\{(x,y,z) \in \mathbb{R}^3 :x^2+3y^2+12z^2 = 0\}$ a vector space?</p>

<p>My inclination is that the only real solution to $x^2+3y^2+12z^2=0$ is $(0,0,0)$, which is the trivial subspace of $\mathbb{R}^3$. However my true/false assignment is telling me that this is incorrect, that this set is in fact NOT a vector space. Why?</p>
",<linear-algebra>
"<blockquote>
  <p>Let $\{e_1,\ldots,e_n\}$ be an arbitrary basis in a finite dimensional inner product space $V$. Prove there exists vectors $\{f_1,\ldots,f_n\}$ such that $(e_i,f_j)=\delta_{ij}$.</p>
</blockquote>

<p>I tried using the the Gram-Schmidt process to obtain the $f_i$'s but the resulting $f_i$'s should apparently be uniquely determined and $(e_i,f_j)=\delta_{ij}$ won't hold.</p>

<p>What other methods could I try to obtain the $\{f_1,\ldots,f_n\}$? Perhaps we could use induction on $\dim(V)$ for a proof?</p>
",<linear-algebra>
"<p>Doing some reviewing and I'm not 100% sure if my thought-process is correct.</p>

<p>I have the following two vectors and need to prove they're a basis for $R^3$:</p>

<p>$$B=
        \begin{bmatrix}
        1 \\
        0 \\
        -1 \\
        \end{bmatrix},
  \begin{bmatrix}
        0 \\
        1 \\
        -1 \\
        \end{bmatrix}
$$</p>

<p>Now these two vectors are linearly independent, and now I have to prove they span $\Re^3$.</p>

<p>So I have two arbitrary scalars: $\alpha$ and $\beta$ that belong to R:</p>

<p>$$\alpha
        \begin{bmatrix}
        1 \\
        0 \\
        -1 \\
        \end{bmatrix}+\beta
  \begin{bmatrix}
        0 \\
        1 \\
        -1 \\
        \end{bmatrix}=
 \begin{bmatrix}
        x \\
        y \\
        z \\
        \end{bmatrix}
$$</p>

<p>Not much solving to do here</p>

<p>$$
        \begin{bmatrix}
        1 &amp; 0 &amp; x \\
        0 &amp; 1 &amp; y \\
        -1 &amp; -1 &amp; z \\
        \end{bmatrix}
$$</p>

<p>I am left with:</p>

<p>$\alpha = x$, $\beta=y$ and $-\alpha - \beta=z$</p>

<p>Now what I'm having difficulties understanding is how this basis can span all of $R^3$ with z being: $-\alpha - \beta=z$</p>
",<linear-algebra>
"<p>Let $V := R^n$ be a vector space and let $I \in O(n)$ be an operator satisfying $I^2 = -Id$. I want to show that the $span\{x,Ix\}$ is an invarient subspace of $I$.</p>

<p>Let $W = span\{x,Ix\}$. I need to show that $IW \subseteq W$. That is, if we transform $W$ by $I$, then we are still within $W$. But, my linear algebra is bad and i'm struggling to figure how to do this. Can anyone show me how to do this or offer advice?</p>
",<linear-algebra>
"<p>$f:G \rightarrow H$  is a group homomorphism</p>

<p>$g\in G$ is an element of order $n$.</p>

<p>(a) Prove that $f (g)$ is a final order and that the order of the element $f (g)$ parts n.</p>

<p>(b) What is the order of the element $g^{-1}$? What is the order of the element $g ^ m$ for $m\in \Bbb N$?</p>

<p>(c) Whether there is any injective homomorphism $(\Bbb Z_{12}, +) \rightarrow (\Bbb Z_{18}, +)$?</p>

<p>What $(\Bbb Z_{12}; +) \rightarrow (\Bbb Z_{24}; +)$? If so, find example.</p>

<p>i did this:</p>

<p>a) $f(g)$- is final order and parts n
$f(g)^n = f(e)=e$
$f(g)$ - is final order</p>

<p>b) $g^n = e \iff (g^-1)=e$
$g^{-1})^m=e$   for $m&lt;n    \implies  g^m=e$</p>

<p>So how can i do second (b) and (c) THANKS</p>
",<linear-algebra>
"<p><img src=""http://i.stack.imgur.com/KfInK.png"" alt=""I am trying to calculate B"">I wondered how to prove a theory I have about calculating a point having only a line and a circle the line is tangent to.
If  $$\Delta{y}^2$$$$\Delta{x}^2$$ Thus the distance between any two integer numbers is $$\sqrt{\Delta{y}^2+\Delta{x}^2}=m$$ After seeing this I decided to ""create"" my very own formula $$n*r=m$$ where r is the distance between the point on the line, m is the gradient and $$n\Delta{y}(initial)= \Delta{y}(new)$$ and $$n\Delta{x}(initial)=\Delta{x}(new)$$ thus the new point will be $$(x+\Delta{x}(new);y+\Delta{y}(new)) or (x-\Delta{x}(new);y-\Delta{y}(new))$$ I believe it will work for all lines and points with a distance. I am actually afraid to post this here but how can I prove my theory?
In this example I am trying to calculate B.</p>
",<linear-algebra>
"<p>I was trying to generate a direct formula for this series but I am not sure whether it is possible to do so.</p>

<p>$$1\ln(1) + 2\ln(2) + 3\ln(3) + 4\ln(4)+\dots+(n-1)\ln(n-1) + n\ln(n)$$</p>
",<linear-algebra>
"<p><strong><em>How exactly can I convert the below equation into the vector form?</em></strong> (i.e. V(i,j,k) form or $a*i+b*j+c*k$ form):
$$\frac{x-5}{-10}=\frac{y-3}{-6}=\frac{z-2}{-4}$$</p>

<p>I'm actually trying to find the angle between two 3D lines, but I only know how to find out angles between vectors, so I'm trying to convert the above equation to vector form so as to carry out what I need to.</p>
",<linear-algebra>
"<p>Given a matrix $M$, we can compute its singular value decomposition $M=U\Sigma V^*$ where $^*$ is the complex conjugate transpose. $U$ and $V$ are unitary, so $UU^*=I$, $VV^*=I$. Let's take the $i$-th column of $U$, $u_i$. I've experimented with a few examples and I found that the matrix $u_iu_i^*$ always has rank 1. What theorem/property is behind this? Or even, is there a short and sweet proof of it?</p>
",<linear-algebra>
"<p>What is the basis for the intersection of the plane $x-2y+3z=0$ with the $xy plane$ in $R^3$ ? Also can the basis and dimensions  of these planes separately be found  ??</p>
",<linear-algebra>
"<p>Let $f : \mathbb{C}^{n}\rightarrow \mathbb{C}^{n}$ be a linear operator with  a simple spectrum, furthermore, let $g : \mathbb{C}^{n}\rightarrow \mathbb{C}^n
$ be a linear operator such that  $f$ and $g$ commute.</p>

<p>Show that there is $P$ a polynomial such that $g=P(f)$.</p>

<p><strong>Remark:</strong> The spectrum is simple when the characteristic polynomial has not multiple roots.</p>
",<linear-algebra>
"<p>\begin{equation}
P = \begin{bmatrix}1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 3 \\ 3 &amp; 0 &amp; 1
\end{bmatrix}
\end{equation}</p>

<p>a) P is the transition matrix from what basis B to the standard
basis S = {e1, e2, e3} for R3?</p>

<p>b) P is the transition matrix from the standard basis
S = {e1, e2, e3} to what basis B for R3?</p>

<p>My attempt:</p>

<p>For a), if PB=S (is this even right?), can we just multiply inverse of P both sides to get B?</p>
",<linear-algebra>
"<p>Some years ago I was in a lecture where I met for the first time the matrix representation of some differential and integral operators (once discretized). Back then, someone mentioned me that every linear operator has its matrix representation. I liked the idea a lot, and I even think it sounded to me like a theorem. Unfortunately I have not been able to find a reference where the same affirmation (together with some worked-out examples) would be. There is this book of Burden and Faires ""Numerical Analysis"", where certainly one can find examples of the equivalence of, say elliptic partial differential equations, with the explicit matrix of the discretized equation (using finite differences), chapter 12, eight edition. But I would hope to find some references with something more general (as I said, with some theorems behind), and also some examples, even if they are confined to finite differences only from which to extract the elements to construct the matrices. </p>
",<linear-algebra>
"<p>Let $A$ and $B$ be real $n\times n$ matrices with $ABA=A$. How can I prove
$\operatorname{tr} AB\le n$?</p>
",<linear-algebra>
"<p>Let $T$ be a diagonalizable operator on a vector space $V$. Prove that the operator
$$a_nT^n + a_{n-1}T^{n-1}+\cdots+a_1T+a_0 Id_V$$
 on $V$ is also diagonalizable for any scalars $a_1, a_1,\dots,a_n$.</p>

<hr>

<p>First off, what is $Id_V$? I've never seen this before.</p>
",<linear-algebra>
"<p>The question:</p>

<p><em>Suppose in a single period investment problem we may divide our wealth between n assets and that the return on the ith security is given by</em></p>

<p><em>$r_i = \alpha + \beta_i\theta + \epsilon_i,$ $i = 1, ..., n$,</em></p>

<p><em>where $\alpha,$ $\beta_i$ are constants, and $\theta$ and $\epsilon_i$ are random variables. $\theta$ represents some underlying common economic variable (such as overall economic growth) that influences the return on all the securities. The $\epsilon_i$ are random variables assumed to be uncorrelated with each other and with $\theta$. Suppose that the $\epsilon_i$ have zero mean and common variance $\sigma_\epsilon^2$.</em></p>

<p><em>Denote the mean and variance of $\theta$ by $\mu_\theta$ and $\sigma_\theta^2$.
Determine the covariance matrix $\Sigma$ of the returns, and verify that $\beta =(\beta_1, ..., \beta_n)$ is one of its eigenvectors.</em></p>

<p><em>What can you say about the other eigenvectors? Hence find the inverse $\Sigma^{-1}$. Find the portfolio that has minimal risk, and determine its variance and expected return.
Let $\beta_i$ = $1$ for all $i = 1,2,... ,n$. Treating $\mu_\theta$, $\sigma_\theta^2$ and $\sigma_\epsilon^2$ as fixed, let $n$ tend to infinity, and comment on the minimal variance portfolio. Interpret this in terms of diversification as an investment strategy.</em></p>

<p>Basically, I have done all of this question except everything after computing $\Sigma^{-1}$. I have the following formula for $\Sigma^{-1}$:</p>

<p>$\Sigma^{-1}$ = $aI  + b$, where $a = \displaystyle\frac{1}{\sigma_\epsilon^2}$ and $b = \left(\displaystyle\frac{1}{\sigma_\epsilon^2 + \sigma_\theta^2\beta^T\beta} - \displaystyle\frac{1}{\sigma_\epsilon^2}\right) \displaystyle\frac{1}{\beta^T \beta}$, </p>

<p>which I'm almost certain is correct. The problem is, that is pretty messy! So when I want to compute the minimal variance portfolio which is given by</p>

<p>$w = \displaystyle \frac{\Sigma^{-1}1}{1^T\Sigma^{-1}1}$, so I can compute the mean return. The variance is just $\displaystyle \frac{1}{1^T\Sigma^{-1}1}$, but this is still messy. </p>

<p>I get the motivation behind the question, which is that the $\epsilon_i$ represent risk due to unforeseen circumstances where as $\theta$ is risk felt by everyone in the market, and that by letting $n \rightarrow \infty$ I can in essence ""diversify away"" the contribution of risk due to the $\epsilon_i$, but I cannot show this explicitly, which is frustrating. </p>

<p>So, does anybody have smart ways of calculating the minimum variance portfolio, weights, variance etc? </p>
",<linear-algebra>
"<p>The way I always understood linear functionals on a vector space $V$ is to consider then as measuring objects which give projections when they are given vectors. Now I wanted to make this a little bit more precise and I did as follows: let $\omega \in V^\ast$ be a linear functional, $\omega \neq 0$. Since $\omega : V\to K$ where $K$ is the underlying field, and since $\omega$ is not null, we have that $\operatorname{Im}(\omega) = K$ since $K$ has dimension $1$.</p>

<p>Because of that, the rank-nullity theorem gives $\dim V = \dim \ker \omega + \dim K$ and this implies $\dim \ker \omega = \dim V - 1$. In that case $\ker \omega$ is a hyperplane and we know we can split $V$ as:</p>

<p>$$V = \ker \omega \oplus W,$$</p>

<p>now ne can show that there exists just one $w\in W$ such that $\omega(w) = 1$. To prove it, suppose there's another, say $\tilde{w}\in W$, then since $W$ is one-dimensional one has $\tilde{w} = kw$. Because of that we have $\omega(\tilde{w}) = k\omega(w)$ and since we suppose that both $w$ and $\tilde{w}$ are such that $\omega(w) = \omega(\tilde{w}) = 1$ one has $k =1 $ and $w = \tilde{w}$.</p>

<p>Given that, one can define the operator $P_W : V\to V$ given by $P_W(v) = \omega(v)w$ where $w$ is the unique vector with $\omega(w) = 1 $. This linear operator is a projection operator because $P_W^2 = P_W$.</p>

<p>In that case we can think of it in the following way: $P_W$ is a projection operator in the direction of $W$ with the property that $\omega$ is the object which measures the projections. So this unique $w$ defines ""one unit"" of $\omega$ and $\omega$ measure projections with respect to this $w$.</p>

<p>Is that the correct way to think about linear functionals? Is that way they can be considered measuring objects capable of giving projections?</p>
",<linear-algebra>
"<p>Show that $ B_1 = \{\textbf{Av}| \textbf{v} \in B\} $ is also a basis for $\mathbb{R}^n.$</p>

<p>I apologize for my informality, but I would really like some feedback as to whether I am using the correct reasoning. </p>

<p>To begin, I noticed that this set $ B_1$ should contain an $\textit{n}\times 1$ vector. So, this means that I need to prove that this vector is linearly independent and spans $\mathbb{R}^n$. </p>

<p>Since no component of $\textbf{v}$ is $0$ because it is linearly independent, and $\textbf{A}$ only has the trivial solution for a homogeneous system, then any $\textbf{Av}$ should be linearly independent. Because the dimensions of $\textit{B}_1$ are the same as $\mathbb{R}^n$, then it is a basis.</p>
",<linear-algebra>
"<ol>
<li>$\{(x,y): x^2+y^2=0, x,y\in\mathbb{C}\}$, is it a subspace of $\mathbb{C}^2$?</li>
</ol>

<p>I thought yes atleast a trivial subspace as $\{(0,0)\}$ , the answer says No!</p>

<ol start=""2"">
<li>$\{(x,y): x^2-y^2=0, x,y\in\mathbb{R}\}$, is it a subspace of $\mathbb{R}^2$?</li>
</ol>

<p>I thought yes as it is  just $y=x$ or $y=-x$ so one dimensional subspace of the plane, but answer says No!</p>

<p>3.$\{(x,y): xy=0, x,y\in\mathbb{R}\}$, is it a subspace of $\mathbb{R}^2$?</p>

<p>I thought yes atleast a trivial subspace as $\{(0,0)\}$ , the answer says No!</p>

<p>please help</p>
",<linear-algebra>
"<p><strong>Background:</strong> Many (if not all) of the transformation matrices used in $3D$ computer graphics are $4\times 4$, including the three values for $x$, $y$ and $z$, plus an additional term which usually has a value of $1$.</p>

<p>Given the extra computing effort required to multiply $4\times 4$ matrices instead of $3\times 3$ matrices, there must be a substantial benefit to including that extra fourth term, even though $3\times 3$ matrices <em>should</em> (?) be sufficient to describe points and transformations in 3D space.</p>

<p><strong>Question:</strong> Why is the inclusion of a fourth term beneficial? I can guess that it makes the computations easier in some manner, but I would really like to know <em>why</em> that is the case.</p>
",<linear-algebra>
"<p>Let $V$ be a finite dimensional real vector space and let $A:V\to V$ be a linear map such that $A^2=A$. Assume that $A\ne0$ and that $A\ne I$. Which of the following statements are true?</p>

<p>a. $ker(A)\ne0$</p>

<p>b. $V=ker(A)\oplus R(A)$</p>

<p>c. The map $I+A$ is invertible</p>

<p>(c) is true since eigen value of $A$ cann't be $-1$. (a) and (b) are too true??. Not sure about them</p>
",<linear-algebra>
"<p>In each of the following cases, describe the smallest subset of $C$ which contains all the eigenvalues of every member of the set $S$.</p>

<p>(a) $S=\{A\in M_n(C) | A=BB^*$ for some $B\in M_n(C)\}$</p>

<p>(b) $ S=\{A\in M_n(C)| A=B+B^*$ for some $B\in M_n(C)\}$</p>

<p>(c) $ S=\{A\in M_n(C)| A+A^*=0 \}$</p>

<p>For $\lambda$ as an eigen value of $B$ , (a) will have $\lambda\lambda^*=|\lambda|^2\ge0.$ Hence The smallest subset will be $\{x\in R|x\ge0\}$.</p>

<p>(b) will have the set as $R$. </p>

<p>(c)  will have the set as the set of purely imaginary numbers</p>
",<linear-algebra>
"<p>Consider an $m\times m$ non-negative matrix $A$ where elements of $A$ can take many different values e.g. they are functions of a variable z. Suppose $A$ is such that one of its eigenvalues is equal to one. Can we say anything about the properties of matrix $A$? </p>

<p>For example, a sufficient condition is that the sum of all columns to be one [plus irreducibility]. Under this condition, irrespective of the values of the elements of the matrix, one eigenvalue is always equal to one. My question: is there a simple necessary condition?</p>
",<linear-algebra>
"<p>If $V$ and $W$ are vector spaces and $T$ is a linear transformation such that $T:V\longrightarrow W$. Furthermore, if $T$ is onto then $\mathrm{rank}(T) = \dim(W)$ right? The range is on the entire codomain then. This is not a homework question I'm reviewing. I know it sounds stupid but I want to make sure I'm not missing something. Shouldn't this be the definition of onto? It's funny because this statement isn't mentioned anywhere in the text.</p>
",<linear-algebra>
"<p>If a man can row at a speed of x m/sec , and the water in the river flows at a speed of  y m/sec ,  then if then man wants to cross the river from one bank to the opposite bank, i.e. he would be moving horizontally in the river , neither upstream nor downstream , then what would be the effect on the speed of rowing of the man . What would be his speed in terms of x and y ?</p>
",<linear-algebra>
"<p>Looking for an elegant proof of $\det(\textbf{A}) = \det(\textbf{A}^{t})$ without Schur decomposition.</p>

<p><strong>Proof 1 with Schur decomposition</strong>
$$\textbf{A} = \textbf{P}^{t}\Delta\textbf{P} \implies\textbf{A}^{t} = (\textbf{P}^{t}\Delta\textbf{P})^{t} = \textbf{P}^{t}\Delta^{t}\textbf{P}$$
So, $\textbf{P}$  is unitary matrix, $\textbf{P}^{t}=\textbf{P}^{-1}$.
$$\det(\textbf{P})=\det(\textbf{P}^{t})= \det(\textbf{P}^{-1})$$
 $\Delta$ is upper triangular matrix.
$$\det(\Delta)=\det(\Delta^{t}) \implies   \det(\textbf{A})=\det(\textbf{A}^{t})$$ </p>
",<linear-algebra>
"<p>How to solve matrix equation $AXH+AHX−BH=0$? All matrices are square, $A$, $B$ known constant matrices and invertible, $H$ can take any value, $X$ represent the solution to be found. </p>

<p>I have seen about the Sylvester Equation like in this post <a href=""http://math.stackexchange.com/questions/39906/solving-a-matrix-equation-ax-xb-in-a-cas"">Solving a matrix equation $AX=XB$ in a CAS</a>, but I'm not sure how to apply it because of the presence of matrix H.</p>
",<linear-algebra>
"<p>I have that the set of vectors:</p>

<p>$$\vec u, \vec v, \vec w$$
is L.I.</p>

<p>This means that:</p>

<p>$$a_1\vec u + a_2\vec v + a_3\vec w = \vec 0 \implies a_1 = a_2 = a_3 = 0$$</p>

<p>I need to prove that the set:</p>

<p>$$(\vec u + \vec v + \vec w, \vec u - \vec v, 3\vec v)$$</p>

<p>Is also L.I.</p>

<p>What I did was to make a linear combination with the vectors, so:</p>

<p>$$k_1(\vec u + \vec v + \vec w) + k_2(\vec u - \vec v) + k_3(3\vec v) = \vec 0 \tag{1}$$</p>

<p>If this set is L.I., then $k_1 = k_2 = k_3 = 0$.</p>

<p>Expanding the sum:</p>

<p>$$k_1\vec u + k_1\vec v + k_1\vec w + k_2\vec u - k_2\vec v + 3k_3\vec v = \vec 0$$</p>

<p>So I have:</p>

<p>$$(k_1+k_2)\vec u + (k_1 -k_2 + 3k_3)\vec v + k_1 \vec w = \vec 0$$</p>

<p>By $(1)$ I have that this is a L.I. set iff 
$$(k_1+k_2) = (k_1 -k_2 + 3k_3) = k_1 = 0$$</p>

<p>Am I correct? How do I prove, then, that $(k_1+k_2) = (k_1 -k_2 + 3k_3) = k_1 = 0$?</p>
",<linear-algebra>
"<p>In general the inverse of a sparse matrix is dense. A notable (but trivial) exception from that rule are diagonal matrices. Is there any other (broad) class of sparse matrices whose inverse is also sparse?</p>
",<linear-algebra>
"<p>There exists a continuous function $f$ whose domain is $[2,5]$ and the range is $(3,4)$. We have to prove that there exists at least one point $p \in (2,5)$ such that $f(p)=p$.</p>

<p>Now this is easy to see intuitively. The values of $f$ increase more slowly than the values of $x$ do. In the beginning, $f(x)&gt;x$, in the end, $f(x)&lt;x$. So, there must be at least one point at which $f(x)=x$.</p>

<p>But how do I prove this rigourously, using some mathematical arguments?</p>
",<linear-algebra>
"<p>(Long time observer, first time asking a question, so excuse me if I get any of the rules wrong)</p>

<p>I am having trouble wrapping my head around this problem and presenting the proof. </p>

<p>If I know A, B is invertible, given: $X^{T}$$(BA)^{T}$$A^{T}$=$V^{T}$ I would like to show X = $A^{-1}$$B^{-1}$$A^{-1}$V </p>

<p>This is what I have so far</p>

<p>Given $X^{T}(BA)^{T}A^{T} = V^{T}$  and since we know $A, B$ is invertible then by property of transposes  $(AB)^{T}= B^{T}A^{T}$;</p>

<p>then $X^{T}A^{T}B^{T}A^{T} =V^{T}$;</p>

<p>$(XABA)^{T} = V^{T}$ by transposing both sides - ""involution"" </p>

<p>$XABA = V$ Then since $ABA $is invertible, it can be moved to the other side</p>

<p>as required I am left with $X = A^{-1}B^{-1}A^{-1}V$</p>

<p>Any comments or suggestions are appreciated!</p>
",<linear-algebra>
"<p>Can we define a vector space structure on $\mathbb {R}^n$ other than usual scalar multiplication and usual addition such that the dimension of $\mathbb {R}^n$ over $\mathbb {R}$ is not $n$ but some $m$ not equal to $n$?</p>
",<linear-algebra>
"<p>An ellipsoid centered at the origin is defined by the solutions $\mathbf{x}$ to the equation $\mathbf{x}^TM\mathbf{x} = 1$, where M is a positive definite matrix.</p>

<p>How can I see why M needs to be positive definite, based on the equation of an ellipse $Ax^2 + Bxy + Cy^2 = 1$ where $B-4AC &lt; 0$? It looks like the idea is to make $B-4AC &lt; 0$ equate to the requirement that all eigenvalues of $M$ are positive for a 2x2 matrix, but I can't seem to make it work.</p>

<p>Also, what other shapes can we represent with $\mathbf{x}^TM\mathbf{x} = 1$ when $M$ is not positive definite?</p>
",<linear-algebra>
"<p>There's a theorem in Linear Algebra which says that if ${\bf A}$ is an $m \times n$ matrix and $m &lt; n$, then the homogeneous system of linear equations ${\bf A}{\bf X}=0$ has a non trivial solution.</p>

<p>I read a proof but what happens if I have a matrix like ${\bf B}$, whereas</p>

<p>B =        \begin{pmatrix}
        1 &amp; 0 &amp; 0 &amp; 0\\
        0 &amp; 0 &amp; 0 &amp; 0\\
        0 &amp; 0 &amp; 0 &amp; 0\\
        \end{pmatrix}</p>

<p>For ${\bf B}$, $m &lt; n$ but the only solution for ${\bf B}$ is the trivial one, unless I am getting something wrong, which I suspect and I would appreciate a lot if you help me find what is it.</p>

<p>Thank you in advance.</p>
",<linear-algebra>
"<p><a href=""http://i.stack.imgur.com/podWE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/podWE.png"" alt=""The problem:""></a></p>

<p><a href=""http://i.stack.imgur.com/LqSh7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LqSh7.png"" alt=""enter image description here""></a></p>

<p>I understand how to find the image($A$). The basis of Im($A$) would be the first two columns of the matrix $A$ (given the two leading 1's in ref are in the first and second columns). </p>

<p>So the $\text{Ker}(B) = [1,1,1,1],[1,2,3,4]$ </p>

<p>But I do not get how to find $B$ based on its kernel? Any ideas? </p>
",<linear-algebra>
"<blockquote>
  <p><strong>Problem Statement</strong>: Let $A=\begin{bmatrix} 2 &amp;&amp; 1 \\ 1 &amp;&amp; 2 \end{bmatrix}$. Find an orthonormal basis for $\mathbb{C}^2$ with respect to the Hermitian form $\bar{x}^TAy$.</p>
</blockquote>

<p>I am trying to figure out this proof, but not sure if what I am currently trying to do will help me. I was goin to start out by taking an arbitrary basis $B=\left\{v_1,v_2\right\}$ and write two arbitrary vectors in $\mathbb{C}^2$ as linear combinations of basis elements:
$$v=x_1v_1+x_2v_2$$ $$w=y_1v_1+y_2v_2$$</p>

<p>Then break down the Hermitian form $$\langle v,w \rangle=\langle x_1v_1+x_2v_2, y_1v_1+y_2v_2 \rangle=\bar{x_1}\langle v_1, y_1v_1+y_2v_2 \rangle+\bar{x_2}\langle v_2, y_1v_1+y_2v_2 \rangle=\bar{x_1}y_1\langle v_1, v_1 \rangle+\bar{x_1}y_2\langle v_1, v_2 \rangle+\bar{x_2}y_1\langle v_2, v_1 \rangle+\bar{x_2}y_2\langle v_2, v_2 \rangle=\sum_{i=1,j=1}^2 \bar{x}_iy_j\langle v_i, v_j\rangle=\bar{x}^TAy$$</p>

<p>Then I want to use the fact that we're given $A$ in order to define $v_1,v_2$, but I am confused with one thing:
In my notes, I have that $$a_{ij}=\langle v_i,v_j\rangle$$ but my notes also say for any orthonormal basis, that $$\langle v_i,v_j\rangle:=\begin{cases} 1\ \mathrm{if}\ i=j \\ 0\ \mathrm{if}\ i\neq j \end{cases}$$
which conflicts with the fact that $A=\begin{bmatrix} 2 &amp;&amp; 1 \\ 1 &amp;&amp; 2 \end{bmatrix}$ because that would mean that $$\langle v_i,v_j\rangle:=\begin{cases} 2\ \mathrm{if}\ i=j \\ 1\ \mathrm{if}\ i\neq j \end{cases}$$</p>

<p>Is there a more straightforward approach to this problem?</p>
",<linear-algebra>
"<p>Suppose that $X$ is $n\times K$ with full column rank and $y$ $n\times 1$. I understand that if $\beta$ satisfies the system $X\beta=y$, then $\beta=(X'X)^{-1}X'y$ (dimension $k\times 1$). But how do I verify the reverse direction
$$
\beta=(X'X)^{-1}X'y\implies X\beta=y?
$$
This question is self-contained but it is related to what I asked <a href=""http://math.stackexchange.com/q/983816/178464"">earlier</a>. Thank you for your help. </p>
",<linear-algebra>
"<p>Consider the following homogeneous equation where $A$ and $X$ are matrices.</p>

<p>$$AX = 0$$</p>

<p>I want to know whether there are non trivial solutions for this equations.
Now, if $A^{-1}$ exists, then I can multiply throughout by it and get $X = 0$, so if $A$ is invertible, only the trivial solution exists.
However, I do not understand why $A$ being non-invertible would imply that non-trivial solutions exists, shouldn't it just imply that no solutions exist?</p>
",<linear-algebra>
"<p>I have been given the quadratic form $$A(x,x) = 2x^2-\frac{1}{2}y^2-2xy-4xz$$ and been asked to diagonalize it, find the change of basis matrix, and find the new basis in which A is diagonalized.</p>

<p>I found the diagonalized version of A to be $$2\xi_1^2-\frac{1}{2}\xi_2^2-2\xi_3^2$$ where $\xi_1 = x-\frac{y}{2}-z$,$\xi_2=y$,$\xi_3=\frac{y}{2}+z$.  I was then able to calculate the change of basis matrix $$B = \begin{pmatrix} 0 &amp;0 &amp;-1\\0 &amp; 1 &amp; -\frac{1}{2} \\ 1 &amp; 2 &amp; 2 \end{pmatrix}$$</p>

<p>This leads to my question.  How do I find the new basis?  Do I just perform Gaussian elimination on the diagonalized version of A?  So the new basis would just be the stardard basis $$\begin{pmatrix} 1 &amp;0 &amp; 0\\0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}$$</p>
",<linear-algebra>
"<p>Let n=1,2,3,... and $i^2=-1$ and:</p>

<p>$$F=[e^{i\frac{2\pi kl}{n}}]_{k,l=0}^{n-1}\in\Bbb{C}^{n,n}$$</p>

<p>Find $F^HF$ and $F^{-1}$.</p>

<p>In this quite challenging (at least for me) problem I started from finding the matrix $F^HF$. In order to do that you need find $F^H$ first. I think the equation of this matrix is $F^H=[e^{-i\frac{2\pi kl}{n}}]_{k,l=0}^{n-1}$. So now let's think about matrix $F^HF$. To do that we need to know what is the k'th row of $F^H$. It looks like this:</p>

<p>$[e^{-i\frac{2\pi k0}{n}} e^{-i\frac{2\pi k1}{n}} e^{-i\frac{2\pi k2}{n}}... e^{-i\frac{2\pi k(n-1)}{n}}]$.</p>

<p>Now what about j'th column of $F$? According to definition it should look like that:
$[e^{i\frac{2\pi 0j}{n}} e^{i\frac{2\pi 1j}{n}} e^{i\frac{2\pi 2j}{n}}... e^{i\frac{2\pi (n-1)j}{n}}]^T$</p>

<p>So let's think about indices k,j of $F^HF$. It should be sum of multiplication of corresponding elements of k'th row of $F^H$ and j'th column of $F$. So the value at indices k,j of $F^HF$ should look like that:</p>

<p>$e^{-i\frac{2\pi k0}{n}}*e^{i\frac{2\pi 0j}{n}} + e^{-i\frac{2\pi k1}{n}}*e^{i\frac{2\pi 1j}{n}}+...+e^{-i\frac{2\pi k(n-1)}{n}}*e^{i\frac{2\pi (n-1)j}{n}}$</p>

<p>So we can write $F^HF$ down as:</p>

<p>$[\sum_{m=0}^{n-1}e^{\frac{i2\pi m}{n}(j-k)}]_{k,j=0}^{n-1}$</p>

<p>Have I done everything right? Or maybe I completely screwed up this part of the problem?</p>

<p>Also, how to proceed with finding $F^{-1}$?</p>
",<linear-algebra>
"<p>I want to find the projection from $\mathbb{R}^n$ onto a vector subspace of $\mathbb{R}^n$. Can I do this by adding the projections to each basis vector, even if the basis vectors are not orthogonal?
Specifically, projecting $x$ onto $V$, can I define the projection $$\operatorname{proj}_V(x) = \sum_i \frac {v_i\cdot x}{v_i\cdot v_i}v_i$$ for basis vectors $v_i$?</p>
",<linear-algebra>
"<p>Let $L_A$ :$R^{3}_{col}$ $\rightarrow$ $R^{3}_{col}$ , X $\rightarrow$ AX be operator of left multiplication by matrix $$A=\begin{bmatrix}1 &amp; 2 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ -1 &amp; 3 &amp;4\end{bmatrix}$$Find bases of :</p>

<p><strong>a.</strong> kernel Ker $L_A$</p>

<p><strong>b.</strong> image Im $L_A$</p>

<p><strong>c.</strong> Ker $L_A$ + Im $L_A$ and Ker $L_A$ $\cap$ Im $L_A$</p>

<p>I found the null space of $L_A$ as [1,-1,1] and the image as [1,0,-1] and [2,1,3].  But I couldn't think of something for part c.
Do we use the fact that dim Ker $L_A$ + dim Im $L_A$= dim $L_A$??</p>
",<linear-algebra>
"<p>Let $V$ be a real vector space equipped with a scalar product $\langle, \rangle$ (i.e. a positive definite symmetric bilinear form). </p>

<p>We say that an endomorphism $J: V \to V$ is an almost complex structure if $J^2=-Id.$ </p>

<p>$J$ is said to be compatible with the scalar product if $\langle J v, J w \rangle = \langle v, w \rangle. $</p>

<p>I'd like a very simple example of a scalar product and almost complex structure such that $J$ FAILS to be compatible with $\langle, \rangle.$   This is very basic -and hopefully trivial- but I can't find any counterexamples. </p>
",<linear-algebra>
"<p>H is an inner product space with inner product $( . , . )$ over the complex numbers, and $T∈L(H,H)$.  Let $R=T+T^*$, $S=T-T^*$ .   Supposing that T is normal and $T(\alpha)=(x+iy)\alpha$, how do I compute $(R∘S)(\alpha)$ in terms of $x,y,\alpha$ if I know that $R∘S=T∘T-T^*∘T^*$?  Thanks for any and all help!</p>
",<linear-algebra>
"<p>I have a problem with the following question.</p>

<p>For which $n$ does the following equation have solutions in complex numbers</p>

<p>$$|z-(1+i)^n|=z $$</p>

<p>Progress so far.</p>

<ol>
<li><p>Let $z=a+bi$.</p></li>
<li><p>Since modulus represents a distance, the imaginary part of RHS has to be 0. This immediately makes $b=0$.</p></li>
<li><p>If solutions are in the complex domain $|a-(1+i)^n|=a $ by 2., and $a$ is Real. </p></li>
<li><p>?</p></li>
</ol>

<p>I don't know where to go from here.  </p>
",<linear-algebra>
"<p>Define operatot $L$ :$R^{3}_{col}$$\rightarrow$ $R^{3}_{col}$ by equation $L(x_1,x_2,x_3)=(3x_1+x_3,-2x_1+x_2,-x_1+2x_2+4x_3)$.</p>

<p><strong>a.</strong> Find matrix L in the standard basis of $R^{3}_{col}$.</p>

<p><strong>b.</strong> and in the basis $f_1=(1,0,1)$, $f_2=(-1,2,1)$,$f_3=(2,1,1)$.</p>

<p><strong>c.</strong> Show that L is invertible and evaluate $L^{-1}(x_1,x_2,x_3)$.</p>

<p>Do we write the linear equation in the form $$A=\begin{bmatrix}3 &amp; 0 &amp; 1 \\ -2 &amp; 1 &amp; 0 \\ -1 &amp; 2 &amp;4\end{bmatrix}$$. And then from here by row reducing to row echelon matrix find the basis.Right??</p>
",<linear-algebra>
"<p>I have two vectors, 1 is the current direction of a moving object and the other is the new direction that I want that object to change to.</p>

<p>What I'm trying to achieve is to get the current direction to gradually change to the new direction but I'm not really sure how to do this. It would be similar a ship slowly turning I guess.</p>

<p>Could someone help me out please?</p>
",<linear-algebra>
"<p>Is it true that there are irreducible hyperbolic polynomials $p(x,y,z) \in \mathbb{R}[x,y,z]$, $p$ homogeneous of any degree? Are there even concrete examples for such polynomials?</p>

<p>I know that there are irreducible polynomials of any degree and that there are hyperbolic polynomials of any degree in this setting, but I do not see whether also polynomials which have both properties exist of any degree. </p>

<p>Thank you for your help.</p>

<p>Edit: By hyperbolic I mean the following: </p>

<p>$p$ is hyperbolic with respect to $\textbf{e} \in \mathbb{R}^3$ if $p(\textbf{e}) &gt; 0$ and for all vectors $\textbf{x} \in \mathbb{R}^3$ the univariate polynomial $t \mapsto p(\textbf{x} - t\textbf{e})$ has only real roots.</p>
",<linear-algebra>
"<p>If $X,Y,Z$ are three random variables with normal distribution, how can we define the joint distribution of $(X,Y,Z)$? Can we define a similar covariance tensor $\Sigma=Cov(X,Y,Z) = E[(X-E[X])(Y-E[Y])(Z-E[Z])]$? </p>

<p>Notice that knowing only the joint distribution of any two is not sufficient to know the joint distribution. We need the know the information of such a tensor $\Sigma$.</p>

<p>My question comes from in the following problem. Consider $\Sigma$ to be symmetric, $\Sigma_{ijk} = \Sigma_{ikj}=\Sigma_{jik}=\Sigma_{jki}=\Sigma_{kij}=\Sigma_{kji}$. </p>

<p>If 
$$\partial_t \Sigma_{ijk} + \Sigma_{sjk} \partial_{x_s} u_i  +\Sigma_{isk} \partial_{x_s} u_j +\Sigma_{ijs} \partial_{x_s} u_k = 0, $$
can we find a function $f=f(\Sigma)$ such that 
$$\partial_t f + \partial_{x_s} u_s = 0$$
Here we use that Einstein summation notation. And for $\sigma$ being a matrix, this is easy. Since if $\partial_t \sigma_{ij} + \sigma_{sj} \partial_{x_s} u_i+\sigma_{is} \partial_{x_s} u_j=0$, we have $f(\sigma)=\ln \det \sigma$ satisfying $f_t + 2 \nabla \cdot u = 0$.</p>

<p>These two questions are related, which I will not show here.</p>
",<linear-algebra>
"<blockquote>
  <p>Let $c_{00} (\mathbb{N})$ denote the space of finitely non-zero sequences, and let $(\beta_n)_{n \in \mathbb{N}} \subset \mathbb{F}$ be a sequence of scalars. Then the subsets
  $$X := \{(x_n)_{n \in \mathbb{N}} \in c_{00} (\mathbb{N}) \; | \; x_{2n} = 0, \; \forall n \in \mathbb{N} \}, \quad Y := \{(x_n)_{n \in \mathbb{N}} \in c_{00} (\mathbb{N}) \; | \; x_{2n-1} + \beta_n x_{2n} = 0, \; \forall n \in \mathbb{N} \}$$ are complementary subspaces of $c_{00} (\mathbb{N})$, that is, the subsets $X, Y$ are closed subspaces and $c_{00} (\mathbb{N})$ is the internal direct sum of $X$ and $Y$.</p>
</blockquote>

<p>It follows readily that $X$ and $Y$ are closed subspaces of $c_{00} (\mathbb{N})$. However, I do not succeed in showing that $c_{00} (\mathbb{N})$ is the internal direct sum of $X$ and $Y$.</p>
",<linear-algebra>
"<p>The theory of commutative absolutely flat rings (a.k.a. commutative von Neumann regular rings) is algebraic and furthermore it is the smallest variety containing all fields. Being a variety the category of (set-theoretic) models is arguably much better behaved than the category of fields.</p>

<blockquote>
  <p>How does linear algebra in modules over commutative absolutely flat rings compare
  to plain linear algebra, i.e. working with vector spaces? Is is it
  basically the same or are there some major differences?</p>
</blockquote>

<p>Let's ask some more concrete questions:</p>

<p>Let $R$ be an commutative absolutely flat ring: Are finitely generated $R$-modules always free? Assuming the axiom of choice: Are <em>all</em> $R$-modules free?</p>
",<linear-algebra>
"<blockquote>
  <p>How to prove that eigenvalues of a rotation matrix in $\text{SO}(3)$ are $e^{(i\theta)}$ , $e^{(−i\theta)}$?</p>
</blockquote>

<p>Here, $\theta$ is the angle of rotation and $i$ is $\sqrt{-1}$ . </p>

<p>Edit 1:</p>

<p>I have been able to prove that there is one eigenvalue of 1, and other two are complex conjugates of each other. This comes from the fact that the rotation matrix can be written as an exponential $R = e^\omega $ where R is the rotation matrix and $\omega$ is a skew symmetric matrix. I also was able to determine that the axis of rotation is the eigenvector corresponding to eigenvalue 1. What I'm not being able to prove is that how are the other eigenvalues related to the angle of rotation. </p>
",<linear-algebra>
"<p>I saw the following question a book of mine:</p>

<blockquote>
  <p>Show that an $n\times n$ orthogonal matrix has $n(n-1)/2$ independent parameters.</p>
</blockquote>

<p>I have no idea what an <em>independent parameter</em> is. Could you explain it to me?</p>
",<linear-algebra>
"<p>List all diagonalizable $2\times 2$ matrices over the a field $F$ consisting of two elements $0$ and $1$.</p>

<p>I want to try and do this using C++, but perhaps this isn't the place to ask. I have an idea as to how I'd do it.</p>
",<linear-algebra>
"<p>I have a differential equation $$N'_x(x)=G(x)N(x)$$ where $N, G$ are $2\times2$ matrices depending on $x$, and $G$ satisfies $\sigma G+G\sigma=0$, $\sigma$ is one half of the pauli matrix, i.e. $$\sigma=\begin{pmatrix}\frac{1}{2}&amp;0\\
0&amp;\frac{-1}{2}\end{pmatrix}$$ My question is:</p>

<blockquote>
  <p>Would $N^{\ast}\sigma N$ then be independent of $x$? Why or why not?</p>
</blockquote>
",<linear-algebra>
"<p>I would like to determine if the following map $T$ is a linear transformation:</p>

<p>\begin{align*}
T: P_{2} &amp;\to P_{2}\\
A_{0} + A_{1}x + A_{2}x^{2} &amp;\mapsto A_{0} + A_{1}(x+1) + A_{2}(x+1)^{2}
\end{align*}</p>

<p>My attempt at solving:</p>

<p>\begin{align}
T(p + q) &amp;= p(x+1) + q(x+1)\\
&amp;= \left[A_{0} + A_{1}(x+1) + A_{2}(x+1)^2\right] + \left[b_{0} + b_{1}(x+1) + b_{2}(x+1)^2\right]\\
&amp;= \left(A_{0} + b_{0}\right) + \left(A_{1} + b_{1}\right)(x+1) + \left(A_{2} + b_{2}\right)(x+1)^2\\
&amp;= T(p) + T(q)
\end{align}</p>

<p>Is this right so far? If not, what am I doing wrong?</p>
",<linear-algebra>
"<p>The question: </p>

<blockquote>
  <p>Show that if $A$, $B$, and $A+B$ are invertible matrices with the
  same size, then: $$A(A^{-1}+B^{-1})B(A+B)^{-1} = I$$</p>
</blockquote>

<p>I began by multiplying the first $A$:</p>

<p>$I+AB^{-1}B(A+B)^{-1}=I$</p>

<p>and then</p>

<p>$I + A(A+B)^{-1} = I$</p>

<p>At this point I'm not sure what to do. Should I just assume $A(A+B)^{-1} = 0$, or does that not work to prove this?</p>
",<linear-algebra>
"<p>Say I have the following <em>second</em> order 7 x 7 system of equations:</p>

<ul>
<li>$x_1'' = 10(x_2- x_1- 1)$ </li>
<li>$x_2'' = 10(x_3- 2x_2+ x_1)$</li>
<li>$x_3'' = 10(x_4- 2x_3+ x_2)$</li>
<li>$x_4'' = 10(x_5- 2x_4+ x_3)$</li>
<li>$x_5'' = 10(x_6- 2x_5+ x_4)$</li>
<li>$x_6'' = 10(x_7- 2x_6+ x_5)$</li>
<li>$x_7'' = 10(x_6- x_7)$.</li>
</ul>

<p>How would I convert this second order 7 x 7 system into a <em>first</em> order 14 x 14 system using the additional equations $v_j = x'_j$, where $j = 1, 2, 3, ..., 7$?</p>
",<linear-algebra>
"<p><strong>Problem:</strong> Solve the following system in function of the parameter $b$:</p>

<p>\begin{align*} \begin{cases} -bx + 2y - (2+b^2)z + bu &amp;= -2 \\ x -2y + bz -u &amp;= 0 \\ x + (2b-4)y + (2-b)z + (b-1)u &amp;= 2 \\ x -2by -(3b+2)z + (4b-5)u &amp;= 2b-4 \end{cases} \end{align*}</p>

<p><strong>Attempt at solution:</strong> We write down the augmented matrix of this system, and then apply the operations: $R_1 \leftrightarrow R_2, R_2 \rightarrow R_2 + b R_1$. This gives us the matrix: \begin{align*} \left(\begin{array}{cccc|c} 1 &amp; -2 &amp; b &amp; -1 &amp; 0 \\ 0 &amp; (2-2b) &amp; (2+b^2) &amp; 0 &amp; -2b \\ 1 &amp; (2b-4) &amp; (2-b) &amp; (b-1) &amp; 2 \\ 1 &amp; -2b &amp; -(3b+2) &amp; (4b+5) &amp; (2b-4) \end{array}\right) \end{align*} After that, we do $R_3 \rightarrow R_3 - R_1$ and $R_4 \rightarrow R_4 - R_1$: \begin{align*}\left(\begin{array}{cccc|c} 1 &amp; -2 &amp; b &amp; -1 &amp; 0 \\ 0 &amp; (2-2b) &amp; (2+2b^2) &amp; 0 &amp; -2b \\ 0 &amp; (2b-2) &amp; (2-2b) &amp; b &amp; 2 \\ 0 &amp; (-2b+2) &amp; (-4b-2) &amp; (4b-4) &amp; (2b-4) \end{array}\right) \end{align*} Now I want a leading $1$ at the position $a_{22}$. </p>

<p><strong>Case 1.</strong> Let $b = 1$. Then our matrix reduces to \begin{align*} \left(\begin{array}{cccc|c} 1 &amp; -2 &amp; 1 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; 4 &amp; 0 &amp; -2 \\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 \\ 0 &amp; 0 &amp; -6 &amp; 1 &amp; -2 \end{array}\right) \end{align*} From the second row we see than that $u =2$. Substituting this in the last equation gives $z = - \frac{2}{3}$. But from the second row $z = - \frac{1}{2}$, which is a contradiction. Does this mean I can conclude the system has no solutions in this case?</p>

<p>Then if $b \neq 1$, should I just proceed with Gauss-elimination untill I hit another case distinction?</p>
",<linear-algebra>
"<p>I was reviewing my homework and it seems I overlooked something crucial while proving some ring has no Invariant Basis Number property. This is exercise VI.1.12 in Aluffi's <em>Algebra: Chapter 0</em></p>

<p>The setup: $V$ is a $k$-vector space and let $R = \mathrm{End}_{k}(V)$.</p>

<ol>
<li>Prove that $\mathrm{End}_{k}(V\oplus V) \cong R^4$ as an $R$-module</li>
<li>Prove that $R$ doesn't satisfy the IBN property if $V = k^{\oplus \mathbb N}$.</li>
</ol>

<p>For the first, I used to the fact that $V \oplus V$ is both the product and coproduct (in $k$-Vect) of $V$ with itself to get the isomorphism. What I just realized is I only showed that the two are isomorphic as groups not $R$-modules. So what would be the $R$-module structure on $\mathrm{End}_{k}(V \oplus V)$? </p>

<p>For the second, I used the fact that $V = k^{\oplus \mathbb N}$ implies $V \cong V \oplus V$ which in turn implies $R = \mathrm{End}_{k}(V) \cong \mathrm{End}_{k}(V \oplus V)$. Again, I just realized that I only showed the latter two are isomorphic as groups. </p>

<p>It may be obvious (and maybe why my professor let it pass?) but I can't come up with a good $R$-module structure that makes the two group isomorphisms $R$-linear.</p>

<p><strong>Edit:</strong></p>

<p>Explicitly, these are the isomorphisms I'm dealing with. Let $\pi_j, i_j$ be the natural projection/inclusion maps of the $j$-th factor resp. and $\psi: k^{\oplus \mathbb N} \oplus k^{\oplus \mathbb N} \to k^{\oplus \mathbb N}$ the isomorphism given by $\psi(e_i, 0)=e_{2i-1}$ and $\psi(0, e_i)=e_{2i}$.</p>

<p>Then the first isomorphism $\mathrm{End}_k(V \oplus V)\to R^4$ is given by $\varphi \mapsto (\pi_1\varphi i_1,\pi_2\varphi i_1,\pi_1\varphi i_2,\pi_2\varphi i_2)$</p>

<p>The second isomorphism $R \to \mathrm{End}_k(V \oplus V)$ is given by $\alpha \mapsto \psi^{-1} \alpha \psi$</p>

<p>The composition doesn't seem to be $R$-linear if I use the obvious $R$-module structure on $R$ and $R^4$.</p>
",<linear-algebra>
"<p>No matter what I do I can't seem to get this in the proper form.  Here is the system:</p>

<p>$$
\left\{
\begin{aligned}
3x+3y+12z&amp;=6 \\
3x_1+x_2-2x_3&amp;=2 \\
2x_1+2x_2+x_3&amp;=10 \\
-x+2y+8z&amp;=4
\end{aligned}
\right.
$$</p>

<p>Here is my base matrix:</p>

<p>$$ \left[
      \begin{array}{cccc|c}
        3&amp;3&amp;12&amp;6\\
        1&amp;1&amp;4&amp;2 \\
        2&amp;5&amp;20&amp;10 \\
        -1&amp;2&amp;8&amp;4
      \end{array}
    \right]$$</p>

<p>$\left(\frac{1}{3}\right)R_1-&gt;R_1$</p>

<p>$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        1&amp;1&amp;4&amp;2 \\
        2&amp;5&amp;20&amp;10 \\
        -1&amp;2&amp;8&amp;4
      \end{array}
    \right]$$</p>

<p>$(-2)R_1+R_3-&gt;R_3$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        1&amp;1&amp;4&amp;2 \\
        0&amp;3&amp;12&amp;6 \\
        -1&amp;2&amp;8&amp;4
      \end{array}
    \right]$$</p>

<p>$R_2&lt;-&gt;R_4$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        -1&amp;2&amp;8&amp;4 \\
        0&amp;3&amp;12&amp;6 \\
        1&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_2&lt;-&gt;R_3$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        0&amp;3&amp;12&amp;6 \\
        -1&amp;2&amp;8&amp;4 \\
        1&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_3+R_4-&gt;R_4$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        0&amp;3&amp;12&amp;6 \\
        -1&amp;2&amp;8&amp;4 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_2&lt;-&gt;R_3$, $R_1+R_2-&gt;R_2$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        0&amp;-1&amp;-4&amp;-2 \\
        0&amp;3&amp;12&amp;7 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$\left(\frac{1}{3}\right)R_3-&gt;R_3$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        0&amp;-1&amp;-4&amp;-2 \\
        0&amp;1&amp;4&amp;2 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_1&lt;-&gt;R_2$
$$ \left[
      \begin{array}{cccc|c}
        0&amp;-1&amp;-4&amp;-2\\
        1&amp;1&amp;4&amp;2 \\
        0&amp;1&amp;4&amp;2 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_1+R_2-&gt;R_2$
$$ \left[
      \begin{array}{cccc|c}
        0&amp;-1&amp;-4&amp;-2\\
        1&amp;0&amp;0&amp;0 \\
        0&amp;1&amp;4&amp;2 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_1&lt;-&gt;R_2$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;0&amp;0&amp;0 \\
        0&amp;-1&amp;-4&amp;-2 \\
        0&amp;1&amp;4&amp;2 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_2&lt;-&gt;R_3$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;0&amp;0&amp;0\\
        0&amp;1&amp;4&amp;2 \\
        0&amp;-1&amp;-4&amp;-2 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_2+R_3-&gt;R_3$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;0&amp;0&amp;0\\
        0&amp;1&amp;4&amp;2 \\
        0&amp;0&amp;0&amp;0 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>I stopped here because I felt like I was going to be going into a circle and I felt like I've done way too many steps.  Please help!</p>

<p>Sorry for any mistakes, it took me awhile to type this up.</p>
",<linear-algebra>
"<p>Apparently, my previous question didn't get no satisfactory answer, when I asked for two equations having a fixed value for each, <em>not necessarily linear</em>. As XenoGraff states, WolframAlpha does the task, but counts permutations of values among variables, and is thus impractical to test any two equations.</p>

<p>Actually, I ask, is it possible that there could be a system of X equations that can be solved for more than X variables, all having whole number values, considering that these X equations have an unique solution?</p>

<p>As Gerry Myerson states in the previous thread, there is the unproven conjecture that $x^{5}+y^{5}=N$ will have only one solution for $x,y$ for a given $N$, which can be modified to satisfy $x^{5}+y^{5}=x+y$ for only one set of values for $x,y$.</p>

<p>So... are there any such equations? What about differential equations (I don't understand them, anyway) and multivariates? And Diophantine equations?</p>
",<linear-algebra>
"<p>I've sort of gotten a grasp on the Chain rule with one variable.  If you hike up a mountain at 2 feet an hour, and the temperature decreases at 2 degrees per feet, the temperature would be decreasing for you at $2\times 2 = 4$ degrees per hour.</p>

<p>But I'm having a bit more trouble understanding the Chain Rule as applied to multiple variables.  Even the case of 2 dimensions </p>

<p>$$z = f(x,y),$$ </p>

<p>where $x = g(t)$ and $y = h(t)$, so</p>

<p>$$\frac{dz}{dt} = \frac{\partial z}{dx} \frac{dx}{dt} + \frac{\partial z}{dy} \frac{dy}{dt}.$$</p>

<p>Now, this is easy enough to <em>""calculate""</em> (and figure out what goes where).  My teacher taught me a neat tree-based graphical method for figuring out partial derivatives using chain rule.  All-in-all, it was rather hand-wavey.  However, I'm not sure exactly how this works, intuitively.</p>

<p>Why, intuitively, is the equation above true?  Why <strong>addition</strong>?  Why not multiplication, like the other chain rule?  Why are some multiplied and some added?</p>
",<linear-algebra>
"<p>I just came back from an intense linear algebra lecture which showed that linear transformations could be represented by transformation matrices; with more generalization, it was later shown that affine transformations (linear + translation) could be represented by matrix multiplication as well.</p>

<p>This got me to thinking about all those other transformations I've picked up over the past years I've been studying mathematics.  For example, polar transformations -- transforming <code>x</code> and <code>y</code> to two new variables <code>r</code> and <code>theta</code>.</p>

<p>If you mapped <code>r</code> to the <code>x</code> axis and <code>theta</code> to the <code>y</code> axis, you'd basically have a coordinate transformation.  A rather warped one, at that.</p>

<p>Is there a way to represent this using a transformation matrix?  I've tried fiddling around with the numbers but everything I've tried to work with has fallen apart quite embarrassingly.</p>

<p>More importantly, is there a way to, given a specific non-linear transformation, construct a transformation matrix from it?</p>
",<linear-algebra>
"<p>If you have the following matrix can $k$ be any number?</p>

<p>\begin{pmatrix}
  1 &amp; 0 &amp; 0 \\
  0 &amp; k &amp; 0 \\
  0 &amp; 0 &amp; 1
 \end{pmatrix}</p>

<p>So this is obviously an assignment question, but I couldn't find a concrete answer anywhere.</p>

<p>I would just liketo double check my reasoning with other people (long distance learning, so no other students to chat too)</p>

<p>I say no, because $k$ cannot be zero. To my understanding, an elementary matrix can only be created using a single row operation on an Identity matrix. I can't think of any operation that would create a row of zeros from an Identity matrix.</p>

<p>Is my assumption correct: $k$ can be any number except for zero.</p>
",<linear-algebra>
"<blockquote>
  <p>Define D:$\wp_{2}$($\mathbb{R}$) $\mapsto$$\wp_{2}$($\mathbb{R}$)
  by $D(p)(x) = p'(x)$ , Find the matrix of $D$ with respect to the basis
  $\{1, 1+x, 1+x+x^2 \}$ </p>
</blockquote>

<p>I was thinking this would be the matrix $\left(\begin{array}[t]{ccc}
0 &amp; 1 &amp; 2\\
0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0
\end{array}\right)$ by differentiating each of the terms in the basis, 
but i have a feeling this is wrong? </p>
",<linear-algebra>
"<p>I am trying to teach my self some linear algebra in preparation for a module in machine learning. I am using Gilbert Strang's text Introduction to Linear Algebra and am having some difficulties.</p>

<p>My specific question is: How is the last equation below an example of integration by parts? It seems to be missing $x(t)y(t)$ to me and I don't know where that has gone.</p>

<p>The book gives the following equations leading up to it:</p>

<p>$$x^Ty = (x,y) = \int_{-\infty}^\infty x(t)y(t)~dt$$ </p>

<p>From what I can tell, the above is saying that the inner product of the vectors $x$ and $y$ is equivalent or at least approximate to taking the integral of those two vectors as functions. Does the $(x,y)$ just mean inner product? I don't really understand what a vector of a function even is though.</p>

<p>$$(Ax)^Ty = x^T(A^Ty)$$</p>

<p>This associative rule for matrix-vector multiplication was given as a more rigorous view of what the transpose of $A$ actually is. It then partnered it with this:</p>

<p>$$\int_{-\infty}^\infty\frac{dx}{dt}y(t)~dt = \int_{-\infty}^\infty x(t)\left(-\frac{dy}{dt}\right)~dt$$</p>

<p>I can see how the two equations relate to eachother and that this suggests that $A^T$ is anti-symmetric. Gil mentions integration by parts here which I can definitely recognise but it doesn't seem complete, there should be an extra $x(t)y(t)$ surely?</p>
",<linear-algebra>
"<p>Suppose T: $\mathbb{R}^{2}$$\rightarrow\mathbb{R}$$^{2}$ is linear
and has matrix $\begin{pmatrix}4&amp;9\\1&amp;1\end{pmatrix}$ with </p>

<p>respect to the standard basis of $\mathbb{R}$$^{2}$. What is the
matrix of T with respect to the basis </p>

<p>$\beta$= {(1,-1), (-3,2)} ?.</p>

<p>How do we approach these sort of problems, commutative diagram? And if so how would it look?</p>
",<linear-algebra>
"<p>Let $$\mathcal{B}=\left \{\frac{1}{\sqrt{2\pi}},\frac{\cos x}{\sqrt{\pi}},\frac{\sin x}{\sqrt{\pi}},\frac{\cos 2x}{\sqrt{\pi}},\frac{\sin 2x}{\sqrt{\pi}},\dots\right \}$$.
This is an orthonormal basis of $L^2(a,a+2\pi)$ since its elements are orthonormal and $\overline{\operatorname{span}_{\mathbb{R}}B}=L^2(a,a+2\pi)$. 
Is it true that these vectors are also linearly independent?</p>
",<linear-algebra>
"<p>Hy i have a small problem. I need to prove that a transformation $$\mathbb{R}_{3}[ x ] \rightarrow \Bbb{R}^{3}$$ $$\phi (p):= [p(-1), p(0), p(1)]^T$$  is linear. 
The $$\mathbb{R}_{3}[ x ]$$ vector space is a degree of max 3.
Then i need to find the basis of the transformation kernel and basis of the image.</p>

<p>How would i do that ?</p>

<p>Thanks</p>
",<linear-algebra>
"<p>I have a problem understanding getting the KERNEL and IMAGE of a linear transformation. We have the following transformation given: 
$$ \mathbb{R}_{2}[ x ] \rightarrow \mathbb{R}_{2}[ x ] $$
$$ (\phi (p))(x) = (x p(x+1))' - 2p(x) $$</p>

<p>We first have to find its matrix in basis $$ \{ 1, x, x^2 \} $$
which I know how to get. The transformation matrix result is:</p>

<p>$$ 
\begin{bmatrix}
 -1&amp; 1&amp; 1\\ 
 0&amp;  0&amp; 4\\ 
 0&amp;  0&amp; 1
\end{bmatrix}
 $$</p>

<p>How do I get the KERNEL and the IMAGE from it ?</p>

<p>Would really appretiate an explanation, not just the result.</p>

<p>THANKS !</p>
",<linear-algebra>
"<p>In $\mathbb{R}$3 we declare  an inner product as follows: $\langle v,u \rangle \:=\:v^t\begin{pmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 2 &amp; 0 \\0 &amp; 0 &amp; 3\end{pmatrix}u$  </p>

<p>we have operator $f \colon V \to V$ , $f\begin{pmatrix}x \\y \\z\end{pmatrix}\:=\begin{pmatrix}1 &amp; 2 &amp; 3 \\4 &amp; 5 &amp; 6 \\7 &amp; 8 &amp; 9\end{pmatrix}\begin{pmatrix}x \\y \\z\end{pmatrix}$</p>

<p>The question is : calculate $f^*$.  </p>

<p>So far, as i know, i need to find orthonormal basis $B$, and find $\left[f\right]_B^B$, and after that just do transpose to $\left[f\right]_B^B$.<br>
 is That correct?  it's a question from  test that i had and i didn't know how to answer it so i forwarding this to you. tnx!</p>
",<linear-algebra>
"<p>In $\mathbb{R}^3$ we declare an inner product as follows: $\langle v,u \rangle \:=\:v^t\begin{pmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 2 &amp; 0 \\0 &amp; 0 &amp; 3\end{pmatrix}u$  </p>

<p>How can I find an orthonormal basis for this inner product space using the Gram–Schmidt process?</p>
",<linear-algebra>
"<blockquote>
  <p>Let matrix $A$ be
  $$\begin{bmatrix}
 -5&amp; 1&amp; 0&amp; 0\\
  a &amp;2&amp; 1 &amp;0\\
  0&amp; 1 &amp;1 &amp;1\\
  0 &amp;0&amp;1&amp; 0
\end{bmatrix}$$ 
  where $a$ is a constant between 1 and 3.</p>
  
  <p>Show that the dominant eigenvalue is real.</p>
</blockquote>

<p>Thanks a lot!!</p>
",<linear-algebra>
"<p>I have a system of linear equations as follows.</p>

<blockquote>
  <p>$$M(p) = 1+\frac{n-p-1}{n}M(n-1) + \frac{2}{n} N(p-1) + \frac{p-1}{n}M(p-1)$$
   $$N(p) = 1+\frac{n-p-1}{n}M(n-1) + \frac{p}{n}N(p-1)$$
   $$M(1) = 1+\frac{n-2}{n}M(n-1) + \frac{2}{n}N(0)$$
   $$N(0) = 1+\frac{n-1}{n}M(n-1)$$</p>
</blockquote>

<p>$M(p)$ is defined for $1 \leq p \leq n-1$.  $N(p)$ is defined for $0 \leq p \leq n-2$.  What is $M(n-1)$?</p>
",<linear-algebra>
"<blockquote>
  <p>Let $P \in \mathbb{R}_{n-1}[X]$ be a polynomial of degree $n-1 \geqslant 0$.</p>
  
  <ol>
  <li><p>Let $\mathbb{R}_{n-1}[X]$ be the vector space of polynomials with degree $\leqslant n-1$ over $\mathbb{R}$. Show that $(P(X),P(X+1),\ldots ,P(X+n-1))$ is a basis of $\mathbb{R}_{n-1}[X]$.</p></li>
  <li><p>Let $M_n = \begin{pmatrix} P(X) &amp; P(X+1) &amp; P(X+2) &amp; \ldots &amp; P(X+n) \\
				P(X+1) &amp; P(X+2) &amp; P(X+3) &amp; \ldots &amp; P(X+n+1) \\
				\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
				P(X+n) &amp; P(X+n+1) &amp; P(X+n+2) &amp; \ldots &amp; P(X+2n) \end{pmatrix}$.</p></li>
  </ol>
  
  <p>Show that $\det{M_n} = 0$ for every $X \in \mathbb{R}$.</p>
</blockquote>

<p>My thoughts on (1): $\mathbb{R}_{n-1}[X]$ is $n$-dimensional, because $(1,X, \ldots ,X^{n-1})$ is a basis of $\mathbb{R}_{n-1}[X]$. So it suffices to show that $(P(X),P(X+1),\ldots ,P(X+n-1))$  is a generating set/linearly independent. I tried proving it with induction and using the binomial theorem, but I am not getting anywhere.</p>

<p>My thoughts on (2): $\det{M_n} = 0$ implies that the columns are linearly dependent. (1) is probably useful here, but I don't even know how to start.</p>

<p>Any help is appreciated, thanks.</p>
",<linear-algebra>
"<p>There is a notation used in many sources (e.g. Wikipedia: <a href=""http://en.wikipedia.org/wiki/Exponential_family"" rel=""nofollow"">http://en.wikipedia.org/wiki/Exponential_family</a>) for the natural parameters of exponential family distributions which I do not understand, and I cannot find a description of.</p>

<p>With vector parameters and variables, the exponential family form has the dot product between the vector natural parameter, ${\boldsymbol\eta}({\boldsymbol\theta})$ and the vector sufficient statistic, ${\mathbf{T}}({\mathbf{x}})$, in the exponent. i.e. $e^{{\boldsymbol\eta}({\boldsymbol\theta})^{\top}{\mathbf{T}}({\mathbf{x}})}$. </p>

<p>However, many examples of these parameters for different distributions are vectors composed of matrices &amp; vectors. E.g. the multivariate Normal distribution has parameter $[\Sigma^{-1}\mu\space\space-\frac{1}{2}\Sigma^{-1}]$ and sufficient statistic $[\mathbf{x}\space\space\mathbf{xx^{\top}}]$.</p>

<p>So what are these ""vectors"" and moreover, how is the dot product between them defined? Does this notation have a name?</p>
",<linear-algebra>
"<p>How to prove the following?</p>

<p><strong>Lemma.</strong> Let $C=[A,A^{\star}]$. $A$ is normal iff $[A,C]=0$.</p>

<p>One direction is trivial. The other direction reduces to showing that $A^2 A^\star+A^\star A^2=0$ implies that $A$ is normal, but I don't see why that holds.</p>
",<linear-algebra>
"<p>I must construct this triangle:</p>

<p>Consider the triangle $ABC$. Take $D$ in the line of $BC$ such that $C$ is the mid point of $BD$ and take $Y$ in the line $AC$ such that the lines $AB$ and $BY$ are parallel. </p>

<p>I constructed the point $D$, but I don't see how I can construct the point $Y$ without this point being $A$. Does another possibility exists? I don't see how.</p>
",<linear-algebra>
"<p>I'm interested in the algorithm of LU decomposition in order to solve a LSE like $Ax=b$, where $A$ is a square matrix.</p>

<p>My question is: When I compute $PA=LU$ do I also need to interchange rows in $L$ whenever I interchange rows in $A$? It is clear, that I get $P$ by interchanging rows in $Id$ whenever I interchange rows in $A$, but is $L$ also affected?</p>

<p>Is it right that after that I need to solve $Lz = Pb$ and $Ux = z$?</p>
",<linear-algebra>
"<p><strong>I do not want the answer given to me, I just want assistance.</strong></p>

<p>Problem: <em>Marcus invests $750 in an account that pays 9.8% interest compounded annually. Write a function that describes the account balance, A, in terms of the number of years, t, that have passed.</em></p>

<p>I know that to find the account balance after one year: </p>

<p>A + (A * 0.098)
A + 0.098A (simplified)</p>

<p>But I don't know how to implement the time variable.</p>

<p>initial balance is $750
after one year 750 + (750*0.098) = 1348.5</p>

<p>after 2 years:
1348.5 + (1348.5*0.098) = 1480.653</p>
",<linear-algebra>
"<p>How to find inverse of an infinite lower triangular matrix all of whose diagonal entries are 1 and the entries of each column are given by coefficients of some power series rings?</p>
",<linear-algebra>
"<ol>
<li>How can one intuitively understand the definition of a bilinear map? Is there some way of looking at it geometrically? I found the following definition:</li>
</ol>

<p>Let $\mathit{A}$,$\mathit{B}$,$\mathit{C}$ be vector spaces. A map $f:\mathit{A}\times \mathit{B}\to C$ is said to be bilinear if for each fixed element $b\in \mathit{B}$, $f(.,b):\mathit{A}\to\mathit{C}$ is a linear map. Similarly, for each fixed element of $\mathit{A}$.</p>

<p>Matrix multiplication is an example of a bilinear map.Following my definition, I can prove that it is a bilinear map, but I don't understand the intuitive idea behind it. In my opinion, it is simply a linear map with one element fixed.</p>

<ol start=""2"">
<li>Is there some formal definition of a bilinear algorithm? I could find an explanation for it only in the context of matrix multiplication: <a href=""http://www.issac-symposium.org/2014/tutorials/ISSAC2014_tutorial_handout_LeGall.pdf"" rel=""nofollow"">http://www.issac-symposium.org/2014/tutorials/ISSAC2014_tutorial_handout_LeGall.pdf</a></li>
</ol>

<p>Kindly help me with these questions.
Thanks!</p>
",<linear-algebra>
"<p>Given that $A$ is a symmetric matrix, find $X$ that solves
$$\mathop {\min }\limits_X {\left\| {A - X{X^T}} \right\|_F}$$</p>

<p>I think that the problem can be solved using eigenvalue or singular value decomposition technics. $XX^T=A$ seems an obvious solution, but the problem is that $XX^T$ is positive semidefinite, while $A$ may not be, although they are both symmetric.</p>

<p>At this point I am thinking about taking eigenvalue decomposition of A, then replacing the negative eigenvalues in the middle diagonal matrix with 0-s (denote the resulting matrix by $\bar{A}$). But am having difficulties to understand why  $XX^T=\bar{A}$ gives the $X$ with minimal distance from A.</p>
",<linear-algebra>
"<p>I hope to solve this problem.</p>

<p>$$\min \quad \left\| CX \right\|_{1} $$
$$ \text{s.t.}\quad AX=b, X &gt;0 $$</p>

<p>where $C \in \mathbb{R}^{m \times m}$, $X \in \mathbb{R}^{m \times n}$, $A \in \mathbb{R}^{k \times m}$, $b \in \mathbb{R}^{k \times n}$. $C$ is known weight, $X$ is unknown matrix. My problem is how to calculate the proximal operator of $ \left\| CX \right\|_{1}$, I know, if without $C$ the proximal operator will be apply Shrinkage elementwise. </p>

<p>This problem will be easy if $x$ is a vector, we just need to solve a LP, but my $X$ is a matrix.</p>

<p>$$ \min \quad c^Tx $$ 
 $$ \text{s.t.}\quad Ax=b , x&gt;0 $$</p>

<hr>

<p>the overall problem I hope to solve is:
$$ \min \left\| CX \right\|_{1} + \lambda \left\| Y \right\|_{*} $$
$$ \text{s.t.}\quad AX+Y=b , X&gt;0 $$
Y has the same dimension with $b \in \mathbb{R}^{k \times n}$. X is known to be sparse.</p>
",<linear-algebra>
"<p>This is a question from a review package that is causing me some trouble.</p>

<p>Let $U,W$ be subspaces of a finite dimensional vector space. Show if $\dim(U+W) = 1+\dim(U \cap W)$, then $\{U+W,U\cap W\}=\{U,W\}$.</p>

<p>I know that for $\{U+W,U\cap W\}=\{U,W\}$ to be true, one of two cases must happen. Either $U\subseteq W$ or $W\subseteq U$, since $U \cap W$ must equal $U$ or $W$. However we can assume without loss of generality that either one is true. </p>

<p>I'm not sure how to show the implication (maybe through contraposition?). Any hints and help is greatly appreciated. </p>
",<linear-algebra>
"<p>In one book on differential equations and dynamical systems I read that if <strong>(1)</strong> $(A-\lambda I)^{k_j} \vec{v_j} = \vec{0}$ then <strong>(2)</strong> $(A-\lambda I)\vec{v_j} = V_j$ and $V_j\in \ker(A-\lambda I)^{k_j-1}$. But I don't see how (2) follows from (1). Can someone please explain?</p>
",<linear-algebra>
"<blockquote>
  <p>I've been given an $(n+1)\times(n+1)$ square matrix, which is written in the form of a block matrix with the following dimensions
  $$ \begin{bmatrix}
    (1\times1)       &amp; (n\times1)\\
    (n\times1)       &amp; (n\times n) 
\end{bmatrix} .$$
  I need to compute the determinant. </p>
</blockquote>

<p>I've tried to understand what is shown <a href=""https://en.wikipedia.org/wiki/Determinant#Block_matrices"" rel=""nofollow"">here</a> on how to solve this but I'm still confused. Can someone offer any insight as to how I would go about solving this? Also, is there any decomposition, factorization, etc I can take advantage of with the off diagonals being $(n\times 1)$ and $(1\times n)$? I feel like there's some simplification I can utilize. Thoughts?</p>
",<linear-algebra>
"<p>Let's say that we have linear subspaces $V$ and $W$ of $Y$.</p>

<p>What is the difference between the following sets:</p>

<ol>
<li>$V+W$ </li>
<li>$V\cup W$ </li>
<li>$V\oplus W$</li>
</ol>
",<linear-algebra>
"<p>Let $u = \left( \begin{matrix} 2 \\-5   \\1\end{matrix} \right)$</p>

<p><strong>Find an operator $T \in L(U) $ such that $T(u)=u $ and $T$ is self-adjoint.</strong> </p>

<p>I have to show that $T=T^*$ to have a self-adjoint operator T but I know how to start off.</p>

<p>I all I can think is $ T(u) = \left( \begin{matrix} 2 \\-5   \\1\end{matrix} \right)$</p>

<p>Any sort of help is appreciated! </p>

<p>Thanks!</p>
",<linear-algebra>
"<p>Given a % of change and the resulting value after the change, how do you calculate the original value?</p>

<p>For example, X increased by 50% = 150. In this case we can easily use guess and check to see that X = 100, but how do you calculate this given any percentage of change and any resulting value?</p>

<p>I though this formula should work, but it does not. As you can see my algebra is quite rusty, so maybe you can help.</p>

<pre><code>start = x
percentage of change = p
resulting value = r

x * (p / 100) + x = r
</code></pre>

<p>Which can be simplified to</p>

<pre><code>2(p / 100)x = r
</code></pre>

<p>which is equivalent to</p>

<pre><code>x = r / 2(p / 100)
</code></pre>

<p>But when I check my work, this is wrong:</p>

<pre><code>100 = 150 / 2(50 / 100)
100 = 150 / 2(.5)
100 = 150 / 1
100 = 150
</code></pre>
",<linear-algebra>
"<p>Suppose that A is an $n\times m$ matrix with $ n\neq m$.</p>

<p>Here's my reasoning.</p>

<p>Every nonpivot column corresponds to a free variable in the system Ax = 0. Each free
variable becomes a parameter, and each parameter is multiplied times a basis vector
of null(A). Therefore the number of nonpivot
columns equals nullity(A). Since rank(A) + nullity(A) = m, the nullity(A) must be greater than zero.</p>

<p>I'm not sure if I'm justified in stating the last sentence. Any suggestions or can you provide a different proof?</p>
",<linear-algebra>
"<p>Let Ax=b be a nonhomogenous system of linear equations with the unknown x ∈ |R^n. Assume that X1 ∈ |R^n and X2 ∈ |R^n are both solutions of the nonhomogenous system. Which ONE of the following statements must be true?</p>

<pre><code>a) x1 + x2 is a solution of Ax = 0
b) x1 - x2 is a solution of Ax = 0
c) x1 + x2 is a solution of Ax = b
d) x1 - x2 is a solution of Ax = b
</code></pre>

<p>I'm not really sure how to approach this question. My current approach is to make up a system of equations such that Ax = b is nonhomogenous.</p>

<p>So I have:</p>

<pre><code>5x - 2y = 1
8x - 3y = 2
</code></pre>

<p>The solutions of (x,y) are (1,2) for both equations.</p>

<pre><code>5(1) - 2(2) = 1
8(1) - 3(2) = 1
</code></pre>

<p>Not sure what to do next, if I plug in ( x , y ) for ( x1 , x2 ) in the answer choices, none of them seem to be consistent.</p>
",<linear-algebra>
"<p>Let $A$ be a non-negative irreducible $n\times n$ matrix. Then the function
$$f(t)=\rho(tA+(1-t)A^T)$$
is increasing on $[0,1/2]$, and is decreasing on $[1/2,1]$.</p>

<p>Here are the notations.</p>

<ol>
<li><p>$A$ is non-negative if any entry of $A$ is greater than or equal to $0$.</p></li>
<li><p>$A$ is irreducible if $A$ is not reducible; and $A$ is reducible if there exists a permutation matrix $P$ such that $$P^T AP=\begin{pmatrix}
B&amp;0\\
C&amp;D\end{pmatrix},$$ or equivalently, there exists a permutation $\sigma$ of $\{1,2,\cdots,n\}$ and a $1\leq k\leq n-1$ such that the sub-matrix of $A$ in rows $\sigma(1),\cdots,\sigma(k)$ and columns $\sigma(k+1),\cdots,\sigma(n)$ being $0$.</p></li>
<li><p>$A^T$ is the transpose of $A$.</p></li>
<li><p>$\rho(A)$ is the spectral radius of $A$, that is, the largest modulus of the eigenvalues of $A$.</p></li>
</ol>

<p>And now I have no idea on it. However, it is intuitively right. As there are more symmetry in the matrix, the spectral radius becomes larger.</p>
",<linear-algebra>
"<p>Let $A$ be a non-negative primitive matrix. Then $$\lim_{n\to\infty}\left[\frac{A}{\rho(A)}\right]^n=xy^T,$$
where $x, y$ are the Perron roots of $A$ and $A^T$ respectively, they satisfy $x^Ty=1$.</p>

<p>Here are the notations.</p>

<ol>
<li><p>$A$ is non-negative if any entry of $A$ is greater than or equal to $0$.</p></li>
<li><p>$A$ is primitive if $A$ is non-negative irreducible, and the number of eigenvalues of $A$ with modulus equal to $\rho(A)$ (the spectral radius of $A$) is $1$.</p></li>
<li><p>$A$ is irreducible if $A$ is not reducible; and $A$ is reducible if there exists a permutation matrix $P$ such that $$P^T AP=\begin{pmatrix}
B&amp;0\\
C&amp;D\end{pmatrix},$$ or equivalently, there exists a permutation $\sigma$ of $\{1,2,\cdots,n\}$ and a $1\leq k\leq n-1$ such that the sub-matrix of $A$ in rows $\sigma(1),\cdots,\sigma(k)$ and columns $\sigma(k+1),\cdots,\sigma(n)$ being $0$.</p></li>
<li><p>The Perron root $x$ of $A$ is an eigenvector $x$ corresponding to the eigenvalue $\rho(A)$, the entries of $x$ are positive.</p></li>
<li><p>$A^T$ is the transpose of $A$.</p></li>
</ol>

<p>It is easy to show that the limit exists. In fact, we could just use Jordan carnonical form to find there exists a invertible matrix $T$ such that 
$$T^{-1}AT=\begin{pmatrix}
\rho(A)&amp;0\\
0&amp;*\end{pmatrix},$$
and thus
$$
\lim_{n\to\infty}T^{-1}\left[\frac{A}{\rho(A)}\right]^nT
=\begin{pmatrix}
1&amp;0\\
0&amp;0\end{pmatrix}.$$
However, I could not prove that the limit if $xy^T$.</p>
",<linear-algebra>
"<p>I have some data points that define a curve and what i need to find is the slope of the lines definedby
line 1 = p1&amp;p2
line 2 = p1&amp;p3
line 3 = p1&amp;p4
.
.
.
line29= p1&amp;p30
line30= p2&amp;p3
line31= p2&amp;p4</p>

<p>linexxx = p29&amp;p30 being the last </p>

<p>and I as I go i need to determine if the lines above  <em>ONLY</em> have 2 points in common with the curve?
image below the blue line is valid the green isnt how can I tell this? </p>

<p><img src=""http://i.stack.imgur.com/Vw0k8.png"" alt=""enter image description here""></p>

<p>point 1     0.128250002 6.235036978 <br>
point 2     0.197718753 6.239911671 <br>
point 3     0.281734379 6.22376425  <br>
point 4     0.336656255 6.233513636 <br>
point 5     0.347343755 6.20761683  <br>
point 6     0.472625007 6.238083661 <br>
point 7     0.491625008 6.205484151 <br>
point 8     0.553968759 6.244786364 <br>
point 9     0.601765634 6.200609458 <br>
point 10    0.740703137 6.243263022 <br>
point 11    0.797703137 6.225287592 <br>
point 12    0.927140639 6.245091032 <br>
point 13    1.078546892 6.239911671 <br>
point 14    1.159890643 6.249661057 <br>
point 15    1.291703145 6.238997666 <br>
point 16    1.404812522 6.262457126 <br>
point 17    1.506937524 6.248747052 <br>
point 18    1.6057969   6.213405528 <br>
point 19    1.684765651 6.243263022 <br>
point 20    1.770859403 6.235341646 <br>
point 21    1.94037503  6.247833047 <br>
point 22    2.059125032 3.903410396 <br>
point 23    2.189453159 3.681916534 <br>
point 24    2.330468786 4.359194189 <br>
point 25    2.398453162 6.237169656 <br>
point 26    2.55728129  6.279213883 <br>
point 27    2.692656292 6.248747052 <br>
point 28    2.844359419 6.254840418 <br>
point 29    2.992203172 6.264589804 <br>
point 30    3.167062549 6.243263022 <br></p>
",<linear-algebra>
"<p>I've been researching for a while and trying to wrap my head around spanning of vector spaces completely (by visualizing them in R3) before moving on to Linear Independence, Basis' and anything else taught after subspaces. (had no calc 3 unfortunately =/)Based on what I'm reading, the span of a set of vectors is every possible linear combination of those vectors. After reading/looking at this figure:</p>

<p><img src=""http://i.stack.imgur.com/iMrLI.png"" alt=""enter image description here""></p>

<p>I think I understand it for two vectors in R3. It looks like any two arbitrary vectors (that arent scalar multiples of eachother) in R3 will span a never ending plane through the origin. </p>

<p>Does this mean that the span of any 3 arbitrary vectors in the vector space R3 will form a never ending 3d shape spanning all of R3? (as long as two of them arent scalar multiples of eachother (EDIT: or as long as they don't span R2?)</p>

<p>also, what if you have any random set of more than 3 vectors (that qualify as being in R3), is the span of that set also all of R3? (as long as the vectors dont end up being scalar multiples of eachother so that the set spans a line)(EDIT: or as long as they don't span R2?)</p>

<p>while I'm at it, does this mean that any 4 (or more?) vectors in R4 that arent multiples of any of the other 4 span all of R4? (EDIT:if not R2 or R3?)</p>

<p>if this is all true, I think I had a big epiphany and everything now makes sense to me..... such as being able to visualize linear dependent and linear independent.. I'm guessing that a set of vectors is L.D. if at least one vector can be written as a scalar multiple of another vector in the set. and LI means they are all unique, as in they have no scalar multiples of eachother, which also means that if the only solution to a set of 3 vectors in R3 is (0,0,0) or the homogeneous solution, or the trivial solution.... then that set spans R3? and the same applies to sets of vectors in R4, such as if there is 4 or 5 vectors satsifying R4 rules, and the only solution to their system of equations is homogeneous, they are LI, they span R4, and the 4x4 systems determinant is not equal to 0....</p>
",<linear-algebra>
"<p>I don't have a clue of what's going on. We haven't learn this in class so I need all the help possible. The more detailed of an explanation, the better. Thanks in advance. The only info I have is that this matrix is Orthogonal. Which means I know the answer, just don't know how to get it.</p>

<p>\begin{bmatrix}
       \cos\theta &amp; -\sin\theta \\
       \sin\theta &amp; \cos\theta 
     \end{bmatrix}</p>
",<linear-algebra>
"<p>Let $T:\mathbb{V} \rightarrow \mathbb{W}$ be an injective linear transformation and $S:\mathbb{W}\rightarrow \mathbb{V}$ be a surjective linear transformation with $\mathbb{V}$ and $\mathbb{W}$ finite-dimensional vector spaces.</p>

<p>How can I show that $S\circ T=id_{\mathbb{V}}$?</p>
",<linear-algebra>
"<p>I have this problem I'm working on, which I cannot entirely solve:</p>

<blockquote>
  <p>Let $V$ be a finite dimensional vectorspace over a field $K$ with a symmetric bilinear
  form $\langle \cdot, \cdot \rangle$. Define for every $v \in V$ the
  map $$ l_v : V \mapsto K: w \mapsto \langle v, w \rangle. $$ Consider
  now the map $$f: V \rightarrow V^{*}: v \mapsto l_v. $$ </p>
  
  <p>(i) Prove that $f$ is a linear map.</p>
  
  <p>(ii) Prove that $f$ is surjective if and only if $\langle \cdot, \cdot
 \rangle $ is non degenerate.</p>
</blockquote>

<p><strong>Attempt:</strong>
(i) Let $v, w \in V$ be vectors, and let $\lambda, \mu \in K$ be scalars. We need to prove that $$f(\lambda v + \mu w) = \lambda f(v) + \mu f(w). $$ This is equivalent to proving $$l_{\lambda v + \mu w} = \lambda l_v + \mu l_w. $$ Let $x \in V$ be another vector. Then the above equality is true since $$ l_{\lambda v + \mu w} (x) = \langle \lambda v + \mu w, x \rangle = \lambda \langle v, x \rangle + \mu \langle w, x \rangle = \lambda l_v(x) + \mu l_w(x). $$ This proves that $f$ is linear.</p>

<p>(ii) Suppose first that $f$ is surjective. To prove that $\langle \cdot, \cdot \rangle$ is non degenerate, we need to prove that $$ \forall w \in V: [ (\forall v \in V: \langle v, w \rangle = 0 ) \Rightarrow w = 0 ]. $$ So let $w \in V$ be arbitrary, and suppose that $\forall v \in V: \langle v, w \rangle = \langle w, v \rangle = 0$, since the bilinear form is symmetric. This means that $\forall v \in V : l_v(w) = l_w(v)= 0. $ </p>

<p>Now I'm not sure how to use the fact that $f$ is surjective to deduce from this that $w = 0$. I know that the linear functional $l_v \in V^{*}$. So since $f$ is surjective, I can take a $v \in V$ such that $f(v) = l_v$. But what can I conclude from this about $w$? </p>

<p>Help is appreciated.</p>
",<linear-algebra>
"<p>I'm trying to implement some determinant routines for some CUDA C++ code that I'm writing. The only issue is, my code is returning nan's and inf's! It turns out that it's my pivoting routine that's bad.</p>

<p>Right now, my overall approach has been ripped straight from rosetta code's C implementation (without those awful macros). But it seems like their pivoting routine isn't robust enough for my needs.</p>

<p>Right now, my pivoting routine will take a column, start at the row corresponding to the column index and then search for the column's largest element below the current row and only swap if a larger value is found.</p>

<p>This is far too naive in practice. Is there a more robust pivoting algorithm I can/should be using?</p>

<p>Here's a link to the source code test: <a href=""https://github.com/LeonineKing1199/cuda-stuff/blob/master/tests/matrix-tests.cu#L182"" rel=""nofollow"">https://github.com/LeonineKing1199/cuda-stuff/blob/master/tests/matrix-tests.cu#L182</a></p>

<p>If you look at that matrix, that's the properly permuted one. In my actual data,  the rows of the matrix can be in any order! I just needed a static test to make sure it would eventually work. So, what kind of pivoting algorithm would I need to help me produce that same matrix assuming the rows were permuted in any order?</p>

<p>Sorry if I haven't given enough information. My main matrix class is here: <a href=""https://github.com/LeonineKing1199/cuda-stuff/blob/master/include/math/matrix.hpp"" rel=""nofollow"">https://github.com/LeonineKing1199/cuda-stuff/blob/master/include/math/matrix.hpp</a></p>
",<linear-algebra>
"<blockquote>
  <p>Let $V$ be a vector space with dimension $n$ and let $T: V\rightarrow V$ satisfy $T^2=0$.</p>
  
  <p>(a) Prove Im$ T \subseteq$  Ker $T$ and $\dim($Ker$(T))\geq \frac{n}{2}$ (SOLVED BY ME)</p>
  
  <p>(b) Assume $n=3, T\neq 0$ Prove there exists a basis $B$ of $V$ such that $[T]_B=\begin{bmatrix}
0 &amp; 0 &amp; 1\\ 
 0&amp;0  &amp; 0\\ 
0 &amp;0 &amp; 0
\end{bmatrix}$</p>
</blockquote>

<p>My attempt at (b):</p>

<p>By (a) we know $\dim(\ker T)\geq \frac{3}{2}$, hence it's either 2 or 3. Assuming it's 3 we get $V=\ker T$ which contradicts $T\neq 0$. So $\dim(\ker T)=2$</p>

<p>Basically we can take a basis of $\ker T$, $(v_1,v_2)$. And we need to show there exists $v_3\in V$ such that $T(v_3)=v_1$ and $(v_1,v_2,v_3)$ is a basic of $V$ (ahm show theyr'e linearly independent). this is have I'm having trouble with.</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>$A\in \mathbb{C}^{n,n}$ is hermitian. Then:<br>
a. $A$ is congruent to some diagonal matrix $D\in\mathbb{R}^{n,n}$.<br>
b. if matrix $A$ is positively defined then all eigenvalues of $A$ are equal to $1$.<br>
c. if matrix $A$ is positively defined then $A$ is congruent to matrix $I_n$.  </p>

<p>b. not true, counterexample:<br>
$\left[ \begin{array}{ccc}
5 &amp; 0 \\
0 &amp; 3 
 \end{array} \right]$.  Of course this matrix is positively defined. Is also hermitian. However, eigenvalues are $5$ and $3$.  </p>

<p>c.  Yes, hermitian positively defined matrix $A$ has unique Cholesky decomposition: $A=LL^H$ where $L$ is lower triangular matrix with real positive (strictly) numbers on diagonal. Then $A=LI_nL^H$ and $L$ is nonsingular. Hence, c. is true.    </p>

<p>What about b, c?   Could you help me with a. ?</p>
",<linear-algebra>
"<p>I need to algebraically prove minimum for $\frac{1}{2}x^TAx-x^Tb$ using $r = A^{-1}x-b.$</p>

<p>I can write $x$ as $A(r+b)$ and whole expression as 
$$
\begin{align}
f(x) &amp;=\frac{1}{2}(r+b)^TA^3(r+b) - (r+b)^TAb \\
&amp;= \frac{1}{2}(Ar)^TA(Ar) - (Ar)^Tb \\
&amp; + \frac{1}{2}(Ab)^TA(Ab) - (Ab)^Tb \\
&amp; + (Ar)^TA^2b \\
\end{align}
$$</p>

<p>And i don't know how to proceed.</p>
",<linear-algebra>
"<p>I need some help here. It's hard to me to tackle this kind of question and I'm not used to write math proofs. I need to find values of $t$ that make $$\langle(x_1, x_2), (y_1, y_2)\rangle = x_1y_1 + tx_2y_2$$ an internal product in $\Bbb R^2$.</p>

<p>I have showed that for $3$ of the $4$ properties, t does not matter at all. But for the property that $$\langle u,u\rangle \gt 0, u \neq 0$$ I have $$x_1^2 +tx_2^2 \gt 0$$ and them $$t \gt -((x_1/x_2)^2)$$ The answer seems to be $t \gt 0$ and I'm lost in this. Could anyone give me the right direction to complete the proof?</p>
",<linear-algebra>
"<p><a href=""http://i.stack.imgur.com/NHbGu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NHbGu.png"" alt=""set of vectors, S""></a></p>

<p>Firstly, my apologies for attaching the question as an image. I was having a lot of problems typing it in MathJax.<br>
The question asks to show S is independent. From my understanding, I have to prove that:<br>
$$
\alpha{\begin{pmatrix}
 1\\
 1\\
 0\\
 \end{pmatrix}} +\beta{\begin{pmatrix}
 1\\
 0\\
 1\\
 \end{pmatrix}}= {\begin{pmatrix}
 0\\
 0\\
 0\\
 \end{pmatrix}}
$$if and only if $\alpha$ =$\beta$ = 0. I try to solve this using an augmented matrix, and reduce it to row-echelon form.
$$ \left[
    \begin{array}{cc|c}
      1&amp;1&amp;0\\
      1&amp;0&amp;0\\
      0&amp;1&amp;0
    \end{array}
\right] $$ $\underrightarrow{R2+R3}$
$$ \left[
    \begin{array}{cc|c}
      1&amp;1&amp;0\\
      1&amp;1&amp;0\\
      0&amp;1&amp;0
    \end{array}
\right] $$ $\underrightarrow{R3-R2}$
$$ \left[
    \begin{array}{cc|c}
      1&amp;1&amp;0\\
      1&amp;1&amp;0\\
      0&amp;0&amp;0
    \end{array}
\right] $$ But, 
$$ \left[
    \begin{array}{cc|c}
      0&amp;0&amp;0\\
    \end{array}
\right] $$ means that the system has infinitely many solutions. Therefore, $\alpha,\beta$ could have infinitely many solutions, and so the set $S$ is dependent.<br>
But the solution says that the set is independent because $\alpha,\beta = 0$. Could someone please assist me by pointing out where I have gone wrong? I would appreciate if you could stick to my method of solving (proving $\alpha,\beta=0$) because this is how I would like to approach it)</p>
",<linear-algebra>
"<p>I am doing some problems outside of class and have a couple of questions that I cannot figure out how to start. </p>

<ol>
<li>If $f$ and $g$ are independent polynomials and $h$ is a nonzero polynomial over $F$, show that $fh$ and $gh$ are independent.</li>
</ol>

<p>I think this is relatively intuitive, but cannot find a proof for it. </p>

<p>Since f and g are independent, that means they should each form a basis for the fields they are over (in terms of polynomials). Consequently, I believe multiplying by a nonzero polynomial is the equivalent of scaling it in the field. So, the only way they would be dependent is if $h=0$ or if $f=g=0$.</p>
",<linear-algebra>
"<p>Suppose I have a matrix $A$, not telling you what it looks like, and the set of eigenvalues associated with $A$ = $\{-1,-1,-1,4\}$</p>

<p>Suppose the geometric multiplicity of $-1$ is $2$, what would be the geometric multiplicity of $4$?</p>

<p>Possible answer could be $1$, $2$, since any more then our jordan form will blow up</p>

<p>Obviously here the algebraic multiplicity of $4$ is one. </p>

<p>Does it equal to the geometric multiplicity?</p>

<p>What is a condition to check when they are equal and how can I see that?</p>
",<linear-algebra>
"<p>a) Suppose $S = \{v_1, v_2, v_3, v_4, v_5\}$, where</p>

<p>$v_1 = \left(  
\begin{array}{c}
    1 \\
    -1 \\
    -1 \\
    2
  \end{array}
\right)$, 
$v_2 = \left(  
\begin{array}{c}
    1 \\
    -1 \\
    0 \\
    -1
  \end{array}
\right)$, 
$v_3 = \left(  
\begin{array}{c}
    5 \\
    -5 \\
    -2 \\
    1
  \end{array}
\right)$, 
$v_4 = \left(  
\begin{array}{c}
    1 \\
    -1 \\
    1 \\
    -4
  \end{array}
\right)$, 
$v_5 = \left(  
\begin{array}{c}
    0 \\
    0 \\
    3 \\
    -9
  \end{array}
\right)$,</p>

<p><strong>Without doing any row operations, explain why $S$ is a linearly dependent set</strong></p>

<p>I don't know how to start by just looking at it, all I can do is just Row Operation and see the leading columns then judge if it is linearly dependent or not. </p>

<p>Would someone please tell me how to judge if the set is a linearly dependent or independent set please?</p>

<p>Thank you.</p>
",<linear-algebra>
"<p>Solve the system: The last column is the vector b
$$
        \begin{bmatrix}
        1 &amp; 1 &amp; 4 &amp; -5 \\
        4 &amp; 3 &amp; -5 &amp; 8 \\
        \end{bmatrix}
$$</p>

<p>I reduced it down to</p>

<p>$$
        \begin{bmatrix}
        1 &amp; 0 &amp; -17 &amp; 23 \\
        0 &amp; 1 &amp; 21 &amp; -28 \\
        \end{bmatrix}
$$</p>

<p>Now I have to express it in terms of:</p>

<p>$$
        \begin{bmatrix}
        x1 \\
        x2 \\
        x3 \\
        \end{bmatrix}
$$</p>

<p>x3 is free and I believe the answer to be something along the lines of</p>

<p>$$
        \begin{bmatrix}
        1 &amp; 0\\
        0 &amp; 1\\
        ? &amp; ?\\
        \end{bmatrix}
$$</p>

<p>Not sure what the ? values are.</p>
",<linear-algebra>
"<p>I'm trying to write a Fortran subroutine to compute a QR factorization using the Householder method. To test my routine, I compute the factorization of the following matrix:
$$
A =
 \begin{pmatrix}
  12 &amp; -51 &amp; 4 \\
  6 &amp; 167 &amp; -68  \\
  -4 &amp; 24 &amp; -41 
 \end{pmatrix},
$$
which, if done correctly, will reduce to the following upper triangular matrix:
$$
R =
 \begin{pmatrix}
  14 &amp; 21 &amp; -14 \\
  0 &amp; 175 &amp; -70  \\
  0 &amp; 0 &amp; 35 
 \end{pmatrix}.
$$
However, the matrix I actually get is:
$$
R =
 \begin{pmatrix}
  -14 &amp; -21 &amp; 14 \\
  0 &amp; -175 &amp; 70  \\
  -0 &amp; 0 &amp; 35 
 \end{pmatrix},
$$
which looks almost correct, except for some strange sign changes. I've been staring at my subroutine all day trying to see where these sign changes are being introduced, but I can't identify the problem. </p>

<p>My algorithm is as follows:</p>

<p>$
for \:\: k \:=\: 1\: to\: n
$</p>

<p>$
\qquad x(k:m) = A(k:m,k)
$ </p>

<p>$
\qquad v(k:m) = \mathtt{sign}(x(k))||x(k:m)||_{2}e1 + x(k:m)
$</p>

<p>$
\qquad v(k:m) = v(k:m)/||v(k:m)||_{2}
$</p>

<p>$
\qquad A(k:m,k:n) = A(k:m,k:n) - 2vv^{\top}A(k:m,k:n)
$</p>

<p>To calculate the factor $2vv^{\top}A(k:m,k:n),$ I made another subroutine called outer_product to compute the outer product of $v$ with itself, i.e. $vv^{\top}$, and then matrix multiply the result into my submatrix $A(k:m,k:n)$. However, I'm not sure if this is legitimate - I suspect herein lies the problem.</p>

<p>I would really appreciate it if someone could glance at my code to see if there is any obvious reason for the incorrect sign changes: </p>

<pre><code>integer, parameter :: dp = selected_real_kind(15)

integer, intent(in) :: m, n
real(dp), dimension(m,n), intent(inout) :: A
real(dp), dimension(m,m), intent(out) :: Q

integer :: k
real(dp) :: two_norm
real(dp), dimension(m) :: x, e1
real(dp), dimension(m,n) :: v
real(dp), dimension(m,m) :: outprod_vv

v = 0.0_dp

do k=1,m
    Q(k,k) = 1.0_dp
end do

!Householder triangularization
do k=1,n

    e1(k) = 1.0_dp

    x(k:m) = A(k:m,k)
    v(k:m,k) = sign( sqrt(dot_product(x(k:m),x(k:m))), x(k) )* &amp;
        e1(k:m) + x(k:m)

    v(k:m,k) = v(k:m,k)/(sqrt(dot_product(v(k:m,k),v(k:m,k))))
    call outer_product(v(k:m,k), m-k+1, outprod_vv(k:m,k:m))

    A(k:m,k:n) = A(k:m,k:n) - &amp;
        2.0_dp*matmul(outprod_vv(k:m,k:m), A(k:m,k:n)) 

    !Form Q implicitly    
    Q(k:m,k:m) = Q(k:m,k:m) - 2.0_dp* &amp;
        matmul(outprod_vv(k:m,k:m), Q(k:m,k:m))

end do

Q = transpose(Q)
</code></pre>
",<linear-algebra>
"<p>What does it mean when someone says ""find a fundamental set of solutions for the system <strong>y'</strong> $=A$ <strong>y</strong>""?</p>

<p>That is, the system</p>

<p>$$ {\bf{y'}} =A {\bf{y}}. $$</p>
",<linear-algebra>
"<p>I've got a section in my textbook about non-parallel vectors, it says:</p>

<p>For two non-parallel vectors <strong>a</strong> and <strong>b</strong>, if $\lambda a + \mu b = \alpha a + \beta b$
then $\lambda  = \alpha $ and $\mu  = \beta $</p>

<hr>

<p>Okay I get that you can equate coefficients and solve for mu and lambda, but how are the two sides of the equation equal in the first place? How can you just equate two different vectors to each other like that? I'm just confused and i'm not entirely sure what about. I've tried googling but not much turns up.. I'd love it if someone could explain in basic terms what this equation is telling me.. (that non parallel vectors are equal?) as I've only just been introduced to this topic recently.. Thank you.</p>
",<linear-algebra>
"<p>The famous identity $\sin^2 x+\cos^2x =1$ can be written as follows:</p>

<blockquote>
  <p>The polynomials $P(x)=x^2$ and $Q(x)=1-x^2$ satisfy 
  $$P(\sin x)= Q(\cos x),\quad \text{for all }x\in\mathbb R$$</p>
</blockquote>

<p>What are other such pairs of polynomials. In other words, what is the sufficient and essential condition for two real polynomials $P(x)$ and $Q(x)$ to satisfy $P(\sin x)= Q(\cos x)$ for all $x$?</p>
",<linear-algebra>
"<p>This question is more general in the sense that I want to know how one finds a particular (say matrix) representation for any object. For the case of Grassmann numbers we have from <a href=""http://en.wikipedia.org/wiki/Grassmann_number#Matrix_representations"" rel=""nofollow"">Wikipedia the following representation</a>: </p>

<blockquote>
  <p>Grassmann numbers can always be represented by matrices. Consider, for example, the Grassmann algebra
  generated by two Grassmann numbers $\theta_1$ and
  $\theta_2$. These Grassmann numbers can be represented by
  4&times;4 matrices:</p>
  
  <p>$$\theta_1 = \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0\\ 1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp;
 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 1 &amp; 0\\ \end{bmatrix}\qquad \theta_2 =
 \begin{bmatrix} 0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0\\ 1&amp;0&amp;0&amp;0\\ 0&amp;-1&amp;0&amp;0\\
 \end{bmatrix}\qquad \theta_1\theta_2 = -\theta_2\theta_1 =
 \begin{bmatrix} 0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0\\ 1&amp;0&amp;0&amp;0\\ \end{bmatrix}.
 $$</p>
</blockquote>

<p>How do one find these matrices? Do you guess them or is there a procedure? What about finding differenct matrix-representations for <strong>Dirac $\gamma$-matrices? How do you find them?</strong></p>
",<linear-algebra>
"<p>I am currently working no a linear algebra question and do not understand how to solve it. The questions gives:</p>

<pre><code>Four corners of a cube are (0,0,0), (2,0,0), (0,4,0) and (0,0,10).
</code></pre>

<p>I am asked to find:</p>

<pre><code>Find the remaining 4 corners.
Find the coordinates of the center point of the cube.
</code></pre>

<p>Can someone help me on the right path to this question?</p>

<p>How would find the other 4 corners of the cube? I dont understand how to find the width of the cube.</p>

<p>Thank you</p>
",<linear-algebra>
"<p>For an uknown 3x3 matrix $A$ we know that $\operatorname{tr} A = 0$, $\det(A) = 1/4$ and we also know that two eigenvalues are the same. Proove that $4A^3 = -3A - I$. Problem says to use Vieta to find characteristic polynomial and Cayley-Hamilton after.</p>

<p>I get that $2L_1 + L_3 = 0$ and $L_1^2 = 1/4$ but i do know how to proceed.</p>

<p>Thanks in advance</p>
",<linear-algebra>
"<p>I was just wondering, for a dynamic system does the origin always have to be an attractor, saddle point, or repellor?</p>

<p>Also if a matrix isn't diagonalizable then does that mean the origin cannot be a repellor the matrix?</p>
",<linear-algebra>
"<p>Determine if $\vec b$ is a linear combination of $\vec a_1,\vec a_2,\vec a_3$.</p>

<p>$\vec a_1 = \left[\begin{array}{c}
1     \\
-2   \\
0    \\
\end{array}\right], \vec a_2 = \left[\begin{array}{c}
0    \\
1   \\
2    \\
\end{array}\right], \vec a_3=\left[\begin{array}{c}
5     \\
-6   \\
8    \\
\end{array}\right], \vec{b} = \left[\begin{array}{c}
2    \\
-1   \\
6    \\
\end{array}\right]$</p>

<p>Okay, so I made my constants $x_{1}, x_{2}, x_{3}$ for $\vec a_1, \vec a_2,\vec a_3$, respectively. I end up getting the following consistent system:
$$\left[\begin{array}{cccc}
1 &amp; 0 &amp; 5 &amp; 2    \\
0 &amp; 1 &amp; 4 &amp; 3   \\
0 &amp; 0 &amp; 0 &amp; 0    \\
\end{array}\right]$$
Which has the general solution: $$
\begin{cases}
x_{1} = 2 - 5x_{3} \\
x_{2} = 3 - 4x_{3} \\
x_{3} = \text{free}.
\end{cases}
$$
So $\vec b$ is equal to infinitely  many linear combinations of $\vec a_1,\vec a_2,\vec a_3$, right? Why does my book say that $\vec b$ is <em>not</em> a linear combination of these three vectors? Must the constants be a unique solution?</p>
",<linear-algebra>
"<p>It is clear that a reordering of the elements in a chosen basis for an n-dimensional vector space induces a permutation on n elements, and conversely such a permutation corresponds to a re-ordering of the basis.</p>

<p>I am wondering why the signature of a permutation is associated to the orientation induced by a basis. I understand that one can do this technically - but is there an intuitive way to understand why this is so? </p>

<p>That is, is there a way to understand  why the concept of orientation, as experienced in 1,2 and 3 dimensional Euclidean space, is formalized (and thereby generalized to abstract vector spaces of arbitrary finite dimension) using the signature? </p>
",<linear-algebra>
"<p>Suppose $A$ is a $2\times2$ matrix. How do I prove that, if $\det(A) &lt; 0$, then $A$ is a diagonalizable matrix over $\mathbb{R}$?</p>
",<linear-algebra>
"<p>I want to find the least squares solution to $\boldsymbol{Ax}=\boldsymbol{b}$ where $\boldsymbol{A}$ is a highly sparse square matrix.<br>
I found two methods that look like they might lead me to a solution: <a href=""http://en.wikipedia.org/wiki/QR_decomposition"" rel=""nofollow"">QR factorization</a>, and <a href=""http://en.wikipedia.org/wiki/Singular_value_decomposition#Pseudoinverse"" rel=""nofollow"">singular value decomposition</a>. Unfortunately, I haven't taken linear algebra yet, so I can't really understand most of what those pages are saying. I can calculate both in Matlab though, and it looks like the SVD gave me a smaller squared error. Why did that happen? How can I know which one I should be using in the future?</p>
",<linear-algebra>
"<p>I have a problem which is interesting: given a real matrix $A_{n\times n}$, when this matrix has a largest real eigenvalue which is strictly bigger than 1. If possible, can you give some conditions that can  guarantee this  statement? Equivalently, this statement says that the entropy of the subshift of finite type is strictly bigger than 0. </p>
",<linear-algebra>
"<blockquote>
  <p>Let $\mathbb{F}$ be an arbitrary field and $A\in M_{n\times n}(\mathbb{F})$ such that $$tr(A)=0$$
  Now show that there exists $P$,$Q$ $\in M_{n\times n}(\mathbb{F})$ such that $$A=PQ-QP$$</p>
</blockquote>

<p>It is so natural because we know that $tr(XY-YX)=0$ for all $X,Y$.</p>

<p>I know some proof by induction and canonical forms. Can somebody say another easy and elementary way.</p>
",<linear-algebra>
"<p>Let $A$ and $B$ be two real $n\times n$ matrices s.t. $AB=BA$. We now that $\det(A^2+B^2) \geq 0$. Is  the similar question true for $n$ matrices which commute with each other? If not, how do I generalize this question?</p>
",<linear-algebra>
"<blockquote>
  <p>Let $V$ be a vector space where dot product is defined. Then the following is true:
  $$\forall x, y\in V \quad \langle x,y\rangle^2  \leq \langle x,x \rangle\langle y,y \rangle$$
  <strong>Proof:</strong></p>
  
  <p>Consider the following linear combination: $z=\langle x,y\rangle x - \langle x,x\rangle y$. Let's find the dot product $\langle z,z\rangle \ge 0$: 
  $$\langle z,z\rangle = \langle \langle x,y\rangle x - \langle x,x\rangle y,\langle x,y\rangle x - \langle x,x\rangle y\rangle\\
\stackrel{?}= \langle x,x\rangle \left[ \langle x,x\rangle\langle y,y\rangle - \langle x,y\rangle^2\right] \ge0.
$$</p>
</blockquote>

<p>This is a proof from my linear algebra course book. I fail too see how the transition which I marked with $?$ was made. I will appreciate a clear explanation.</p>
",<linear-algebra>
"<p>Let $T,S: V\to V$ be 2 linear transformations, and $\ker(T)=\{0\}$.
I need to prove why $\ker(T\circ S)=\ker(S)$.<br>
I have no idea how to prove it.</p>
",<linear-algebra>
"<p>Let $A$ be a matrix so $A=A^2$.</p>

<p>I need to show that the eigenvalues of $A$ are only $1$ or $0$.</p>

<p>I tried some ways but none of them help.</p>
",<linear-algebra>
"<p>Let $x_{k+1} = Bx_k + c$
where $B$ is $n \times n$ matrix $c$ is a vector.</p>

<p>Assume $\|B\| \le \beta &lt;1$</p>

<p>$\|x_k - x_{k-1}\| \le \varepsilon$ for some $k$</p>

<p>Show that $\| x - x_k\| \le \dfrac{\beta\varepsilon}{1 - \beta}$</p>

<p>Thanks a lot...</p>
",<linear-algebra>
"<p>Naturally we can describe graphs via tables of ""yes there is an edge"" or ""no there is not"" between each pair of vertices, so the definition of an adjacency matrix is easily understood.  Thinking of these tables as <em>matrices</em>, however, adds structure - specifically, an interpretation as a linear operator.  Why do we look at them in this light?  Is it just for application - for example, efficiently obtaining a lot of data about a graph by computing its spectrum?  Or is there also an intuitive geometric (or algebraic) motivation behind the adjacency matrix?</p>

<p>For example, the $2$-path <img src=""http://i.stack.imgur.com/bSdoS.png"" alt=""2-path""> has adjacency matrix
 $$\mathcal{A}(P_2)=\left(\begin{array}{cc} 0 &amp; 1\\1 &amp; 0\end{array}\right)$$ which acts on a $2$-dimensional vector space by flipping the coordinates, $(x,y)\mapsto (y,x)$.  Can we somehow intuitively connect this action to the $2$-path?  What about for other simple graphs?</p>
",<linear-algebra>
"<p>Write vector 
u = $$\left[\begin{array}{ccc|c}2 \\10 \\1\end{array}\right]$$</p>

<p>as a linear combination of the vectors in S. Use elementary row operations on an augmented matrix to find the necessary coefficients. </p>

<p>S = {
$v1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , v2\left[\begin{matrix}4\\2\\1\end{matrix}\right],  
v2\left[\begin{matrix}5\\4\\1\end{matrix}\right]
$ }. If it is not possible, explain why?</p>

<hr>

<p>This is what i have so far: </p>

<p>S = {
$v1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , v2\left[\begin{matrix}4\\2\\1\end{matrix}\right],  
v3\left[\begin{matrix}5\\4\\1\end{matrix}\right].
v4\left[\begin{matrix}2\\10\\1\end{matrix}\right]
$ } 
<br><br>
$c1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , c2\left[\begin{matrix}4\\2\\1\end{matrix}\right],  
c3\left[\begin{matrix}5\\4\\1\end{matrix}\right].
c4\left[\begin{matrix}2\\10\\1\end{matrix}\right]
=\left[\begin{matrix}0\\0\\0\end{matrix}\right]$</p>

<p>$c1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , c2\left[\begin{matrix}4\\2\\1\end{matrix}\right],  
c3\left[\begin{matrix}5\\4\\1\end{matrix}\right].
c4\left[\begin{matrix}2\\10\\1\end{matrix}\right]$</p>

<p>$
\begin{bmatrix}
1 &amp; 4 &amp; 5 &amp; 2\\
2 &amp; 2 &amp; 4 &amp; 10\\
2 &amp; 1 &amp; 1 &amp; 1\\
\end{bmatrix}
$</p>

<p>Now i don't know how to do this. Help will greatly be appreciated.</p>

<p>Thanks</p>
",<linear-algebra>
"<p>I am stuck with the following linear algebra problem:</p>

<p>given a basis $ \{e_{1} ... e_{n}\} $, I can define products $ \{p_{1} ... p_{m} \} $ as linear combination of the basis and the products itself:</p>

<p>$$
p_{i} = \sum_{j=1}^m u_{ij} p_{j} + \sum_{j=1}^n a_{ij}e_{ij} 
$$</p>

<p>My goal is to represent each product only as a combination of the basis. </p>

<p>$$
p_{i} = \sum_{j=1}^n s_{ij} e_{j} 
$$</p>

<p>any hint for a solution?</p>
",<linear-algebra>
"<p>I have the following question :</p>

<p>Let $A_{n \times n}$ that implies : $A^2-2A+I=0$ </p>

<ul>
<li>Proof $1$ is an eigevalue of $A$</li>
</ul>

<p>I don't really know how to approach this this what I manage to do (its not much though):</p>

<ul>
<li>$A(A-2I)=-I$ </li>
</ul>

<p>We know that if $\lambda$ is an eigenvalue then $Ax=\lambda x$ $(x \neq 0)$</p>

<p>$$A(A-2I)=I$$
Can I say now that since $Ax=\lambda x$ and Let $x=(A-2I)$ but $x$ is vector, not a matrix.</p>

<p>I don't really understand what to do next.</p>

<p>Any help will be be dearly appreciated, Thanks.</p>
",<linear-algebra>
"<p>Let $A$ be a linear operator which acts on the vector space $V=\langle x_1,x_2, \ldots,x_n\rangle$. Suppose we know its eigenvalues - $\lambda_1, \lambda_2, \ldots, \lambda_n.$</p>

<p>Now consider the vector space $V^{(2)} \subset {\rm Sym}^2 V$ generated by elements $x_i x_j, i&lt;j,$ $\dim V^{(2)}=\binom{n}{2}.$ Let us extend the operator $A$ on $V^{(2)}$    by linearity and by $A(x_i x_j)=A(x_i)A(x_j)$. Denote the extension by $A^{(2)}$  <em>and suppose that $A^{(2)}$ is an injective endomorphism of $V^{(2)}$.</em></p>

<p><strong>Question.</strong> What is the eigenvalues of the $A^{(2)}?$ 
My first answer    was that the set of eigenvalues consists of elements  $\lambda_i \lambda_j, i&lt;j$ but simple examples with small $n$  show  that is wrong answer.</p>

<p><strong>Edit.</strong> We may assume  that $A$ is  a permutation of the basis vectors.</p>
",<linear-algebra>
"<p>Let's say I am using the $Ham(2,11)$ code - the hamming code for which r = 2 and q=11 (length of alphabet). Can I detect errors caused by the transposition of 2 letters? If not, why?</p>
",<linear-algebra>
"<p>Assuming I have a $[n,k,d]$ linear code, how can it be shown that $d \leq n-k+1$ ?</p>
",<linear-algebra>
"<p>Suppose $A$ is a $m \times n$ matrix and $V$ is a $m \times 1$ matrix with both $A$ and $V$ having rational entries and suppose the system $AX=V$ has a solution in $\mathbb{R}^n$. Then the equation has a solution with rational entries.</p>

<p>Is the above statement true or false?</p>
",<linear-algebra>
"<p>$A$ is invertible, but it does not say that $A$ is symmetric.
By $B^T$ I mean that $B$ is transposed.</p>
",<linear-algebra>
"<p>Not all matrix with positive eigenvalues is positive definite, i.e. $\mathbf{x}^\mathsf{T}A\mathbf{x}&gt;0$ for all non zero vector $\mathbf{x}$. For example consider matrix</p>

<p>$$A = \begin{bmatrix} 1 &amp; -3 \\ 0 &amp; 1 \end{bmatrix}.$$</p>

<p>How to prove that if we add symmetry into hypothesis then the assertion is true? That is, a symmetric matrix with positive eigenvalues is positive definite. </p>
",<linear-algebra>
"<p>If $A\in L(X)$ then prove that $A^{-1}$ is linear and invertible.</p>

<p><strong>Proof:</strong> Since $A$ is invertible then $A$ is injective and surjective. We know that $A^{-1}$ defines by $A^{-1}(Ax)=x$. </p>

<p><em>Remark:</em> Also we can prove that $A(A^{-1}x)=x$. Indeed, if $x\in X$ and $A$ is surjective then exists $y\in X$ such that $x=Ay$. Hence $A(A^{-1}x)=A(A^{-1}Ay)=Ay=x$.</p>

<p>$A^{-1}(x+y)=A^{-1}(Ax_0+Ay_0)=A^{-1}A(x_0+y_0)=x_0+y_0=A^{-1}x+A^{-1}y$ and<br>
$A^{-1}(\alpha x)=A^{-1}(\alpha Ax_0)=A^{-1}A(\alpha x_0)=\alpha x_0=\alpha A^{-1}x$. Hence $A^{-1}$ is linear operator.</p>

<p>We have to prove that $A^{-1}$ is invertible, i.e. $A^{-1}$ is injective and surjective. </p>

<p>If $A^{-1}x=A^{-1}y$ then $x=Ax_0$ and $y=Ay_0$ for some $x_0,y_0\in X$. Then $A^{-1}Ax_0=A^{-1}Ay_0$ $\Rightarrow$ $x_0=y_0$ $\Rightarrow$ $x=y$.</p>

<p>For any $x\in X$ exists $y\in X$ such that $x=A^{-1}y$. For example, we can take $y=Ax$.</p>

<p>Thus, $A^{-1}$ is invertible and $\exists$ $(A^{-1})^{-1}$. We have prove that $(A^{-1})^{-1}=A$. Let $A^{-1}=F$ and $F^{-1}=G$.</p>

<p>For any $x\in X$ we have: $Gx=F^{-1}x$ since $F$ is invertible $\Rightarrow$ surjective then $\exists $$y\in X$ such that $x=Fy.$ Hence $$Gx=F^{-1}x=F^{-1}Fy=y=Ax.$$ Thus $(A^{-1})^{-1}=A.$</p>

<p>Please can anyone check my solution? I would be very grateful for any answer.</p>
",<linear-algebra>
"<p>The following is a discussion on the following second differential equation</p>

<p>$$  \frac{dy^2}{dx} - y = 0 $$</p>

<p>So, let us introduce the following, convention and definition, represent the derivative operator by</p>

<p>$$ D = \frac{d}{dx} $$</p>

<p>So that our first equation can be represented as</p>

<p>$$ \left( D^2 - 1 \right) y(x) = 0 $$</p>

<p>My professor discussed on the idea, on ""factoring"" (The space on which the derivative acts?) the operators, his discussion lead to the apparent nonuniqueness of factoring as either</p>

<p>$$ (D - \tanh(x))(D + \tanh(x)) y = 0 \quad \textrm{and } \quad (D - 1)(D + 1) = 0 $$</p>

<p>My first question is that how is that  $ (D - \tanh(x))(D + \tanh(x))  ``="" (D^2 - 1) $ and how is unfoiled?, because I think is not commutative because</p>

<p>$$ f(x)\frac{d}{dx}y \neq \frac{d}{dx}f(x)y(x) $$</p>

<p>One is first derive something and multiply by $f$, and the other is derive the product.</p>

<p>Second, my professor move to the general</p>

<p>$$ y'' + a(x)y' + b(x) = 0 $$</p>

<p>or</p>

<p>$$ \big(D^2 + a(x)D + b(x)\big)y(x) = 0 $$</p>

<p>and he wants to factor the above as</p>

<p>$$ \big( D + A(x)\big)\big(D + B(x)\big)y(x) = 0 $$</p>

<p>So then he unfoils the above equation as:</p>

<p>$$ \big( D^2 + AD + AB + B' + BD \big)y = 0 $$</p>

<p>and he argues that the terms $B'$ and $BD$ comes from the fact that either we take the derivative of $B$ or take the derivative and multiply by B (Why that doesn't apply to the terms $ AD  $ and we should add $ A'$ ? </p>

<p>Also, do you happen to know where to find reference on solving ODEs by this approach? Thanks</p>
",<linear-algebra>
"<p>Find the jordan form of the matrix
$$A = \begin{pmatrix}
 1 &amp; 1 &amp; 2 &amp; 2\\
 1 &amp; -2 &amp; -1 &amp; -1\\
 -2 &amp; 1 &amp; -1 &amp; -1\\
1 &amp; 1 &amp; 2 &amp; 2 \\
\end{pmatrix}$$</p>

<p>Hint: check that the matrix A has a single eigenvalue, and $trace(A) = 0$.</p>

<p>How can I check that the matrix has a single eigenvalue without using the determinant?</p>

<p>That's what I managed to do so far:</p>

<p>Let $G$ the Jordan form of $A$, using the hint that there is a single eigenvalue:
$$trace(A) = 0\implies 0 = trace(A) = trace(G) = 4a \implies a = 0$$</p>

<p>meaning that theres a single eigenvalue which is zero.</p>

<p>Then the characteristic polynomial is $A^4$  and the minimal polynomial is $A^3$.</p>

<p>Then:
$G = diag\{J_3(0), J_1(0)\}$</p>

<p>But how do I prove that there is a single eigenvalue?</p>
",<linear-algebra>
"<p>What is  an example of  a  $C^{*}$  algebra with an idempotent $e$ such that $e$ is  not Murray-von Neumann equivalent to $e^{*}$?</p>
",<linear-algebra>
"<p>For any $i \in \{1,2,3\}$, let:</p>

<ul>
<li>$w_i \in [0,1]$ is an <em>unknown</em> number such that $\sum_{i \in \{1,2,3\}} w_i = 1$.</li>
<li>$t$ is a <em>known</em> number in $[0,1]$. Suppose that $t = 0.8$.</li>
<li>$f_i$ is also a <em>known</em> number in $[0,1]$. Suppose that $f_1 = 0.2$, $f_2=0.6$, and $f_3=0.5$.</li>
</ul>

<p>Which are related by the equation: $t = \sum_{i \in \{1,2,3\}} w_i \times f_i$. It is assumed that there is only one true solution, and the rest are not true (maybe close estimations).</p>

<p>The questions are: </p>

<ul>
<li>What are the possible values of $w_1, w_2, w_3$ that satisfy the equation above? </li>
<li>How to find them?</li>
<li>In case too many possible values exist, what is the best method in estimating the values of $w_1,w_2,w_3$?</li>
</ul>

<h2>A brute-force by Python</h2>

<p>By brute-forcing answers, I found these which give a priority to ensuring that the sum $w_1+w_2+w_3=1$ (without ensuring that $t=0.8$):</p>

<pre><code>w1, w2, w3, w1+w2+w3, t
0, 0.01, 0.99, 1.0, 0.501
0, 0.02, 0.98, 1.0, 0.502
0, 0.02, 0.99, 1.01, 0.507
0, 0.03, 0.98, 1.01, 0.508
0, 0.03, 0.99, 1.02, 0.513
0, 0.04, 0.98, 1.02, 0.514
0, 0.04, 0.99, 1.03, 0.519
0, 0.05, 0.98, 1.03, 0.52
0, 0.05, 0.99, 1.04, 0.525
0, 0.06, 0.98, 1.04, 0.526
0, 0.06, 0.99, 1.05, 0.531
0, 0.07, 0.98, 1.05, 0.532
0, 0.07, 0.99, 1.06, 0.537
0, 0.08, 0.98, 1.06, 0.538
0, 0.08, 0.99, 1.07, 0.543
0, 0.09, 0.98, 1.07, 0.544
0, 0.09, 0.99, 1.08, 0.549
0, 0.1, 0.98, 1.08, 0.55
0, 0.1, 0.99, 1.09, 0.555
0, 0.11, 0.98, 1.09, 0.556
</code></pre>

<p>There are solutions, but none of them correspond to a $t$ that is close enough to 0.8.</p>

<p>Here I gave a high priority to ensuring that $t=0.8$ (while ignoring the sum = 1):</p>

<pre><code>w1, w2, w3, w1+w2+w3, t
0, 0.51, 0.99, 1.5, 0.801
0, 0.55, 0.94, 1.49, 0.8
0, 0.6, 0.88, 1.48, 0.8
0, 0.65, 0.82, 1.47, 0.8
0, 0.7, 0.76, 1.46, 0.8
0, 0.75, 0.7, 1.45, 0.8
0, 0.8, 0.64, 1.44, 0.8
0, 0.85, 0.58, 1.43, 0.8
0, 0.9, 0.52, 1.42, 0.8
0, 0.95, 0.46, 1.41, 0.8
0.01, 0.53, 0.96, 1.5, 0.8
0.01, 0.58, 0.9, 1.49, 0.8
0.01, 0.63, 0.84, 1.48, 0.8
0.01, 0.68, 0.78, 1.47, 0.8
0.01, 0.73, 0.72, 1.46, 0.8
0.01, 0.78, 0.66, 1.45, 0.8
0.01, 0.83, 0.6, 1.44, 0.8
0.04, 0.82, 0.6, 1.46, 0.8
0.06, 0.83, 0.58, 1.47, 0.8
0.07, 0.81, 0.6, 1.48, 0.8
0.09, 0.82, 0.58, 1.49, 0.8
0.1, 0.8, 0.6, 1.5, 0.8
0.11, 0.83, 0.56, 1.5, 0.8
0.12, 0.81, 0.58, 1.51, 0.8
0.13, 0.54, 0.9, 1.57, 0.8
0.13, 0.59, 0.84, 1.56, 0.8
0.13, 0.64, 0.78, 1.55, 0.8
0.13, 0.69, 0.72, 1.54, 0.8
0.13, 0.74, 0.66, 1.53, 0.8
0.13, 0.79, 0.6, 1.52, 0.8
0.14, 0.52, 0.92, 1.58, 0.8
0.14, 0.57, 0.86, 1.57, 0.8
0.14, 0.62, 0.8, 1.56, 0.8
0.14, 0.67, 0.74, 1.55, 0.8
0.14, 0.72, 0.68, 1.54, 0.8
0.14, 0.77, 0.62, 1.53, 0.8
0.14, 0.82, 0.56, 1.52, 0.8
0.15, 0.5, 0.94, 1.59, 0.8
0.15, 0.55, 0.88, 1.58, 0.8
0.15, 0.6, 0.82, 1.57, 0.8
0.15, 0.65, 0.76, 1.56, 0.8
0.15, 0.7, 0.7, 1.55, 0.8
0.15, 0.75, 0.64, 1.54, 0.8
0.15, 0.8, 0.58, 1.53, 0.8
0.16, 0.53, 0.9, 1.59, 0.8
0.16, 0.58, 0.84, 1.58, 0.8
0.16, 0.63, 0.78, 1.57, 0.8
0.16, 0.68, 0.72, 1.56, 0.8
0.16, 0.73, 0.66, 1.55, 0.8
0.16, 0.78, 0.6, 1.54, 0.8
0.16, 0.83, 0.54, 1.53, 0.8
0.17, 0.51, 0.92, 1.6, 0.8
0.17, 0.56, 0.86, 1.59, 0.8
0.17, 0.61, 0.8, 1.58, 0.8
0.17, 0.66, 0.74, 1.57, 0.8
0.17, 0.71, 0.68, 1.56, 0.8
0.17, 0.76, 0.62, 1.55, 0.8
0.17, 0.81, 0.56, 1.54, 0.8
0.18, 0.54, 0.88, 1.6, 0.8
0.18, 0.59, 0.82, 1.59, 0.8
0.18, 0.64, 0.76, 1.58, 0.8
0.18, 0.69, 0.7, 1.57, 0.8
0.18, 0.74, 0.64, 1.56, 0.8
0.18, 0.79, 0.58, 1.55, 0.8
0.19, 0.52, 0.9, 1.61, 0.8
0.19, 0.57, 0.84, 1.6, 0.8
0.19, 0.62, 0.78, 1.59, 0.8
0.19, 0.67, 0.72, 1.58, 0.8
0.19, 0.72, 0.66, 1.57, 0.8
0.19, 0.77, 0.6, 1.56, 0.8
0.19, 0.82, 0.54, 1.55, 0.8
0.2, 0.5, 0.92, 1.62, 0.8
0.2, 0.55, 0.86, 1.61, 0.8
0.2, 0.6, 0.8, 1.6, 0.8
0.2, 0.65, 0.74, 1.59, 0.8
0.2, 0.7, 0.68, 1.58, 0.8
0.2, 0.75, 0.62, 1.57, 0.8
0.2, 0.8, 0.56, 1.56, 0.8
0.21, 0.53, 0.88, 1.62, 0.8
0.21, 0.58, 0.82, 1.61, 0.8
0.21, 0.63, 0.76, 1.6, 0.8
0.21, 0.68, 0.7, 1.59, 0.8
0.21, 0.73, 0.64, 1.58, 0.8
0.21, 0.78, 0.58, 1.57, 0.8
0.21, 0.83, 0.52, 1.56, 0.8
0.22, 0.51, 0.9, 1.63, 0.8
0.22, 0.56, 0.84, 1.62, 0.8
0.22, 0.61, 0.78, 1.61, 0.8
0.22, 0.66, 0.72, 1.6, 0.8
0.22, 0.71, 0.66, 1.59, 0.8
0.22, 0.76, 0.6, 1.58, 0.8
0.22, 0.81, 0.54, 1.57, 0.8
0.23, 0.54, 0.86, 1.63, 0.8
0.23, 0.59, 0.8, 1.62, 0.8
0.23, 0.64, 0.74, 1.61, 0.8
0.23, 0.69, 0.68, 1.6, 0.8
0.23, 0.74, 0.62, 1.59, 0.8
0.23, 0.79, 0.56, 1.58, 0.8
0.24, 0.52, 0.88, 1.64, 0.8
0.24, 0.57, 0.82, 1.63, 0.8
0.24, 0.62, 0.76, 1.62, 0.8
0.24, 0.67, 0.7, 1.61, 0.8
0.24, 0.72, 0.64, 1.6, 0.8
0.24, 0.77, 0.58, 1.59, 0.8
0.24, 0.82, 0.52, 1.58, 0.8
0.25, 0.5, 0.9, 1.65, 0.8
0.25, 0.55, 0.84, 1.64, 0.8
0.25, 0.6, 0.78, 1.63, 0.8
0.25, 0.65, 0.72, 1.62, 0.8
0.25, 0.7, 0.66, 1.61, 0.8
0.25, 0.75, 0.6, 1.6, 0.8
0.25, 0.8, 0.54, 1.59, 0.8
0.26, 0.53, 0.86, 1.65, 0.8
0.26, 0.58, 0.8, 1.64, 0.8
0.26, 0.63, 0.74, 1.63, 0.8
0.26, 0.68, 0.68, 1.62, 0.8
0.26, 0.73, 0.62, 1.61, 0.8
0.26, 0.78, 0.56, 1.6, 0.8
0.26, 0.83, 0.5, 1.59, 0.8
0.27, 0.51, 0.88, 1.66, 0.8
0.27, 0.56, 0.82, 1.65, 0.8
0.27, 0.61, 0.76, 1.64, 0.8
0.27, 0.66, 0.7, 1.63, 0.8
0.27, 0.71, 0.64, 1.62, 0.8
0.27, 0.76, 0.58, 1.61, 0.8
0.27, 0.81, 0.52, 1.6, 0.8
0.28, 0.54, 0.84, 1.66, 0.8
0.28, 0.59, 0.78, 1.65, 0.8
0.28, 0.64, 0.72, 1.64, 0.8
0.28, 0.69, 0.66, 1.63, 0.8
0.28, 0.74, 0.6, 1.62, 0.8
0.28, 0.79, 0.54, 1.61, 0.8
0.29, 0.52, 0.86, 1.67, 0.8
0.29, 0.57, 0.8, 1.66, 0.8
0.29, 0.62, 0.74, 1.65, 0.8
0.29, 0.67, 0.68, 1.64, 0.8
0.29, 0.72, 0.62, 1.63, 0.8
0.29, 0.77, 0.56, 1.62, 0.8
0.29, 0.82, 0.5, 1.61, 0.8
0.3, 0.5, 0.88, 1.68, 0.8
0.3, 0.55, 0.82, 1.67, 0.8
0.3, 0.6, 0.76, 1.66, 0.8
0.3, 0.65, 0.7, 1.65, 0.8
0.3, 0.7, 0.64, 1.64, 0.8
0.3, 0.75, 0.58, 1.63, 0.8
0.3, 0.8, 0.52, 1.62, 0.8
0.31, 0.53, 0.84, 1.68, 0.8
0.31, 0.58, 0.78, 1.67, 0.8
0.31, 0.63, 0.72, 1.66, 0.8
0.31, 0.68, 0.66, 1.65, 0.8
0.31, 0.73, 0.6, 1.64, 0.8
0.31, 0.78, 0.54, 1.63, 0.8
0.31, 0.83, 0.48, 1.62, 0.8
0.32, 0.51, 0.86, 1.69, 0.8
0.32, 0.56, 0.8, 1.68, 0.8
0.32, 0.61, 0.74, 1.67, 0.8
0.32, 0.66, 0.68, 1.66, 0.8
0.32, 0.71, 0.62, 1.65, 0.8
0.32, 0.76, 0.56, 1.64, 0.8
0.32, 0.81, 0.5, 1.63, 0.8
0.33, 0.54, 0.82, 1.69, 0.8
0.33, 0.59, 0.76, 1.68, 0.8
0.33, 0.64, 0.7, 1.67, 0.8
0.33, 0.69, 0.64, 1.66, 0.8
0.33, 0.74, 0.58, 1.65, 0.8
0.33, 0.79, 0.52, 1.64, 0.8
0.34, 0.52, 0.84, 1.7, 0.8
0.34, 0.57, 0.78, 1.69, 0.8
0.34, 0.62, 0.72, 1.68, 0.8
0.34, 0.67, 0.66, 1.67, 0.8
0.34, 0.72, 0.6, 1.66, 0.8
0.34, 0.77, 0.54, 1.65, 0.8
0.34, 0.82, 0.48, 1.64, 0.8
0.35, 0.5, 0.86, 1.71, 0.8
0.35, 0.55, 0.8, 1.7, 0.8
0.35, 0.6, 0.74, 1.69, 0.8
0.35, 0.65, 0.68, 1.68, 0.8
0.35, 0.7, 0.62, 1.67, 0.8
0.35, 0.75, 0.56, 1.66, 0.8
0.35, 0.8, 0.5, 1.65, 0.8
0.36, 0.53, 0.82, 1.71, 0.8
0.36, 0.58, 0.76, 1.7, 0.8
0.36, 0.63, 0.7, 1.69, 0.8
0.36, 0.68, 0.64, 1.68, 0.8
0.36, 0.73, 0.58, 1.67, 0.8
0.36, 0.78, 0.52, 1.66, 0.8
0.36, 0.83, 0.46, 1.65, 0.8
0.37, 0.51, 0.84, 1.72, 0.8
0.37, 0.56, 0.78, 1.71, 0.8
0.37, 0.61, 0.72, 1.7, 0.8
0.37, 0.66, 0.66, 1.69, 0.8
0.37, 0.71, 0.6, 1.68, 0.8
0.37, 0.76, 0.54, 1.67, 0.8
0.37, 0.81, 0.48, 1.66, 0.8
0.38, 0.54, 0.8, 1.72, 0.8
0.38, 0.59, 0.74, 1.71, 0.8
0.38, 0.64, 0.68, 1.7, 0.8
0.38, 0.69, 0.62, 1.69, 0.8
0.38, 0.74, 0.56, 1.68, 0.8
0.38, 0.79, 0.5, 1.67, 0.8
0.39, 0.52, 0.82, 1.73, 0.8
0.39, 0.57, 0.76, 1.72, 0.8
0.39, 0.62, 0.7, 1.71, 0.8
0.39, 0.67, 0.64, 1.7, 0.8
0.39, 0.72, 0.58, 1.69, 0.8
0.39, 0.77, 0.52, 1.68, 0.8
0.39, 0.82, 0.46, 1.67, 0.8
0.4, 0.5, 0.84, 1.74, 0.8
0.4, 0.55, 0.78, 1.73, 0.8
0.4, 0.6, 0.72, 1.72, 0.8
0.4, 0.65, 0.66, 1.71, 0.8
0.4, 0.7, 0.6, 1.7, 0.8
0.4, 0.75, 0.54, 1.69, 0.8
0.4, 0.8, 0.48, 1.68, 0.8
0.41, 0.53, 0.8, 1.74, 0.8
0.41, 0.58, 0.74, 1.73, 0.8
0.41, 0.63, 0.68, 1.72, 0.8
0.41, 0.68, 0.62, 1.71, 0.8
0.41, 0.73, 0.56, 1.7, 0.8
0.41, 0.78, 0.5, 1.69, 0.8
0.41, 0.83, 0.44, 1.68, 0.8
0.42, 0.51, 0.82, 1.75, 0.8
0.42, 0.56, 0.76, 1.74, 0.8
0.42, 0.61, 0.7, 1.73, 0.8
0.42, 0.66, 0.64, 1.72, 0.8
0.42, 0.71, 0.58, 1.71, 0.8
0.42, 0.76, 0.52, 1.7, 0.8
0.42, 0.81, 0.46, 1.69, 0.8
0.43, 0.54, 0.78, 1.75, 0.8
0.43, 0.59, 0.72, 1.74, 0.8
0.43, 0.64, 0.66, 1.73, 0.8
0.43, 0.69, 0.6, 1.72, 0.8
0.43, 0.74, 0.54, 1.71, 0.8
0.43, 0.79, 0.48, 1.7, 0.8
0.44, 0.52, 0.8, 1.76, 0.8
0.44, 0.57, 0.74, 1.75, 0.8
0.44, 0.62, 0.68, 1.74, 0.8
0.44, 0.67, 0.62, 1.73, 0.8
0.44, 0.72, 0.56, 1.72, 0.8
0.44, 0.77, 0.5, 1.71, 0.8
0.44, 0.82, 0.44, 1.7, 0.8
0.45, 0.5, 0.82, 1.77, 0.8
0.45, 0.55, 0.76, 1.76, 0.8
0.45, 0.6, 0.7, 1.75, 0.8
0.45, 0.65, 0.64, 1.74, 0.8
0.45, 0.7, 0.58, 1.73, 0.8
0.45, 0.75, 0.52, 1.72, 0.8
0.45, 0.8, 0.46, 1.71, 0.8
0.46, 0.53, 0.78, 1.77, 0.8
0.46, 0.58, 0.72, 1.76, 0.8
0.46, 0.63, 0.66, 1.75, 0.8
0.46, 0.68, 0.6, 1.74, 0.8
0.46, 0.73, 0.54, 1.73, 0.8
0.46, 0.78, 0.48, 1.72, 0.8
0.46, 0.83, 0.42, 1.71, 0.8
0.47, 0.51, 0.8, 1.78, 0.8
0.47, 0.56, 0.74, 1.77, 0.8
0.47, 0.61, 0.68, 1.76, 0.8
0.47, 0.66, 0.62, 1.75, 0.8
0.47, 0.71, 0.56, 1.74, 0.8
0.47, 0.76, 0.5, 1.73, 0.8
0.47, 0.81, 0.44, 1.72, 0.8
0.48, 0.54, 0.76, 1.78, 0.8
0.48, 0.59, 0.7, 1.77, 0.8
0.48, 0.64, 0.64, 1.76, 0.8
0.48, 0.69, 0.58, 1.75, 0.8
0.48, 0.74, 0.52, 1.74, 0.8
0.48, 0.79, 0.46, 1.73, 0.8
0.49, 0.52, 0.78, 1.79, 0.8
0.49, 0.57, 0.72, 1.78, 0.8
0.49, 0.62, 0.66, 1.77, 0.8
0.49, 0.67, 0.6, 1.76, 0.8
0.49, 0.72, 0.54, 1.75, 0.8
0.49, 0.77, 0.48, 1.74, 0.8
0.49, 0.82, 0.42, 1.73, 0.8
0.5, 0.5, 0.8, 1.8, 0.8
0.5, 0.55, 0.74, 1.79, 0.8
0.5, 0.6, 0.68, 1.78, 0.8
0.5, 0.65, 0.62, 1.77, 0.8
0.5, 0.7, 0.56, 1.76, 0.8
0.5, 0.75, 0.5, 1.75, 0.8
0.5, 0.8, 0.44, 1.74, 0.8
0.51, 0.53, 0.76, 1.8, 0.8
0.51, 0.58, 0.7, 1.79, 0.8
0.51, 0.63, 0.64, 1.78, 0.8
0.51, 0.68, 0.58, 1.77, 0.8
0.51, 0.73, 0.52, 1.76, 0.8
0.51, 0.78, 0.46, 1.75, 0.8
0.51, 0.83, 0.4, 1.74, 0.8
0.52, 0.51, 0.78, 1.81, 0.8
0.52, 0.56, 0.72, 1.8, 0.8
0.52, 0.61, 0.66, 1.79, 0.8
0.52, 0.66, 0.6, 1.78, 0.8
0.52, 0.71, 0.54, 1.77, 0.8
0.52, 0.76, 0.48, 1.76, 0.8
0.52, 0.81, 0.42, 1.75, 0.8
0.53, 0.54, 0.74, 1.81, 0.8
0.53, 0.59, 0.68, 1.8, 0.8
0.53, 0.64, 0.62, 1.79, 0.8
0.53, 0.69, 0.56, 1.78, 0.8
0.53, 0.74, 0.5, 1.77, 0.8
0.53, 0.79, 0.44, 1.76, 0.8
0.54, 0.52, 0.76, 1.82, 0.8
0.54, 0.57, 0.7, 1.81, 0.8
0.54, 0.62, 0.64, 1.8, 0.8
0.54, 0.67, 0.58, 1.79, 0.8
0.54, 0.72, 0.52, 1.78, 0.8
0.54, 0.77, 0.46, 1.77, 0.8
0.54, 0.82, 0.4, 1.76, 0.8
0.55, 0.5, 0.78, 1.83, 0.8
0.55, 0.55, 0.72, 1.82, 0.8
0.55, 0.6, 0.66, 1.81, 0.8
0.55, 0.65, 0.6, 1.8, 0.8
0.55, 0.7, 0.54, 1.79, 0.8
0.55, 0.75, 0.48, 1.78, 0.8
0.55, 0.8, 0.42, 1.77, 0.8
0.56, 0.53, 0.74, 1.83, 0.8
0.56, 0.58, 0.68, 1.82, 0.8
0.56, 0.63, 0.62, 1.81, 0.8
0.56, 0.68, 0.56, 1.8, 0.8
0.56, 0.73, 0.5, 1.79, 0.8
0.56, 0.78, 0.44, 1.78, 0.8
0.56, 0.83, 0.38, 1.77, 0.8
0.57, 0.51, 0.76, 1.84, 0.8
0.57, 0.56, 0.7, 1.83, 0.8
0.57, 0.61, 0.64, 1.82, 0.8
0.57, 0.66, 0.58, 1.81, 0.8
0.57, 0.71, 0.52, 1.8, 0.8
0.57, 0.76, 0.46, 1.79, 0.8
0.57, 0.81, 0.4, 1.78, 0.8
0.58, 0.54, 0.72, 1.84, 0.8
0.58, 0.59, 0.66, 1.83, 0.8
0.58, 0.64, 0.6, 1.82, 0.8
0.58, 0.69, 0.54, 1.81, 0.8
0.58, 0.74, 0.48, 1.8, 0.8
0.58, 0.79, 0.42, 1.79, 0.8
0.59, 0.52, 0.74, 1.85, 0.8
0.59, 0.57, 0.68, 1.84, 0.8
0.59, 0.62, 0.62, 1.83, 0.8
0.59, 0.67, 0.56, 1.82, 0.8
0.59, 0.72, 0.5, 1.81, 0.8
0.59, 0.77, 0.44, 1.8, 0.8
0.59, 0.82, 0.38, 1.79, 0.8
0.6, 0.5, 0.76, 1.86, 0.8
0.6, 0.55, 0.7, 1.85, 0.8
0.6, 0.6, 0.64, 1.84, 0.8
0.6, 0.65, 0.58, 1.83, 0.8
0.6, 0.7, 0.52, 1.82, 0.8
0.6, 0.75, 0.46, 1.81, 0.8
0.6, 0.8, 0.4, 1.8, 0.8
0.61, 0.53, 0.72, 1.86, 0.8
0.61, 0.58, 0.66, 1.85, 0.8
0.61, 0.63, 0.6, 1.84, 0.8
0.61, 0.68, 0.54, 1.83, 0.8
0.61, 0.73, 0.48, 1.82, 0.8
0.61, 0.78, 0.42, 1.81, 0.8
0.61, 0.83, 0.36, 1.8, 0.8
0.62, 0.51, 0.74, 1.87, 0.8
0.62, 0.56, 0.68, 1.86, 0.8
0.62, 0.61, 0.62, 1.85, 0.8
0.62, 0.66, 0.56, 1.84, 0.8
0.62, 0.71, 0.5, 1.83, 0.8
0.62, 0.76, 0.44, 1.82, 0.8
0.62, 0.81, 0.38, 1.81, 0.8
0.63, 0.54, 0.7, 1.87, 0.8
0.63, 0.59, 0.64, 1.86, 0.8
0.63, 0.64, 0.58, 1.85, 0.8
0.63, 0.69, 0.52, 1.84, 0.8
0.63, 0.74, 0.46, 1.83, 0.8
0.63, 0.79, 0.4, 1.82, 0.8
0.64, 0.52, 0.72, 1.88, 0.8
0.64, 0.57, 0.66, 1.87, 0.8
0.64, 0.62, 0.6, 1.86, 0.8
0.64, 0.67, 0.54, 1.85, 0.8
0.64, 0.72, 0.48, 1.84, 0.8
0.64, 0.77, 0.42, 1.83, 0.8
0.64, 0.82, 0.36, 1.82, 0.8
0.65, 0.5, 0.74, 1.89, 0.8
0.65, 0.55, 0.68, 1.88, 0.8
0.65, 0.6, 0.62, 1.87, 0.8
0.65, 0.65, 0.56, 1.86, 0.8
0.65, 0.7, 0.5, 1.85, 0.8
0.65, 0.75, 0.44, 1.84, 0.8
0.65, 0.8, 0.38, 1.83, 0.8
0.66, 0.53, 0.7, 1.89, 0.8
0.66, 0.58, 0.64, 1.88, 0.8
0.66, 0.63, 0.58, 1.87, 0.8
0.66, 0.68, 0.52, 1.86, 0.8
0.66, 0.73, 0.46, 1.85, 0.8
0.66, 0.78, 0.4, 1.84, 0.8
0.66, 0.83, 0.34, 1.83, 0.8
0.67, 0.51, 0.72, 1.9, 0.8
0.67, 0.56, 0.66, 1.89, 0.8
0.67, 0.61, 0.6, 1.88, 0.8
0.67, 0.66, 0.54, 1.87, 0.8
0.67, 0.71, 0.48, 1.86, 0.8
0.67, 0.76, 0.42, 1.85, 0.8
0.67, 0.81, 0.36, 1.84, 0.8
0.68, 0.54, 0.68, 1.9, 0.8
0.68, 0.59, 0.62, 1.89, 0.8
0.68, 0.64, 0.56, 1.88, 0.8
0.68, 0.69, 0.5, 1.87, 0.8
0.68, 0.74, 0.44, 1.86, 0.8
0.68, 0.79, 0.38, 1.85, 0.8
0.69, 0.52, 0.7, 1.91, 0.8
0.69, 0.57, 0.64, 1.9, 0.8
0.69, 0.62, 0.58, 1.89, 0.8
0.69, 0.67, 0.52, 1.88, 0.8
0.69, 0.72, 0.46, 1.87, 0.8
0.69, 0.77, 0.4, 1.86, 0.8
0.69, 0.82, 0.34, 1.85, 0.8
0.7, 0.5, 0.72, 1.92, 0.8
0.7, 0.55, 0.66, 1.91, 0.8
0.7, 0.6, 0.6, 1.9, 0.8
0.7, 0.65, 0.54, 1.89, 0.8
0.7, 0.7, 0.48, 1.88, 0.8
0.7, 0.75, 0.42, 1.87, 0.8
0.7, 0.8, 0.36, 1.86, 0.8
0.71, 0.53, 0.68, 1.92, 0.8
0.71, 0.58, 0.62, 1.91, 0.8
0.71, 0.63, 0.56, 1.9, 0.8
0.71, 0.68, 0.5, 1.89, 0.8
0.71, 0.73, 0.44, 1.88, 0.8
0.71, 0.78, 0.38, 1.87, 0.8
0.71, 0.83, 0.32, 1.86, 0.8
0.72, 0.51, 0.7, 1.93, 0.8
0.72, 0.56, 0.64, 1.92, 0.8
0.72, 0.61, 0.58, 1.91, 0.8
0.72, 0.66, 0.52, 1.9, 0.8
0.72, 0.71, 0.46, 1.89, 0.8
0.72, 0.76, 0.4, 1.88, 0.8
0.72, 0.81, 0.34, 1.87, 0.8
0.73, 0.54, 0.66, 1.93, 0.8
0.73, 0.59, 0.6, 1.92, 0.8
0.73, 0.64, 0.54, 1.91, 0.8
0.73, 0.69, 0.48, 1.9, 0.8
0.73, 0.74, 0.42, 1.89, 0.8
0.73, 0.79, 0.36, 1.88, 0.8
0.74, 0.52, 0.68, 1.94, 0.8
0.74, 0.57, 0.62, 1.93, 0.8
0.74, 0.62, 0.56, 1.92, 0.8
0.74, 0.67, 0.5, 1.91, 0.8
0.74, 0.72, 0.44, 1.9, 0.8
0.74, 0.77, 0.38, 1.89, 0.8
0.74, 0.82, 0.32, 1.88, 0.8
0.75, 0.5, 0.7, 1.95, 0.8
0.75, 0.55, 0.64, 1.94, 0.8
0.75, 0.6, 0.58, 1.93, 0.8
0.75, 0.65, 0.52, 1.92, 0.8
0.75, 0.7, 0.46, 1.91, 0.8
0.75, 0.75, 0.4, 1.9, 0.8
0.75, 0.8, 0.34, 1.89, 0.8
0.76, 0.53, 0.66, 1.95, 0.8
0.76, 0.58, 0.6, 1.94, 0.8
0.76, 0.63, 0.54, 1.93, 0.8
0.76, 0.68, 0.48, 1.92, 0.8
0.76, 0.73, 0.42, 1.91, 0.8
0.76, 0.78, 0.36, 1.9, 0.8
0.76, 0.83, 0.3, 1.89, 0.8
0.77, 0.51, 0.68, 1.96, 0.8
0.77, 0.56, 0.62, 1.95, 0.8
0.77, 0.61, 0.56, 1.94, 0.8
0.77, 0.66, 0.5, 1.93, 0.8
0.77, 0.71, 0.44, 1.92, 0.8
0.77, 0.76, 0.38, 1.91, 0.8
0.77, 0.81, 0.32, 1.9, 0.8
0.78, 0.54, 0.64, 1.96, 0.8
0.78, 0.59, 0.58, 1.95, 0.8
0.78, 0.64, 0.52, 1.94, 0.8
0.78, 0.69, 0.46, 1.93, 0.8
0.78, 0.74, 0.4, 1.92, 0.8
0.78, 0.79, 0.34, 1.91, 0.8
0.79, 0.52, 0.66, 1.97, 0.8
0.79, 0.57, 0.6, 1.96, 0.8
0.79, 0.62, 0.54, 1.95, 0.8
0.79, 0.67, 0.48, 1.94, 0.8
0.79, 0.72, 0.42, 1.93, 0.8
0.79, 0.77, 0.36, 1.92, 0.8
0.79, 0.82, 0.3, 1.91, 0.8
0.8, 0.5, 0.68, 1.98, 0.8
0.8, 0.55, 0.62, 1.97, 0.8
0.8, 0.6, 0.56, 1.96, 0.8
0.8, 0.65, 0.5, 1.95, 0.8
0.8, 0.7, 0.44, 1.94, 0.8
0.8, 0.75, 0.38, 1.93, 0.8
0.8, 0.8, 0.32, 1.92, 0.8
0.81, 0.53, 0.64, 1.98, 0.8
0.81, 0.58, 0.58, 1.97, 0.8
0.81, 0.63, 0.52, 1.96, 0.8
0.81, 0.68, 0.46, 1.95, 0.8
0.81, 0.73, 0.4, 1.94, 0.8
0.81, 0.78, 0.34, 1.93, 0.8
0.81, 0.83, 0.28, 1.92, 0.8
0.82, 0.51, 0.66, 1.99, 0.8
0.82, 0.56, 0.6, 1.98, 0.8
0.82, 0.61, 0.54, 1.97, 0.8
0.82, 0.66, 0.48, 1.96, 0.8
0.82, 0.71, 0.42, 1.95, 0.8
0.82, 0.76, 0.36, 1.94, 0.8
0.82, 0.81, 0.3, 1.93, 0.8
0.83, 0.54, 0.62, 1.99, 0.8
0.83, 0.59, 0.56, 1.98, 0.8
0.83, 0.64, 0.5, 1.97, 0.8
0.83, 0.69, 0.44, 1.96, 0.8
0.83, 0.74, 0.38, 1.95, 0.8
0.83, 0.79, 0.32, 1.94, 0.8
0.84, 0.52, 0.64, 2.0, 0.8
0.84, 0.57, 0.58, 1.99, 0.8
0.84, 0.62, 0.52, 1.98, 0.8
0.84, 0.67, 0.46, 1.97, 0.8
0.84, 0.72, 0.4, 1.96, 0.8
0.84, 0.77, 0.34, 1.95, 0.8
0.84, 0.82, 0.28, 1.94, 0.8
0.85, 0.5, 0.66, 2.01, 0.8
0.85, 0.55, 0.6, 2.0, 0.8
0.85, 0.6, 0.54, 1.99, 0.8
0.85, 0.65, 0.48, 1.98, 0.8
0.85, 0.7, 0.42, 1.97, 0.8
0.85, 0.75, 0.36, 1.96, 0.8
0.85, 0.8, 0.3, 1.95, 0.8
0.86, 0.53, 0.62, 2.01, 0.8
0.86, 0.58, 0.56, 2.0, 0.8
0.86, 0.63, 0.5, 1.99, 0.8
0.86, 0.68, 0.44, 1.98, 0.8
0.86, 0.73, 0.38, 1.97, 0.8
0.86, 0.78, 0.32, 1.96, 0.8
0.86, 0.83, 0.26, 1.95, 0.8
0.87, 0.51, 0.64, 2.02, 0.8
0.87, 0.56, 0.58, 2.01, 0.8
0.87, 0.61, 0.52, 2.0, 0.8
0.87, 0.66, 0.46, 1.99, 0.8
0.87, 0.71, 0.4, 1.98, 0.8
0.87, 0.76, 0.34, 1.97, 0.8
0.87, 0.81, 0.28, 1.96, 0.8
0.88, 0.54, 0.6, 2.02, 0.8
0.88, 0.59, 0.54, 2.01, 0.8
0.88, 0.64, 0.48, 2.0, 0.8
0.88, 0.69, 0.42, 1.99, 0.8
0.88, 0.74, 0.36, 1.98, 0.8
0.88, 0.79, 0.3, 1.97, 0.8
0.89, 0.52, 0.62, 2.03, 0.8
0.89, 0.57, 0.56, 2.02, 0.8
0.89, 0.62, 0.5, 2.01, 0.8
0.89, 0.67, 0.44, 2.0, 0.8
0.89, 0.72, 0.38, 1.99, 0.8
0.89, 0.77, 0.32, 1.98, 0.8
0.89, 0.82, 0.26, 1.97, 0.8
0.9, 0.5, 0.64, 2.04, 0.8
0.9, 0.55, 0.58, 2.03, 0.8
0.9, 0.6, 0.52, 2.02, 0.8
0.9, 0.65, 0.46, 2.01, 0.8
0.9, 0.7, 0.4, 2.0, 0.8
0.9, 0.75, 0.34, 1.99, 0.8
0.9, 0.8, 0.28, 1.98, 0.8
0.91, 0.53, 0.6, 2.04, 0.8
0.91, 0.58, 0.54, 2.03, 0.8
0.91, 0.63, 0.48, 2.02, 0.8
0.91, 0.68, 0.42, 2.01, 0.8
0.91, 0.73, 0.36, 2.0, 0.8
0.91, 0.78, 0.3, 1.99, 0.8
0.91, 0.83, 0.24, 1.98, 0.8
0.92, 0.51, 0.62, 2.05, 0.8
0.92, 0.56, 0.56, 2.04, 0.8
0.92, 0.61, 0.5, 2.03, 0.8
0.92, 0.66, 0.44, 2.02, 0.8
0.92, 0.71, 0.38, 2.01, 0.8
0.92, 0.76, 0.32, 2.0, 0.8
0.92, 0.81, 0.26, 1.99, 0.8
0.93, 0.54, 0.58, 2.05, 0.8
0.93, 0.59, 0.52, 2.04, 0.8
0.93, 0.64, 0.46, 2.03, 0.8
0.93, 0.69, 0.4, 2.02, 0.8
0.93, 0.74, 0.34, 2.01, 0.8
0.93, 0.79, 0.28, 2.0, 0.8
0.94, 0.52, 0.6, 2.06, 0.8
0.94, 0.57, 0.54, 2.05, 0.8
0.94, 0.62, 0.48, 2.04, 0.8
0.94, 0.67, 0.42, 2.03, 0.8
0.94, 0.72, 0.36, 2.02, 0.8
0.94, 0.77, 0.3, 2.01, 0.8
0.94, 0.82, 0.24, 2.0, 0.8
0.95, 0.5, 0.62, 2.07, 0.8
0.95, 0.55, 0.56, 2.06, 0.8
0.95, 0.6, 0.5, 2.05, 0.8
0.95, 0.65, 0.44, 2.04, 0.8
0.95, 0.7, 0.38, 2.03, 0.8
0.95, 0.75, 0.32, 2.02, 0.8
0.95, 0.8, 0.26, 2.01, 0.8
0.96, 0.53, 0.58, 2.07, 0.8
0.96, 0.58, 0.52, 2.06, 0.8
0.96, 0.63, 0.46, 2.05, 0.8
0.96, 0.68, 0.4, 2.04, 0.8
0.96, 0.73, 0.34, 2.03, 0.8
0.96, 0.78, 0.28, 2.02, 0.8
0.96, 0.83, 0.22, 2.01, 0.8
0.97, 0.51, 0.6, 2.08, 0.8
0.97, 0.56, 0.54, 2.07, 0.8
0.97, 0.61, 0.48, 2.06, 0.8
0.97, 0.66, 0.42, 2.05, 0.8
0.97, 0.71, 0.36, 2.04, 0.8
0.97, 0.76, 0.3, 2.03, 0.8
0.97, 0.81, 0.24, 2.02, 0.8
0.98, 0.54, 0.56, 2.08, 0.8
0.98, 0.59, 0.5, 2.07, 0.8
0.98, 0.64, 0.44, 2.06, 0.8
0.98, 0.69, 0.38, 2.05, 0.8
0.98, 0.74, 0.32, 2.04, 0.8
0.98, 0.79, 0.26, 2.03, 0.8
0.99, 0.52, 0.58, 2.09, 0.8
0.99, 0.57, 0.52, 2.08, 0.8
0.99, 0.62, 0.46, 2.07, 0.8
0.99, 0.67, 0.4, 2.06, 0.8
0.99, 0.72, 0.34, 2.05, 0.8
0.99, 0.77, 0.28, 2.04, 0.8
0.99, 0.82, 0.22, 2.03, 0.8
</code></pre>

<p>It seems that the closest solution is:  $w_1=0$, $w_2=0.9$, $w_3=0.52$, which gives the sum $w_1+w_2+w_3=1.42$, and maintains $t=0.8$.</p>

<p>Among all of the runs, I found this: $w_1=0$, $w_2=0.29$, $w_3=0.99$, $w_1+w_2+w_3=1.28$, $t=0.669$ --- is this a good compromise? (just an example).</p>

<p>Is there a mathematical method in identifying a solution that generally minimizes the error $((w_1+w_2+w_3) - 1)^2 + (t-0.8)^2$. Feel free to propose an alternative error metric.</p>

<h2>A preliminary guess of mine:</h2>

<p>How about finding $w_1,w_2,w_3$ that minimize $\Big(\big((w_1+w_2+w_3) - 1\big)^2 + (t-0.8)^2\Big)$?</p>

<p>Is there a way to do this using calculus?</p>
",<linear-algebra>
"<p>I have a cyclotomic field $\mathbb{Q}(\zeta_3)$, and want to know how I can find a minimal polynomial of  $\zeta_{10}$, and $\zeta_{12}$. I have determined that both the polynomials should be of degree 2. I am trying to use a basis argument for both of them. That is: </p>

<p>$(x+m\zeta_3)(x+n\zeta_3)=0$; here $x=\zeta_{10}$</p>

<p>$(y+p\zeta_3)(y+q\zeta_3)=0$; here $y=\zeta_{12}$</p>

<p>and trying to find some explicit restrictions for m,n and p,q. But I am having trouble doing that.</p>
",<linear-algebra>
"<p>Suppose I have a vector field $F(x)=Ax$ where $A$ is a matrix. How can I express $Sx$ without $A$ (use $F$ instead)? Here $S=\dfrac{A+A^T}2$ is symmetric part of $A$.</p>
",<linear-algebra>
"<p>Find all matrices similar <em>only</em> to themselves, i.e., $PTP^{-1}=T$ for any invertible $P$.</p>

<p>My attempt: $PT = TP$.</p>

<p>Am I going about this correctly? If so, how do I find all matrices that are commutative (where $P$ is invertible)?</p>
",<linear-algebra>
"<p>Let $S$ and $W$ be subsets of a vector space $V$. Show that if $S$ is a subset of $W$, then $\mathrm{span}(S)$ is a subspace of $\mathrm{span}(W)$.</p>

<p>Ok I'm finally understanding what each of these things mean.. but I'm running out of ideas on how to actually show it without using arbitrary vector space examples like Rn. Since sets and subspaces are kind of new its hard to figure out how to write correct proofs with them. </p>

<p>I can see how if $V = \mathbb{R}^3$ and $S = \{(1,1,1)\}$ and $W = \{(1,1,1), (1,1,2)\}$ then $\mathrm{span}(S) = R1(line), span(W) = R2(plane)$ and that R1 is a subspace of R2... im just having trouble showing this officially when all vector spaces and sets are completely in general....</p>

<p>I could get as far as writing out $S=(u_1,u_2,\dots,u_n)$, $u_i \in V$ and same with $W$, but I can't really figure out what else to do in this sort of proof.</p>
",<linear-algebra>
"<p>I know that if $L$ is a linear transformation from $V$ to $W$ where $V,W$ are finite dimensional, then we can conclude that the dimension of image (rank) of $L$ is same as that of its transpose, i.e., $L^t$.</p>

<p>But what happens when: $\dim V,\dim W=\infty$, or just one of them has infinite dimension? If there is any difference in the above statement, why such difference arises?</p>
",<linear-algebra>
"<p>I had a question on some h/w that asked if $row(A)=col(A)$ then $A = A^t$.</p>

<p>I answered false and found somewhere that if $A^t = - A$ then $row(A) = col(A)$</p>

<p>does this go the other way as well? </p>
",<linear-algebra>
"<p>Given two vectors $\vec{u}$, $\vec{v}$ indexed by $2^X$ for some finite set $X$, define $\vec{u} \star \vec{v}$ as the vector of similar type whose dimension indexed by $S \subseteq X$ is:
$$\sum_{\begin{array}{c}S = A \cup B\\[-2em]A\cap B = \emptyset\end{array}} u_A v_B\enspace.$$
<strong>Question:</strong> Does this operation have a name?  Am I seeing that from the wrong point of view?</p>

<p>As a concrete example, if $X = {1, 2}$, $\vec{u} = (x, x_1, x_2, x_{12})$ and $\vec{v} = (y, y_1, y_2, y_{12})$, where the components are, in order, indexed by $\emptyset, \{1\}, \{2\}, \{1, 2\}$, then:
$$\vec{u} \star \vec{v} = (xy,\quad x_1y + xy_1, \quad x_2y + xy_2, \quad x_{12}y + x_1y_2 + x_2y_1 + xy_{12})\enspace.$$</p>
",<linear-algebra>
"<p>Here's the entire question: Let $A$ be an 8 x 5 matrix of rank 3, and let $b$ be a nonzero vector in $N(A^T)$.</p>

<p><strong>a) Show that the system $Ax = b$ must be inconsistent.</strong>
Gonna take a wild stab at this one... If the rank is 3, that means the dimension of the column space is 3. But $A$ has 5 columns, so they are not all linearly independent and therefore $Ax = b$ is inconsistent.</p>

<p><strong>b) How many least squares solutions will the system $Ax = b$ have? Explain.</strong></p>

<p>On previous problems, I found the best least squares linear fit, where the approximation of $x$ was a vector that contained sometimes regular numbers, and sometimes variables. Does this mean that there must be either 1 linear solution or infinite (because you can always find an approximation)? In the example that apparently had an infinite number of least squares solutions, it appeared that one row of $A^TA$ was a constant multiple of another row, leading to a row of zeros in reduced row echelon form. From this problem I know that $A^TA$ is a 5x5 matrix, but I don't think I can prove that any rows are a scalar multiple of other rows, so I'm guessing I have to use some other means of figuring this out.</p>

<p>Sorry if I sound like I have no idea what I'm talking about. Just wanted to try out the problem to my best ability before asking about it.</p>
",<linear-algebra>
"<p>Sorry but I'm not a mathematician, so please bear with me.
I understand what idempotence is from a communications point of view, and am wondering if it is correct to include linear algebraic formulas as being idempotent.  After all, if <code>y= 2x</code>, then for a given input X, you always get the same result.  Is <code>y=2x</code> idempotent?</p>

<p>Cheers,
Nap</p>
",<linear-algebra>
"<p>Find the coordinates of bivector u⊗v with the respect to cannonical basis and basis M = ((1,2),(1,3)), u = (1,1) v=(1,-2). Please help, does it even have the solution? After the tensor multiplication the matrix is singlular?</p>
",<linear-algebra>
"<p>To prove: </p>

<blockquote>
  <p>A symmetric matrix has only real eigenvalues.</p>
</blockquote>

<p>For this I took a symmetric matrix $A$, an eigenvalue $k$ and an eigenvector $X$. </p>

<p>$AX=kX$</p>

<p>Taking $X$ transpose on both sides</p>

<p>$X'AX=X'kX$</p>

<p>Taking transpose</p>

<p>$(X'AX)'=(X'kX)'$</p>

<p>After solving<br>
$X'AX=k'X'X$<br>
$X'kX=k'X'X$<br>
$(k-k')X'X=0$<br>
$k-k'=0$<br>
$k=k'$</p>

<p>$k$ is equal to the transpose of $k$. How does it prove that $A$ symmetric matrix has only real eigenvalues?</p>
",<linear-algebra>
"<p>Cayley Hamilton Theorem states that if $A$ is an $n \times n$ matrix over the field $F$ then $p(A) = 0$.</p>

<p>We note that $p(\lambda) = \det(\lambda I - A)$. Hence, why can't we just substitute $\lambda$ with $A$ and directly prove Cayley-Hamilton Theorem by saying that $p(A) = p(\lambda) = \det(\lambda I - A) = \det(AI - A) = 0$?</p>
",<linear-algebra>
"<p>$x_1 - x_2 - 2x_3 + x_4 = 0 \\
-3x_1 + 3x_2 + x_3 - x_4 = 0 \\
2x_1 - 2x_2 + x_3 = 0$</p>

<p>How do I solve this system of equations? I know this is a homogenous system. 
By applying elementary row operations, I get the following:</p>

<p>$x_1 - x_2 + 1/5x_4 = 0$</p>

<p>$x_3 - \frac{2}{5}x_4 = 0$</p>
",<linear-algebra>
"<p>Let's assume that $V$ and $W$ are vector spaces over a field $\mathbb{K}$, $\lambda\in\mathbb{K}$, $\lambda\neq0$.</p>

<p>$S: V\rightarrow W$ and $T: W\rightarrow V$ are linear maps. Prove, that</p>

<p>$\lambda$ is an eigenvalue of $TS\iff\lambda$ is an eigenvalue of $ST$</p>

<p>What can be stated about the eigenvalues of the maps $TS$ and $ST$?
Would it also be correct if $\lambda=0$?</p>

<p>Proof:</p>

<ol>
<li>$\lambda$ is an eigenvalue of $TS\Rightarrow\lambda$ is an eigenvalue of $ST$ </li>
</ol>

<p>$TSv=\lambda v$ that is $S(TSv)=S(\lambda v)$ that is $ST(Sv)=\lambda (Sv)$</p>

<ol start=""2"">
<li>$\lambda$ is an eigenvalue of $ST\Rightarrow\lambda$ is an eigenvalue of $TS$</li>
</ol>

<p>$STw=\lambda w$ that is $T(STw)=T(\lambda w)$ that is $TS(Tw)=\lambda (Tw)$</p>

<p>I do not understand why the statement is proven by finding two eigenvectors. What would happen if we couldn't construct $Sv$ out of $v$ and $Tw$ out of $w$?
I also don't understand how to answer the latter two questions.</p>
",<linear-algebra>
"<p>I'm having trouble finding the Eigen values for this matrix:</p>

<p>$$ A =\begin{pmatrix} 0&amp;1&amp;-2 \\ 1&amp;3&amp;0 \\ -2&amp;0&amp;5 \end{pmatrix} $$</p>

<p>I did $A - \lambda I $ and ended up with this matrix:</p>

<p>$$ A - \lambda I =\begin{pmatrix} -\lambda&amp;1&amp;-2 \\ 1&amp;3-\lambda&amp;0 \\ -2&amp;0&amp;5-\lambda \end{pmatrix} $$</p>

<p>I then took the determinant and got $ -\lambda^3 + 8 \lambda^2 - 10\lambda - 17 $, but I don't know what I can do from here. The above polynomial is not factorable. How would I find the Eigen values? </p>
",<linear-algebra>
"<p>The second order equation </p>

<p>$\frac{d^2\vec{x}}{dt^2} = A\vec{x}\ + \vec{g}(t)$</p>

<p>models an earthquake's effect on a 7-story building. Let $x_j(t)$ be the displacement of the $j$th floor with respect to its equilibrium position. The ground moves with displacement $g(t)$. </p>

<p>Here</p>

<p>$\vec{x} = 
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_7
\end{pmatrix}$</p>

<p>$\vec{g}(t) = 
\begin{pmatrix}
g(t)\\
0\\
\vdots\\
0
\end{pmatrix}$ . </p>

<p>A <em>second</em> order $7\times7$ system in $x_j(t)$ is given by</p>

<ul>
<li>$x_1'' = 10(x_2- x_1- 1)$ </li>
<li>$x_2'' = 10(x_3- 2x_2+ x_1)$</li>
<li>$x_3'' = 10(x_4- 2x_3+ x_2)$</li>
<li>$x_4'' = 10(x_5- 2x_4+ x_3)$</li>
<li>$x_5'' = 10(x_6- 2x_5+ x_4)$</li>
<li>$x_6'' = 10(x_7- 2x_6+ x_5)$</li>
<li>$x_7'' = 10(x_6- x_7)$.</li>
</ul>

<p>Write the above second order system as a <em>first</em> order $14\times14$ system using the additional equations $v_j = x'_j$.</p>
",<linear-algebra>
"<p>Need some help and hints on how to prove this one:</p>

<p>Let $F=\mathbb{R}$ or $\mathbb{C}$, and $_FV=M_{n,1}(F)$. Let $A \in M_n(F)$ be Hermitian (i.e $A^* = \bar{A}^T=A$) and $f(x,y)=x^*Ay$, for all $x,y \in V$. Show that $f$ is a Hermitian form, and that $f$ is an inner product on $_FV$ if and only if all the eigenvalues of $A$ are positive. </p>

<p>I already proved that $f$ is a Hermitian form. </p>

<p>Can I have some help on the if and only if part? Thanks a lot. </p>
",<linear-algebra>
"<p>It's an exercise of the book Linear Algebra Done Right.
I'm not clear about how to prove these problems, would you please offer me some suggestion about how to improve this kind of ability, thanks a lot.</p>
",<linear-algebra>
"<blockquote>
  <p>I'm asked to find the equation of plane satisfying the given conditions:</p>
  
  <ul>
  <li>Passing through the line given by:
  \begin{cases}
x+y=2 \\
y-z=3
\end{cases}</li>
  <li>Perpendicular to the plane:
  $$
2 x+3 y+4 z=5
$$
  Knowing that the normal to the plane is 
  $2 i+3 j+4 k$</li>
  </ul>
</blockquote>

<p>I would have hade no problems finding this out if I was given the point. However I am not able to figure it out.
My first tought was to find the point where these lines intersect and then use this point to create the plane with these coinditions, 
$$
x+y-2=y-z-3\Rightarrow z=-x-1
$$</p>

<p>Which I could have expected since I am dealing with tree variables. </p>

<p>Now how could I solve this?</p>

<p>Answer should be $x+6 y-5 z=17$</p>
",<linear-algebra>
"<p>I am having struggle with this question.</p>

<p>suppose I have two unitary matrices.</p>

<p>Is their sum is  normal ?</p>

<p>I am try to give an example to show it is not true and I can not find.</p>

<p>I try to proof and I reach this results:</p>

<p>$T$,$S$ are unitary then:</p>

<p>$(T+S)(T+S)^{*}=(T+S)^{*}(T+S)$</p>

<p>$(T+S)(T^{*}+S^{*})=(T^{*}+S^{*})(T+S)$</p>

<p>$(T+S)T^{*}+(T+S)S^{*}=(T^{*}+S^{*})T+(T^{*}+S^{*})S$</p>

<p>$TT^{*}+ST^{*}+TS^{*}+SS^{*}=T^{*}T+S^{*}T+T^{*}S+S^{*}S$</p>

<p>$ST^{*}+TS^{*}=S^{*}T+T^{*}S$</p>

<p>and I can not continue from here.</p>

<p>I can not tell is this equation is true or not.</p>

<p>and BTW </p>

<p>matrix is unitary iff matrix is symmetric ?</p>

<p>Thanks in advanced !!</p>
",<linear-algebra>
"<p>Given a transition matrix of a Markov chain, $P$, I want to solve the left eigenvector of $P$, namely a row vector $\alpha$ such that
$$
\alpha P = \alpha
$$</p>

<p>I know the algorithm to solve a linear equation takes $O(n^3)$, using LU decomposition.</p>

<p>I wonder if there is any faster algorithm?</p>

<p>By the way, I may not need the exact solution, approximate one is OK.</p>
",<linear-algebra>
"<p>I asked this on mathoverflow as well and apologies for cross-posting. I am trying to compute this so-called bending energy matrix. The bending energy of a thin plate in 3D is given by:</p>

<p>$$
BE = \int_0^{X}\int_0^{Y}\int_0^{Z} \sum_{d=1}^{3}\left\{\left(\frac{\partial^2u_d}{\partial x^2}\right)^2+\left(\frac{\partial^2u_d}{\partial y^2}\right)^2+\left(\frac{\partial^2u_d}{\partial z^2}\right)^2 + 2\left[\left(\frac{\partial^2u_d}{\partial x \partial y}\right)^2 + \left(\frac{\partial^2u_d}{\partial x \partial dz}\right)^2 + \left(\frac{\partial^2u_d}{\partial y \partial z}\right)^2\right]\right\}dx dy dz
$$</p>

<p>Now, I am trying to embed this so that the above equation would be equal to </p>

<p>$$
BE = u'\Sigma u
$$</p>

<p>where u would be a vector that defines the components of the field $u_d$ in the bending energy equation. Now, I need to find this matrix $\Sigma$ so that the above equality can be specified. This should be a large and sparse matrix. However, I have been struggling all day to formulate the form of this matrix and what it's entries should be. I was hoping someone here might give me some pointers on how to construct this matrix.</p>

<p>I would really appreciate any help you can give me.</p>
",<linear-algebra>
"<p>I just started studying vectors in linear algebra, and I didn't understand the idea of the geometric description of a vector.</p>

<p><strong>Why do we treat the vector entries as coordinates?</strong> </p>

<p>As far as I understand, the entries of a column vector are the coefficients of the same variable of different equations. If I'm right on the previous sentence then why do we use them as a coordinate $(x,y)$ (for $\Bbb R^2$) and why do we treat them as different entries?</p>

<p>My English is poor, so if you didn't understand me let me try it with an example.</p>

<p>Let's say there are two equations with two variables.</p>

<p>$$\begin{align}
2x + 3y &amp;= 4 \\
x + 5y &amp;= 15 
\end{align}$$</p>

<p>$$\left[\begin{matrix} 
2 &amp; 3 &amp;|&amp; 4 \\
1 &amp; 5 &amp;|&amp; 15 
\end{matrix}\right]$$</p>

<p>If I take $(2, 1)$ as a column vector, $2$ is a run and $1$ is a rise. This is what I didn't understand as far as $2$ and $1$ are the same $x$ value, why do we use one as a run and the other as a rise? </p>

<p>Thank you.</p>

<p>correction<br />
 i just changed the 2nd entire from 1 to 3 just to make it more clear and understandable </p>
",<linear-algebra>
"<p>I'm learning Linear Algebra using MIT's Open Courseware <a href=""http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2005/"">Course 18.06</a></p>

<p>Quite often, the professor says ""... assuming that the matrix is invertible ..."".</p>

<p>Somewhere in the lecture he says that using a determinant on an $n \times n$ matrix is on the order of $O(n!)$ operations, where an operation is a multiplication and a subtraction.</p>

<p><strong>Is there a more efficient way?</strong>  If the aim is to get the inverse, rather than just determine the invertibility, what is the most effecient way to do this?</p>
",<linear-algebra>
"<p>In my AP chemistry class, I often have to balance chemical equations like the following:</p>

<p>$$ \mathrm{Al} + \text O_2 \to \mathrm{Al}_2 \mathrm O_3 $$</p>

<p>The goal is to make both side of the arrow have the same amount of atoms by adding compounds in the equation to each side.</p>

<p>A solution:</p>

<p>$$ 4 \mathrm{Al} + 3 \mathrm{ O_2} \to 2 \mathrm{Al}_2 \mathrm{ O_3} $$</p>

<p>When the subscripts become really large, or there are a lot of atoms involved, trial and error is impossible unless performed by a computer. What if some chemical equation can not be balanced?  (Do such equations exist?) I tried one for a long time only to realize the problem was wrong.</p>

<p>My teacher said trial and error is the only way. Are there other methods?</p>
",<linear-algebra>
"<p>What is the dimension of the space of $\{A\ {^t\!A}: A\in M(n\times n,\mathbb{R})\}$? I think it should be $n(n+1)/2$ if one knows already the dimension of the special orthogonal group, but I would love to derive the latter from the former.</p>
",<linear-algebra>
"<p>I have an operator</p>

<p>$$h: \mathbb R^4 \to \mathbb R^3\text{ given by } h(x, y, u, v) = (2x + 3y - u + 2v, x - 5y + 6v, 2y + u + v)$$</p>

<p>What is the easiest way to proove that this operator is linear?</p>

<p>I looked over on wiki etc., but I didn't really find the way to prove it mathematically.</p>
",<linear-algebra>
"<p>Homework problem.</p>

<p>Let $a_1, a_2, ..., a_n$ and $b_1,b_2,...,b_n$ be sets of real numbers.  Show that: 
$$ \left(\sum_{k=1}^n a_kb_k\right)^2 \leq \left(\sum_{k=1}^n ka_k^2\right) \left(\sum_{k=1}^n\frac{b_k^2}{k}\right)$$</p>

<p>for all $n \geq 1$.</p>

<hr>

<p>The hint given to us was not to prove this with induction, but to think of the problem ""in linear algebra terms"".</p>

<p>I've pondered this for a few days now, and come up with this: You can think of the $a$'s as a vector $\langle a_1,...,a_n\rangle$, and the $b$'s as a vector $\langle b_1,...,b_n\rangle$ and then the problem can be rephrased as inner products:
$$\langle A,B\rangle\langle A,B\rangle  \space \leq \space \langle A,A\rangle\langle K^{-1}B,B\rangle\;,$$</p>

<p>where $A$ and $B$ are defined above and $KA$ is $\langle 1a_1, 2a_2, ..., na_n\rangle$ and $K^{-1}B$ is 
$\langle 1b_1, \frac{1}{2}b_2,...,\frac{1}{n}b_n\rangle$.</p>

<p>I'm aware of the similarity with the Cauchy-Schwarz inequality, but can't figure out how to manipulate what I have any further.</p>

<p>Any insights are appreciated.</p>
",<linear-algebra>
"<p>This is a question from Serre's Exercise book in Matrix theory. I don't even know how to start. Any help would be appreciated. </p>

<p>Assume that the characteristic of the field $k$ is not equal to 2. Given $M \in GL_n(k)$, show that the matrix
\begin{align}
\begin{pmatrix}0_n &amp; M^{-1} \\ M &amp; 0_n\end{pmatrix}
\end{align}</p>

<p>is diagonalizable. Find its eigenvectors and eigenvalues. More generally, show that every involution ($A^2=I$) is diagonalizable. </p>
",<linear-algebra>
"<p>I've been working through ""Groups and Symmetry"" (Armstrong) and came across this problem in chapter 9 which I can't figure out. Any hints/help would be greatly appreciated!</p>

<p>Show that every $2\times 2$ unitary matrix has the form</p>

<p>$$
\left(\begin{array}{c c}
w &amp; z \\
-e^{i \theta} z^{*} &amp; e^{i \theta} w^{*}
\end{array}\right)
$$</p>

<p>for some $\theta\in\mathbb{R}$ and $w,z\in\mathbb{C}$. (A matrix is said to be <em>unitary</em> if it is invertible with its adjoint as the inverse. The symbol ""*"" denotes complex conjugate.)</p>
",<linear-algebra>
"<p>Hi could you help me with the following:</p>

<blockquote>
  <p>Show that for a symmetric positive definite matrix $B$, $$b_{ij} + b_{jk} + b_{ki} \leqslant b_{ii} + b_{jj} + b_{kk}$$ holds for any $1 \leqslant i,j,k \leqslant n$ with $b_{ij}$ being the entry at $(i,j)$ of matrix $B$.</p>
</blockquote>

<p>Thanks a lot.</p>
",<linear-algebra>
"<p>For any non-zero $\mathbf{y}\in\mathbb{R}^\mathcal{l}$  one half-space through origin is defined by $H_{\mathbf{y}}^{\leq}(\mathbf{0})=\left\{\mathbf{x}\in\mathbb{R}^{\mathcal{l}}:\mathbf{y}\cdot \mathbf{x}\leq\mathbf{0} \right\}$. I want to show $\mathbf{a}\in H^{\leq}_{\mathbf{y}}(0)$ iff $\mathbf{a}=\mathbf{x}-s\mathbf{y}$, where $\mathbf{x}$ is orthogonal to $\mathbf{y}$ and $s\geq 0$.</p>

<p>$\Leftarrow$ side is straightforward, but I couldn't prove $\Rightarrow$ side. I defined $x_i=a_i$ if $ y_i=0$ and $x_i=0$ if $y_i\neq 0$, but couldn't come up with something.Thanks for any help.</p>
",<linear-algebra>
"<p>Suppose $A$ is a $m\times n$ matrix. Show that $\mbox{rank}\,A=m$ if and only if there exists a $n\times m$ matrix $B$ such that $AB=I_m$.<br>
I have proved the case  $AB=I_m$ eventuates $\mbox{rank}\,A=m$, but the main part, the inverse, I couldn't establish.<br>
Would be grateful for your help.</p>
",<linear-algebra>
"<p>If $A$ and $B$ are two positive definite matrices such that $A - B$ is nonnegative definite, is it true that $B^{-1} - A^{-1}$ is positive definite?</p>

<p>The doubt came to me when working with confidence regions in multivariate statistics that are usually obtained as hyper ellipsoids.</p>
",<linear-algebra>
"<blockquote>
  <p>Example: Find all quadratic polynomials that are orthogonal to the function $e^x$ with respect to the $L^2$ inner product on the interval $[0,1]$.</p>
  
  <p>Solution: $p(x)=a((e-1)x-1)+b(x^2-(e-2)x)$ for any $(a,b\in\mathbb{R})$.</p>
</blockquote>

<p>The textbook didn't give any further explanations or steps.  Can someone please help me understand how to get this result??</p>
",<linear-algebra>
"<p>Given $\{U_i\}_{i\in\mathbb N}=\{U_1,U_2,U_3,...\}$ an infinite family of subspaces of $V$ is $\bigcap_{i\in\mathbb N}U_i$ a subspace of V?</p>

<p>I know that it's right for $n$ subspaces with a pretty simple proof, but I don't know how to deal with the infinity.</p>
",<linear-algebra>
"<p>How to find a matrix $A$ when you are given some parameters and the basis for the null space?</p>

<p>The problem I've been scratching my head over is this. The basis  for the null space of $A-4I$ is</p>

<p>$$\left\{
\begin{pmatrix} 1\\0\\0\end{pmatrix}, 
\begin{pmatrix} 1\\1\\1\end{pmatrix}
\right\}
$$</p>

<p>We also know that the matrix $A$ is square.</p>

<p>I'm pretty confused about what to do; this seems to be implying that there are two free variables but I'm not sure how. </p>
",<linear-algebra>
"<p>Let's say I have 2 linear codes, $C_1 = [n,k_1]$ and $C_2 = [n,k_2]$, and I have the parity check matricies $H_1,H_2$ for them. I use the Plotkin construction to create the code $C$ out of them (for every $u\in C_1$, $v\in C_2$, $(u|u+v)\in C$). How can I construct the parity check matrix $H$ of $C$?</p>
",<linear-algebra>
"<p>Assume that we have $6$ vectors in $\mathbb R^4$ such that every two of them is independent. can we generate $\mathbb R^4$ with them?</p>
",<linear-algebra>
"<p>I came across such an exercise:</p>

<blockquote>
  <p>Let $V$ be a linear space over $K$ such that $\dim V = n$. Show that for any $\alpha_1, \alpha_2, \dots, \alpha_m$ with $ m &gt; n + 1$ there exist
  $a_1, \dots, a_m \in K$ such that 
  $\sum_{i=1}^m a_i \triangleright \alpha_i = 0_V$
  and
  $\sum_{i=1}^m a_i = 0_K$</p>
</blockquote>

<p>But then the answer is trivial: it is $a_1 = \dots = a_m = 0$. What could've this exercise been meant to be?</p>
",<linear-algebra>
"<p>How do I prove or disprove if $\{1, \cos x, \cos 2x,..., \cos nx\}$ is linearly independent?</p>

<p>I tried solving the problem using the definition of linear independence,</p>

<p>$\sum_{k=0}^n a_k\cos kx = 0$</p>

<p>$\Rightarrow a_k =0 $</p>

<p>but I am not able to prove/disprove it.</p>
",<linear-algebra>
"<p><a href=""http://i.stack.imgur.com/MDMX0.png"" rel=""nofollow"">Question</a></p>

<p>I have some methodological questions with this exercise:</p>

<blockquote>
  <p><strong>1.</strong> You are given that the transition matric $P_{\mathcal C,\mathcal B}$ from a basis $\mathcal B=\{b_1,\ b_2,\ b_3\}$ to a basis $\mathcal C=\{c_1,\ c_2,\ c_3\}$ is
  $$\frac12\begin{bmatrix}0&amp;-1&amp;1\\-1&amp;1&amp;1\\1&amp;0&amp;0\end{bmatrix}$$</p>
  
  <p>$(a)$ Compute the vector $u=b_1+b_2+2b_3$ as a linear combination of the vectors in $\mathcal C$ and from this write down $[u]_\mathcal C$ (i.e. the coordinates of the vector $u$ w.r.t. $\mathcal C$)</p>
  
  <p>$(b)$ Calculate $P_{\mathcal B,\mathcal C}$</p>
  
  <p>$(c)$ Suppose
  $$\begin{matrix}c_1=(1,2,3),&amp;c_2=(1,2,0),&amp;c_3=(1,0,0)\end{matrix}$$
  Compute $P_{\mathcal S,\mathcal B}$, where $\mathcal S$ is the standard basis, and from this read off the explicit for of the vectors $b_1,\, b_2,\,b_3$.</p>
</blockquote>

<p>$1.a)$
The question asks to find $u$ in terms of $\mathcal C$. To do this do I simply use the transition matrix on $u$ in the form of $(1, 1, 2)$ to get $\frac12(1, 2, 1)$?<br>
If so, what would $[u]_\mathcal C$ in this case? Have we just found it?</p>

<p>$1.c)$ Do I just use the formula here? (the transition matrix one, I can't quite recall it on the spot).</p>

<p>Thanks</p>
",<linear-algebra>
"<p>The solution to a linear algebra problem I'm working on reads: </p>

<blockquote>
  <p>$$\det(A-\lambda I) = \det\begin{pmatrix}-\lambda &amp; 1 &amp; 0 \\ 0 &amp; -\lambda &amp; 1\\ 1 &amp; -1 &amp; 1-\lambda\end{pmatrix} = -\lambda(-\lambda(1-\lambda)+1)+1$$
  This may be written as $\lambda^2(1-\lambda)+(1-\lambda) = (\lambda^2+1)(1-\lambda)$.</p>
</blockquote>

<p>I understand how the determinant is calculated, but am struggling to understand the algebraic manipulation at the end. By my math, $-\lambda(-\lambda(1-\lambda)+1))+1$ simplifies to 
 $-\lambda(-\lambda + \lambda^2 +1)+1$, which simplifies to $(\lambda^2 - \lambda^3 - \lambda)+1$ </p>

<p>What am I misunderstanding here? </p>
",<linear-algebra>
"<p>Suppose there is a square binary matrix (Adjacency matrix of a graph), $A$.</p>

<p>I got that, the matrices, $A^2$ and $A^3$ are distinct but the set of eigenvalues are same for $A^2$ and $A^3$. It is to be noted that the set of eigenvalues of $A$ is different from the same of $A^2$ and $A^3$. Other powers of $A$ are same as $A^3$. </p>

<p>What does the above result interpret?</p>

<p>Please let me know. </p>

<p>Thanks in advance!  </p>
",<linear-algebra>
"<p>Is strict/weak negative/positive definiteness/semidefiniteness of matrices preserved under matrix addition?</p>

<p>I tried to do this for 2x2 matrix but even this wasn't easy. (I tried to use the principal minors definition of definiteness)</p>
",<linear-algebra>
"<p>This is the problem: Let $A$ be a real symmetric $n \times n$ matrix with non negative entries. Prove that $A$ has an eigenvector with non-negative entries</p>

<p>I looked at the answer key and don't quite understand it. In the expression containing max, why should it correspond to the eigenvalue $\lambda_0$? I thought that this may be because if Ax is parallel to x, then the dot product between $Ax$ and $x$ is maximised, but is it not possible that it still attains a large value if $A$ transforms $x$ in a way that scales x by so much that Ax is large enough to make $\langle Ax,x\rangle$ large even though they may not be parallel?</p>

<p>Solution(as in answer key):</p>

<p>Let $\lambda_0$ be the largest eigenvalue of $A$. We have</p>

<p>$$\lambda_0 = \max{\{\langle Ax, x\rangle\mid x\in\mathbb{R}^n,\|x\| = 1\}}$$</p>

<p>and the maximum it attains precisely when $x$ is an eigenvector of $A$ with 
eigenvalue $\lambda_0$. Suppose $v$ is a unit vector for which the maximum is attained, and let $u$ be the vector whose coordinates are the absolute values of the coordinates of $v$. Since the entries of $A$ are nonnegative, we have</p>

<p>$$\langle Au,u \rangle \ge \langle Ax,x\rangle =\lambda_0$$ implying that $\langle Au,u\rangle = \lambda_0$, so that $u$ is an eigenvector of $A$ for the eigenvalue $\lambda_0$.</p>
",<linear-algebra>
"<p>I have tried it in the following manner. We know that $$rank(XY)\ge rank(X)+rank(Y)-n$$ where $X$ and $Y$ are two matrices of order $n$ and also $$rank(XY) \le \min{\{rank(X),rank(Y)\}}$$ If we take $X=Y=A^3$ then using the above two inequalities we obtain $1\le rank(A^6)\le 2$ i.e. rank of $A^6=1 \text{ or } 2$. Is it correct at all? I have failed to obtain a definite rank of $A^6$ by the above process. Is it ok?</p>
",<linear-algebra>
"<p>Let $W$ be a linear subspace  of $\mathbb{R}^n$ of dimension at most $n-1$. Determine which of the following hold:<br>
(1) $W$ is nowhere dense.<br>
(2) $W$ is closed.<br>
(3) ${\mathbb{R}^n}\setminus W$ is connected.<br>
(4) $\mathbb{R}^n\setminus W$ is not connected.    </p>

<p>""$W$ is closed"" is  equivalent to saying that $W'=\mathbb{R}^n\setminus W$ is open. Now by hypothesis $W'$ is of dimension at least 1. In standard basis, this would mean that $W'$ would consist of the <em>n</em>-th column vectors whose at least one component is non-zero. Take any such $t$ in $W'$, take $\epsilon=||t||$, then i guess it is very clear that $B(t;\epsilon)$ is a subset of $W''$. So this proves that $t$ is an interior point of $W$. 
I can not do the parts. Any hint will be well appreciated. </p>
",<linear-algebra>
"<p>What can we conclude about a square matrix $A$ if we know the following?</p>

<ol>
<li>The characteristic polynomial of the matrix is $f(t)=(t-3)^4(t-2)^3$ and</li>
<li>$(A-2I)(A-3I)^2=0$</li>
</ol>

<p>Extracting information from 1) is easy but I don't know what to conclude from 2). Maybe something about the minimal polynomial? </p>
",<linear-algebra>
"<p>Find all values of $t$ such that 
$$
        \begin{pmatrix}
        t &amp; 7 \\
        3 &amp; t \\
        \end{pmatrix}
$$
is not a basis for $\mathbb{R}^2$</p>

<p>$$
        \begin{pmatrix}
        t &amp; 0 \\
        1 &amp; t \\
        \end{pmatrix}
$$
is a basis for $\mathbb{R}^2$</p>

<p>For first question I know i have to show that its either linear dependent or it's span is not $\mathbb{R}^2$ but I'm not sure how I would go about showing that. I'm new to linear algebra please help me as much as possible sorry.</p>
",<linear-algebra>
"<p>From the structure of this all i getting is that</p>

<blockquote>
  <p>If $V$ an n-dimensional vector space with an ordered basis
  $\beta=(x_1,x_2,x_3,\dots,x_n)$ among them (say) first $k$-vectors
  form the basis for $W$.Let $\beta*=\{f_1,f_2,f_3,\ldots,f_n\}$ be the
  dual basis for $V$, then the functional elements of$\beta* $
  corresponding to last $n-k$ vectors of $\beta$ forms the basis for
  $W^0$</p>
</blockquote>

<p>Showing $\{f_{k+1},f_{k+2},f_{k+3},\ldots,f_n\}$ is a basis for $W^0$ .</p>

<p>We know that,inorder to show $T=\{f_{k+1},f_{k+2},f_{k+3},\ldots,f_n\}$ to be a basis for $W^0$ it is sufficient to show that $\operatorname{span} T=W^0$.Since $T$ is a subset of $\beta*  $ so $T $ is linearly independent. Since $W^0$ is a subset of $V*$,every element of $f$ in $W^0$ we could write as a linear combinations of elements of $\beta*$.</p>

<p>From,here I lost the track.</p>
",<linear-algebra>
"<p>I am doing a self study in linear algebra and I am trying to solve the problem bellow. </p>

<blockquote>
  <p>Suppose $P$ is the projection matrix onto the subspace $\mathbf{S}$ and $Q$ is the projection onto the orhogonal complements $\mathbf{S}^{\perp}$. What are $P+Q$ and $PQ$? Show that $P-Q$ is its own inverse. </p>
</blockquote>

<p>Given $P$ and $Q$ and a vector $b$ we take the projection of $b$ onto $\mathbf{S}$ and $\mathbf{S}^{\perp}$ by $p = Pb$ and $q = Qb$ respectively. </p>

<ul>
<li><p>Geometrically, we may say that $b$ equals $p+q$, but is it possible to prove this algebraically?</p></li>
<li><p>Given $b=p+q$, we have $b = p + q = Pb+ Pq = (P+Q)b$. It seems that $P+Q=I$ but how we prove this algebraically? [taking $(I-P-Q)b=0 \implies P+Q=I$ give me doubts.]</p></li>
<li><p>Finally, applying $PQ$ in $b$ geometrically (think in 3D), $b$ projects in zero vector. How can we prove this algebraically? [taking $PQb=0\implies PQ = 0$ again does not fill very right to me.]</p></li>
</ul>

<p>Any help would by priceless.</p>

<p>Thanks.</p>
",<linear-algebra>
"<p>What is exact relationship between matrix R and input matrix A in QR factorization? Say, R gives the structure of A or R is a representation of A. How? We have Q'A = R. Does it mean A is projected to the subspcae of Q? If so, is R a representation of A in another space?</p>
",<linear-algebra>
"<p>If $A$ is a $ \displaystyle  10 \times 10 $ matrix such that $A^{3} = 0$ but $A^{2}  \neq 0$ (so A is nilpotent) then I know that $A$ is not invertible, but why does at least one eigenvalue of $A$ have to be equal to zero? How would one show that all eigenvalues of $A$ are equal to zero?</p>
",<linear-algebra>
"<p>$A$ is a $ \displaystyle  10 \times 10 $ matrix such that $A^{3} = 0$ but $A^{2}  \neq 0$ and therefore, by definition, $A$ is nilpotent. Is there a non-zero vector that lies in both the column space and null space of $A$? This would mean that the $\text{col}(A) \cap \text{nul}(A) \neq {0}$, right?</p>
",<linear-algebra>
"<p>Find the eigenvectors of
$$
A = \begin{bmatrix} 0 &amp; 2 \\ 1 &amp; 1 \end{bmatrix}.
$$
I know you can solve $ \det(A - \lambda I) = 0 $ to find the eigenvalues of $ A $, but I keep getting no free variables. However, I thought this was impossible, but I know this problem works.</p>
",<linear-algebra>
"<p>I'm going through Spivak's Calculus on Manifolds, and I'm currently working on Problem 2-13 part (b). The problem statement is</p>

<blockquote>
  <p>If $f,g: \mathbb{R} \rightarrow \mathbb{R}^{n}$ are differentiable and $h: \mathbb{R} \rightarrow \mathbb{R}$ is defined by $h(t) = \langle f(t),g(t)\rangle$, show that
  $h'(a) = \langle f'(a)^T,g(a) \rangle + \langle f(a),g'(a)^T \rangle.$ </p>
</blockquote>

<p>What's confusing me is that $f'(a)$ is a vector in $\mathbb{R}^{n}$ and taking its transpose yields a vector in the dual space to $\mathbb{R}^{n}$, so taking the inner product with $g(a)$, which is a vector in $\mathbb{R}^{n}$ yields a scalar in $\mathbb{R}$, corresponding to the left term in the equation. Whereas the right term in the equation is an outer product, so it sends a vector and a dual vector to a linear map in Hom($\mathbb{R}^{n}$), since the vector $f(a)$ is $n\times 1$ and the dual vector $g'(a)^T$ is $1\times n$ so their product is $n\times n$. How can these two terms be added to yield a scalar in $\mathbb{R}$?</p>
",<linear-algebra>
"<p>If we're given a $ \displaystyle  2 \times 2 $  Markov Matrix (so all entries are non-negative and columns add to 1) <strong>M</strong>$(a,b)$ such that $$M = M(a,b) := \begin{bmatrix}1 - a &amp; b\\a &amp; 1 - b \end{bmatrix}$$ where $a$ and $b$ are $ 0 ≤ a ≤ 1, 0 ≤ b ≤ 1$, how could you define $N := 1 - M$? I'm confused how to show that if $⟨µ,u⟩$ is an eigenpair for <strong>M</strong>, why is $⟨1 − λ,u⟩$ in an eigenpair for <strong>N</strong>?</p>
",<linear-algebra>
"<p>I know that generally, for any square matrix $ B $, $ \text{rank}(B^{2}) $ is less than or equal to $ \text{rank}(B) $, but I’m having trouble with this proof.</p>
",<linear-algebra>
"<p>I want to find possible solution satisfying both the equation:</p>

<p>$\sum_{i=1}^{n} f_i^{2} = n$</p>

<p>$\sum_{i=1}^{n} f_i=0$</p>

<p>As the number of equations less than number of variables can we just comment on some properties that solutions will have like the following below:</p>

<p>For example a possible solution to the above equation set can be(I got it by hit and trail and intuition) </p>

<p>$f_i$ =
   \begin{cases} 
      \sqrt{\frac{|\overline{A}|}{|A|}} &amp; v_i \in A \\
      \sqrt{\frac{|A|}{|\overline{A}|}} &amp; v_i \in \overline{A} \\
   \end{cases}</p>

<p>where $A$ is any set and $|A|=n$</p>
",<linear-algebra>
"<p>I am stuck on the following problem :  </p>

<blockquote>
  <p>Let $v_1 = (1, 0); v_2 = (1,-1) \space\text{and} \space v_3 = (0, 1).$ How many linear transformations
  $T \colon \Bbb R^2 \to \Bbb R^2$ are there such that $Tv_1 = v_2; Tv_2 = v_3$ and $Tv_3 = v_1?$ The options are as follows:   </p>
  
  <p>(A) $3!$<br>
  (B) $3$<br>
  (C) $1$<br>
  (D) $0$  </p>
</blockquote>

<p>What I observed that $v_2=v_1-v_3$ and so $T(v_2)=T(v_1)-T(v_3) \implies v_3=v_2-v_1$.<br>
But I do not know how to progress from here. Any idea?</p>
",<linear-algebra>
"<p>Consider the strictly convex quadratic function $f(x) = \frac{1}{2}x^tPx - q^tx + r,$ where $P \in \mathbb{R}^{n \times n}$ is a positive definite matrix, $q \in \mathbb{R}^n$ and $r \in \mathbb{R}.$ Let $\mathcal{H} := \{H: H \text{ is a }k- \text{dimensional subspace in } \mathbb{R}^n\}.$ Clearly, the restriction of $f$ to any $H \in \mathcal{H}$ is again a strictly convex function. For any $H \in \mathcal{H},$ we will use $x_H$ to denote the <em>unique</em> optimal point of the following problem</p>

<p>\begin{equation*}
\underset{x \in H}{\text{min.}} \;  f(x).
\end{equation*}</p>

<p>Now consider the map, $\psi(H) = x_H.$ </p>

<p>Prove / Disprove: The map $\psi$ is bijective.</p>

<p>Remark: It is assumed that $P$ is invertible and $q \neq \mathbf{0}.$</p>
",<linear-algebra>
"<p>As a follow-up of <a href=""http://math.stackexchange.com/questions/1011644/which-rings-containing-a-field-are-as-a-vector-space-over-the-field-is-i"">this question</a>, I would like to ask, what are the $2$-dimensional algebras over $\mathbb R$, $\mathbb Q$, or any arbitrary field? Can we classify them?</p>
",<linear-algebra>
"<p>Problem: If $A$ and $B$ are positive semidefinite matrices such that $A^2 = B^2$, show $A = B$, where $A, B$ are $n$-by-$n$ matrices.</p>

<p>This problem is taken out of Linear Algebra (4th edition) by Friedberg, Insel, and Spence. </p>

<p>EDIT: Problem is in Section $6.4$ number $17(d)$</p>

<p>Before posting my question, I looked at this specific question on the website: <a href=""https://math.stackexchange.com/questions/889963/a-b-in-lx-is-positive-semidefinition-hermitian-operators-and-a2-b2-then"">$A,B\in L(X)$ is positive semidefinition hermitian operators and $A^2=B^2$, then $A=B.$</a></p>

<p>This seems like the answer draws from materials outside this textbook. I am not familiar with the square root of a matrix as denoted in that thread. This is not a homework question, but I suspect that there should be a shorter and simpler proof (whether there is one or not) that does not draw materials outside of this textbook.</p>

<p>However, I am stumped as to show how given the hypothesis above (been at it for an hour), how I can deduce that $A = B$. I also attempted to show the contraposition but I am uncertain as how to proceed other than using the fact that there exists an orthonormal basis $\beta$ for $\mathbb{R}^n$ consisting of eigenvectors of $A$ since it is symmetric. </p>

<p>I would appreciate it if anyone can point me in the right direction. </p>
",<linear-algebra>
"<blockquote>
  <p>Let a linear transformation $T:\mathbb{R}^3\to \mathbb{R}^3$ defined as $T(v_1, v_2, v_3) = (v_1, v_3 - 2v_2, -v_3)$. Calculate $f(T)$ where $f(X) = -X^2 + 2 \in \mathbb{R}[X]$</p>
</blockquote>

<p>I'm not so sure how to evaluate $f(T)$. I'll be glad for an explanation.</p>

<p>Maybe $T(v)$ can be viewed as the polynomial $v_1 + (v_3 - 2v_2)x -v_3x^2$?</p>
",<linear-algebra>
"<p>Let $A, B \in \mathbb{R}^{m,n}$, with SVDs $A = U_A \Sigma_A V_A^T$ and $B = U_B \Sigma_B V_B^T$. I want to show that
$$
  || \Sigma_A - \Sigma_B ||_F \leq || A - U B V^T ||_F
$$ 
where $U, V$ are arbitrary unitary matrices of appropriate dimension (this exercise comes from <a href=""http://math.ecnu.edu.cn/~jypan/Teaching/books/SVD.pdf"" rel=""nofollow"">http://math.ecnu.edu.cn/~jypan/Teaching/books/SVD.pdf</a>, 17.5, problem 5). </p>

<p>I know that by unitary invariance of the Frobenius norm we have $ || B ||_F = || U B V ||_F$ for any appropriately sized $U, V$. Thus, I can show that
$$
  || \Sigma_A - \Sigma_B ||_F = || A - U_A U_B^T B V_B V_A^T ||_F
$$</p>

<p>How do I relate this to $U$,$V$? </p>
",<linear-algebra>
"<blockquote>
  <p>Let $A$ be some matrix over $\mathbb{Q}$ (then it's also over
  $\mathbb{R}$). Suppose $A$ is invertible over $\mathbb{R}$ (that is,
  $A^{-1}$ is over $\mathbb{R}$). Prove that $A^{-1}$ is also over
  $\mathbb{Q}$.</p>
</blockquote>

<p>I know that I have to prove that $A^{-1}$ contains no irrational numbers but I fail to do so. I would appreciate any suggestions.</p>
",<linear-algebra>
"<p>My task is to figure out determinant of following matrix depending on $n$.  I want to solve it without altering the rows!
$$
A^{n,n} = \begin{vmatrix} 
0  &amp;    &amp; ... &amp; 0  &amp; -1\\
   &amp;    &amp;     &amp; -1 &amp; 0 \\
   &amp;    &amp; ... &amp;    &amp;   \\
0  &amp; -1 &amp;     &amp;    &amp;   \\
-1 &amp; 0  &amp; ... &amp;    &amp;0  \\
 \end{vmatrix}
$$</p>

<p>This will be $\pm1$ depending on following two things.</p>

<ol>
<li>Multiplication of $-1$. This is simply $-1^n$</li>
<li>The number of left-down positions - 1. For $1\times1$ det is simply $(+1)*(-1)$. For $2\times2$ it's $(-1)*(1)$ and so on.</li>
</ol>

<p>So the rule should look like so:</p>

<p>$$det(A) = -1^n \times -1^{ld_positions - 1}$$</p>

<p>Now the only complicated thing was to figure out the nuber of left-down positions to know the second part. I believe it's $$\sum_{0}^{n-1}n$$ where $n$ is the matrix dimension.</p>

<p>But my solution did not pass my test calculations, so it must be wrong.</p>
",<linear-algebra>
"<p>Let $(\theta,A\theta)=\theta_i A_{ij}\theta_j$ where $A$ is some $(2\times2)$ antisymmetric matrix. </p>

<p>I want to generalize the following </p>

<p>$$I(A) =\int d\theta_1d\theta_2~ \exp\Bigg[\frac{1}{2}(\theta,A\theta)\Bigg]=\int d\theta_1d\theta_2~ (1+\theta_1\theta_2A_{12}) = A_{12}=\sqrt{\det A}$$</p>

<p>to the $n$-tuple case. </p>

<p>Let now $$A:=\begin{bmatrix}
0    &amp; 1       &amp; \;     &amp; \;    \\
\;-1    &amp; 0     &amp;  &amp; \;    \\     
\;     &amp; \;      &amp; 0 &amp; 1     \\
\;     &amp; \;      &amp; -1 &amp; 0     \\
\;     &amp; \;      &amp; \;     &amp; \, &amp;\ddots   \\
\end{bmatrix}.$$</p>

<p>I evaluate, I get the following 
$$I(A) = \int d\theta_n\dots d\theta_1\,\exp\Bigg[\frac{1}{2}(\theta,A\theta)\Bigg]\\ = 
\int d\theta_n\dots d\theta_1\, (\theta_1\theta_2+\theta_3\theta_4+\cdots)\\=0$$</p>

<p>The answer should be $$I(A) = 1.$$ </p>

<p>In the above I use (perhaps incorrectly?) </p>

<p>$$\int d\theta_n\dots d\theta_1\, \theta_n\dots \theta_1\, = 1 $$
and
$$\int d\theta_n\dots d\theta_1= 0. $$</p>

<p>Where do I err? </p>

<p>EDIT: I think I know how to fix this: it is the last term in the expansion of the exponential the contributes. All other terms give zero (just like the one above). I will add the solution later. </p>
",<linear-algebra>
"<blockquote>
  <p>Let $A \in M_2(\mathbb R)$ be a matrix which is not a diagonal matrix . Which of the following statements are true??</p>
  
  <p>a. If $tr(A)=-1$ and $detA=1$, then $A^3=I$.</p>
  
  <p>b. If $A^3=I$, then $tr(A)=-1$ and $det(A)=1$.</p>
  
  <p>c. If $A^3=I$, then $A$ is diagonalizable over $\mathbb R$.</p>
</blockquote>

<p>For (a), it is clear that $A$ will satisfy $\lambda^2+\lambda+1=0$ giving $A^2+A+I=0$. Multiplying $A$ through out gives $A^3+A^2+A=0\implies A^3=-A^2-A=I$</p>

<p>For (b), the only possibilities of eigen values are $1, \omega, \omega^2$. Now if the eigen values are only $1$ and $1$ then $A$ will satisfy $(\lambda-1)^2=0$. We already know that $A^3=I$. From these two facts it is not difficult to see that $A=kI$. Hence the only possible eigen values can be $\omega, \omega^2$. Hence (b) is true.</p>

<p>For(c), $A$ is definitely diagonalizable over $\mathbb C$. Is there any condition which would force a matrix to be diagonalizable over $\mathbb R$ when it is already diagonalizable over $\mathbb C$?</p>
",<linear-algebra>
"<p>In my linear algebra class, we just talked about determinants. So far I’ve been understanding the material okay, but now I’m very confused. I get that when the determinant is zero, the matrix doesn’t have an inverse. I can find the determinant of a $2\times 2$ matrix by the formula. Our teacher showed us how to compute the determinant of an $N \times N$ matrix by breaking it up into the determinants of smaller matrices, and apparently there is a way by summing over a bunch of permutations. But the notation is really hard for me and I don’t really know what’s going on with them anymore. Can someone help me figure out what a determinant is, intuitively, and how all those definitions of it are related?</p>
",<linear-algebra>
"<p>In least-squares approximations the normal equations act to project a vector existing in N-dimensional space onto a lower dimensional space, where our problem actually lies, thus providing the ""best"" solution we can hope for (the orthogonal projection of the N-vector onto our solution space).  The ""best"" solution is the one that minimizes the <strong>Euclidean distance (two-norm)</strong> between the N-dimensional vector and our lower dimensional space. </p>

<p>There exist other norms and other spaces besides $\mathbb{R}^d$, what are the analogues of least-squares under a different norm, or in a different space?</p>
",<linear-algebra>
"<p>The notion (rank-2) ""tensor"" appears in many different parts of physics, e.g. stress tensor, moment of inertia tensor, etc.</p>

<p>I know mathematically a tensor can be represented by a 3x3 matrix. But I can't grasp its geometrical picture — unlike scalar (a number) and vector (an arrow with direction and magnitude) which I can easily see what's going on.</p>

<p>How to visualize a tensor?</p>
",<linear-algebra>
"<p>I asked this question on Stack Overflow but it was closed as ""not programming related"". So I think this is probably the best place for it...</p>

<hr>

<p>I read over the wikipedia <a href=""http://en.wikipedia.org/wiki/Linear_programming"">article</a>, but it seems to be beyond my comprehension. It says it's for optimization, but how is it different than any other method for optimizing things?</p>

<p>An answer that introduces me to linear programming so I can begin diving into some less beginner-accessible material would be most helpful.</p>
",<linear-algebra>
"<p>The absolute value of a $2 \times 2$ matrix determinant is the area of a corresponding parallelogram with the $2$ row vectors as sides.</p>

<p>The absolute value of a $3 \times 3$ matrix determinant is the volume of a corresponding parallelepiped with the $3$ row vectors as sides.</p>

<p>Can it be generalized to $n-D$? The absolute value of an $n \times n$ matrix determinant is the volume of a corresponding $n-$parallelotope?</p>
",<linear-algebra>
"<p>I know there is a double covering map between SU(2) and SO(3) but I have no idea how I would go about proving this or showing this. </p>

<p>can someone point me in the right direction please?</p>
",<linear-algebra>
"<p>Let's suppose we have  a function $Y=A\cdot t^B$ and the values for $Y$ are $30,60,90,120,150$ and the values for $t$ are respectively $0.974, 1.331, 1.718, 1.971, 2.356$. Can you find $A$ and $B$ with the method of linear regression? I have to do a lab work and this is a very small part of it, which does not count but I still have to do it and I have never done linear regression, I need this now? please?</p>
",<linear-algebra>
"<p>Let's look at the well-known definition of orthogonal vectors:</p>

<blockquote>
  <p>Let $V$ be a vector space. Two vectors $x, y \in V$ are <em>orthogonal</em> to each other when the following condition is fulfilled: $$\langle x,y \rangle = 0$$</p>
</blockquote>

<p>Let me explain, where I see a contradiction in this definition.</p>

<p>As I understand the inner product $\langle\cdot \rangle$ of two vectors is not unambiguous (like a norm of a vector, for instance). It can be defined in many ways for different vector spaces, it just needs to satisfy <a href=""http://mathworld.wolfram.com/InnerProduct.html"" rel=""nofollow"">some properties</a>.</p>

<p>As inner product can be defined differently, it can presumably take different values for the same two vectors, depending on which 'version' of the inner product is applied to these vectors. Just analogically with the definition of the norm. Let me illustrate this on an example:</p>

<blockquote>
  <p>Consider a vector $x=(1,-3,2)^T$. Its <a href=""http://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm"" rel=""nofollow"">Euclidean norm</a> is $\lVert x\rVert_2=\sqrt {14} = 3.74\dots$, its <a href=""http://en.wikipedia.org/wiki/Norm_(mathematics)#Maximum_norm_.28special_case_of:_infinity_norm.2C_uniform_norm.2C_or_supremum_norm.29"" rel=""nofollow"">Maximum norm</a> is $\lVert x\rVert_\infty = \max(\lvert 1 \rvert, \lvert -3 \rvert, \lvert 2 \rvert) =3.$ Clearly these two norms are not equal.</p>
</blockquote>

<p>So I guess the same generalization is applied to the definition of the inner product: for some vector space it can be defined in many different ways and thus can take different values upon the applied definition. So theoretically in some special case it might happen so, that the value of an inner product of two vectors, that was defined in one way can be equal $0$ which will imply that the vectors are orthogonal, while the value of an inner product defined in another way applied to the same two vectors will not be equal $0$, which will imply that the vectors are not orthogonal. There's an obvious contradiction here. </p>

<p>There's no such contradiction in the definition of norm as there's a theorem that specifies equivalence of two norms:</p>

<blockquote>
  <p>For some finite-dimensional vector space $V$ let $\lVert x\rVert$ and $\lVert x \rVert '$, $x\in V$ be two different norms. Then $$\exists c \ge 0 \; \forall x \in V: \quad \frac 1c \lVert x\rVert ' \leq \lVert x\rVert \leq c\lVert x\rVert '$$</p>
</blockquote>

<p>Clearly when some norm evaluates for some definite vector to $0$ (thus the vector has 'zero length') any other norm will evaluate to $0$ according to the equivalence theorem. So it cannot happen so, that a zero vector can have a 'non-zero length'. </p>

<p>Therefore the one way to solve this contradiction is to prove that for some vector space $V$ an inner product is defined unambiguously. Another way would be to set an analogical theorem for inner products stating their equivalence, from which will follow, that if some inner product is evaluated to zero, all other will do so. Let's again consider Euclidean space, for which the inner product (also called dot product) is defined as $$\langle x,y \rangle = a_xa_y + b_xb_y,$$ where $a_i$ and $b_i$ are the corresponding coordinates of the vectors $x$ and $y$. This is really an inner product as it fulfills the properties. However has it been proved that in the Euclidean space there can't exist any other prescription, which, when applied to two vectors will fulfill the properties of the inner product and thus be another type of the inner product for this space? Or does some theorem exist that states equivalence of two inner products?</p>
",<linear-algebra>
"<p>I'm using Linear Algebra by Jim Hefferon (freely available, links below with solution).</p>

<p>I'm having trouble understanding Exercise 1.18 on page 117.</p>

<p>1.18 Decide if each is a basis for $P_2$.
(a) $(x^2 + x - 1, 2x + 1, 2x - 1)$</p>

<p>First, I try to prove that it spans $P_2$ $(ax^2 + bx + c)$. However, I do not understand how to set up the matrix. I usually do not have any trouble when there are column vectors given to me and I simply have to row-reduce using Gauss' Method, however whenever given equations with variables I have trouble. </p>

<p>Can someone walk through this step by step? That would be really helpful. I'm trying to teach myself Linear Algebra so there may be many missing gaps of knowledge.</p>

<p>Book: <a href=""http://joshua.smcvt.edu/linearalgebra/book.pdf"" rel=""nofollow"">http://joshua.smcvt.edu/linearalgebra/book.pdf</a></p>

<p>Answer Key: <a href=""http://joshua.smcvt.edu/linearalgebra/jhanswer.pdf"" rel=""nofollow"">http://joshua.smcvt.edu/linearalgebra/jhanswer.pdf</a></p>
",<linear-algebra>
"<p>I've just seen a proof of the statement: ""Given $\alpha$ in a commutative ring $K$ there is a unique alternating multilinear function $f$ with $f(Id)=\alpha$.""</p>

<p>The determinant is defined as the unique $f$ such that $f(Id)=1$. I don't understand why for each alternating multilinear function $f$ we have $$f(A)=\det(A)f(Id)$$</p>

<p>I would appreciate if anyone could explain me why this is true. Thanks in advance.</p>
",<linear-algebra>
"<p>How to find the linear transformation $T: \mathbb R^3 \to \mathbb R^3$ such that the set of all vectors satistfying $4x_1-3x_2+x_3=0$ is</p>

<p>a) Null space of $T$</p>

<p>b) Range of $T$</p>

<p>I'm not able to approach this problem.
Any help would be appreciated.</p>
",<linear-algebra>
"<blockquote>
  <p>Let $A$ be a square matrix of order $3$. Prove that 
  $$
\operatorname{adj}(A) 
= \tfrac{1}{2} \bigl[ 
(\operatorname{tr} A)^2 - \operatorname{tr}(A^2) \bigr] I_3 
- [\operatorname{tr} A] A + A^2
$$ 
  where $\operatorname{tr}A$ is the trace of $A$.</p>
</blockquote>

<p>To begin, I'm not quite sure how to start proving this. I've considered using brute force, but I suspect there should be a much more elegant way of doing it. Is there a need to prove this for both $(i,i)$ and $(i,j)$ entries? I'd appreciate an explanation that is not too complex. Thanks in advance!</p>
",<linear-algebra>
"<p>Given that $A$ is a complex square matrix of order $n$, $\lambda$ is an eigenvalue of $A$ with geometric and algebraic multiplicity $1$, and $x,y$ are entrywise nonzero vectors such that $Ax=\lambda x$ and $y^*A=\lambda y^*$. Show that every proper principal submatrix of $\lambda I-A$ has nonzero determinant.</p>

<p>I know that $\operatorname{adj}(\lambda I -A) = \gamma xy^*$ where $\gamma$ is nonzero, hence has only nonzero entries. So I know every principal submatrix of  $\lambda I-A$ of size $n-1$ has nonzero determinant. I don't know how to prove this is true for smaller principal submatrices.</p>

<p>Any help is appreciated.</p>
",<linear-algebra>
"<p>Assume that the following is used:</p>

<p>$$ 
A = \begin{pmatrix}
 0&amp;  1&amp;\\
 2&amp;  3&amp;\\ 
 4&amp;  5&amp;\\
 6&amp;  7&amp;\\
 8&amp;  9&amp; 
\end{pmatrix}
$$</p>

<p>Then calculating the Coveriance matrix, which, gives me:</p>

<p>$$ 
C = \begin{pmatrix}
 40&amp;  40&amp;\\
 40&amp; 40&amp;\\ 
\end{pmatrix}
$$</p>

<p>Then using the following:</p>

<p>$$
det = (a+b) \cdot (a+b)-4 \cdot(a \cdot b - c \cdot c),
$$</p>

<p>where in this case, $a = 40, b = 40, c = 40$ gives the answer:</p>

<p>$$
\lambda_{1} = 80, \\
\lambda_{2} = 0,
$$</p>

<p>These are therefore the correct Eigen values. However, using this formula, if I have the following:</p>

<p>$$ 
A = \begin{pmatrix}
 -4&amp;  -2&amp;\\
 -1&amp;  -3&amp;\\ 
 4&amp;  5&amp;\\
 6&amp;  7&amp;\\
 8&amp;  9&amp; 
\end{pmatrix},
$$</p>

<p>where the Covariance matrix is given: </p>

<p>$$
C = \begin{pmatrix}
 99.2&amp;  103.4&amp;\\
 103.4&amp; 116.8&amp;\\ 
\end{pmatrix},
$$</p>

<p>gives the Eigenvalues as: </p>

<p>$$
\lambda_{1} = 218.119 \\
\lambda_{2} = -15.5189
$$</p>

<p>When the actual values are:
$$
\lambda_{1} =211.774 \\
\lambda_{2} = 4.226
$$</p>

<p>Could anyone tell me where I am calculating this wrong please?</p>

<p>EDIT:</p>

<p>For
$\lambda_{1} = (a + b + det)/2 \\
 \lambda_{2} = (a + b - det)/2
$</p>
",<linear-algebra>
"<p>I have a 10x10 symmetrical variance-covariance matrix, such that the variances for 10 vectors are on the main diagonal and the covariance between all vectors are on the off-diagonals.</p>

<p>I want to quantify the amount of variance in total. I can easily take the matrix trace as the sum of the eigenvalues on the main diagonal.</p>

<p>However, the matrix can be split into meaningful (biologically meaningful, in my case) sub-matrices: 4 submatrices, 5x5 each, in each corner of the original matrix. If I then want to quantify the variation within each sub-matrix  using the matrix trace, I run into some trouble with the top-right/bottom-left sub-matrices. These are formed of covariance estimates and are therefore not necessarily positive. My question is, what is the correct way to calculate the matrix trace here? If I sum the eigenvalues, I will have some negative values subtracting from the total, so should I use absolute values? Is the matrix trace the best method to use here or is there a more appropriate way of summarising the amount of variance in the sub-matrices?</p>

<p>Any guidance would be gratefully received.</p>

<p>Thanks,</p>

<p>Fiona </p>
",<linear-algebra>
"<ol start=""2"">
<li>Find matrix of a given linear transformation L->M in given new bases:</li>
</ol>

<p>a) $L =&lt; e_1,e_2,e_3 &gt;, M =&lt; g_1,g_2 &gt;, f(e_1) = g_1 − 2g_2, f(e_2) = g_1 + g_2, f(e_3) = 2g_1 + 3g_2,
$
$
\bar e_1 = 2e_1 − e_3, \bar e_2 = e_2 + e_3,  \bar e_3 = e_1 − e_2, \bar g_1 = g_1 + 2g_2, \bar g_2 = 2g_1 − g_2
$$</p>

<p>I know similar questions have been asked but they didn't help me because I am unsure if my data matches the data in the other answers. Please note there is a difference between $e_1$ and $\bar e_1 $.</p>
",<linear-algebra>
"<p>What is the dimension of a kernel with the basis {[0,0,0]}?</p>

<p>I'm confused because the definition of the dimension is number of vectors in a basis. So there is 1 vector here which is [0,0,0]. </p>

<p>Why does my professor say that the dimension of kernel is zero? He mentioned something about the zero vector space.</p>
",<linear-algebra>
"<p>Let $T$ be the Toeplitz operator on $\ell_p$ with symbol $\alpha(\lambda)=a/2\cdot \lambda-(a+1/2)+\lambda^{-1}$, where $a$ is complex. I want to solve the following </p>

<p>$$
Tx=y
$$</p>

<p>for $x\in \ell_p$ and $y=(1,q,q^2,\ldots),|q|&lt;1$. Therefore, I (Wiener-Hopf) Factorized the symbol:</p>

<p>$$\alpha(\lambda)=\alpha_{-}(\lambda)\alpha_{+}(\lambda)=(1/2-\lambda^{-1})(a\lambda-1).$$</p>

<p>So, $T^{-1}=T_{\alpha_+^{-1}}T_{\alpha_-^{-1}}$ (only if $a\not=1/\lambda$, not sure if this is true though. When is $T$ one-sided invertible?). Hence (again, not sure),</p>

<p>$$T^{-1}= 2a(S-2I)^{-1}(I-aS_{backw.})^{-1}$$</p>

<p>Now, I want tot compute $T^{-1}y$ to obtain $x$, but I got stuck here. Any hints? My second question is: How is the spectrum of $T$ defined, is there an easy way to compute $\sigma(T)$?</p>
",<linear-algebra>
"<p>I am asked to determine all faces of the $n$-dimensional hypercube
$$C_n = \left\lbrace x\in\mathbb R^n \;|\;\forall i\in\lbrace1\ldots n\rbrace : |x_i|\leq1\right\rbrace $$</p>

<p>I already know that the the $k$-dimensional faces of $C_n$ are defined by $n-k$ equalities $|x_i|=1$. <br/>
So in total there are $2^{n-k}{n\choose k}$ of those $k$-dimensional faces.</p>

<p>I understand how those faces look like and for a fixed particular $n$ I would be able to write them all down one by one, but I am struggeling to write down some general expression for all faces.</p>
",<linear-algebra>
"<p>Find a basis of subspace $ U_1 +U_2 $ of a vector space $V$. $ U_1, U_2 \subseteq V$:
$V = \mathbb R[t]$, $U_1 = \{ f \mid t^2-4t+3 \text{ divides } f \}, U_2 = \{g \mid t^2-5t+4 \text{ divides } g\}. $</p>
",<linear-algebra>
"<p>I don't understand this really good and couldnt find anything helpful on internet. I only found in book the following: A is the matrix with reflection over line through the origin with direction vector $\left(\cos(\frac{\alpha }{2} ) , \sin(\frac{\alpha }{2} ) \right) ^{T}$
$A=\begin{pmatrix} \cos(\alpha ) &amp; \sin(\alpha ) \\ \sin(\alpha )  &amp; -\cos(\alpha )    \end{pmatrix}$
I'm not sure how to connect this from book to solve it, because there is another direction vector.</p>
",<linear-algebra>
"<p>How to prove that $\det(A^{T}A) \neq 0$ if coloumns of $A$ are linearly independent, without using Cauchy-Binet formula? $A$ is real matrix.</p>
",<linear-algebra>
"<p>How do I prove this proposition:</p>

<blockquote>
  <p>If $A$, $B$ are similar matrices then for every $\lambda$ $\in$
  $\mathbb{R}$ the matrices $A-\lambda I$ and $B-\lambda I$ are similar.</p>
</blockquote>

<p>Now, from what I was given I know $A = P^{-1} B P$ and $B = PBP^{-1}$, and</p>

<p>I need to show that $A-\lambda I = M^{-1} (B-\lambda I)M$ and I am done.</p>

<p>I can't see how to solve this, I hope someone does,</p>

<p>Thank you in advance.</p>
",<linear-algebra>
"<p><strong>For 5 months!</strong> I have been struggling to solve the following equations analytically without numeric method (i,e, Newton method):</p>

<blockquote>
  <p><strong>Main equation:</strong></p>
  
  <p>$$
 \biggl(M^2-\cfrac{\mathbf{x^{\text{T}}}M^2\mathbf{x}}{\mathbf{x^{\text{T}}}\mathbf{x}}E\biggr)\mathbf{x}=\mathbf{1}
$$</p>
  
  <p><strong>Constraint equations:</strong></p>
  
  <p>$$
\begin{cases}
 \mathbf{x^{\text{T}}1}=0 \\
\\
\mathbf{x^{\text{T}}x}=u 
\end{cases} $$</p>
  
  <p>where $\{M,E\}\in\mathbf{R}^{n \times n}$ and $\{\mathbf{1},\mathbf{x}\}\in\mathbf{R}^n$ are defined, then $M$ is an arbitrary symmetric matrix, $E$
  is an identical matrix,
  $\mathbf{1}$ is all one vector, $\mathbf{x}$ is a
  variable vector and $u\in\mathbf{R}$ is a scalar.
  Furthermore, as a knowledge, the below equation form is called <a href=""https://en.wikipedia.org/wiki/Rayleigh_quotient"" rel=""nofollow"">Rayleigh
  quotient</a> $R(M^2,\mathbf{x})$:</p>
  
  <p>$$R(M^2,\mathbf{x}):=\cfrac{\mathbf{x^{\text{T}}}M^2\mathbf{x}}{\mathbf{x^{\text{T}}}\mathbf{x}}$$</p>
</blockquote>

<p>Now, we attempt to estimate the $\mathbf{x}$. Does the analytic solution or method exist? My ability is shortage but, I guess that this problem has a beautiful solution. Also, main equation is a simultaneous cubic equation. Theoretically, this is solvable. Just, this is my theme question.</p>

<p>Furthermore, same question is already asked on <a href=""http://mathoverflow.net/questions/238431/explicit-solution-to-a-rayleigh-quotient-equation"">math overflow</a>. Then answerers provided worthful information which may be solution to clue. </p>
",<linear-algebra>
"<p>The property states, ""A square matrix A is invertible iff it can be written as the product of elementary matrices""</p>

<p>I'm confused on the part of the theorem where they're trying to show that if A is invertible, then it can be written as the product of elementary matrices.</p>

<p>This is that section of the proof:</p>

<p>""Assume A is invertible. You know the system of linear equations represented by Ax=0 has only the trivial solution. But this implies that the augmented matrix [A 0] can be rewritten in the form [I 0] (using elementary row operations corresponding to E<sub>1</sub>,E<sub>2</sub>,...,E<sub>k</sub>). So, E<sub>k</sub>,...,E<sub>2</sub>,E<sub>1</sub>A I and it follows that A = E<sub>1</sub>-1E<sub>2</sub>-1...E<sub>k</sub>-1 . A can be written as the product of elementary matrices.""</p>

<p>I just don't get how knowing that Ax=0 has only the trivial solution implies that [A 0] can be written in the form [I 0]. Wasn't it already obvious that A can be rewritten as I since it's invertible? And obviously if there's a 0 matrix adjoined A to it it's going to stay the zero matrix no matter what row operations are done on it? What's the point of doing that?</p>

<p>I'm just generally confused on this proof</p>
",<linear-algebra>
"<p>Compute $det(OE[A](t))$ with $tr(A)=0$, where $OE[A]$ is defined <a href=""http://en.wikipedia.org/wiki/Ordered_exponential"" rel=""nofollow"">here</a>.</p>

<p>Attempt: First I computed the following derivative $\frac{d}{dt}det(OE[A](t))=tr((OE[A])^{-1}\frac{d}{dt}OE[A])det(OE[A](t))$. Since the ordered exponential can be written as a product of infinitesimal exponentials, I can use the product rule and simplify:
$tr((OE[A])^{-1}\frac{d}{dt}OE[A])=tr(\Delta t \sum_{i=1}^\infty A(t_i))=\Delta t \sum_{i=1}^\infty tr(A(t_i))=0$. Hence the determinant is independent on the parameter $t$ and now I choose $t=0$ (it's arbitrary) and get $det(OE[A](t))=1$.</p>

<p>Are there built errors? Are there formulas for the determinant of a ordered exponentials?
Hints would be greatly appreciated.</p>
",<linear-algebra>
"<blockquote>
  <p>Suppose $A,B$ are $n\times n$ positive definite. Then which of the followings are positive definite:</p>
  
  <ol>
  <li><p>$A+B$</p></li>
  <li><p>$ABA$</p></li>
  <li><p>$A^2+I$</p></li>
  <li><p>$AB$</p></li>
  </ol>
</blockquote>
",<linear-algebra>
"<p>I would like if someone could look over my proof. It feels odd to me.</p>

<blockquote>
  <p>Let $W$ be a subspace of a vector space $V$ over a field $F$. Prove that $v + W = \{v + w \mid w \in W\}$ is a subspace of $V$ if and only if $v \in W$.</p>
</blockquote>

<p><strong>Proof:</strong> ($\Rightarrow$) Suppose $v + W$ is a subspace of $V$. Note that $v = v + 0_W \in v + W$ and as a result $(-1)v = -v \in v + W$ as $v + W$ is a subspace. Therefore, $-v = v + w$ for some $w \in W$. Solving for $w$, we get $w = -2v$ and since $W$ is a subspace $(\frac{-1}{2})w = (\frac{-1}{2})(-2v) = v \in W$ as desired. </p>

<p>($\Leftarrow$) Suppose $v \in W$. As $W$ is a subspace, $-v \in W$ and therefore $0 = v + (-v) \in v + W$. If $x, y \in v + W$ then $x = v + w$ and $y = v + w'$ for some $w,w' \in W$. Then $x + y = (v + w) + (v + w') = v + (v + w + w') \in v + W$ as $v,w,w'\in W$ and $W$ is a subspace. Furthermore, note that $cx = c(v + w) = cv + cw = v + (cv + cw - v) \in v + W$ as $v,w \in W$ and $W$ is a subspace. Therefore, $v + W$ is a subspace of $V$ as it contains the $0$ element and it is closed under addition and scalar multiplication. $_\Box$</p>

<p>I'd appreciate any feedback. Thank-you. </p>
",<linear-algebra>
"<p>I am trying to optimize the output of a given neural network with a single hidden layer.  To accomplish this, I intend to find solve for all combinations of inputs where the derivative of the neural network = 0 and select the input vector with the highest (or lowest, depending on the problem) neural network output.  It uses the activation function</p>

<p>$$
H_i,_j = \frac{1}{(1 + e^{-t})}
$$</p>

<p>where </p>

<p>$$
t = X_i\theta_j
$$</p>

<p>for a given input vector i and hidden node j. </p>

<p>The activation values of each hidden node are multiplied by a separate weight matrix to produce the outputs.  The output k of a given input vector i is the product of the hidden node activation values i and the weight vector k.</p>

<p>$$
O_i,_k = H_iW_k
$$</p>

<p>Could someone please explain the steps I would use to create the derivative formula for an input vector of arbitrary length, an arbitrary number of hidden nodes, a single hidden node layer, and a given output k?  Thank you so much.</p>
",<linear-algebra>
"<p>I've a task to find the distance in $E^4$ between:</p>

<p>$L = [1,2,-1,4] + \text{lin}((1,2,-1,0))$</p>

<p>and</p>

<p>$M = [2,3,1,5] + \text{lin}((2,1,0,2))$</p>

<p>My efforts to find the correct solution:</p>

<p>Let</p>

<p>$\alpha_{1}=(1,2,-1,0)
 , \alpha_{2}=(2,1,0,2)$</p>

<p>$p_{0}=[2,3,1,5]$</p>

<p>$p\in R^{4}
  p\in L$</p>

<p>$p=(1+t,2+2t,-1-t,4)$</p>

<p>$\overrightarrow{p_{0}p}=(t-1,2t-1,2t,-1)$</p>

<p>$f(t)=
 d(p,M)$</p>

<p>Then (W is a determinant of grammian matrix)</p>

<p>$f(t)=\sqrt{\frac{W(\alpha_{2},\overrightarrow{p_{0}p})}{W(\alpha_{2})}}$</p>

<p>$&lt;\alpha_{2},\alpha_{2}&gt;=3$</p>

<p>$W(\alpha_{2})=3$</p>

<p>$W(\alpha_{2},\overrightarrow{p_{0}p})=\det\left(\begin{array}{cc}
&lt;\alpha_{2},\alpha_{2}&gt; &amp; &lt;\alpha_{2},\overrightarrow{p_{0}p}&gt;\\
&lt;\overrightarrow{p_{0}p},\alpha_{2}&gt; &amp; &lt;\overrightarrow{p_{0}p},\overrightarrow{p_{0}p}&gt;
\end{array}\right)$</p>

<p>$&lt;\overrightarrow{p_{0}p},\overrightarrow{p_{0}p}&gt;=\sqrt{(t-1)^{2}+(2t-1)^{2}+4t^{2}+1}=\sqrt{t^{2}-2t+1-4t^{2}+1+4t^{2}+1}=\sqrt{t^{2}-2t+1}=\sqrt{(t-1)^{2}}=t-1$</p>

<p>$&lt;\alpha_{2},\overrightarrow{p_{0}p}&gt;=&lt;\overrightarrow{p_{0}p},\alpha_{2}&gt;=\sqrt{2t-2+2t-1-2}=\sqrt{4t-5}$</p>

<p>$W(\alpha_{2},\overrightarrow{p_{0}p})=\det\left(\begin{array}{cc}
3 &amp; \sqrt{4t-5}\\
\sqrt{4t-5} &amp; \mid t-1\mid
\end{array}\right)=\\3\cdot\mid t-1\mid-\mid4t-5\mid=\begin{cases}
-3t+3+4t-5 &amp; \iff t&lt;1\\
3t-3+4t-5 &amp; \iff t\in[1,\frac{5}{4})\\
3t-3-4t+5 &amp; \iff t&gt;\frac{5}{4}
\end{cases}=\begin{cases}
t-2 &amp; \iff t&lt;1\\
7t-8 &amp; \iff t\in[1,\frac{5}{4})\\
-t+2 &amp; \iff t\geq\frac{5}{4}
\end{cases}$</p>

<p>From that I noticed, that distance can be negative, and I don't know how to fix it</p>

<p>Could you help me to point, where I made an error in reasoning?</p>

<p>Thanks in advance for all advices! </p>
",<linear-algebra>
"<p>We are given the following problem:</p>

<blockquote>
  <p>Let $S$ be the set of all functions $y$ that satisfy the following differential equation
  $$2\dfrac{d^2y}{dx^2}  - 3\dfrac{dy}{dx} + y = 0.$$
  Show that $S$ is a subspace of the vector space $A$, where $A$ is the set of all functions $f : \mathbb{R} \rightarrow \mathbb{R}$.</p>
</blockquote>

<p>I do not know how to approach the problem.</p>
",<linear-algebra>
"<p>So I was able to figure out the first part of this problem, but I have no concept of how it relates to Schur complements, so I'm not sure (no pun intended) how to proceed. The question is as follows:</p>

<p>Consider $2x^2 + 2xy + 2y^2 + z^2 + 2xz$. Write the symmetric matrix representing this quadratic form. Now, express this as a sum of squares by using this symmetric matrix and Schur complements.</p>

<p>I determined the symmetric matrix representation as:</p>

<p>$$\begin{bmatrix}
        2 &amp; 1 &amp; 1 \\
        1 &amp; 2 &amp; 1 \\
        1 &amp; 0 &amp; 1 \\
        \end{bmatrix} $$</p>

<p>And that's as far as I've gotten. Any help would be much appreciated.</p>
",<linear-algebra>
"<p>If we're given a $ \displaystyle  2 \times 2 $  Markov Matrix (so all entries are non-negative and columns add to 1) <strong>M</strong>$(a,b)$ such that $$M = M(a,b) := \begin{bmatrix}1 - a &amp; b\\a &amp; 1 - b \end{bmatrix}$$ where $a$ and $b$ are $ 0 ≤ a ≤ 1, 0 ≤ b ≤ 1$, I know that $λ1 = 1$ is an eigenvalue for $M(a,b)$, but how would I find a corresponding eigenvector $u_{1}(a, b)$ such that when normalized, $e^{T}u1(a, b) = 1$? There should also be a second eigenvalue / eigenvector right?</p>
",<linear-algebra>
"<p>Let T:P2->P2 be a linear transformation and A be the matrix of the linear transformation. Prove that if det(A) does not equal 0 then T is one-to-one.</p>

<p>I know that for T to be 1-1 then the kernel is a zero vector and therefore A would reduce to an identity matrix I'm just not sure how to tie that into the determinant?</p>
",<linear-algebra>
"<p>Define a function $ T: P_{3} \to \text{M}_{2 \times 2} $ by
$$
  T \! \left( a_{0} + a_{1} x + a_{2} x^{2} + a_{3} x^{3} \right)
= \begin{pmatrix} a_{3} &amp; a_{0} \\ a_{2} &amp; a_{1} \end{pmatrix}.
$$
I know how to show that $ T $ is a linear transformation, i.e.,
$$
T(\vec{u} + k \cdot \vec{v}) = T(\vec{u}) + k \cdot T(\vec{v}).
$$
I also know how to show that $ T $ is an isomorphism (one-to-one and onto), but how do I find the matrix representation of $ T $ with respect to the standard bases of $ P_{3} $ and $ \text{M}_{2 \times 2} $?</p>
",<linear-algebra>
"<p>I am not seeing why a subspace must include $ 0 $. From what I am told, this inclusion means that the subspace is not “empty”, but I cannot see how the inclusion of $ 0 $ does this. For instance, can you not have a subspace of $ \Bbb{R}^{2} $ that is a line represented by $ y = x + 1 $, which will not intersect $ (0,0) $ of $ \Bbb{R}^{2} $ at all? This subspace appears to exists and to be an element of $ \Bbb{R}^{2} $ by addition and scalar multiplication, but unless I am mistaken, it does not satisfy this $ 0 $-vector inclusion requirement (unless this means it is NOT a subspace, but then that is not making sense).</p>
",<linear-algebra>
"<p>I'm having a bit trouble with this excercise:</p>

<p><strong>The problem:</strong><br>
Let there be a polynomial $f(x)=a_1x^{t_1} + a_2x^{t_2} + ... + a_nx^{t_n}$
Where $t_1, t_2, ..., t_n$ are not-negative integers.
The polynomial has a root $b$ which occurs $n$ times.
Prove that $b = 0$.</p>

<p><strong>What I have so far:</strong><br>
I can presume that $a_1, a_2, ..., a_n \neq 0$<br>
If $n = 1$, then it's obviously true.<br>
If $n = 2$, I tried using this:<br>
$f^{(n-1)}(b) = 0
\\
f^{(n)}(b) = 0$<br>
So:<br>
$f(b) = a_1b^{t_1} + a_2b^{t_2} = 0
\\
f'(b) = t_1*a_1b^{(t_1-1)} + t_2*a_2b^{(t_2-1)} = 0
\\
f''(b) = (t_1-1)t_1*a_1b^{(t_1-2)} + (t_2-1)t_2*a_2b^{(t_2-2)} \neq 0$<br>
Why can't $b \neq 0$ be true?</p>
",<linear-algebra>
"<p>Let $V$ be a vector space of dimension $n$ over $\mathbb{F}_q$, and let $U$ be a subspace of dimension $k$. I want to compute the number of subspaces $W$ of $V$ of dimension $m$ such that $W\cap U=0$.</p>

<p>I know why the number of subspaces of $V$ that contain $U$ and have dimension $m$ is $\binom{n-k}{m-k}_q$, but I don't understand why $q^{km}\binom{n-k}{m}_q$ is number of these subspaces?</p>
",<linear-algebra>
"<p>how to prove $\|(A^HA)^k\| =||A||^{2k}$ using singular value decomosition. $A^H$ is a hermitian matrix. $A$ element of $C^{p\times q}$, for every positive integer $k$.</p>
",<linear-algebra>
"<p>Suppose one has the following system of linear equations
$$(A + \Delta A) x = b$$
where $A$ and $\Delta A$ are large sparse matrices and $\Delta A$ is ""small"" compared to $A$, furthermore vector $x$ is unknown (the solution) and vector $b$ is known. </p>

<p>The system needs to be solved many times, in which only $\Delta A$ varies (the perturbation). Therefore it is relatively cheap to obtain $A^{-1}$ and may be considered to be known.</p>

<p>The best solution I've found thus far is to apply the Neumann series expansion on the inverse
$$A(I + A^{-1} \Delta A) x = b$$
$$A(I + P) x = b$$
$$\Rightarrow \quad x = (I + P)^{-1} A^{-1} b = \lbrace I - P + P^2 - P^3 + \cdots \rbrace A^{-1} b$$</p>

<p>Does any one know of a better alternative, preferably a method that doesn't require a series expansion?</p>
",<linear-algebra>
"<p>I have a polynomial $p_a(x,y)= x^2F(a)+y^2G(a)-xH(a)-I(a)$ where $F(a)$, $G(a)$, $H(a)$ and $I(a)$ some real fuctions of $a$ are. Which conditions must satisfy $a$ so that I can factorize the polynomial $p_a(x,y)$ in lineal real factors?</p>
",<linear-algebra>
"<p>If both roots of the equation $(a-b)x^2+(b-c)x+(c-a)=0$ are equal, prove that $2a=b+c$.</p>

<blockquote>
  <p><strong>Things should be known:</strong></p>
  
  <ul>
  <li><p>Roots of a Quadratic Equations can be identified by:</p>
  
  <p>The roots can be figured out by: 
  $$\frac{-b \pm \sqrt{d}}{2a},$$ 
  where
  $$d=b^2-4ac.$$</p></li>
  <li><p>When the equation has equal roots, then $d=b^2-4ac=0$.</p></li>
  <li><p>That means $d=(b-c)^2-4(a-b)(c-a)=0$</p></li>
  </ul>
</blockquote>
",<linear-algebra>
"<p>My question is: </p>

<p>How to solve this equation:</p>

<p>$ax²+by²+cxy=0$</p>

<p>with respect to $x$ and $y$ in the same time. Here $a,b,c$ are real constants.</p>
",<linear-algebra>
"<p>How would one go about proving that there is no embedding of a vector space into it's dual that is independent of a choice of basis? Thanks</p>
",<linear-algebra>
"<p>Given the matrix $A= \begin {pmatrix} 1 &amp; 1 &amp;1  \\ -1 &amp; 1 &amp; 0 \\ 0 &amp; 2 &amp;1 \end{pmatrix}$.</p>

<p>(i) Determine the orthogonal projection $p:\mathbb{R}^3 \rightarrow \mathbb{R}^3$ on $Im(A)$</p>

<p>(ii) Calculate an orthonormal basis of $(\ker(A))^{\perp}$</p>

<p>(iii) Determine the pseudoinverse $A^+$ of $A$</p>

<p>I was wondering about the sequence of the subtasks. Normaly, i would do (iii) first and then (i) &amp; (ii) using that $AA^+:\mathbb{R}^m\rightarrow Im (A)$ and $A^+A:\mathbb{R}^n\rightarrow(\ker(A))^{\perp}$. So my question is: Is there a way to to (i) and (ii) without determine the pseudoinverse?</p>
",<linear-algebra>
"<p>$T:P(\mathbb{R}) \mapsto P(\mathbb{R})$ defined by</p>

<p>$(Tp)(x) = x^2p(x)$</p>

<p>Verify that multiplication by $x^2$ is a linear map.</p>

<p>Additivity: $x^2(p+q) = x^2p+x^2q$</p>

<p>Homogeneity: $x^2(ap) = a(x^2p)$</p>

<p>Is this a correct verification?</p>
",<linear-algebra>
"<p>Let $$V=\{x \in \mathbb{R} : x&gt;0\}$$</p>

<p>For $x,y,a \in \Bbb{R}$, define $x\oplus y=xy$ and $x\odot a=x^a$. Is $V$ a vector space under these operations? Justify your answer.</p>
",<linear-algebra>
"<p>I know that normally for commutators that [A,B]=-[B,A] where A and B are operators.  But under what conditions does [A,B]=[B,A]?</p>
",<linear-algebra>
"<blockquote>
  <p>Can someone give me some examples of unit vectors that's in the same direction as vector, let's say $v=(1,2,-3)^T$ for:</p>
</blockquote>

<p>(i) Euclidean norm</p>

<p>(ii) Weighted norm $||V||^2=2V_1^2+v_2^2+\frac13v_3^2$</p>

<p>(iii) The 1 norm</p>

<p>(iv) The infinite norm</p>

<p>(v) The norm based on their inner product $2v_1w_1-v_1w_1-v_2w_1+2v_2w_2-v_2w_3-v_3w_2+2v_3w_3$.</p>

<p>Ty.</p>
",<linear-algebra>
"<p>How can I show that for a particle in an infinite square well in a stationary state, that the expectation value $\langle[\hat{H},\hat{O}]\rangle=0$ where $\hat{H}$ is the Hamiltonian operator and $\hat{O}$ is an arbitrary operator?</p>
",<linear-algebra>
"<p>I'm studying for an exam and I don't understand how my prof finds the basis for eigenspaces using the matrix representation of a linear map. Once I find an eigevalue then how do I find the basis for its eigenspace. I've attached a screenshot of the part that I don't understand (from an example). Can someone please explain it to me in detail? Thanks<img src=""http://i.stack.imgur.com/78iPC.png"" alt=""![enter image description here"">]<a href=""http://i.stack.imgur.com/78iPC.png"" rel=""nofollow"">1</a></p>
",<linear-algebra>
"<blockquote>
  <p>Even if an isomorphism between two linear spaces $L$ and $M$ over a field $\mathbb{K}$ exists, it is defined uniquely only in two cases:</p>
  
  <ol>
  <li><p>$L=M=\{0\}$ and</p></li>
  <li><p>$L$ and $M$ are one-dimensional, while $\mathbb{K}$ is a field consisting of two elements.</p></li>
  </ol>
</blockquote>

<p>How can I show this fact? Does anyone have any hints?</p>
",<linear-algebra>
"<p>I would like to generate a semi-unitary matrix, i.e., $UU^T=~I$ where U is a non-square matrix whose number of rows is bigger than its number of columns.
I tried it by solving the optimization problem $\min_U\|UU^T - I\|_F$ but didn't work at all since the problem is not convex. </p>
",<linear-algebra>
"<p>I have a question:
Suppose I have a  $n\times n$ matrix:
$$
        \begin{bmatrix}
        1 &amp; 1 &amp;...&amp; 1 \\
        1 &amp; 1 &amp;...&amp;1 \\
        \vdots&amp;\vdots &amp;\ddots &amp; \vdots&amp;\\
        1 &amp; 1 &amp; ...&amp;1 \\
        \end{bmatrix}
$$
,then is there a easy way to compute the eigenvalues of the matrix?</p>

<p>How can I compute this matrix eigenvalue?</p>
",<linear-algebra>
"<p>Find a system of linear equations whose solution set is the line in the 3 dimensional space.</p>

<p>$$  \begin{pmatrix} x \\ y \\ z \\  \end{pmatrix} =\begin{pmatrix} t-4 \\ t-10 \\ 2t-20 \\  \end{pmatrix}$$</p>

<p>What i tried</p>

<p>The question is asking to link the variables $x$ $y$ and $z$ into a single equation while removing the $t$. I first equate $x=t-4$,$y=t-10$ and $z=2t-20$. Then i tried combining the equations to form $$x=y+6$$ and $$z=2x-12$$ and $$z=2y$$. However i could not combine all these 3 variable into a single equation. Could anyone explain. Thanks</p>
",<linear-algebra>
"<p>I have a panel 1200 pixels wide, and am filling in smaller subpanels to fill the length. Each sub-panel is of a different color ($p$ = purple, $g$ = green, etc). It's for a navigation bar on a website, each subpanel corresponds to a link to another page.</p>

<p>In one situation, there's three purple subpanels, a yellow subpanel, two blue subpanels, one each of green and red subpanels; in another situation, there's 3 purple subpanels, a yellow ($e$ for clarification) subpanel, two more purple subpanels, and a green subpanel. The width of each subpanel (irrespective of situation) must add up to 1200px. </p>

<p>Lastly, $e$ is always the middle panel, with some panels to the left or right. The combined widths of the left must equal the combined widths of the right in both situations.</p>

<p>Since all subpanel widths must add up to 1200px irrespective of situation, two equations can be made:</p>

<p>$$\begin{align}
3p + e + 2b + g + r &amp;= 1200 \\
3p + e + 2p + g &amp;= 1200
\end{align}$$</p>

<p>Since $e$ is in the center of the panel, subpanels to the left and right of $e$ in the above equations must equal. This gives me two more:</p>

<p>$$\begin{align}
3p &amp;= 2b + g + r \\
3p &amp;= 2p + g
\end{align}$$</p>

<p>I think I can derive two more equations, but I'm not sure their relevance. Since $e$ is in the middle ($e$'s midpoint is in the middle of the panel), the left and right halves each contain half of $e$:</p>

<p>$$\begin{align}
3p + \frac{1}{2}e &amp;= \frac{1}{2}e + 2b + g + r \\
3p + \frac{1}{2}e &amp;= \frac{1}{2}e + 2p + g
\end{align}$$</p>

<p>Alternatively:</p>

<p>$$\begin{align}
3p + \frac{1}{2}e &amp;= 600 \\
\frac{1}{2}e + 2b + g + r &amp;= 600 \\
\frac{1}{2}e + 2p + g &amp;= 600
\end{align}$$</p>

<p>I want to find <em>any solution</em> given a couple of rules:</p>

<p>$$\begin{align}
p &amp;&gt; 150\\
b &amp;&gt; 100 \\
e &amp;&gt; 90 \\
g &amp;&gt; 80 \\
r &amp;&gt; 60
\end{align}$$</p>

<p>As long as any solution fits those minimum values, things are kosher.</p>

<p>Forming a matrix and bringing to RREF yields three free variables, which is why I'm stumped.</p>

<p>The illustrate the first situation, $3p + e + 2b + g + r = 1200$:</p>

<p><a href=""http://i.stack.imgur.com/hOst4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hOst4.png"" alt=""enter image description here""></a></p>
",<linear-algebra>
"<p>In $P_2 = {ax^2 + bx + c: a,b,c \in\mathbb{R}}$, why do coefficients in $ax^2$ form reduce with coefficients in $bx$ or $c$?</p>

<p>For example, lets look at the set {$x^2 + x - 1, 2x + 1, 2x - 1$}</p>

<p>If we wanted to check to see if its independent, we would rewrite this as </p>

<p>$C_1* (x^2 + x -1) + C_2 * (2x +1) + C_3 * (2x-1) = \vec{0} $</p>

<p>In matrix form this would be,</p>

<p>\begin{bmatrix}
    1 &amp; 0 &amp; 0 \\
    1 &amp; 2 &amp; 2 \\
    -1 &amp; 1 &amp; -1
\end{bmatrix}</p>

<p>When we apply Gaussian Elimination, in my head, when eliminating the $1$ found in the 1st column 2nd row and $-1$ in the 1st column 3rd row, it seems like I am subtracting x with $x^2$ and -1 with $x^2$ respectively. I understand that the first column all belongs to $C_1$. </p>

<p>I understand that we are subtracting $C_1$ from $C_1$ but why can we simply ignore the $x^2$ and $x$. I guess my problem with how this is all set up is that I'm relating it back to high school algebra where you had equations like.</p>

<p>$x + y = 5$</p>

<p>$x -y = 10$ </p>

<p>And you solved for $x$. Why can we ignore the $x$ or $x^2$?</p>

<p>Thanks.</p>
",<linear-algebra>
"<p>Let $A$ and $B$ be two real symmetric matrices in $M_n(\mathbb{R})$. I would like to learn about necessary and sufficient conditions for knowing when $B \in \overline{GL_n(\mathbb{R})\cdot A}$; where:
$$
GL_n(\mathbb{R})\cdot A:=\{(g^{-1})^{T} A g^{-1} : g \in GL_n(\mathbb{R})\},
$$
$(g^{-1})^{T}$ is the transpose of $g^{-1}$ and $\overline{GL_n(\mathbb{R})\cdot A}$ is the closure of $GL_n(\mathbb{R})\cdot A$ with respect to the usual topology of $M_n(\mathbb{R})$.</p>

<p>Help would be appreciated!</p>

<p>Thanks!</p>
",<linear-algebra>
"<blockquote>
  <p>If $V=\mathbb R[x]_k=\{\sum\limits_{i=1}^ka_ix^i:a_i\in\mathbb R, \forall i\}$ is a vector space of dimension $k+1$ over $K=\mathbb R$ and $\mathcal B=\{1,x,\dots,x^k\}$ is a basis of $V$. The dual space of $V$ is the vector space $V^*=\{F:V\to K:F\ \text{is linear}\}$ for all $i$;  $x^{i^*}$ is defined as $x^{i^*}(x^j)=\delta_{ij}=\begin{cases} 1 &amp; \textrm{if i=j}\\0 &amp; \textrm{else}\end{cases}$, let $\mathcal B^*=\{x^{i^*}:i\in\{0,\dots,k\}\}$ and $F:V\to K$ with $p(x)\mapsto\int_0^1 p(x)$ write $F$ as a linear combination of elements of $\mathcal B^*$</p>
</blockquote>

<p>First $\mathcal B^*$ is basis of $V^*$ because $Hom(V,K)\simeq Mat_{1\times(k+1)}(K)$ with $f\mapsto(f(1),\dots f(x^k)$(I can show it is injective and with our chosen $x^{i^*}$ also surjective, and since $x^{i^*}$s are linearly independent it is a basis, but how can I write $F$; </p>

<p>$F=\lambda_01^*+\lambda_1x^*+\dots\lambda_kx^{k^*}$, for examples if $p(x)=x$ then $\int_0^1 p(x)=\frac12$, so $F=\frac12x^*$ and in general $\lambda_k=\frac{1}{k+1}$, am I correct ?</p>
",<linear-algebra>
"<blockquote>
  <p>Given $A_{n\times n},B_{n\times n} \in \mathbb C$ then:</p>
  
  <ol>
  <li><p>if $A$ is unitary and the characteristic polynomial $f_A(x)=f_B(x)$ then $B$ is also unitary. </p></li>
  <li><p>if $A$ is normal and $f_A(x)=f_B(x)$ then $B$ is also normal. </p></li>
  <li><p>if $A$ is unitary and $f_A(x)=f_B(x)$ and $m_A(x)=m_B(x)$ then $A$ is similar to $B$. </p></li>
  </ol>
</blockquote>

<p>(1) if the matrix is unitary then the eigenvalues are $\pm 1$ but why would having eigenvalues that are  $\pm 1$ mean that the matrix is unitary? So that's probably false.</p>

<p>(2) similar reasoning, why would the normal property carry over because of the same eigenvalues? Probably false.</p>

<p>(3) since $A$ is diagonalizable because its unitary, then it has a similar diagonal matrix, and since $B$ has the same diagonal matrix then it has to be similar to it? How does the minimal polynomial help here?</p>
",<linear-algebra>
"<p>I am going through Linear Algebra right now, we are using the book Elementary Linear Algebra by Andrilli. In one of the theorems he uses this notation without really introducing it. Here is the theorem:</p>

<p>Theorem 2.3 Let <strong>AX</strong>=<strong>B</strong> be a system of linear equations. If [<strong>C</strong>|<strong>D</strong>] is row equivalent to [<strong>A</strong>|<strong>B</strong>], then the system <strong>CX</strong>=<strong>D</strong> is equivalent to <strong>AX</strong>=<strong>B</strong>.</p>

<p>I just don't know how to read the [C|D] notation there. I want to say C divides D because that's the symbol for integer division, but I know that's not right.</p>

<p>Thanks!</p>
",<linear-algebra>
"<p>The notion of <a href=""https://en.wikipedia.org/wiki/Linear_independence""><em>linear independence</em></a> is very well-known and well-understood.</p>

<p>However, is there a way to generalize the definition to other types of independence -- such as perhaps ""quadratic independence"", ""polynomial independence"", ""harmonic independence"", etc.?</p>

<p>(Sorry if <a href=""/questions/tagged/linear-algebra"" class=""post-tag"" title=""show questions tagged &#39;linear-algebra&#39;"" rel=""tag"">linear-algebra</a> isn't a good tag; I couldn't think of a better one.)</p>
",<linear-algebra>
"<p>Could anyone help me with this proof without using determinant? I tried two ways. </p>

<blockquote>
  <p>Let $A$ be a matrix. If $A$ has the property that each row sums to zero, then there does not exist any matrix $X$ such that $AX=I$, where $I$ denotes the identity matrix. </p>
</blockquote>

<p>I then get stuck. The other way was to prove by contradiction, and I failed too. </p>
",<linear-algebra>
"<p>Let $A$ be a $n\times n$ matrix. Choose the correct option.</p>

<p>a) if $A^2 =0$ then $A$ is diagonalisable.</p>

<p>b) if $A^2 =I$ then $A$ is diagonalisable.</p>

<p>c) if $A^2 - A =0$ then $A$ is diagonalisable.</p>

<p>Now... for a) I tried by transforming them using minimal polynomial (cayley Hamilton) so it should satisfy $t^2 =0$ hence the eigen values are $0$ (twice) hence its not convertible and not diagonalisable... similar arguments for b) and c).</p>

<p>But I am not sure whether I am right... please guide me..</p>
",<linear-algebra>
"<p>How to find a matrix $A$ over $\mathbb{R}$ such that $A^2-5A+6I=0$ is diagonalisable??</p>

<p>I tried by considering $A$ a particular matrix and putting the values in the given equation but it gives nine equations in nine variables and those too very complex .. is there any other way ??</p>
",<linear-algebra>
"<p>I am quite confused at the notation used for $J$ in the following question: </p>

<blockquote>
  <p>For a normed space $\Omega$, let $\Omega^{**}$ denote the dual space of the dual space. Let $J: \Omega \rightarrow \Omega^{**}$ be defined by $\langle x^*, J(x)\rangle   =  \langle x, x^*\rangle  = x^*(x)$. Show that $J$ is a linear isometry.</p>
</blockquote>

<p>Where, for $x^* \in \Omega$, $\langle x, x^*\rangle = x^*(x)$. </p>

<p>I am confused on how to decipher the notation for $J$. I am normally use to things like $J(x) = $ fill in the blank. Could someone please explain this notation? </p>
",<linear-algebra>
"<p>I have here a linear transformation $T : P_3(\mathbb{R})\rightarrow P_3(\mathbb{R}) $ defined by:</p>

<blockquote>
  <p>$ T(at^3 + bt^2 + ct + d) = (a-b)t^3 + (c-d)t $</p>
</blockquote>

<p>I'm very very new in this subject and I'm not going well with polynomials. I need find the $ Kernel $ and the $ Image $ of the transformation. Look what I've been thinking:</p>

<blockquote>
  <p>$Ker(T) =  \{ T(p) = 0 / p \in P_3\} $</p>
  
  <p>$ T(at^3 + bt^2 + ct + d) = (a-b)t^3 + (c-d)t = 0 $</p>
  
  <p>$(a-b) = 0 \ ;\ \ (c-d) = 0 \ ;\ \ a = b \ ; \ \ c = d $</p>
  
  <p>$ Ker(T) = \{ at^3 + at^2 + ct +c\ /\ a,c \in \mathbb{R} \} $</p>
</blockquote>

<p>And what about the $ Image $? I know that $Im(T) = \{ T(p) / p \in P_3 \}$, but how can I show it? And how can I test if a polynomial such as $ p(t) = t^3 + t^2 + t -1 \in Im(t)$?</p>
",<linear-algebra>
"<p>Let $T: V\rightarrow W$ be a linear mapping. Let $M$ be a linear subspace such that $M \subset ker(T)$. Let $Q$ be the quotient mapping  $Q:V \rightarrow V/M$ then I have to show there is unique mapping $S: V/M \rightarrow W$ such that $T=SQ$. </p>

<p>Thoughts: Is it a reasonable choice to choose $S(x+M) = T(x+m) = T(x)$ for any coset $x+M$ and any $m \in M$ since $M \subset ker (T)$ or have I drifted down the wrong path.</p>
",<linear-algebra>
"<p>Let $A\in\{0,1\}^{m\times n}$ where $m \gg n$. Take this matrix to be over $\mathbb{R}^{m\times n}$ (not the binary field). What is the probability that said matrix will have full rank? Is there some condition on the difference between $m$ and $n$ so that the probability approaches $1$? </p>
",<linear-algebra>
"<p>Assume $A$ is a $m \times n$ matrix. We want to see whether the linear system $Ax=b$ has any solution for $x$ given $b$. One way to check this is:</p>

<p>""This linear system of equation has a solution if the b is contained in the column space of A.""</p>

<p>1- Does anybody know a good reference for this?</p>

<p>2- How can we check that b is within the column space of A using projection? Any reference for this method?</p>
",<linear-algebra>
"<p>I was given as an assignment to diagonalize the following matrix:</p>

<blockquote>
  <p>$\left(\begin{array}{cc}
\cos\theta &amp; -\sin\theta\\
\sin\theta &amp; \cos\theta
\end{array}\right)$ </p>
</blockquote>

<p>I started by finding the eigenvectors and got:</p>

<blockquote>
  <p>$v_{1}=\left(\begin{array}{c}
1\\
-i
\end{array}\right)$,  $v_{2}=\left(\begin{array}{c}
1\\
i
\end{array}\right)$</p>
</blockquote>

<p>then I normalized the vectors and composed a unitary matrix:</p>

<blockquote>
  <p>$U=(v_1|v_2)=\frac{1}{\sqrt{2}}\left(\begin{array}{cc}
1 &amp; 1\\
-i &amp; i
\end{array}\right)$</p>
</blockquote>

<p>The problem is in the final step:</p>

<blockquote>
  <p>$U^{*}AU=\left(\frac{1}{\sqrt{2}}\left(\begin{array}{cc}
1 &amp; i\\
1 &amp; -i
\end{array}\right)\right)\left(\begin{array}{cc}
\cos\theta &amp; -\sin\theta\\
\sin\theta &amp; \cos\theta
\end{array}\right)\left(\frac{1}{\sqrt{2}}\left(\begin{array}{cc}
1 &amp; 1\\
-i &amp; i
\end{array}\right)\right)$</p>
</blockquote>

<p>This doesn't produce a diagonal matrix.</p>

<p>Is there any mistake in these stages?</p>

<p>Many thanks.</p>
",<linear-algebra>
"<p><strong>Question:</strong> 
Let V be the vector space of all functions $\Bbb R\to \Bbb R$.
Show that $V=U \oplus W$
for $U=${$f | f(x)=f(-x) \forall x$}$, $W={$f | f(x)=-f(-x) \forall x$}</p>

<p><strong>What I did</strong>:</p>

<p>I did prove that $U \cap W$={$0$}. But proving that any function from R to R can be displayed as a sum of odds and evens wasn't a success. I tried saying that for $v \in V, w \in W: v=v-w+w$ and proving that $v-w \in U$ but that didn't work (That trick worked with some linear transformations we saw, but this isn't a linear transformation).</p>
",<linear-algebra>
"<p>I'm not clear about the sum(and direct sum) operator on subspaces, how to use them? please offer me some applications about this operation, really really appreciate it.</p>
",<linear-algebra>
"<p>Let $V$ be the space of $n \times 1$ matrices over $F$ and let $W$ be the space of all $m \times 1$ matrices over $F$. Let $A$ be a fixed $m \times n$ matrix over $F$ and $T$ be a linear transformation $T : V\to W$ defined by $T(X)=AX$. Prove that $T$ is the zero transformation if and only if $A$ is the zero matrix.</p>

<p><strong>My try</strong></p>

<ol>
<li>I am thinking that if I set the matrix $A=0$, that is, the $0$ matrix, then the $0$ matrix times any other matrix will be zero. I don't know whether this is right or not. Can someone please check it?</li>
<li>What if I had to prove the converse of this? How am I supposed to prove that?</li>
</ol>
",<linear-algebra>
"<p>The $U_e$ and $U_o$ denote the set of all real-valued even/odd function on $\mathbb R$ respectively.</p>
",<linear-algebra>
"<p>I'm interested in algorithms to compute matrix multiplications. Is the Coppersmith-Winograd algorithm similar to the Strassen algorithm ?</p>

<p>I have two other questions:</p>

<p>1) Are the multiplications done at the end of the recursion, like the Strassen one ?</p>

<p>2) The $O(n^{2.38})$ refers to the number of multiplications or to any basic operations (additions...) ?</p>

<p>Thank you</p>
",<linear-algebra>
"<p>\begin{array}{rrrrr|r}
    b &amp; a &amp; a &amp; \cdot \cdot \cdot &amp; a \\
    a &amp; b &amp; a &amp; \cdot \cdot \cdot &amp; a \\
    a &amp; a &amp; b &amp; \cdot \cdot \cdot &amp; a \\
    \cdot &amp; \cdot &amp; \cdot &amp; \space &amp; \cdot\\
\cdot &amp; \cdot &amp; \cdot &amp; \space &amp; \cdot\\
a &amp; a &amp; a &amp; \cdot \cdot  \cdot &amp; b
  \end{array}</p>

<p>I have the above matrix $A\in M_{n\times n}(F)$ where $F$ is a field and $n\geq1$, $a,b\in F$.</p>

<p>I'm trying to find out how to use row operations to make it into an upper triangular matrix in order to figure out the determinant. But I'm not sure how I would approach it.</p>
",<linear-algebra>
"<p>Page 2 (506), line 18 of
<a href=""http://www-personal.umich.edu/~orosz/articles/NonlinScipublished.pdf"" rel=""nofollow"">http://www-personal.umich.edu/~orosz/articles/NonlinScipublished.pdf</a></p>

<p>says that ""The presence of translational symmetry in the nonlinear equations gives rise to a relevant zero eigenvalue in the linearized system at any of the trivial solutions"".</p>

<p>It looks like a general statement, but I don't see why (is it trivial?). Where can I find a precise statement about this and on what conditions does that hold? (I'd like to know the proof too.) Thank you.</p>
",<linear-algebra>
"<p>Let $A$ be a Hermitian matrix of size $n$ such that $A^5+A^3+A=3I_n$ , then is it true that $A=I_n$ ? What I got is if $a$ is an eigenvalue then $a^5+a^3+a-3=0=(a-1)(a^4+a^3+2a^2+2a+3)$ this doesn't seem to get anywhere , Please help . </p>
",<linear-algebra>
"<p><strong>Given data in the problem</strong> </p>

<ol>
<li>${\psi'(t)}_{3 \times 3}=A_{3 \times 3}\psi(t)_{3 \times 3}, \psi(0)_{3 \times 3}=R^{cl}_{3 \times 3}  \\
\phi'(t)_{3 \times 3}=t\hspace{.1cm}B_{3 \times 3} \phi(t)_{3 \times 3},\phi(0)_{3 \times 3}=R^{cl}_{3 \times 3}   \tag 1$</li>
<li>$A,B ,R^{cl}$ are constant matrices. A,B are skew symmetric matrices. $R^{cl}$ is a rotation matrix</li>
<li>We know the solutions of  equation (1). Implies we know about what is  $\psi(t)=e^{At}R^{cl}_{3 \times 3},\phi(t)=e^{B\frac{t^2}{2}}R^{cl}_{3 \times 3} \tag 2 $</li>
</ol>

<p><strong>Question</strong></p>

<p>What is the  the solution of $ R'(t)=AR(t)+tBR(t)\tag 3$ in closed form?</p>
",<linear-algebra>
"<p>If $T:\mathbb C^n \to \mathbb C^n$ is a linear transform such that $\ker(T-aI)=\ker(T-aI)^n , \forall a\in \mathbb C$ , then is $T$ diagonalizable ? </p>
",<linear-algebra>
"<p>Consider $\mathbb{R}^3$ with the standard inner product. Let $W$ be the plane spanned by $(1,1,1)$ and $(1,1,-2)$. Let $U$ be the linear operator defined as: $U$ is rotation through the angle $\theta$, about the straight line through the origin which is orthogonal to $W$. How to find the matrix of $U$ in the standard ordered basis.</p>
",<linear-algebra>
"<p>I have taken a Quadratic form and performed simultaneous row and column operations on it.  I started with $$A = \begin{pmatrix} 1 &amp; 2 &amp; 1\\2 &amp; 3 &amp; 4\\ 1 &amp; 4 &amp; 5\end{pmatrix}$$ and diagonalized to $$D = \begin{pmatrix} 1 &amp; 0 &amp; 0\\0 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; 8\end{pmatrix} S = \begin{pmatrix} 1 &amp; 0 &amp; 0\\-2 &amp; 1 &amp; 0 \\ -5 &amp; 2 &amp; 1\end{pmatrix}$$ where $SAS^T = D$.  How do I find the new basis for the diagonalized version of A?</p>

<p>EDIT:  I typed A incorrectly.</p>
",<linear-algebra>
"<p>I am currently working on a Computer Algebra System and was wondering for suggestions on methods of finding roots/factors of polynomials. I am currently using the Numerical Durand-Kerner method but was wondering if there are any good non-numerical methods (primarily for simplifying fractions etc).</p>

<p>Ideally this should work for equations in multiple variables.</p>
",<linear-algebra>
"<p>I understand what the Hodge dual is, but I can't quite wrap my head around the dual space of vector space. They seem very similar, almost the same, but perhaps they are unrelated. </p>

<p>For instance, in R^3, the blade a^b gives you a subspace that's like a plane, and the dual is roughly the normal to the plane.</p>

<p>Is there a similarly simple example for the dual space of a vector space, or is there a way to describe the vector space dual in terms of the Hodge dual?</p>
",<linear-algebra>
"<p>I have a simple linear operator:</p>

<p>$$\begin{align}g: \Bbb{R^4} &amp;\to \Bbb{R^3}\\g (x, y, u , v) &amp;= ( x + u, x + v, y + u)\end{align}$$</p>

<blockquote>
  <p>How would I determine the image of this linear operator?</p>
</blockquote>

<p>I thought that putting in the vectors</p>

<blockquote>
  <p>$(0, 0, 0, 1) \implies \dots$<br>
  $(0, 0, 1, 0) \implies \dots$<br>
  $\;\;\;\vdots$</p>
</blockquote>

<p>would yield the $4$ vectors of the image.</p>

<p>But the solution says there are only $3$ vectors in the image of this operator.</p>

<p>What am I missing?</p>
",<linear-algebra>
"<p>First I'll define what I talk about:</p>

<p>A <strong>bilinear form</strong> on a vector space V is a mapping:</p>

<p>$F: V \times V \rightarrow \mathbb{R}, (a,b) \mapsto F(a,b)$</p>

<p>which is linear in every argument:</p>

<p>$a, b, c \in V$ and $\lambda, \mu \in \mathbb{R}$:</p>

<ul>
<li>$F(\lambda a + \mu b, c) = \lambda F(a, c) + \mu F(b, c)$</li>
<li>$F(a, \lambda b + \mu c) = \lambda F(a, b) + \mu F(a, c)$</li>
</ul>

<p>If I get an expression which could be a bilinear form, I check those two. This can be quite long.</p>

<p>A bilinear form F is <strong>symmetric</strong>, if:</p>

<p>$\forall a, b \in V: F(a, b) = F(b, a)$</p>

<p>Now my <strong>question</strong>:
If I know that a mapping is symmetric, can I make the checks for bilinearity shorter? Something like that:</p>

<p>$F(\lambda a + b, c) = \lambda F(a, c) + F(b, c)$?</p>

<p>If it is not possible, do you have counterexamples where it doesn't work?</p>
",<linear-algebra>
"<p>Let $V$ a vector space and $W$ be its linear subspace. Give an example of a linear map that satisfies $\mathrm{im}(f)=W$ and $\ker(f) \oplus \mathrm{im}(f)=V$, but $f^2 \neq f$.</p>

<p>Would $f(v)=2v$ be the right example? Since the kernel of it is $0$ and the map itself is surjective so the condition $\ker(f)\oplus \mathrm{im}(f)=V$ satisfied, also $W=V$ in this case and $f^2 \neq f$ is also satisfied.</p>

<p>A next question is once I restrics $f$ to $W$ then will it be always isomorohism. My approach was that the map will be surely surjective, since $f:W\rightarrow W$ and $W=\mathrm{im}(f)$. But the injectivity I check by using the given $\ker(f)\oplus \mathrm{im}(f)=V$. Can someone help me on that. I am stuck. Please.</p>
",<linear-algebra>
"<p>I want to show the following:</p>

<p>Let $A,B \subseteq \mathbb{R}^n$ disjoint, nonempty, closed and convex sets. Then there exists a $h \in \mathbb{R}^n$, such that $A$ and $B$ gets separated in the following way:
$$
 \langle b, h \rangle \le \langle a, h \rangle \quad \forall a \in A, b \in B.
$$
I have the following proof: Consider $C := B - A$, which is convex too. Because $A$ and $B$ are disjoint, it must be that $0 \notin C$. (*) Then there exists a $h$ such that $\langle c, h \rangle \le 0$ for all $c \in C$ or $\langle c, h \rangle \ge 0$ for all $c \in C$. WLOG let $\langle c, h \rangle \le 0$, then
$\langle b - a, h \rangle \le 0$, which means $\langle b, h \rangle - \langle a, h \rangle \le 0$, i.e. $$\langle b, h \rangle \le \langle a, h \rangle.$$</p>

<p>But (*) uses the fact that: For every convex set $X$ and a point $u \notin X$, there exists a $h$ such that $\langle u, h \rangle = 0$ and $\langle x, h \rangle \le 0$  for all $x \in X$ or $\langle x, h \rangle \ge 0$  for all $x \in X$.</p>

<p>Which I feel is geometrically true because the Elements $h$ could be identified with hyperplanes, but I am not sure how to proof this?</p>
",<linear-algebra>
"<p>Show that the set of functions $1,x,x^2,x^3...x^n...$ is linearly independent on any interval $[a,b]$.</p>

<p>If $$c_1+xc_2+x^2c_3+x^3c_4...=0$$ we should show $$c_i=0,\quad i=1,2, \ldots$$
how could I start?</p>

<p>My second question: is it linearly independent on $C[0,1]$?</p>
",<linear-algebra>
"<p>Suppose I have an infinite set $U$ and let $M$ be the linear subspace of all real-valued functions $\nu$ on $2^U$ such that $\nu(\emptyset) = 0$. Here the sum of two such functions (and the product of such a function by a scalar) is taken simply; i.e. pointwise. </p>

<p>Now fix a member $\bar{\nu} \in M$. I am interested in linear transformations $T_{\bar{\nu}}$ of $M$ into itself of the following form: $$(T_{\bar{\nu}}\nu)(S) = \bar{\nu}(S)(\nu(S)), \quad S \subset U.$$ I would like to know if these types of transformations (or equivalent) have been studied before and, if so, under what name. Note I am not requiring that $\bar{\nu}$ be a measure, or even an outer measure, but would be willing to start there.</p>

<p>Thanks in advance for any pointers.</p>
",<linear-algebra>
"<p>I am taking linear algebra, and have learned about the vector dot product and cross product.  Is there a vector product defined by :
    $(a_1, a_2, \dots ,a_n)\times (b_1, b_2, \dots,b_n) = (a_1b_1, a_2b_2, \dots ,a_nb_n)$ ?
If so, what it it called?</p>
",<linear-algebra>
"<p>Suppose $v_1, \dots v_m$ is a linearly independent list in V. Show that there exists $w \in V$ such that $\langle w, v_j \rangle &gt; 0$ for all $j \in {1, \dots ,m}$.</p>

<p>I understand this question is saying given a linearly independent list, there is $w \in V$ such that the vector $w$ is not orthogonal to any $v$ in that linearly independent set. I'm also confused as to why it is significant that the inner product be greater than zero and instead of just $\neq 0$. Can someone give me a hint on how to do this problem?</p>

<p>I know that $\langle v, v \rangle &gt;0$ for all $v$ not equal to zero, and since $v_1, \dots v_m$ is linearly independent, then none of the $v_j$ will be zero, but it is impossible to have w equal to all $v_j$?</p>
",<linear-algebra>
