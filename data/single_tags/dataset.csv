"<p>Suppose that $f(x) \leq g(x) \text{ for all }x$. Prove that $\lim_{x\to a} f(x) \leq \lim_{x \to a} g(x)$ provided these limits exist.</p>

<p>Attempt: I am not even sure how to start this question.  I had made an attempt but it was getting nowhere when I did it. I found a solution to the question, but I still do not understand what is going on:</p>

<p>Suppose $$ l = \lim_{x\to a}f(x) \geq \lim_{x\to a}g(x) = m$$. Let $$\epsilon = l - m &gt; 0$$ </p>

<p>Then  there is $$\delta &gt; 0 \text{ such that if } 0 &lt;|x-a|&lt;\delta \Rightarrow |l-f(x)|&lt; \frac{\epsilon}{2} \text{ and } |m-g(x)| &lt; \frac{\epsilon}{2}$$ </p>

<p>Thus for $$0 &lt;|x-a|&lt;\delta \text{ we have } g(x) &lt; m + \frac{\epsilon}{2} = l - \frac{\epsilon}{2} &lt; f(x)$$ </p>

<p>Contradicting the hypothesis.</p>

<p>My problem is I do not see clearly what the hypothesis is and of equal importance I don't see how this proof accomplishes the objectives of the question.</p>
",<calculus>
"<p>My attempt: I wrote the equation of a circle for x belongs to [0,xo]. But I had no clue about what should I do for x belongs to (xo,infinity).</p>

<p><a href=""http://i.stack.imgur.com/IMabt.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/IMabt.jpg"" alt=""""></a></p>
",<calculus>
"<p>The question is:
Show that $y = 11x + 5$ is a tangent to curve $y = 3x^2 + 5x + 8$.</p>

<p>I have no clue about how to go about figuring this out. 
Should I graph both curves? Or should I use a certain formula?</p>
",<calculus>
"<p><a href=""http://i.stack.imgur.com/RSKkW.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/RSKkW.png"" alt=""enter image description here""></a></p>

<p>I'm a little confused on how to go about approximating the function (Or part 2 of the 3 part question as shown in the image). Would I use the polynomial I got from part a but substitution a for x? Or would I use the formula $cos(0) - sin(0)(x-0) - \frac{cos(0)}{2!}(x-0)^2 + \frac{sin(0)}{3!}(x-0)^3 + \frac{cos(0)}{4!}(x-0)^4$? If I were to use the second formula, how would I go about simplifying the ratio using the formula? Any kind of guidance on this would be greatly appreciated.</p>
",<calculus>
"<p>This is the integral
$$\int\frac{dx}{(x+1)(n-x)}=\int kdt$$
I just need some assistance on how to begin the left side integral and I will most likely be able to continue it from there thank you.</p>
",<calculus>
"<p>I have a function of $3$ variables which are all functions of $t$.</p>

<p>$$x = \frac{v_1t-y}{\sqrt{(v_2/\dot{x})^2 -1}} \tag 1 $$ </p>

<p>In the equation $v_1,v_2$ are constant and $x$ and $y$ are both function of $t$ (also $\dot{x}$ is $\frac{dx}{dt}$). I am trying to differentiate $(1)$ with respect to $t$, but I am not sure how to do this as there are three variables and an $\dot{x}$ already. I tried holding $x,y$ constant but that instinctively does not make sense as these change with respect to $t$.</p>
",<calculus>
"<p>Consider the sequence $a_{n+2}=f(a_1,a_2)$ where $f(x,y)$ is the mean of $x, y$ (geometric/arithmetic/harmonic) and $a_1,a_2$ are positive real numbers.</p>

<p>In detail: </p>

<p>Geometric - $a_{n+2}=\sqrt{a_{n+1}a_n}$</p>

<p>Arithmetic - $a_{n+2}={1 \over 2}(a_{n+1}+a_n)$</p>

<p>Harmonic - $a_{n+2}={2 \over {a_{n+1}^{-1}+a_n}^{-1}}$</p>

<p>Now, it's easy to show that each sequence converges (though if you have an interesting method to show that, share it with us!), but the trick is to calculate the limit. Here I give my way to compute these limits and challenge you to do so for a recursive sequence that each step takes the mean of its three or more previous element, namely $a_{n+N}=f(a_{n+N-1},a_{n+N-2},...,a_{n})$.</p>
",<calculus>
"<p>I just finished reading the Wikipedia article on the <a href=""http://en.wikipedia.org/wiki/Cauchy%27s_condensation_test"" rel=""nofollow"">Cauchy condensation test</a>. I understand the trapezoidal view, but apparently ""the 'condensation' of terms is analogous to a substitution of an exponential function"". Can anyone explain what is meant by this?</p>
",<calculus>
"<p>For a given $k>0$ constant, assuming that $x,y>0$. Also this equation has a particular name, or some mathematician associated with it?</p>
",<calculus>
"<p>What is an upper bound on $e^{-W_{-1}(c_1)}$ and $e^{-W(c_1)}$, where $W$ is the Lambert W function?</p>
",<calculus>
"<p>Let $h : \mathbb{R}^2 \rightarrow \mathbb{R}$ be a function defined by $$h(x,y) =\begin{cases} x^2 + y^2 &amp; : (x,y) \in \mathbb{Q} \times \mathbb{Q} \\[1ex] 0 &amp; :  \mbox{otherwise}\end{cases}$$</p>

<p>Show that $h$ is continuous only at $(0,0)$, and differentiable there. </p>

<p>I can show the continuity of $h$ at $(0,0)$. Also, I can show the discontinuity of $h$ at rational pair which is not $(0,0)$. However, I cannot show the discontinuity of other points. Also, the differentiation. Could anyone give a hint ?</p>
",<calculus>
"<p>Suppose you have two infinite sequences $\{a_n\}, \{b_n\}$, with $0 &lt; a_n &lt; b_n$ for each $n$, such that $b_n \to 0$ as $n \to \infty$. Does there exist a sequence $\{s_n\}$ with $s_n \to 1$ as $n \to \infty$ such that $a_n s_n \geq b_n$ for $n$ large enough? Thank you. </p>
",<calculus>
"<p>I am asked to prove this statement $^{*}$. I am trying now, but it is getting to small and tiny steps that I even loose my way. my steps are as follows: 
$$\lim_{n\rightarrow \infty}(\sqrt[3]{n+\sqrt{n}}-\sqrt[3]{n})=0^{*}$$ </p>

<p>$\lim_{n\rightarrow \infty}(\sqrt[3]{n+\sqrt{n}}-\sqrt[3]{n})=\dfrac{(\sqrt[3]{n+\sqrt{n}}-\sqrt[3]{n}) \cdot (\sqrt[3]{n+\sqrt{n}}+\sqrt[3]{n})}{(\sqrt[3]{n+\sqrt{n}}+\sqrt[3]{n})}=\dfrac{(\sqrt[3]{n+\sqrt{n}})^2-(\sqrt[3]{n})^2}{(\sqrt[3]{n+\sqrt{n}}+\sqrt[3]{n})}=\dfrac{\sqrt[3]{n^2+2n\sqrt{n}+n}}{\sqrt[3]{n+\sqrt{n}}+\sqrt[3]{n}}=\dfrac{(n+\sqrt{n})^{\frac{2}{3}}-n^{\frac{2}{3}}}{(n+\sqrt{n})^{\frac{1}{3}}+n^{\frac{1}{3}}}= .. help = 0$ $$if \quad n\rightarrow \infty$$</p>
",<calculus>
"<p>I am given this statement and I need to prove it. the statement is for all $n\ge 1$: $$\sqrt[n]{n}\leq 1+\frac{2}{\sqrt{n}}$$</p>

<p>I am trying to prove with induction. But I am stuck for step n=k+1, how can I then decompose the step:  $\sqrt[n+1]{n+1}$? </p>

<p>Thanks for help in advance</p>
",<calculus>
"<p>In Thomas's calculus $10^{th}$ edition, chapter $5$, exercises $5.3$, problem $9$, I am asked to find the length of an arc with the equation:</p>

<p>$$x=\int^y_0\sqrt{\sec^4(t)-1} dt$$</p>

<p>and $\displaystyle {\frac{-\pi}{4}} \leq y \leq {\frac{\pi}{4}}$.</p>

<p>The problem is, I'm not entirely sure what the equation is supposed to be... The previous problems were relatively straight forward, with simple $x=f(y)$ equations. The second part, the parameters of $y$, is confusing me-- is this a parametric of some sort (considering the ""$t$"")? I've looked around the chapter and can't seem to find another example like it...</p>

<p>If someone could point me in the right direction, that'd be really helpful.</p>
",<calculus>
"<p>For the integral: $$\int_{-1}^{5} \left( x^{2} -4 \right) dx$$</p>

<p>My calculations:</p>

<p>$$\begin{align*}\Delta x &amp;= \frac6n\\\\
x_i &amp;= -1 + \frac{6i}n\\\\
f(x_i) &amp;= 1 + \frac{36i^2}{n^2} -4\\\\
A&amp;=72
\end{align*}$$</p>

<p>I'm unsure if this is correct as it is my first attempt at doing this type of problem.</p>
",<calculus>
"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://math.stackexchange.com/questions/141987/is-my-riemann-sum-correct"">Is my Riemann Sum correct?</a>  </p>
</blockquote>



<p>This is my second attempt, the answer seems rather odd so I thought I would have it checked as well.
For the integral: $$\int_{-5}^{2} \left( x^{2} -4 \right) dx$$</p>

<p>My calculations:</p>

<p>$$\begin{align*}\Delta x &amp;= \frac7n\\\\
x_i &amp;= -5 + \frac{7i}n\\\\
f(x_i) &amp;= 21 - \frac{70i}{n} + \frac{49i^2}{n^2} \\\\
A&amp;=-738
\end{align*}$$</p>
",<calculus>
"<blockquote>
  <p>A viscous liquid is poured on to a flat surface. It forms a circular patch whose area grows at a steady rate of $5 \text{ cm}^2/s$. Find in terms of $π$,</p>
  
  <p>(a) the radius of the patch 20 seconds after pouring has commenced</p>
  
  <p>(b) the rate of increase of the radius at this instant.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/F37Oz.jpg"" alt=""enter image description here""></p>
",<calculus>
"<p>Can someone walk me through how to do the following problem so I can attempt a few more practice problems?</p>

<p>If:
$$\int_{1}^{5} f(x) dx = 12  $$
and
$$\int_{4}^{5} f( x) dx = 3.6$$
find:
$$\int_{1}^{4} f( x) dx$$</p>

<p>Would it simply be $12 - 3.6$ ?</p>

<hr>

<p><strong>EDIT</strong></p>

<p>If:
$$\int_{0}^{9} f(x) dx = 37  $$
and
$$\int_{0}^{9} g( x) dx = 16$$
find:
$$\int_{0}^{9} 2f(x)+3g(x) dx$$</p>

<p>Would this simply be: $2 \times 37 + 3 \times 16$?</p>
",<calculus>
"<p>Let $$f(x) = x\cos\frac{1}{x}$$ for x in $[1,\infty)$</p>

<p>Now I need to prove or disprove the difference $$f(x+2) - f(x) &gt; 2$$ for all $x$ in the domain.</p>

<p>I tried a lot but I don't seem to be getting anywhere. My approach was to try and use graphs, but somehow that doesn't seem to work out that well.</p>

<p>Any ideas or suggestions please?</p>
",<calculus>
"<p>I've try solve this question, but I haven't sucess...</p>

<p>The problem is the following:</p>

<p>A continuous functions $f:[a,b]\rightarrow \mathbb{R}$ assume positive and negative values in its domain, show that there exists $a_1,a_2,\ldots,a_k$, k numbers that are arithmetic progression and it is in the domain such that
$$f(a_1)+f(a_2)+\cdots+f(a_k)=0$$</p>

<p>Someone can help me with this question?</p>
",<calculus>
"<p>Can you help me prove this identity: $\tan x\sin x = \sec x-\csc x$?
I began working on the left side, by first changing $\tan x$ to $\sin x/\cos x$ and then multiplying by $\sin x$, I can get it down to $\sec x - \cos x$ but I can't figure out how to change the $\cos x$ to $\csc x$.</p>
",<calculus>
"<p>Can you help me prove the identity $$\tan x\sec x= \sin^3x \sec^2x +\sin x$$
I began working on the left side and changed everything to sine and cosine terms.</p>
",<calculus>
"<p>I am working on a differentiation question and my answer to simplify is </p>

<p>$2x\ln \left(2x-1\right)+\frac{2}{2x-1}x^2$</p>

<p>Why cant I cancel out a x when multiplying to get 
$\frac{2x}{2-1}$= $2x$ ?</p>

<p>So my final answer would be 
$2x(\ln \left(2x-1\right)+1)$</p>

<p>Instead they factorize to get this answer
$2x\left(\frac{x}{2x-1}+\ln \left(2x-1\right)\right)$</p>

<p>I can recognize how they arrive at that final answer but is my answer wrong?</p>
",<calculus>
"<p>As h approaches $0$, show $\sin(x+h)=\sin(x)+h\cos(x)+o(h)$. What I've done is basically substituted $h=0$ and therefore $LHS=RHS$, but I realise I'm supposed to use limits and I somehow can't get rid of the denominator $h$ everywhere.</p>
",<calculus>
"<p>I just can't seem to get this one. I know the process at least what we learned is to:</p>

<p>Get all the y's on one side and the x's on another
Integrate each side
Solve for some $C$ given a $x$ and $y$ value.
I just want to know how to set up the integration.</p>

<p>$$\frac{dy}{dx} - x * e^y = 2 * e^y$$</p>
",<calculus>
"<p>What approach would be ideal in finding if and where $f(x)$ is continuous and/or differentiable when $f$ is a <strong>piecewise</strong> defined function?  </p>

<p>A concrete example is below, but I'm interested in <em>general strategy</em>, illustrated on this function.</p>

<p>$$
f(x) = \begin{cases}     \ln(x) &amp; : x &gt; e\\
    x/e &amp; : x \leq e
  \end{cases}
$$</p>
",<calculus>
"<p>How can I solve this limit.  (Here $x$ belongs to natural numbers $\Bbb{N}$.) 
$$
\lim_{x\to\infty} \dfrac{5\cdot5^x+3^x-4^x}{5^x +2^x+27\cdot9^x}$$</p>

<p>My try:  I tried using L'Hospital, expansions of all terms using Taylor's series, and did work out.  I was just thinking if there is any simpler method?</p>
",<calculus>
"<p>Given:</p>

<p><img src=""http://i.stack.imgur.com/5pSa7.jpg"" alt=""enter image description here""></p>

<p>How can I show that:</p>

<p>1) If $ \beta &gt;0$ , then the sequence is increasing ? </p>

<p>2) The sequence converges if and only if $0&lt;\beta \leq 1 $ </p>

<p>I tried estimating $ a_{n+1} / a_n$ , but without any success. Induction might help , but I can't prove that $a_1 &lt; a_2 $ . </p>

<p>Will you please help ? </p>

<p>Thanks in advance </p>
",<calculus>
"<p>I have determined a tangent plane to be
$$z = a(-b+y) + x(b-1)$$
$$ab = x(b-1) + ay - z$$
At the point (a,b)</p>

<p>I want to determine the normal to this plane as a function of a and b. I am not entirely sure as to what this means ""As a function of a and b"".</p>

<p>So I assume I will find the normal using the dot product of plane to normal = 0.
$$(b-1, a,-1) * (\zeta,\beta,\gamma) = 0$$
$$\zeta(b-1) + \beta*a - \gamma = 0$$
$$-\frac{\zeta(b-1) + \gamma,}\beta = a$$
$$\frac{a\beta - \gamma,}\zeta + 1 = b$$</p>

<p>I am not sure if this is what is wanted, or if there is a better way to solve this. If anyone can shed some light, that would be appreciated. This is from a past exam 2012S2 UQ, for course 'Multivariate Calculus and Ordinary Differential Equations'.</p>
",<calculus>
"<p><img src=""http://i.stack.imgur.com/nJHyr.png"" alt=""Question""></p>

<p>Really have no idea what to do. I have tried letting z = 0 = (e^iz + e^-iz)/2i but all that does is cos z = pi/2. Doesn't give me a formula for cos^-1 z and doesn't help with part (b).</p>
",<calculus>
"<p>Given $f(x)$ a function that has derivatives of all orders in $\Bbb R$,</p>

<p>and $R_n(x)$ the $n^{th}$ order Lagrange form of the remainder,</p>

<p>Prove or disprove:</p>

<p>if $$lim_{x \to 0} {\frac {R_n(x)} {x^n}} = 0$$
for every $n$, than the radius of convergence of the series
$$\sum_{n=0}^\infty \frac {f^{(n)}(0)} {n!} x^n$$
is larger than $\frac 12$.</p>

<p>My try:
Let 
$$a_n = \frac {f^{(n)}(0)} {n!}$$
By d'Alembert rule
$$R = lim_{n \to \infty} \left| \frac {a_n} {a_{n+1}}\right| = 
lim_{n \to \infty} \left| \frac {\frac {f^{(n)}(0)} {n!}} {\frac {f^{(n+1)}(0)} {(n+1)!}}\right| = $$
$$
 = lim_{n \to \infty} \left| 
\frac {f^{(n)}(0) \cdot  (n+1)!} {f^{(n+1)}(0) \cdot n!}
\right| = 
lim_{n \to \infty} \left| 
\frac {f^{(n)}(0) } {f^{(n+1)}(0) }\cdot  (n+1)
\right|
$$ 
stuck here.</p>

<p>Also tried Cauchy</p>

<p>$$\frac 1R = \overline {lim} \sqrt[n] {\left| a_n \right|} = 
\overline {lim} \sqrt[n] {\left| \frac {f^{(n)}(0)} {n!} \right|} = 
\overline {lim} 
\frac 
{
  \sqrt[n] {
        \left| 
           f^n(0)
        \right|
  }
} 
{
\sqrt[n] {n!}
} 
= ?
$$
again stuck.</p>

<p>Maybe the statement is false and there's a simple counter example?</p>
",<calculus>
"<p>Consider the surface formed by revolving $y=\sin(x)$ about the line $y=c$ from some $0\le{c}\le{1}$ along the interval $0\le{x}\le{\pi}$.</p>

<p><a href=""//i.stack.imgur.com/2ZAiQ.gif"" rel=""nofollow""><img src=""//i.stack.imgur.com/2ZAiQ.gif"" alt=""graph""></a></p>

<p>Set up and evaluate an integral to calculate the volume $V(c)$ as a function of $c$.</p>

<p>(My attempt) 
$$
\begin{align}
V &amp;= \pi\int_0^\pi[(\sin(x))^2-c^2]dx \\
  &amp;= \pi\int_0^\pi[(\sin^2(x))-c^2]dx \\
  &amp;= \pi\int_0^\pi\left[\frac12(1-\cos(2x))-c^2\right]dx \\
  &amp;= \pi\left[\frac12(x-\sin(x)(\cos(x))-\frac{c^2x}{2}\right]_0^\pi \\
  &amp;= \pi\left[\left(\frac12(\pi-\sin(\pi)\cos(\pi)-\frac{\pi c^2}{2}\right)-\left(\frac12(0-\sin(0)\cos(0)-0\right)\right]
\end{align}
$$</p>

<p>So far... is this correct?</p>

<p>The second part of the question:</p>

<p>What value of c maximises the volume $V(c)$?</p>

<p>^ no idea on that one. help appreciated.</p>
",<calculus>
"<p>I was given this problem by a friend:</p>

<p>$$
\def\limit{\lim_{x\to5}}
\def\top{\sqrt{x}-2}
g(x) = \frac{\top}{x-5}\\
\limit g(x) = \quad?
$$</p>

<p>This caught me by surprise, because I can't remember how to do this problem with basic Calculus. Intuitively, the limit doesn't exist since $\limit(\top) \ne 0$ but $\limit(x-5) = 0$. And the limit doesn't even approach either infinity since the bottom is an odd-power polynomial.</p>

<p>However, how can I prove this by using basic Calculus that a new Calculus student would understand?</p>

<hr>

<p>I tried to do this as follows, but it seems overly complicated:</p>

<p>I spent some time and found this function:</p>

<p>$$
f(x)=\frac1{\sqrt{x-5}}\\
f(x) &lt; \frac\top{x-5}\\
\text{when }5 &lt; x &lt; \frac{81}{16}
$$</p>

<p>And we know that $\lim_{x\to5^+} f(x) = \infty$, so therefore,</p>

<p>$$\lim_{x\to5^+} g(x) = \infty$$</p>

<p>However, $f(x)$ doesn't work for the LH limit. However:</p>

<p>$$
g(x) &gt; 0 \quad\text{if}\quad x &gt; 5\\
g(x) &lt; 0 \quad\text{if}\quad 4 &lt; x &lt; 5
$$</p>

<p>So that means that $\lim_{x\to5^-} g(x) &lt; 0 \ne \infty$ so the limit does not exist, and we can't even say that the limit is one of the infinities.</p>

<p>Isn't there an easier way to do this (assuming graphing isn't allowed)?</p>
",<calculus>
"<p>What is the solution to the following integral?  </p>

<p>$$\int_0^\infty\frac{x^n}{e^{ax}-e^{bx}} dx$$</p>
",<calculus>
"<p>$$\int \frac{1}{(1-x^2)^{\frac{1}{3}}}\,dx$$</p>

<p>$\underline{\bf{\text{My Try}}}$ Let $(1-x^2) = t^3$. then $-2x\,dx = 3t^2\,dt$ or $\displaystyle dx = -\frac{3t^2}{-2x}\,dt = \frac{3t^2}{-2\cdot \sqrt{1-t^3}}\,dt$</p>

<p>So $\displaystyle  = -\frac{3}{2}\int t^1\cdot (1-t^3)^{-\frac{1}{2}}\,dt$</p>

<p>Now our integral is converted into in this form $\int x^m\cdot(a+bx^n)^p \, dx$</p>

<p>So $m=1$ and $n=3$ and $\displaystyle p=-\frac{1}{2}$</p>

<p>Now How can I solve it</p>

<p>Help Required</p>

<p>Thanks</p>
",<calculus>
"<p>Consider $(\log_b(x))^p$ where $b$ is   a  constant $&gt;1$; $x, p \in \mathbb R_+$.</p>

<p>As we increase the value of $p$ (starting from 1), at specific value of $p$, the curve changes its shape from concave to convex, specifically for $x\ge 1$.  <a href=""https://www.desmos.com/calculator/1fzp8z5ris"" rel=""nofollow"">See Curve Transition</a>.</p>

<p>So the question is: </p>

<ol>
<li>At what value of $p$, in terms of $b$, the transition of curvature (from concave to convex) occurs? </li>
<li>How to find it mathematically?</li>
</ol>
",<calculus>
"<p>Help me please with this indefinite trigonometric integral. How can I solve this kind of integrals?</p>

<p>$$\int\limits \frac{1}{\cos^4x \cdot  \sin^2x}dx$$</p>
",<calculus>
"<p>This is a question from my exam in Calculus 1.</p>

<blockquote>
  <p><strong>Problem 6</strong> </p>
  
  <p>Let $f: [0,\infty[ \to \mathbb{R}$ be continuously differentiable and $\lim_{x \to \infty} f'(x) = 0$.</p>
  
  <p>a) Show that for $n \in \mathbb{N}: \lim_{n \to \infty} \left( f(n+1) - f(n)\right) = 0$.</p>
  
  <p>b) Is $f$ necessarily bounded? Justify your answer.</p>
</blockquote>

<p>I started (a) with</p>

<p>$$ \lim_{x \to \infty} f'(x) = 0 $$
$$ \lim_{x \to \infty} \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} = 0 $$</p>

<p>But how does that help when $h \to 1$?</p>
",<calculus>
"<p>I can't grasp this concept of an instantaneous change of rate. How could a point on a function graph have a rate of change in the first place?</p>

<p>In this moment I just know that it is named the derivative and that it is the slope of the tangent line at that point. We can find that slope by finding the limit of closer and closer to the point slopes. I still don't understand what does it really represent. </p>
",<calculus>
"<p>So i want to prove that
$$x^2e^x=1$$ has at least one solution for $$x\in\mathbb{R}$$</p>

<p>I am kinda lost and would appreciate any help. This is suppose to be solved using basic  calculus but i am not sure what to use.</p>
",<calculus>
"<p><a href=""http://fooplot.com/#W3sidHlwZSI6MSwiZXEiOiIxK3Npbih0aGV0YSkiLCJjb2xvciI6IiMwMDgwY2MiLCJ0aGV0YW1pbiI6IjAiLCJ0aGV0YW1heCI6IjJwaSIsInRoZXRhc3RlcCI6Ii4wMSJ9LHsidHlwZSI6MSwiZXEiOiIxK2Nvcyh0aGV0YSkiLCJjb2xvciI6IiMwMDgwY2MiLCJ0aGV0YW1pbiI6IjAiLCJ0aGV0YW1heCI6IjJwaSIsInRoZXRhc3RlcCI6Ii4wMSJ9LHsidHlwZSI6MSwiZXEiOiIiLCJjb2xvciI6IiMwMDgwY2MiLCJ0aGV0YW1pbiI6IjAiLCJ0aGV0YW1heCI6IjJwaSIsInRoZXRhc3RlcCI6Ii4wMSJ9LHsidHlwZSI6MTAwMCwid2luZG93IjpbIi0zLjAzMTA0IiwiMy42MjQ5NTk5OTk5OTk5OTk3IiwiLTEuNTc2OTYwMDAwMDAwMDAwMSIsIjIuNTE5MDQiXX1d"" rel=""nofollow"">Fooplot graph</a>:</p>

<hr>

<p><a href=""http://i.stack.imgur.com/kK6Xx.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/kK6Xx.png"" alt=""enter image description here""></a></p>

<hr>

<p>I think the formula is</p>

<p>$$A = \frac 1 2 \int_{\alpha}^{\beta} (\text{outer})^2 - (\text{inner})^2 d\theta$$</p>

<p>where $\alpha, \beta$ are <a href=""http://www.wolframalpha.com/input/?i=sin(theta)%3Dcos(theta),%200%20%3C%3D%20theta%20%3C%3D%202pi"" rel=""nofollow"">where they intersect</a> in $[0, 2\pi]$.</p>

<p>This is what I got based on that</p>

<p>$$A = \frac 1 2 \int_{3\pi/4}^{5\pi/4} (1+\sin \theta)^2 - (1+\cos \theta)^2 d\theta$$</p>

<p>Is that right?</p>
",<calculus>
"<p>$ \boldsymbol x = f(\boldsymbol X,t)$ is the position of a particle in an instant of time</p>

<p>$\boldsymbol X$ is the initial position</p>

<p>$t$ time</p>

<p>$\boldsymbol u$ velocity</p>

<p>In my opnion $f$ is continuos...</p>

<p>Considering:</p>

<p>$$u_i={{\partial x_i}\over{\partial t}}$$</p>

<p>Then:</p>

<p>$${\partial \over {\partial x_j}} \left(\partial x_i \over \partial t \right) = {{\partial u_i}\over{\partial x_j}}=\boldsymbol{\nabla}\boldsymbol{u}$$</p>

<p>But we <strong>can't</strong> Invert the Order of the partial derivative, otherwise we would have:</p>

<p>$${\partial \over {\partial t}} \left(\partial x_i \over \partial x_j \right) \neq 0$$</p>

<p>That is not the same result.</p>
",<calculus>
"<p><a href=""http://fooplot.com/#W3sidHlwZSI6MSwiZXEiOiIyL3Npbih0aGV0YSkiLCJjb2xvciI6IiMwMDgwY2MiLCJ0aGV0YW1pbiI6IjAiLCJ0aGV0YW1heCI6IjJwaSIsInRoZXRhc3RlcCI6Ii4wMSJ9LHsidHlwZSI6MSwiZXEiOiIxMHNpbih0aGV0YSkiLCJjb2xvciI6IiMwMDgwY2MiLCJ0aGV0YW1pbiI6IjAiLCJ0aGV0YW1heCI6IjJwaSIsInRoZXRhc3RlcCI6Ii4wMSJ9LHsidHlwZSI6MTAwMCwid2luZG93IjpbIi0xMy4zMTkwOTE3OTY4NzQ5OTUiLCIxMi4wNzE1MzMyMDMxMjQ5OTUiLCItMy44MTkzMzU5Mzc0OTk5OTgyIiwiMTEuODA1NjY0MDYyNDk5OTk2Il19XQ--"" rel=""nofollow"">Fooplot graph</a>:</p>

<hr>

<p><a href=""http://i.stack.imgur.com/jC42p.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jC42p.png"" alt=""enter image description here""></a></p>

<hr>

<p>I think the formula is</p>

<p>$$A = \frac 1 2 \int_{\alpha}^{\beta} (\text{outer})^2 - (\text{inner})^2 d\theta$$</p>

<p>where $\alpha, \beta$ are <a href=""http://www.wolframalpha.com/input/?i=10sinx+%3D+2cscx,+0+%3C%3D+x+%3C%3D+2pi"" rel=""nofollow"">where they intersect</a> in $[0, 2\pi]$.</p>

<p>This is what I got based on that</p>

<p>$$A = \frac 1 2 \int_{x}^{\pi-x} (10 \sin \theta)^2 - (2 \csc \theta)^2 d\theta$$</p>

<p>where $x= \sin^{-1}(\frac {1}{\sqrt{5}} )$</p>

<p>Is that right?</p>
",<calculus>
"<p><strong>QUESTION:</strong> Given function is $$E=\frac{1}{4}\cdot \frac{F^2}{m}\cdot \frac{\omega_0^2+\omega^2}{(\omega_0^2-\omega^2)^2+4\alpha^2\omega^2}$$</p>

<p>We have to maximise $E$ with respect to $\omega$.</p>

<p><strong>MY ATTEMPT FOR SOLUTION:</strong> $$E=\frac{F^2}{4m}\cdot \phi(\omega^2)$$</p>

<p>Now, $\frac{dE}{d(\omega^2)}=\frac{d\{\phi(\omega^2)\}}{d(\omega^2)}=0$ when $E$ is maximum.</p>

<p>So we have that $$\frac{(\omega_0^2-\omega^2)^2+4\alpha^2\omega^2-(\omega_0^2+\omega^2)\left[-2(\omega_0^2-\omega^2)+4\alpha^2\right]}{\left[(\omega_0^2-\omega^2)^2+4\alpha^2\omega^2\right]^2}=0$$
Or, $$(\omega_0^2-\omega^2)^2-4\alpha^2\omega_0^2+2(\omega_0^2+\omega^2)(\omega_0^2-\omega^2)=0$$
Or, $$(\omega_0^2-\omega^2)\left[(\omega_0^2-\omega^2)+2(\omega_0^2+\omega^2)\right]=4\alpha^2\omega_0^2$$
Or, $$(\omega_0^2-\omega^2)(3\omega_0^2+\omega^2)=4\alpha^2\omega_0^2$$</p>

<p>This results in a Fourth degree equation in $\omega$ and it yields a very complex result.</p>

<p>But in the book, it is given that $E$ is maximum when $\omega=\omega_0$.</p>

<p>Where did I go wrong? Please help.</p>
",<calculus>
"<p><a href=""http://fooplot.com/#W3sidHlwZSI6MSwiZXEiOiJzcXJ0KDhjb3MoMnRoZXRhKSkiLCJjb2xvciI6IiMwMDgwY2MiLCJ0aGV0YW1pbiI6IjAiLCJ0aGV0YW1heCI6IjJwaSIsInRoZXRhc3RlcCI6Ii4wMSJ9LHsidHlwZSI6MSwiZXEiOiIyIiwiY29sb3IiOiIjMDA4MGNjIiwidGhldGFtaW4iOiIwIiwidGhldGFtYXgiOiIycGkiLCJ0aGV0YXN0ZXAiOiIuMDEifSx7InR5cGUiOjEsImVxIjoiLXNxcnQoOGNvcygydGhldGEpKSIsImNvbG9yIjoiIzAwODBjYyIsInRoZXRhbWluIjoiMCIsInRoZXRhbWF4IjoiMnBpIiwidGhldGFzdGVwIjoiLjAxIn0seyJ0eXBlIjoxMDAwLCJ3aW5kb3ciOlsiLTcuMjIzNzc5Mjk2ODc0OTk4IiwiOS4wMjYyMjA3MDMxMjQ5OTUiLCItNS43MzE4MzU5Mzc0OTk5OTgiLCI0LjI2ODE2NDA2MjQ5OTk5OSJdfV0-"" rel=""nofollow"">Fooplot graph</a>:</p>

<hr>

<p><a href=""http://i.stack.imgur.com/zFlk1.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/zFlk1.png"" alt=""enter image description here""></a></p>

<hr>

<p>I think the formula is</p>

<p>$$A = \frac 1 2 \int_{\alpha}^{\beta} (\text{outer})^2 - (\text{inner})^2 d\theta$$</p>

<p>where $\alpha, \beta$ are <a href=""http://www.wolframalpha.com/input/?i=4+%3D+8cos(2x),+0+%3C%3D+x+%3C%3D+2pi"" rel=""nofollow"">where they intersect</a> in $[0, 2\pi]$.</p>

<p>This is what I got based on that</p>

<p>By symmetry, we have</p>

<p>$$\frac A 4 = \frac 1 2 \int_{0}^{\pi/6} (8 cos 2\theta) - (2)^2 d\theta$$</p>

<p>Is that right?</p>
",<calculus>
"<p>Given that an arithmetic progression is such that the 8th term is twice the second term, and the 11th term is 18.
Find:
1) The first term and common difference.
2) The sum of the first 26 terms.
3) The smallest of the progression whose values exceed 126?</p>

<p>How on earth am I meant to solve this? I'm guessing you try and find a formula for the nth term, but I have no clue how to get there.
Any suggestions?</p>
",<calculus>
"<p>$f(x)=\begin {cases}\sin x\ln x^2 &amp; x\neq 0\\ 0 &amp; x=0\end{cases}$</p>

<p>When I try to find the derivative on $x=0$ with the defintion I get:</p>

<p>$\displaystyle\lim_{h\to 0}\frac {f(h+0)-f(0)}{h-0}=\lim_{h\to 0}\frac {f(h)}{h}=\lim_{h\to 0}\frac {\sin h \ln h^2} h=\lim_{h\to 0}\ln h^2=-\infty$</p>

<p>But I see from the graph that the slope on $x=0$ is supposed to be finite:</p>

<p><img src=""http://i.stack.imgur.com/3iVNH.png"" alt=""enter image description here""></p>

<p>Why does the derivative definition don't work here?</p>
",<calculus>
"<blockquote>
  <p>Find how many solutions does $f(x)=\ln x-kx$ has for $k&gt;\frac 1 e$.</p>
</blockquote>

<p>$f&lt;0$ at $x\to \infty$ and $x\to 0$. The derivative has a solution only at $x=\frac 1 k$.</p>

<p>So place that point in $f$ and we'll want to check when $f(\frac 1 k)&gt;0$ and $f(\frac 1 k)&lt;0$</p>

<p>Now I'm getting confused here:</p>

<p>$f(\frac 1 k)&gt;0\to \ln \frac 1 k &gt; 1$ so $e^1&gt;\frac 1 k\to k&gt;\frac 1 e$ but it should be the opposite, so my question is why does the inequality 'flip'? </p>
",<calculus>
"<p>Intuitively, it is rather obvious that</p>

<p>$$\lim_{l\to\infty}\sum_{n=-\infty}^{\infty}f(n\Delta x)\Delta x = \int_{-\infty}^{\infty}f(x)dx \tag{1}$$</p>

<p>where $\Delta x = \frac{1}{l}$, assuming $f$ is integrable and the limit exists.</p>

<p><strong>The fact that this equality is true is the core part of deriving Fourier transform from Fourier series, see page 4, eq. 4.7 in <a href=""http://people.seas.harvard.edu/~schan/class/Su11/Chapter4.pdf"" rel=""nofollow"">this document</a>.</strong> Or maybe we cannot consider this derivation as formal, as it was never intended to be formal, but I thought n mathematics there's no place for informal thinking.</p>

<p>My question is how can we prove it's true from the definitions and properties of improper integral, definite integral and limits?</p>

<p>I've listed the important definitions below in case you would like to refer to some of these in your answers.</p>

<p>Oh, and please ignore mrf's answer - it doesn't refer to my question anymore, I've reformulated it.</p>

<hr>

<p>If function $f$ is integrable on $[a,b]$, then:
$$\int_{a}^{b}f(x)dx=\lim_{n\to\infty}\sum_{i=1}^{n}f(x_i)\Delta x \tag{2}$$
where $\Delta x = \frac{b-a}{n}$ and $x_i = a+i\Delta x$.</p>

<p>Improper integral definitions</p>

<p>$$\int_{a}^{\infty}f(x)dx=\lim_{t\to\infty}\int_{a}^{t}f(x)dx \tag{3}$$</p>

<p>$$\int_{-\infty}^{b}f(x)dx=\lim_{t\to-\infty}\int_{t}^{b}f(x)dx \tag{4}$$</p>

<p>$$\int_{-\infty}^{\infty}f(x)dx=\int_{a}^{\infty}f(x)dx + \int_{-\infty}^{a}f(x)dx \tag{5}$$</p>
",<calculus>
"<p>I have an integration and I am splitting this one in two parts ;</p>

<p>$\displaystyle\int_{0}^{a-b}=\int_{0}^{a}+\int_{a}^{a-b}=\int_{0}^{a}-\int_{a-b}^{a}$</p>

<p>and </p>

<p>$\displaystyle a\geq b$</p>

<p>Is it correct to write this integral in two part like this ?
The fact that bothers me is that I make the split on $a$ which is for sure greater then $a-b$.</p>
",<calculus>
"<p>I'm introducing myself to the $(\varepsilon, \delta)$ definition of limits, and I'm encountering a few issues.</p>

<p>When proving the $\lim_{x \to c}f(x) = L$
$$
\forall \varepsilon  &gt; 0, \ \exists \delta = \delta(\varepsilon) &gt; 0 : 0 &lt; |x - c| &lt; \delta \implies |f(x) - L| &lt; \varepsilon
$$</p>

<p>When considering $\lim_{x \to 2}(2x - 5) = -1$</p>

<p>Let $\forall \varepsilon &gt; 0$</p>

<p>Choose $\delta = \dfrac{\varepsilon}{2}$</p>

<p>Assume $0 &lt; |x - 2| &lt; \delta$</p>

<p>Then,</p>

<p>$$
|2x - 5 - (-1)| &lt; \varepsilon,
$$</p>

<p>$$
\\|2x - 4| &lt; \varepsilon
\\2|x - 2| &lt; \varepsilon
\\|x - 2| &lt; \delta
\\2|x - 2| &lt; 2\delta
\\ \therefore \delta = \dfrac{\varepsilon}{2}
$$</p>

<p>Forgive my mistakes, I'm still quite new to this. I believe that my proof is mostly accurate (do correct me if I'm wrong, please).</p>

<p>My biggest issue comes with solving this other problem:</p>

<p>I am to suppose $|f(x)-7| &lt;  0.2$ whenever $0 &lt; x &lt; 7$.</p>

<p>Find all values of $\delta &gt; 0$ such that $|f(x) - 7| &lt; 0.2$ whenever $0 &lt; |x-2| &lt; \delta$.</p>

<p>I've not got a good idea of how to approach this with an arbitrary $f(x)$.</p>
",<calculus>
"<p>Does it ask that we can express any positive real number as square root of something? like 4 is equal to square root of 16?</p>
",<calculus>
"<p>Show that the series $\sum\limits_{j=1}^\infty  = \frac{2^j+j}{3^j-j}$ converges.</p>

<p>I know that if I look at each sequential case j=1,2,3... the limit of the partial sums approaches 0 as $j \rightarrow \infty$, but how can I show that the series converges more explicitly?</p>
",<calculus>
"<p>I evaluated the correct answers for the limit with the function $(8+x)^{\frac{1}{x}}$ using a different method, but I do understand how L'Hopitals works, its just rewriting this function into $\frac{f(x)}{g(x)}$ I am very confused about. I am able to do so with other functions such as $x\ln(x)$, but I am not sure how to do so with an exponent. </p>
",<calculus>
"<p>$$
\int\frac {x^3 + 5x^2 +2x -4}{(x^4-1)}dx
$$
A bit confused with how to integrate this question. I though it was partial fractions but was unsure about the what to do after that.</p>
",<calculus>
"<p>Does the following series converge or diverge? 
$\sum\limits_{j=1}^\infty \frac{(1+\frac{1}{j})^{2j}}{e^j}$</p>

<p>Using the ratio test, I found $|\frac{a_{j+1}}{a_j}|$ to be $|\frac{(1+\frac{1}{j+1})^{2j+2}}{e(1+\frac{1}{j})^{2j}}|$</p>

<p>To find the lim sup of this, I looked at the ratio as $j \rightarrow \infty$, and arrived at $\frac{1}{e}&lt;1$, so by the ratio test, the series would converge absolutely.</p>

<p>Does this look valid? I wasn't sure about the step of taking the limit as $j \rightarrow \infty$ to find the lim sup.</p>

<p>Thank you!</p>
",<calculus>
"<p>After looking at calculations, I realized that the exponent needs to be 1/4 instead of -1/4</p>

<p>I have this equation and I am trying to solve the integral of it. </p>

<p>$$((R^2) - (y^2))^{1/4} dy$$</p>

<p>I tried to put it into wolfram alpha, and I got an answer, but I wanted to know how they arrived at the answer. </p>

<p>Any advice would be greatly appreciated. If you could please show me how to do this integral, I would be appreciate it very much. </p>

<p>I know you need to use hyper geometric functions; however, I am not sure how.</p>

<p>Thank-you very much </p>
",<calculus>
"<p>$$\lim_{x\to 0} \int_x^{x+1} \sqrt {\arctan {t}}\space dt$$</p>

<p>I think it does not exist because we can't talk about limit from $x\to 0^-$, but what if we just look for $\lim_{x\to 0^+}$? I see there less and less area of the graph so it should be $0$, but how we show it?</p>
",<calculus>
"<p>The question reads:</p>

<p>Let $f(x)$ be a function whose graph is contained in the first quadrant but does not pass through the x-axis and let $Q = (a,0)$ . Let $P=(x_o, f(x_o))$ be the point on the graph closest to Q. Prove that the line PQ is perpendicular to the tangent line to the graph at $x_o$. ($Hint:$ Recall that the slope $m'$ of a line $L'$ perpendicular to a line $L$ of slope $m$ is $m' = \frac{-1}{m}$ .)</p>

<p>Edit (second attempt):</p>

<p>$$x\geq 0, y\geq 0 $$
$$D = \sqrt{(f(x_o)-0)^2 + (x_o - a)^2}$$
$$D' = \frac{2(f(x_o))f'(x_o) + 2(x_o-a)(1)}{2\sqrt{(f(x_o)-0)^2 + (x_o - a)^2}}$$
$$D' = 0$$
$$2(f(x_o))f'(x_o) + 2(x_o-a)(1) = 0$$
$$(x_o-a)=-(f(x_o))f'(x_o)$$
$$-\frac{(x_o-a)}{f(x_o)}=f'(x_o)$$
$$-\frac{1}{f'(x_o)} = \frac{f(x_o)}{(x_o-a)}$$</p>

<p>Which is the desired result.</p>
",<calculus>
"<p>I am not sure if this question is appropriate here but would appreciate any help. Let $n>2$ and $x_1,\cdots,x_n$ be real numbers. What is the infimum of:</p>

<p>$$A = \sum_{i=1}^n \frac{1}{(1+x_i)^2} $$ </p>

<p>subject to the constraint $\prod_{i=1}^nx_i=1$?</p>

<p>Similar question for: </p>

<p>$$B = \sum_{i=1}^n \frac{1}{(1-x_i)^2} $$</p>

<p>Thanks in advance!</p>
",<calculus>
"<p>The exercise asks me to prove 2 things:</p>

<p>1) $f(x,y) $ is continuous in $(x_0,y_0)$, $f(x_0,y_0)&gt;0$ then there is a neighborhood such that $f(x,y)&gt;\frac{1}{2}f(x_0,y_0)$</p>

<p>My idea:</p>

<p>$f$ is continuous, then $|(x,y)-(x_0,y_0)|&lt; \delta\implies |f(x,y)-f(x_0,y_0)|&lt; \epsilon \implies f(x_0,y_0) -\epsilon &lt;f(x,y) &lt; f(x_0,y_0) + \epsilon$. It's true for all $\epsilon$, so if I choose $\epsilon = \frac{1}{2}f(x_0,y_0)$
we have:</p>

<p>$$\frac{1}{2}f(x_0,y_0)&lt;f(x,y) &lt; f(x_0,y_0) + \frac{1}{2}f(x_0,y_0)$$</p>

<p>2) Suppose $f$ is continuous in a domain $D$. Suppose that $f(x,y)$ is positive for at least $1$ point of $D$ and negative for at least one point of D. Then $f(x,y) = 0$ for at least one point of $D$. (suggestion: use $1$)</p>

<p>How to use $1$ to prove $2$? As I know, this can be understood as the mean value theorem for multivariables, but I couldn't find a proof that used $1$.</p>
",<calculus>
"<p>I need an upper bound for 
$$\frac{x+y}{ax+y}$$</p>

<p>I know that $$1\leq a&lt; 2$$ 
$x\geq 0 $ and $y\geq 0 $ . This upper bound can include just $a$ and constant numbers not $x$ or $y$.</p>

<p>thanks a lot.</p>
",<calculus>
"<p>$$\int \frac{\sqrt{9-4x^{2}}}{x}dx$$
How Can I attack this kind of problem?</p>
",<calculus>
"<p>Why is $$ \int \frac{\sin x (b-a\cos x)}{(b^2+a^2-2ab \cos x)^{3/2}}\,dx = \frac{a-b\cos x}{b^2 \sqrt{a^2-2ab\cos x + b^2}}\text{ ?}$$</p>

<p>Constant of integration omitted.</p>
",<calculus>
"<p>I've encountered a homework problem which has had me a little bit confused, as it doesn't <em>feel</em> as though it marries up very closely with the content in my textbook. I guess that what I'd appreciate most is if someone could verify that I have answered this correctly, and potentially clarify any errors that I've made.</p>

<p>Here is the problem statement:</p>

<blockquote>
  <p>Let $S$ define the region inside the cardioid $r=1+\sin(\theta)$, above the x-axis. Using polar coordinates, evaluate $\iint_{S}\frac{1}{\sqrt{x^2+y^2}}dA$.</p>
</blockquote>

<p>Here's my attempt at the problem:</p>

<blockquote>
  <p>Defining $S$ as $S=\{(r,\theta)\ |\ 0\le\theta\le\pi,\ 0\le r\le1+\sin(\theta)\}$,</p>
  
  <p>$\iint_{S}\frac{1}{\sqrt{x^2+y^2}}dA = \int_0^\pi \! \int_0^{1+\sin(\theta)} \frac{1}{r}r\ dr\ d\theta = \int_0^\pi \! \int_0^{1+\sin(\theta)} 1\ dr\ d\theta=\pi+2.$</p>
</blockquote>

<p>Thank you very much!</p>
",<calculus>
"<p>I need an upper bound for 
$$\frac{ax}{x-2}$$
I know that $1\leq a&lt; 2$ and $x\geq 0$.</p>

<p>This upper bound can include just $a$ and constant numbers not $x$.</p>

<p>thanks a lot.</p>
",<calculus>
"<blockquote>
  <p>$$\int_0^1\int_\sqrt[3]{x}^1 4\cos(y^4)\,\mathrm dy\,\mathrm dx$$</p>
</blockquote>

<p>What I got was</p>

<p>$\sin(1)x+\cos(x^2) dx$ and now I am stuck.</p>

<p>I suddenly froze. Could someone help me? Haven't done calculus for a long time. </p>
",<calculus>
"<p><img src=""http://i.stack.imgur.com/plnRC.png"" alt=""enter image description here""></p>

<p>This seems super easy. But i am just a little bit stuck here. Haven't done much calculus recently. Can someone help me out real quick?</p>

<p>Thank you in advance!</p>
",<calculus>
"<p>Let a, b, c be vectors, f(x, y, z) be a scalar ﬁeld, F(x,y,z) be a vector ﬁeld. Which of the
following expressions are meaningful?</p>

<p>I. (a×b)×(c×b)</p>

<p>II. |a|(b· c) +|a|(b+c)</p>

<p>III. ∇ ×(f F)</p>

<p>IV. (∇ ×F)·(f ×F)</p>

<p>I know that (I) makes sense. Vector x Vector.
(II) is not meaningful</p>

<p>I am confused about grad and curls. I know that div and curl apply to vector fields, grad to
scalar fields.</p>

<p>So. curl(f F). is (f F) scalar and is (f x F) vector? </p>
",<calculus>
"<p>The maximum value of the function $f(x, y) = xy$, and subject to condition $x^2+y^2=1$:</p>

<p>So do I apply Lagrange's Multiplier method to find the maximum value?
I tried to find the numbers just by applying few numbers but it doesn't seem to work.</p>
",<calculus>
"<p>Apologizes if I'm missing something in my question or if my question seems trivial; this is my first question on this site. As motivation for my question, consider the following standard first year calculus question.</p>

<blockquote>
  <p>Consider this piecewise function:
  $
   f(x) = \left\{
     \begin{array}{lr}
       ax^2+b &amp; \text{ if } x \le-2\\
       12x-5 &amp; \text{ if } x &gt;-2
     \end{array}
   \right.
$</p>
  
  <p>For what values of $a$ and $b$ will $f(x)$ be differentiable?</p>
</blockquote>

<p>To solve this question, I would like to propose the following theorem:</p>

<blockquote>
  <p>$\mathbf{Theorem:}$ A function $f(x)$ is differentiable iff $f'(x)$ is continuous.</p>
</blockquote>

<p>If this theorem is true, then I can solve for $a$ first by noting that:
$
   f'(x) = \left\{
     \begin{array}{lr}
       2ax &amp; \text{ if } x \le-2\\
       12 &amp; \text{ if } x &gt;-2
     \end{array}
   \right.
$</p>

<p>Thus, since by my theorem $f'(x)$ must be continuous, we have:</p>

<p>$$\begin{align*}
\lim_{x \rightarrow -2^-}f'(x) &amp;= \lim_{x \rightarrow -2^+}f'(x)\\
\lim_{x \rightarrow -2^-}2ax &amp;= \lim_{x \rightarrow -2^+}12\\
2a(-2) &amp;= 12\\
-4a &amp;= 12 \\
a &amp;= -3 \\
\end{align*}$$</p>

<p>Hence, since differentiability implies continuity, we can solve for $b$ as follows:</p>

<p>$$\begin{align*}
\lim_{x \rightarrow -2^-}f(x) &amp;= \lim_{x \rightarrow -2^+}f(x)\\
\lim_{x \rightarrow -2^-}-3x^2+b &amp;= \lim_{x \rightarrow -2^+}12x-5\\
-3(-2)^2+b &amp;= 12(-2)-5\\
b-12 &amp;= -29\\
b &amp;= -17 \\
\end{align*}$$</p>

<p>so that our differentiable function is:</p>

<blockquote>
  <p>$$
   f(x) = \left\{
     \begin{array}{lr}
       -3x^2-17 &amp; \text{ if } x \le-2\\
       12x-5 &amp; \text{ if } x &gt;-2
     \end{array}
   \right.
$$</p>
</blockquote>

<p>Anyways. My question is: <strong>Is my proposed theorem actually a thing?</strong> I've looked through my calculus textbook and it doesn't seem to explicitly state it, yet I don't know how to solve this question otherwise. If this theorem turns out to be false, how else can you solve this problem? Thanks in advance. =]</p>
",<calculus>
"<p>I need to find a number $x$ such that
$$\sum_{n=1}^\infty\frac{n^x}{2^n n!} = \frac{1539}{64}e^{1/2}.$$
What is the best approach to this problem?</p>
",<calculus>
"<p>Test for convergence the series
$$\sum_{n=1}^{\infty}\frac{1}{n^{(n+1)/n}}$$
I'd like to make up a collection with solutions for this series, and any new<br>
solution will be rewarded with upvotes.   Here is what I have at the moment</p>

<p><strong>Method 1</strong> </p>

<p>We know that for all positive integers $n$, $n&lt;2^n$, and this yields
$$n^{(1/n)}&lt;2$$
$$n^{(1+1/n)}&lt;2n$$
Then, it turns out that 
$$\frac{1}{2} \sum_{n=1}^{\infty}\frac{1}{n} \rightarrow \infty \le\sum_{n=1}^{\infty}\frac{1}{n^{(n+1)/n}}$$
Hence
$$\sum_{n=1}^{\infty}\frac{1}{n^{(n+1)/n}}\rightarrow \infty$$
<strong>EDIT:</strong>  </p>

<p><strong>Method 2</strong></p>

<p>If we consider the maximum of $f(x)=x^{(1/x)}$ reached for $x=e$<br>
and denote it by $c$, then
$$\sum_{n=1}^{\infty}\frac{1}{c \cdot n} \rightarrow \infty \le\sum_{n=1}^{\infty}\frac{1}{n^{(n+1)/n}}$$</p>

<p>Thanks!</p>
",<calculus>
"<p>Suppose $f(x) \in C[a,b]$ and $\varphi(x)$ is Riemann integrable and satisfy : $\int_a^b\varphi(x)\mathscr{dx}=0$.   $\int_a^bf(x)\varphi(x)\mathscr{dx}=0$       can we conclude that $f(x)\equiv c$?</p>

<p><hr>
My approach:</p>

<p>for $f$ is continuous on $[a,b]$, then there exists $M$ and $m$ in $\Bbb{R}$,satisfy: $m \leq f(x) \leq M$,so we have $M-f(x)\geq0$,so I can use the mean value theorem:
$$\int_a^b(M-f(x))\varphi(x)\mathscr{dx}=\mu\int_a^b(M-f(x))\mathscr{dx}=0 \Rightarrow \int_a^b f(x)\mathscr{dx} =M(b-a)$$
where $\inf\varphi(x)\leq\mu \leq \sup \varphi(x)$<br>
use MVT to : $$\int_a^b(f(x)-m)\varphi(x)\mathscr{dx}$$we get $\int_a^bf(x)\mathscr{dx}=m(b-a)$ this implies $m=M$,then $f$ is constant on $[a,b]$</p>

<p>Am I right?</p>

<hr>
",<calculus>
"<p>Let $\lambda_i \in \mathbb C$  $(i=1,2,\cdots,n)$ be $n$ different complex numbers and $p_i \in \mathbb C[t]$ $(i=1,2,\cdots,n)$ polynomials. If we have the relation
$${e^{{\lambda _1}t}}{p_1}(t) + {e^{{\lambda _2}t}}{p_2}(t) +  \cdots  + {e^{{\lambda _n}t}}{p_n}(t) = 0$$ for all $t \geq 0$, can we conclude that all $p_i(t)=0$?</p>
",<calculus>
"<p>In Gauss Divergence Theorem , $$\int \int_{\partial V} \textbf{F}\cdot\textbf{n}\,dS =\int \int \int_V \nabla\cdot\textbf{F}\,dx\,dy\,dz,$$ is there any restriction on the vector field $\textbf{F}$ ? Does it need to be in dimension three only ? Can I let it be $\textbf{F}=xyz$ where $x,y,z\in\mathbb{R}$ ?</p>
",<calculus>
"<p>i can show $\sum |x|^2=\int_a^b|f(x)|^2dx$ in term of integral, or this one  $|\sum x\overline y|^2=|\int_a^bf(x)\overline {g(x)}dx|^2$ but i don't know how to show this one $\sum_{i}^{n-1}\sum_{j=i+1}^{n}|(x_i\overline y_j-x_j\overline y_i)|^2$ in term of integral</p>

<p>$\sum_{i}^{n-1}\sum_{j=i+1}^{n}|(x_i\overline y_j-x_j\overline y_i)|^2=?$</p>
",<calculus>
"<p>Imagine I have this limit:</p>

<p>$$\lim_{x\to 0}\frac{\ln(1+2x)}x$$</p>

<p>Using the <em>L'Hospital's rule</em> the result is $2$.</p>

<p>Using this result is it possible to calculate</p>

<p>$$\lim_{n\to \infty}\ n\ln\bigg(1+\frac{4}{\sqrt{n}}\bigg) \quad ?$$</p>

<p>Sorry if this is an easy question, but many years have passed since I've learned calculus.</p>
",<calculus>
"<p>I did the following problem in class $f(x)=x\sqrt{x+2}$ and I needed to find the local maxima. I said the domain was $[-2,\infty)$ and the left end point $(-2,0)$ on the graph is a local maxima because the graph has a negative derivative there, but my instructor said that it was wrong.</p>

<p>Why is that end point not a local maxima?</p>

<p>Thanks</p>
",<calculus>
"<blockquote>
  <p>Solve $\space \begin{align*} \lim_ {x \to+\infty} \left [ \frac{4 \ln(x+1)}{x}\right]  
\end{align*}$.</p>
</blockquote>

<p>I did this way:</p>

<p>$$\begin{align*}
\lim_ {x \to+\infty} \left [ \frac{4 \ln(x+1)}{x}\right] &amp; = 4\lim_ {x \to+\infty} \left [\frac{1}{x} \ln(x+1) \right]= \\\\=4\lim_ {x \to+\infty} \left [ \ln(x+1)^{\frac{1}{x}}\right] &amp;= 4 \ln \left[\lim_ {x \to+\infty}(x+1)^{\frac{1}{x}}\right] =4 \cdot \ln(1)=0
\end{align*}$$</p>

<p>What is the rule behind the shift that I made between the $\ln$ and the $limit$?</p>

<p>I'm in the high school.Thanks</p>
",<calculus>
"<p>Is there any way to show that</p>

<p>$$\sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}} = \frac{1}{a} + \sum\limits_{k = 1}^\infty  {{{\left( { - 1} \right)}^k}\left( {\frac{1}{{a - k}} + \frac{1}{{a + k}}} \right)}=\frac{\pi }{{\sin \pi a}}} $$</p>

<p>Where $0 &lt; a = \dfrac{n+1}{m} &lt; 1$</p>

<p>The infinite series is equal to</p>

<p>$$\int\limits_{ - \infty }^\infty  {\frac{{{e^{at}}}}{{{e^t} + 1}}dt} $$</p>

<p>To get to the result, I split the integral at $x=0$ and use the convergent series in $(0,\infty)$ and $(-\infty,0)$ respectively:</p>

<p>$$\frac{1}{{1 + {e^t}}} = \sum\limits_{k = 0}^\infty  {{{\left( { - 1} \right)}^k}{e^{ - \left( {k + 1} \right)t}}} $$</p>

<p>$$\frac{1}{{1 + {e^t}}} = \sum\limits_{k = 0}^\infty  {{{\left( { - 1} \right)}^k}{e^{kt}}} $$</p>

<p>Since $0 &lt; a &lt; 1$</p>

<p>$$\eqalign{
  &amp; \mathop {\lim }\limits_{t \to 0} \frac{{{e^{\left( {k + a} \right)t}}}}{{k + a}} - \mathop {\lim }\limits_{t \to  - \infty } \frac{{{e^{\left( {k + a} \right)t}}}}{{k + a}} = \frac{1}{{k + a}}  \cr 
  &amp; \mathop {\lim }\limits_{t \to \infty } \frac{{{e^{\left( {a - k - 1} \right)t}}}}{{k + a}} - \mathop {\lim }\limits_{t \to 0} \frac{{{e^{\left( {a - k - 1} \right)t}}}}{{k + a}} =  - \frac{1}{{a - \left( {k + 1} \right)}} \cr} $$</p>

<p>A change in the indices will give the desired series.</p>

<p>Although I don't mind direct solutions from tables and other sources, I prefer an elaborated answer. </p>

<hr>

<p>Here's the solution in terms of $\psi(x)$. By separating even and odd indices we can get</p>

<p>$$\eqalign{
  &amp; \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \sum\limits_{k = 0}^\infty  {\frac{1}{{a + 2k}}}  - \sum\limits_{k = 0}^\infty  {\frac{1}{{a + 2k + 1}}}   \cr 
  &amp; \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a - k}}}  = \sum\limits_{k = 0}^\infty  {\frac{1}{{a - 2k}}}  - \sum\limits_{k = 0}^\infty  {\frac{1}{{a - 2k - 1}}}  \cr} $$</p>

<p>which gives</p>

<p>$$\sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \frac{1}{2}\psi \left( {\frac{{a + 1}}{2}} \right) - \frac{1}{2}\psi \left( {\frac{a}{2}} \right)$$</p>

<p>$$\sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a - k}}}  = \frac{1}{2}\psi \left( {1 - \frac{a}{2}} \right) - \frac{1}{2}\psi \left( {1 - \frac{{a + 1}}{2}} \right) + \frac{1}{a}$$</p>

<p>Then</p>

<p>$$\eqalign{
  &amp; \sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  + \sum\limits_{k = 0}^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a - k}}}  - \frac{1}{a} =   \cr 
  &amp;  = \left\{ {\frac{1}{2}\psi \left( {1 - \frac{a}{2}} \right) - \frac{1}{2}\psi \left( {\frac{a}{2}} \right)} \right\} - \left\{ {\frac{1}{2}\psi \left( {1 - \frac{{a + 1}}{2}} \right) - \frac{1}{2}\psi \left( {\frac{{a + 1}}{2}} \right)} \right\} \cr} $$</p>

<p>But using the reflection formula one has</p>

<p>$$\eqalign{
  &amp; \frac{1}{2}\psi \left( {1 - \frac{a}{2}} \right) - \frac{1}{2}\psi \left( {\frac{a}{2}} \right) = \frac{\pi }{2}\cot \frac{{\pi a}}{2}  \cr 
  &amp; \frac{1}{2}\psi \left( {1 - \frac{{a + 1}}{2}} \right) - \frac{1}{2}\psi \left( {\frac{{a + 1}}{2}} \right) = \frac{\pi }{2}\cot \frac{{\pi \left( {a + 1} \right)}}{2} =  - \frac{\pi }{2}\tan \frac{{\pi a}}{2} \cr} $$</p>

<p>So the series become</p>

<p>$$\eqalign{
  &amp; \sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \frac{\pi }{2}\left\{ {\cot \frac{{\pi a}}{2} + \tan \frac{{\pi a}}{2}} \right\}  \cr 
  &amp; \sum\limits_{k =  - \infty }^\infty  {\frac{{{{\left( { - 1} \right)}^k}}}{{a + k}}}  = \pi \csc \pi a \cr} $$</p>

<p>The last being an application of a trigonometric identity.</p>
",<calculus>
"<p>$$x^{\frac{2}{3}}-2x^{\frac{1}{3}}-3=0$$
I need some help solving this reducible to quadratic equation.</p>
",<calculus>
"<p>I'm studying for a test and this question has me really stumped:</p>

<p>$f(x) = 2x^3+5x+3$. Find x if $f^{-1}(x) = 1$</p>

<p>I don't know how I am supposed to figure out the inverse of this polynomial. I used <a href=""http://www.wolframalpha.com/widgets/view.jsp?id=d08726019e4a2a15cb1d49092e4d0522"" rel=""nofollow"">this widget</a> from Wolfram|Alpha to find the inverse, and it returned an enormous and confusing formula which doesn't seem humanly possible to calculate:</p>

<p><img src=""http://i.stack.imgur.com/QbzVB.gif"" alt=""Wolfram|Alpha output""></p>

<p>(not sure if the formula image generated by the widget will get deleted or not)</p>

<p>How would one even go about solving that type of problem?</p>

<p>Thanks</p>
",<calculus>
"<h2><strong>Updated Question</strong></h2>

<p>Assuming I want to differentiate function using Chain Rule,  $\frac {x^5}{(3+ 2x^{8})},$ 
The Chain Rule says, $(g\circ f)'(x) = f'(x)\cdot g'(f (x))$</p>

<p>So what's the logic or steps to determine $f(x)$ and $g(x)$?</p>

<p>PS: I have the answer using Quotient Rule.</p>

<hr>

<p><strong>Here is how I solve it finally using <em>arbitrary</em> function f(x) and g(x).</strong> </p>

<ol>
<li>separate $x^{5}$ as h(x)</li>
<li>$f(x) = (3+2x^{8})$</li>
<li>$g(x) = x^{-1} = g(f(x)) = (3+2x^{8})^{-1}$</li>
<li>$\frac{d}{dx} h(x).g(x)$ = $\frac{d}{dx} x^{5}.[x^{-1}]$</li>
<li><p>using Product rule, 
$\frac{d}{dx} x^{5}.[x^{-1}] = 5x^{4}.g(x) + x^{5}.g'(x)$;
This g'(x) = the derivative of composition function, $(g\circ f)'(x)$</p></li>
<li><p>applyg Chain rule to get g'(x), the composition function,
$$(g\circ f)'(x)$$ = inside function's derivative . outside function's derivative.</p></li>
<li><p>$f'(x) = 16x^{7}$</p></li>
<li>$(g\circ f)'(x) = 16x^{7} . (-1)[x^{-2}]$</li>
<li>plug in f(x) into outside function's x,
$$(-1)[x^{-2}] = \frac{-1}{(3+2x^{8})^{2}} $$</li>
<li>Thus, 
$(g\circ f)'(x) = 16x^{7} .\frac{-1}{(3+2x^{8})^{2}} $</li>
<li>going back to where we pause at Product rule at Step 5 and applying each solved segment,
$\frac{d}{dx} x^{5}.[x^{-1}] = 5x^{4}.g(x) + x^{5}.(g\circ f)'(x) = \frac{5x^{4}}{(3+2x^{8})} -\frac{16x^{7}}{(3+2x^{8})^{2}} $</li>
</ol>
",<calculus>
"<p>So I have a two fold question, one I believe is simple but my algebra seems to be off, the other involves the trapezoidal rule of integration using Mathematica as an aid. Here they are:</p>

<p>$1.\quad \displaystyle \int_{-\infty}^{\infty} \frac{\operatorname{sech}(x)}{x^2+1} dx = \int_{-1}^{1} \operatorname{sech}\left(\frac{1}{1-t^2}\right)\frac{t^2+1}{t^4-t^2+1} dt$</p>

<p>I know I need to let $x = \frac{t}{1-t^2}$ and take the limits as $t \to \infty$,change the limits of integration and do the same for $t \to -\infty$ but I can't seem to nail it down. Why are my limits going to be $-1$ and $1$?</p>

<p>$2$. Space five points equally from $-1$ to $1$ and compute the four trapezoid approximation of
$\int_{-1}^{1} \mathrm{sech}(\frac{1}{1-t^2})\frac{t^2+1}{t^4-t^2+1} dt$ using Mathematica to evaluate $\operatorname{sech}(x)$. To be honest, I'm not really sure what the question is asking. Am I breaking the integral up into four integrations the first of which is from $-1$ to $-0.5$? How do I use Mathematica to evaluate? Any help is appreciated.</p>
",<calculus>
"<p>Why $g_n =\inf _{k \geq n} f_k $ is a nondecreasing sequence? Given that $f_k$ are non negative for all $k$?</p>

<p>I believe it is because if $m &gt; n$, then $\{f_n\} \subseteq \{ f_m \} $ and hence $\inf_{k \geq m} f_k &gt; \inf_{k \geq n} f_k $. Is this correct?</p>
",<calculus>
"<p>This is the integral and the solution has the following steps outlined </p>

<p>$$\int \frac{\sqrt{x+4}}{x}dx$$</p>

<p>$$u=\sqrt{x+4}$$
$$u^2=x+4$$
$$2u\,du=dx$$</p>

<p>$$\int \frac{u}{u^2-4}(2u\,du)$$
$$\int \frac{2u^2}{u^2-4}\,du$$</p>

<p>I'm very comfortable with doing all of the above... no issues there, but the next step is where I get lost:</p>

<p>$$\int \left(2+ \frac{8}{u^2-4}\right) \, du$$</p>

<p>It's probably something very small I'm overlooking, but how did they get the term $2$ and $8$ in the numerator of the other term?</p>
",<calculus>
"<p>So I wish to find for each of these functions a Lipschitz constant or prove that none exists. So my definition for a function to be Lipschitz is:</p>

<p>A function $f:[a,b] \rightarrow \mathbb{R}$ is Lipschitz if there exists a $L$ such that $|f(x) - f(y)| \leq L|x-y|$ for all $x,y \in [a,b]$.</p>

<ol>
<li><p>$f(x) = \frac{1}{x}$ for $x \in (0, 1]$ </p></li>
<li><p>$f(x) = e^x$ for $x \in \mathbb{R}$</p></li>
<li><p>$f(x) = \sqrt{1-x^2}$ for $x \in [-1,1]$</p></li>
</ol>

<p>My attempt for 1) to prove $f$ is not Lipschitz is via contradiction. Suppose that $x, y \in (0,1]$ and $f$ is Lipschitz. Then there exists a $L$ such that,</p>

<p>$$|\frac{1}{x} -\frac{1}{y}| = \frac{|x-y|}{|xy|} \leq L |x-y|$$ for all $x,y \in (0, 1]$. This would imply that, $\frac{1}{|xy|} \leq L$ for all $x,y \in (0, 1]$. But such a $L$ cannot exist since we can make $x, y$ as small as we like, the fraction will grow.</p>

<p>So in conclusion I attempted 1) but not sure if I am correct. I am stuck on 2 and 3. Anybody can give me some hints? I'm thinking of using the Mean Value Theorem on question 2. Other than that I have no idea how to start.</p>
",<calculus>
"<p>Please help with this: </p>

<p>Let 
$$f(z,b) = \begin{cases} z\sin(1/b), &amp; b \ne 0, \\ 0, &amp; b=0. \end{cases} $$</p>

<p>I'm trying to calculate the limit $f(z,b)$ for $(z,b)$ approach to $0$, in order to check if $f$ is continuously at $(0,0)$.</p>

<p>Please help with this way if is the right way...</p>
",<calculus>
"<p>I am trying to express the derivative of the outer product of the $(n\times m)$-matrix 
$\mathbf{A}=\mathbf{A}(\mathbf{x})$ with respect to the $p$-vector $\mathbf{x}$. This is, I want to rewrite $\frac{\partial \mathbf{A}\mathbf{A}^T}{\partial \mathbf{x}}$ using a product rule. My intuition tells me that I must have something like
$$
\frac{\partial \mathbf{A}\mathbf{A}^T}{\partial \mathbf{x}}=\mathbf{A}\otimes\frac{\partial \mathbf{A}}{\partial \mathbf{x}}+\frac{\partial \mathbf{A}}{\partial \mathbf{x}}\otimes\mathbf{A}.
$$
Any help or confirmation on this? Thanks!</p>
",<calculus>
"<p>For the function $$f(x)=b^x-1 = x_1 \qquad g(x)=\log(1+x)/\log(b) $$ and its iterative notation $$ x_0=x \qquad x_h=f(x_{h-1})=g(x_{h+1}) \qquad x_{-1}=g(x_0) $$ with <em>b</em> from the interval $1 \lt b \lt e$ we can define alternating series with infinitely many terms with negative as well with positive indexes. Let us denote the idea
$$A(x) = \ldots + x_{-2} - x_{-1} + x - x_1 + x_2 - \ldots $$
and define it by
$$ \begin{eqnarray}A(x) &amp;=&amp; A_n(x) + A_p(x) - x \\
\text{ where } \qquad A_n &amp;=&amp; \sum_{k=0}^\infty (-1)^k x_{-k} \\
\text{ and } \qquad A_p &amp;=&amp; \sum_{k=0}^\infty (-1)^k x_{k} \\
 \end{eqnarray} $$
With, say, $b=1.3$ and $x=3$ the series $A_p$ is convergent and $A_n$ must be evaluated using Cesaro/Euler-summation. It is obvious that this is 2-periodic, that means $$A(x)=A(x_2)=A(x_4)=...=A(x_{-2})=... $$<br>
such that it is irrelevant where I define the center $x$  as long as I choose the iterates in steps of <em>2</em>.
<hr>
I expect, that this should be true for the derivatives in the same way, but with some examples I find, that this is not the case. 
$$ ... \ne A'(x_{-2}) \ne A'(x) \ne A'(x_2) \ne ... \\  ... \ne A''(x_{-2}) \ne A''(x) \ne A''(x_2) \ne ... $$<br>
where I compute the derivatives in various ways, for instance simply by 
$$A'(x)=\lim_{h\to0} {A(x+h/2)-A(x-h/2) \over h} $$
<strong>Q:</strong> I think I must have some basic misunderstanding here (which, I think, is not specific to the given example function $f(x)$ and $g(x)$ here). Can someone explain this to me? 
<hr>
<strong>[Added:]</strong> It seems, I've found one hint myself: checking numerically I found that 
$$A(x_0)' = - x_1' \cdot A(x_1)' \quad \text{ where } \quad x_1' = \lim_{h\to0} {f(x_0+h/2)-f(x_0-h/2)\over h}$$
So at least the result is not completely random (and in particular not depending on problems with the Cesaro/Euler-summation), but my intuition is still unable to grasp this conceptually...
<hr>
Numerical example using $ b=1.3 , x_0=3.2 $
$$ \small
\begin{array} {rclll}
   &amp;   x_0 &amp;         A(x_0) &amp;         A'(x_0) &amp;  A''(x_0) \\       
    &amp; 3.20000000000 &amp; -0.00119822450167&amp; 0.0175377529574&amp; 0.000817628416425 \\
   &amp;   x_1 &amp;         A(x_1) &amp;         A'(x_1) &amp;  A''(x_1) \\       
 &amp;1.31536107268&amp; 0.00119822450167&amp; -0.0288702496568&amp; 0.0102533145478 \\
   &amp;   x_2 &amp;         A(x_2) &amp;         A'(x_2) &amp;  A''(x_2) \\
     &amp;0.412136407584&amp; -0.00119822450167&amp; 0.0779236358328&amp;   -0.129878114856
\end{array} $$</p>
",<calculus>
"<p>I know that's a very common theorem in calculus but when I try to prove it with $\epsilon-\delta$ definition of continuity, I found that it is not so obvious.</p>

<p>Attempts: Let $f:\mathbb{R}\to\mathbb{R}$ be a function differentiable at point $a$ $\implies$ $\forall \epsilon&gt;0, \exists \delta&gt;0 \text{ s.t. } |\frac{f(x)-f(a)}{x-a}-f'(a)|&lt;\epsilon$ for any $|x-a|&lt;\delta$. So what we want to show is $\forall \epsilon&gt;0$, we can find an $\delta&gt;0$s.t. $|f(x)-f(a)|&lt;\epsilon$ for any $|x-a|&lt; \delta$. First of all, we can applies the triangular inequality$|f(x)-f(a)|\le |f(x)-f(a)-f'(a)(x-a)|+|f'(a)(x-a)|&lt;\epsilon+|f'(a)(x-a)|$ but I found that $|f'(a)(x-a)|$ could be very large even $\epsilon$ can be any real number. Thx</p>
",<calculus>
"<p>I am given that $\sum\limits_{n=1}^\infty a_n$ is convergent. </p>

<p>I need to determine whether $\sum\limits_{n=1}^\infty (a_n)^\frac{1}{3}\;$ and $\;\sum\limits_{n=1}^\infty (a_n)^2\;$ are also convergent.</p>

<p>Imagine that $a_n = \dfrac{1}{n^4}.\;$ I believe that this is convergent because it's converging to $0$.</p>

<p>Following  the same thought, if $\displaystyle a_n = \left(\frac{1}{n^4}\right)^2,\;$ it's convergent because it's converging to $0$.</p>

<p>Am I doing this correctly or there is some other way to prove this?</p>
",<calculus>
"<p>Solve the differential equation
$$y''+(1-2x \cos x \cos 2x)y=0 \space $$</p>
",<calculus>
"<p>we hae a cylindrical tank which is full and depth is 10 meters. The rate at which the depth of liquid drops is proportional to the square root of the depth of the liquid. After draining for 20 minutes, the tank has now depth 5 meters. How long it takes for tank to empty?</p>

attempt:

<p>Let $H(t)$ be the height of the tank. We know $H(0)=10$ an $H(20)=5$. Also, since </p>

<p>$$ H' = \sqrt{H} \implies \int \frac{ \mathrm{d} H }{\sqrt{H}} = \int \mathrm{d} t \implies 2 \sqrt{H} = t + C$$</p>

<p>So, $2 \sqrt{5} = 20+ C $ so $C = 2 \sqrt{5}-20$. Thus,</p>

<p>$$ \sqrt{H} = \frac{t}{2} + \sqrt{5} - 10  $$</p>

<p>so, $H=0$ if $ t = \dfrac{ 10 - \sqrt{5} }{2} $</p>

<p>is this correct?</p>
",<calculus>
"<p>There is a nice $\exp$-like function, which can be defined on $\mathbb{R}$ by its Taylor series or by an integral:</p>

<p>$$p(x)=\sum_{k=0}^\infty \frac{x^k}{(k+1)^{k+1}}=\int_0^1 u^{-u x}du$$</p>

<p>I had a thought to define $\cos$-like and $\sin$-like functions the same way they are defined for the exponent:</p>

<blockquote>
  <p>$$pc(x)=\frac{1}{2}(p(ix)+p(-ix))=\sum_{k=0}^\infty \frac{(-1)^k x^{2k}}{(2k+1)^{2k+1}}=\int_0^1 \cos (x ~u \ln u)du$$</p>
</blockquote>

<p><a href=""http://i.stack.imgur.com/Rkzb7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Rkzb7.png"" alt=""enter image description here""></a></p>

<blockquote>
  <p>$$ps(x)=\frac{1}{2i}(p(ix)-p(-ix))=\sum_{k=0}^\infty \frac{(-1)^k x^{2k+1}}{(2k+2)^{2k+2}}=-\int_0^1 \sin (x~ u \ln u)du$$</p>
</blockquote>

<p><a href=""http://i.stack.imgur.com/TUp7n.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/TUp7n.png"" alt=""enter image description here""></a></p>

<p>I'm not sure how to use either of these definitions to find the zeroes (or the maxima/minima, etc) of $pc(x)$ and $ps(x)$.</p>

<blockquote>
  <p>I guess like for Bessel functions there might be no closed form expressions for these zeroes. But how to find them numerically in an effective way, without trial and error and computing either the series or the integral several times?</p>
</blockquote>
",<calculus>
"<p>I have the problem:</p>

<p>$$a_{n} = \frac{e^{n+5}}{\sqrt{n+7}(n+3)!}$$</p>

<p>I am told to use the ratio test to determine convergence or divergence (or the test could be inconclusive).</p>

<p>So I take the limit:</p>

<p>$$\lim_{n\to \infty} \frac{a_{n+1}}{a_{n}}$$
$$\lim_{n\to \infty} a_{n+1}\frac{1}{a_{n}}$$</p>

<p>$$\lim_{n\to \infty} \frac{e^{n+6}}{\sqrt{n+8}(n+4)n!} \frac{\sqrt{n+7}(n+3)n!}{e^{n+5}}$$
$$= (\lim_{n\to \infty} \frac{e^{n+6}}{e^{n+5}}) (\lim_{n\to \infty} \frac{\sqrt{n+7}}{\sqrt{n+8}}) (\lim_{n\to \infty} \frac{n+3}{n+4})$$</p>

<p>I reorganized and canceled the <strong><em>n!</em></strong> terms.</p>

<p>I keep getting:
$$(\lim_{n\to \infty} \frac{e^{n+6}}{e^{n+5}}) = e$$
$$(\lim_{n\to \infty} \frac{\sqrt{n+7}}{\sqrt{n+8}}) = 1$$
$$(\lim_{n\to \infty} \frac{n+3}{n+4}) = 1$$</p>

<p>So my limit is <strong><em>e</em></strong>, which indicates the series diverges. However, according both to MathWorks and WolframAlpha this is incorrect. The limit is actually 0, indicating the series converges.</p>

<p>Where am I going wrong?</p>
",<calculus>
"<p>Why does $$\lim_{x \to 1}\arccot\left(\frac{x^2+1}{x^2-1}\right)$$ diverge? In my textbook it says that from the positive side it's zero, and from the negative side it's $\pi$. However, when entering it into <a href=""https://www.symbolab.com/solver/step-by-step/%5Clim_%7Bx%5Cto1%7D%5Cleft(arccotan%5Cleft(%5Cfrac%7B%5Cleft(x%5E%7B2%7D%2B1%5Cright)%7D%7Bx%5E%7B2%7D-1%7D%5Cright)%5Cright)/?origin=enterkey"" rel=""nofollow"">Symbolab</a> the solution is zero (according to them, it converges). I also got $0$ in both cases, using my caveman-plug-in-the-values technique. I guess I'm solving this limit incorrectly.</p>

<p>What is the real solution? If I was analyzing this function and I messed up the vertical asymptote (which I'm doing in this particular problem), the rest of my analysis would be incorrect.</p>

<p>Can someone clear this out for me? </p>
",<calculus>
"<p>I'm trying to solve this integral with trigonometric substitution but am having a ton of trouble:
$$\int\limits_{0}^{a}{\frac{dx}{(a^2+x^2)^{\frac{3}{2}}}}$$</p>

<p>I tried $x=a\tan{\theta}$ and thus $dx=a\sec^2{\theta}$ but I cannot get anywhere.</p>
",<calculus>
"<p>Find $x$ such that 
\begin{equation}
x\tanh(x\sqrt{2\alpha})=\frac{2}{\sqrt{2\alpha}}
\end{equation}</p>
",<calculus>
"<p>This <em>limit</em> thing keeps coming up in my calculus textbook.  What is it?</p>
",<calculus>
"<p>I keep seeing this symbol $\nabla$ around and I know enough to understand that it represents the term ""gradient."" But what is a gradient? When would I want to use one mathematically?</p>
",<calculus>
"<p>I keep hearing about this weird type of math called calculus. I only have experience with geometry and algebra. Can you try to explain what it is to me?</p>
",<calculus>
"<p>I've come across statements in the past along the lines of ""function $f(x)$ has no closed form integral"", which I assume means that there is no combination of the operations:</p>

<ul>
<li>addition/subtraction</li>
<li>multiplication/division</li>
<li>raising to powers and roots</li>
<li>trigonometric functions</li>
<li>exponential functions</li>
<li>logarithmic functions</li>
</ul>

<p>which when differentiated gives the function $f(x)$. I've heard this said about the function $f(x) = x^x$, for example.</p>

<p>What sort of techniques are used to prove statements like this? What is this branch of mathematics called?</p>

<hr>

<p>Merged with ""<a href=""http://math.stackexchange.com/questions/2328/"">How to prove that some functions don't have a primitive</a>"" by <a href=""http://math.stackexchange.com/users/918/ismael"">Ismael</a>:  </p>

<p>Sometimes we are told that some functions like $\sin(x)/x$ don't have a indefinite integral, or that it can't be expressed in term of other simple functions.</p>

<p>I wonder how we can prove that kind of assertion?</p>
",<calculus>
"<blockquote>
  <p>Find an equation of the tangent line to the curve $2·(x^2+y^2)^2=25·(x^2−y^2)$ at the point 
  $(3,1)$.</p>
</blockquote>

<p>Any hints?</p>
",<calculus>
"<p>I have to draw the graphic of ""group of points given by the equation $$(|z^2|-3|z|+2)(z^4+4)=0$$ I solved the first part by factoring and obtaining $|z|=1$ and $|z|=2$ so in the graphic I have the points that stay on two circumferences with the respective radii. For the second part: $$z^4=-4 \to z=\sqrt2*i^{(1/4)}$$ so I'm sure I have 4 points that ""form a square"" but I don't know how to proceed correctly. I saw a question similar to mine but the answers were given using exponential forms that I don't know and that are not in my program. I need  a guideline that show me how to proceed when I have to solve this kind of equation or also the simpler equation form $z=v^{n/n}$, I need to have clear ideas and I'll be grateful if you can help me. Thank you in advance!</p>
",<calculus>
"<p>Find the Integral: </p>

<blockquote>
  <p>$$\int\limits_{0}^{\pi/6}\sin{x}\cos^2{x}\,\mathrm dx$$</p>
</blockquote>

<p>The answer is 1/3- square root of 3 over 8.</p>

<p>What method should one use to solve a problem like this?¨</p>

<p><img src=""http://i.stack.imgur.com/tSKzp.jpg"" alt=""enter image description here""></p>
",<calculus>
"<blockquote>
  <p><strong>Problem:</strong> Find the limit $ \displaystyle L = \lim_{x \to \infty} \left( x \ln x + 2 x \ln \sin \frac{1}{\sqrt{x}} \right) $.</p>
</blockquote>

<p>Please suggest how to proceed in this problem. I will be grateful to you. Thanks.</p>
",<calculus>
"<p>Problem : </p>

<p>Consider $f(x) =3x^2+2x+a$ where a is parameter such that $\frac{da}{dt}=3$ Let $a =0$, when $t =0$ and $A(t) =\int^t_0 \{f(x)\}\,dx$ ( where $\{\cdot\}$ denotes the fractional part function ). If $A(2)$ can be expressed as sum of $n$ integrals then find the minimum value of $n$.</p>

<p>My approach : </p>

<p>$$\frac{da}{dt}=3$$</p>

<p>Integrating both sides we get : </p>

<p>$$a =3t +c $$</p>

<p>Please suggest how to proceed further will be of great of help. Thanks.</p>
",<calculus>
"<p>Could someone please indicate precisely the difference between a scalar and a vector field? I find no matter how many times I try to understand, but I always am confused in the end. So what exactly makes them different?</p>
",<calculus>
"<p>$$\int_1^2 {1 \over (1-x)^2}dx$$
$$u = 1 - x$$
$$-du = dx$$</p>

<p>$$\lim_{a \to 0^-}-\int_0^{-1} {du \over u^2} = \lim_{a \to 0^-}\int_{-1}^{a} {du \over u^2}$$</p>

<p>$$\lim_{a \to 0^-} {1 \over u}|_{-1}^a $$
$$\lim_{a \to 0^-} {1 \over a} - {1 \over -1} = \infty + 1 = \infty \ = D.N.E. $$</p>
",<calculus>
"<p>Can I use the comparison test for the following problem?
$$\sum _{n=1}^{\infty }\:\frac{1}{n^2-6n+10}$$</p>

<p>The denominator has a negative coefficient so i'm not sure if its valid to compare it to a p-series. Does this effect anything, or is it valid to to say this is convergent because it's less than 1/n^2?</p>

<p>Should I use the integral test instead?</p>
",<calculus>
"<p>Evaluate $\int_0^x \frac{dt}{1+\cos^2t}$ $\forall x \in \mathbb{R}$</p>

<p>I got this question in an analysis exam, and I did what everybody does (<a href=""http://math.stackexchange.com/questions/1244055/integrate-frac11-cos2x-probably-need-using-some-trigonometric-identity"">this</a>), I made $u=\tan t$ and I got $\frac{1}{\sqrt2}\tan^{-1}\left(\frac{\tan x}{\sqrt2}\right)$ but I know this is wrong because $\tan x$ is not continuous in every $[0,x]$, <a href=""http://math.stackexchange.com/questions/638025/definite-integration-problem/638035#638035"">here</a> it's solved until $\pi$, but my limit is $x$ so I'm not sure what should I do.</p>

<p>Also, we have the integral of a positive amount, so my answer should be an increasing function...</p>

<p><a href=""http://math.stackexchange.com/questions/170885/a-little-integration-paradox"">Here</a> is explained why this happens (which I already know), and the answers there don't solve the integral between $0$ and $x$. One answer gives a possible result (that have to be adapted, since there it is indefinite), but no deduction of it. The answer given here is much more complete and perfectly address my specific question.</p>
",<calculus>
"<p>Show that for any finite value of $z$
$$e^z=e+e\sum_{n=1}^\infty \frac{(z-1)^n}{n!}$$</p>

<p>For $z=1$
$$f(z)=f(z_0)+\sum f^{(n)}(z_0)\frac{(z-z_0)^n}{n!}$$</p>

<p>equality is checked, but I do not know how to show for any finite value of z, I tried to apply the principle of induction but not worked well.</p>
",<calculus>
"<p>Let</p>

<ul>
<li>$A\in\mathbb{R}^{n\times n}$ and $b\in\mathbb{R}^n$</li>
<li>$f\in C^2(\mathbb{R}^n)$ and $\tilde{f}(x):=f(Ax+b)$ for $x\in\mathbb{R}^n$</li>
</ul>

<p>It's easy to prove that $$\nabla\tilde{f}(x)=A^T\nabla f(x)$$ But I'm not able to prove that the Hessian matrix $$\nabla^2\tilde{f}(x)=A^T\nabla^2 f(x)A$$</p>

<hr>

<p>Shouldn't we have $$\nabla^2\tilde{f}(x)=\left(\begin{matrix}\frac{\partial}{\partial x_1}\\\vdots\\\frac{\partial}{\partial x_n} \end{matrix}\right)A^T\nabla f(x)=A\left(\begin{matrix}\frac{\partial}{\partial x_1}\\\vdots\\\frac{\partial}{\partial x_n} \end{matrix}\right)\nabla f(x)=A\nabla^2f(x)\;?$$</p>
",<calculus>
"<p>$$\int {\sqrt{x^2-49} \over x}\,dx $$
$$ x = 7\sec\theta$$
$$ dx = 7\tan\theta \sec\theta \,d\theta$$
$$\int {\sqrt{7^2\sec^2\theta - 7^2} \over 7\sec\theta}\left(7\tan\theta \sec\theta \,d\theta\right) = \int \sqrt{7^2\sec^2\theta - 49} \left(\tan\theta d\theta\right)$$
$$ \int\sqrt{7^2(\sec^2\theta - 1)} (\tan\theta \,d\theta) = 7\int\sqrt{\sec^2\theta - 1} (\tan\theta \,d\theta)$$
$$ 7\int \tan^2\theta \,d\theta = 7\int \sec^2\theta - 1 \,d\theta  $$
$$ 7\int \sec^2\theta - 7\int d\theta $$ 
$$ 7\tan\theta - 7\theta + C  = 7(\tan\theta - \theta) + C$$</p>

<p>This makes: $$ \theta = \sec^{-1}\left(x \over 7\right)$$</p>

<p>And plugging back in to the indefinite integral:</p>

<p>$$ 7\left(\left(\sqrt{x^2-49} \over 7 \right) - \sec^{-1}\left(x \over 7 \right)\right) + C $$</p>

<p>My question really is, how can I evaluate $\sec^{-1}\left(x \over 7 \right)$ ?</p>
",<calculus>
"<p>I'm having trouble with this problem because it has $n$ as an exponent. Which I'm not really sure on how to deal with.</p>

<p>$$\lim_{n \to \infty} \frac{4+(-1)^n}{n^2}$$</p>
",<calculus>
"<p><img src=""http://i.stack.imgur.com/90mdk.png"" alt=""enter image description here""></p>

<p>Can someone help me with these limits?
At (a) I found the onesided limits and the right one was 0 and the left one was -infinite. I don't if i am correct</p>
",<calculus>
"<p>I need help proving that $g(x)= 1 - e^{-x}$ has unique fixed point on $[1,2]$.</p>

<p>Sorry in my previous post I accidentally type 1+e^-x, as opposed to 1-e^-x</p>
",<calculus>
"<p><img src=""http://i.stack.imgur.com/dBAqc.png"" alt=""enter image description here""></p>

<p>I think that it has to into specific cases but I don't really know how many cases would there be, can someone explain it even if they dont sketch it</p>
",<calculus>
"<p><strong>Question</strong>
Given $K_n=\cases 0$ elsewhere , $n- n^2|x|$ for $x&lt;|\frac 1n|$
,
 $f$ is continuous, $2\pi$ periodic $\Bbb R \to \Bbb C$
. </p>

<p>$f_n(x)=\int _{-\pi}^ \pi f(t)Kn(x-t)$ </p>

<p>prove that $f_n\to^uf$.</p>

<p><strong>Thoughts</strong>
I know it has something to do with Fourier series and Fejer kernel. Since this is not the same kernel - I don't really know how to handle it.</p>
",<calculus>
"<blockquote>
  <p>I tried to prove that it does not converge by showing that the series $b_n=\frac{\sin a_n}{a_n}$ does not converge to $0$. But I am stuck with a limit of the kind $\frac{0}{0}$. </p>
</blockquote>

<p>Thanks for helping me proceeding from here. </p>
",<calculus>
"<p>I trying to find Fourier series and I see a lot of times that $\int^\pi_{-\pi}$ of absolute $|\cos| \text{ and } |\sin| $ is 4.</p>

<p>for example, $\int^\pi_{-\pi} (2-|x|) \cos(x) = 4 \text{ and } \int^\pi_{-\pi} |\sin(x)| = 4$.</p>

<p>What is the trick behind integrals over $[-\pi, \pi]$ of absolute-valued cosine and sine?</p>

<p>Thanks in advance!</p>
",<calculus>
"<p>I would appreciate if somebody could help me with the following problem</p>

<p>Q: Find the maximum of $$f(x) =\frac{x-\ln(1+x)}{x\ln(1+x)};\quad(x&gt;0)$$</p>
",<calculus>
"<p>The poisson kernel is sometimes written as
$$
\frac{1}{2\pi}\int_0^{2\pi} \frac{R^2-r^2}{R^2 - 2Rr\cos(\varphi-\vartheta) + r^2} \mathrm{d}\vartheta = 1 \ , \ \ R&gt;r&gt;0
$$
Where $\varphi$ is some arbitary angle. 
This much used in complex analysis amongst other fields. Is there some basic, elementary way of showing that the integral is independent on $R$ and $r$?</p>

<p>Splitting the integral at $\int_0^\pi + \int_\pi^{2\pi}$ and using the Weierstrass substitution seems like somewhat ugly and a  messy way to approach the problem.</p>
",<calculus>
"<p>How do I solve:
$$ 
\frac{1}{\pi}(x \tan x) = \frac{1}{8}
$$
for $x$ where $x$ is in $[0,\pi]$?</p>

<p>The problem states: </p>

<blockquote>
  <p>""Determine if the graph of the curve $y = (1/\pi)(x\tan x)$ crosses
  the line $y = (1/8)$ at some point $x$ in $[0, \pi]$. Justify answer.""</p>
</blockquote>

<p>I have already plotted this and see that there is 2 intersection points at around ±0.588, my problem is that I cannot solve the function numerically thus unable to prove how i got to the answer.</p>

<p>Any help is appreciated.</p>

<p><a href=""http://i.stack.imgur.com/lsiAKm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/lsiAKm.png"" alt=""graph of y""></a> (<a href=""http://i.stack.imgur.com/lsiAK.png"" rel=""nofollow"">Large version</a>)</p>
",<calculus>
"<blockquote>
  <p>I would like to prove
  $$ \forall x &gt;0,  \quad \sqrt{x +2} - \sqrt{x +1} \neq \sqrt{x +1}-\sqrt{x}$$</p>
  
  <ul>
  <li>I'm interested in more ways of proving it</li>
  </ul>
</blockquote>

<p><strong>My thoughts</strong>:</p>

<p>\begin{align}
\sqrt{x+2}-\sqrt{x+1}\neq \sqrt{x+1}-\sqrt{x}\\
\frac{x+2-x-1}{\sqrt{x+2}+\sqrt{x+1}}&amp;\neq \frac{x+1-x}{\sqrt{x +1}+\sqrt{x}}\\
\frac{1}{\sqrt{x+2}+\sqrt{x+1}}&amp;\neq \frac{1}{\sqrt{x +1}+\sqrt{x}}\\
\sqrt{x +1}+\sqrt{x} &amp;\neq \sqrt{x+2}+\sqrt{x+1}\\
\sqrt{x} &amp;\neq \sqrt{x+2}\\
\end{align}</p>

<ul>
<li>Is my proof correct? </li>
<li>I'm interested in more ways of proving it.</li>
</ul>
",<calculus>
"<p>I want to evaluate this integral:</p>

<blockquote>
  <p>$$\int{\frac{ax+b}{(x^2+2px+q)^n}}dx$$</p>
</blockquote>

<p>The book only says to integrate by parts $\int{\dfrac{1}{(x^2+2px+q)^{n-1}}dx}$,
for simplicity if $n = 2$ I get:
$\int{\dfrac{1}{(x^2+2px+q)}dx}=\dfrac{x}{x^2+2px+q}+\int{\dfrac{2x^2+2px}{(x^2+2px+q)^2}}dx$.</p>

<p>Now I don't know what to do.</p>
",<calculus>
"<p>Is there a not identically zero, real-analytic function $f\colon\mathbb R\to\mathbb R$, which satisfies</p>

<p>$$f(n)=f^{(n)}(a),n\in\mathbb N \text{ or }\mathbb N^+?$$</p>

<p>and $a\in \mathbb R$</p>

<p><a href=""http://math.stackexchange.com/questions/91855/is-there-a-function-with-the-property-fn-fn0/91927#91927"">I saw a special case when $a=0$</a></p>

<p>I try to solve it by :</p>

<p>$$f(x)=e^{cx}$$
$$f(n)=e^{nc}$$
$$f^{(n)}(x)=c^ne^{cx}$$
$$f^{(n)}(a)=c^ne^{ca}$$
so $$e^{nc}=c^ne^{ca}$$
<a href=""http://www.wolframalpha.com/input/?i=solve%20%28c%5En%29%2a%28e%5E%28c%28a-n%29%29%29=1%20for%20c"">so</a> $$c=\frac{nW(\frac{a-n}{n})}{a-n}$$</p>

<p>the problem is we always see  n with c 
but the special case when a=0 give </p>

<p>$$c=\frac{nW(\frac{0-n}{n})}{0-n}$$
$$c=\frac{W(\frac{-1}{1})}{-1}=-W(-1)$$</p>

<p>I think there is no solution when $a\neq 0$</p>

<p>may be there is another function can solve it</p>

<p>Is there any solution in general?</p>

<p>thanks for all</p>
",<calculus>
"<p>I've just read the proof of a theorem which states that if a function of several variables(two in this case) has partial derivatives in some neighborhood of a point (x,y) and these derivatives are continuous at (x,y) then the function is differentiable at that point. The proof uses the mean value theorem and ,analytically, I understand the proof but I can't feel why it is necessary to have these derivatives in a neighborhood not just in a point. Can someone experienced share his intuitive view on this question and show me some intuitive reasons for this being necessary?</p>
",<calculus>
"<p>By ""curved"", I mean that there is no portion of a graph of a function that is a straight line. (Let us first limit the case into 2-dimensional cases.. and if anyone can explain the cases in more than 2-dimensional scenarios I would appreciate.. but what I really want to know is the 2-dimensional case.)</p>

<p>Let us also say that a function is surjective (domain-range relationship).</p>

<p>So, can every continuous and curved function be described as a formula, such as sines, cosines, combinations of them, or whatever formula?</p>

<p>thanks.</p>
",<calculus>
"<p>We are given a continuous function $f \colon [0,1] \to \mathbb{R}$. What is the value, if it exists, of the limit $$\lim_{t \to +\infty} \frac{1}{t} \log \int_0^1 \cosh (t f(x))\, \mathrm{d}x \ ?$$</p>

<p>PS: this is not homework. It's a question contained in a Ph.D test, and I am unable to make progress toward the solution :-(</p>
",<calculus>
"<p>The sphere is </p>

<p>$x^2 + y^2 + z^2 = 81$</p>

<p>and the point is $(5,6,9)$</p>

<p>I am using Langrane multipliers , but the answers I am getting are so far off. I will post my system of equations soon. </p>

<p>I found the gradient of $F: (2(x-5), 2 (y-6), 2(z-9))$<br>
Gradient of $G: (2x, 2y, 2z)$</p>

<p>$2x-10 = 2 \lambda x$<br>
$2y-12 = 2 \lambda y$<br>
$2z-18 = 2 \lambda z$<br>
$x^2+y^2+z^2 = 81$</p>

<p>This system seems very challenging to solve.</p>
",<calculus>
"<p>Find equation of the straight line tangent to the curve at the point indicated:</p>

<p>$y=2x^2 -5$ at $(2,3)$</p>

<p>I think I have to use $y=m(x-x_o)+y_0$ etc but I'm not sure how to find the $m$! Thanks for tips/solutions!</p>
",<calculus>
"<p>Please help me to prove the identity
$$_2F_1\!\left(\frac76,\frac12;\,\frac13;\,-\phi^2\right)=0,$$
where $\phi=\frac{1+\sqrt5}2$ is the golden ratio.</p>
",<calculus>
"<p>Can you give me a very precise demonstration of this result please because it's very difficult for me to understand the demonstration on the pic :( </p>

<p>$$
\sum_{(p,q) \in {\mathbb{N}^*}^2 \text{, } p \land q =1} \frac{1}{p^2 q^2} = \frac{5}{2}
$$ </p>

<p><img src=""http://i.stack.imgur.com/JXdQT.png"" alt=""enter image description here""></p>

<p>Thank you ! 
Shadock </p>
",<calculus>
"<p>Find the area (to three decimal places) bounded by $f(x)=x^2e^x$ and $q(x)=4-x^2$</p>

<p>So far I've gotten $x^2(e^x+1)-4=0$
and the two $x$ values that make the equation $0$ are $1.027$ and $-1.86$
next I tried to integrate the function to find the area.</p>

<p>I got $A= \int(4-x^2-x^2e^x)dx= \left[4x-\frac{x^3}{3}-e^x(x^2-2x+2)\right]_{-1.86}^{1.027}$.</p>

<p>when I calculated my answer and it was 9.788. This is wrong b/c here are the choices of answers: (A)7.0 (B)7.676 (C)7.333 (D)7.555</p>

<p>Can someone help me out?</p>
",<calculus>
"<p>It is the indefinite integral: $\int \frac{1}{2x-6}$ </p>

<p>I am trying to understand it and looking the last step goes from $\frac12 \log(2(x-3))$ to $\frac12 \log(x-3)$</p>

<p>Can someone explain to me why the two just disappears and the answer isn't just $\frac12 \log(2x-6)$</p>
",<calculus>
"<blockquote>
  <p>Suppose $f$ is (Riemann) integrable over $[c,b]$ for every $c\in (a,b)$ and the improper integral $\int_{a}^{b}|f|dx$ exists. Then $\int_{a}^{b}fdx$ also exists. Does the inverse hold?</p>
</blockquote>

<p>Here is what I've tried.</p>

<p>We know $|f|=f^++f^-$ (the positive and negative part of $f$, which are integrable over any $[c,b]$). By Comparison Test, $\int_{a}^{b}f^+dx$ and $\int_{a}^{b}f^-dx$ exist. Hence the improper integral of $f=f^+-f^-$ exists.</p>

<p>Is this okay?</p>

<p>Would anyone help me with the counterexample of the inverse? I'd appreciate it so much.</p>
",<calculus>
"<p>I am trying to take the following limit
$$
\lim_{t\to\infty} \frac{1}{t}\exp{\left(\frac{\sqrt[4]{2} \left(\sqrt{\pi }-1\right) t^B e^{-\frac{t^{2 B}}{2 \sqrt{2}}}}{\pi }-\frac{\left(2 \sqrt{2} \left(\sqrt{\pi }-1\right) t^{2 B}+\sqrt{\pi }+2\right) \operatorname{erfc}\left(\frac{t^B}{2^{3/4}}\right)}{4 \sqrt{\pi }}+\frac{1}{2}\right)},
$$
where 
$\operatorname{erfc} \left(z\right)$ is the complementary error function given by
\begin{equation}
\operatorname{erfc}(z) = \frac{2}{\sqrt{\pi}}\int_z^{\infty} \mathrm{e}^{-t^2} \,dt 
\end{equation}
and $0 \le B \le 1$.
I have tried using Lhospital's rule but the numerator started to unwind and I was not sure how to proceed after the first derivative.
Thank you.</p>
",<calculus>
"<p>Evaluate $$\int \frac{3x^2+1}{(x^2-1)^3}dx$$ I tried breaking the numerator in terms of the denominator but it didn't help much. I also tried a few substitutions buy most of them were useless. Please give some hints. Thanks.</p>
",<calculus>
"<p>Give me some hints to <strong>start</strong> with this problem: $${\displaystyle\int}\dfrac{2x^{12}+5x^9}{\left(x^5+x^3+1\right)^3}\,\mathrm{d}x$$</p>
",<calculus>
"<p>Prove $$\int^\infty_0 \frac{x^{n-1}}{1+x}dx= \frac{\pi}{\sin(n\pi)}$$</p>

<p>I would like to consider two circles of radii $\epsilon &lt; 1 &lt; R$ centered at the origin and connected by a corridor of width $\delta &gt; 0$. Then I want to choose a branch of the logarithm in $ \mathbb{C}$ \ $[0,\infty]$ and study its behavior on both sides of the corridor. But I don't know how to go about this to get the final result. Any help appreciated!</p>
",<calculus>
"<p>Let $f(x,y)$ be a differentiable two variable function such that $f(2,-1)=4, f_{y}(2,-1)=-2 , f_{y}(2,-1)=3$. Find $g_{x}(2,-1)$ and $g_{y}(2,-1)$ if:</p>

<p>a) $g(x,y)=f(3x+4y,2xy^2-5)$</p>

<p>b) $g(x,y)=f(\frac{1}{2}f(x,y),xy+1)$</p>

<p>can someone help me with some hints as how to tackle this problem because I haven't seen this kind of exercise. Thank you.</p>
",<calculus>
"<p>Does this diverge or converge ?? $$\sum_{n=1}^{\infty}(\frac{H_n}{p_n}-\frac{n}{n^n})$$
where $H_n$ is the nth harmonic number, $p_n$ is the nth prime.</p>

<p>My impression is that it diverges, but I don't see how I can prove it!
I tried on wolframalpha but no clue.</p>
",<calculus>
"<p>Hi I am trying to calculate
$$
I:=\int_0^\pi \theta^2 \ln^2\big(2\sin\frac{\theta}{2}\big)d \theta.
$$
Here is a related  <a href=""http://math.stackexchange.com/questions/699746/integral-int-0-pi-theta2-ln2-big2-cos-frac-theta2-bigd-theta"">Integral...$\int_0^\pi \theta^2 \ln^2\big(2\cos\frac{\theta}{2}\big)d \theta$.</a>.  This paper may also be of interest to people here : <a href=""http://www.math.uwo.ca/~dborwein/cv/zeta4.pdf"">http://www.math.uwo.ca/~dborwein/cv/zeta4.pdf</a>.</p>

<p>We can expand the log in the integral to obtain three interals, one trivial, the other 2 are not so easy, any ideas? I tried doing the following
$$
\left( \ln 2 +\ln \sin \frac{\theta}{2} \right)^2=\ln^2(2)+\ln^2\sin\frac{\theta}{2}+2\ln (2)\ln \sin\big(\frac{\theta}{2}\big).
$$
We can write I as 
$$
I=\ln^2(2)\int_0^\pi \theta^2d\theta  +\int_0^\pi\theta^2 \ln^2 \sin \frac{\theta}{2}d\theta+2\ln 2 \int_0^\pi\theta^2 \ln \sin{\frac{\theta}{2}}d\theta.
$$
Change of variables $x=\theta/2$ and performing the trivial integral we obtain
$$
I=\frac{\pi^3\ln^2 2}{3}+8\int_0^{\pi/2} x^2 \ln^2 \sin x\, dx+16\ln 2\int_0^{\pi/2} x^2 \ln \sin x \, dx.
$$
I am stuck at this point, I was trying to somehow work these two integrals into the form of $$
\int_0^{\pi/2} \ln \sin x dx= \frac{-\pi\ln(2)}{2}\approx -1.08879
$$
but couldn't do so.  Thanks.</p>
",<calculus>
"<p>Let $f$ be a function of $x$ and $f'$ be the derivative, at a point $(x_0, f(x_0))$
the slope is $f'(x_0)$, we know from any calculus book that
the line $g(x) = f(x_0) + f'(x_0)(x - x_0)$ is tangent to the curve of $f$, 
but how do you rigorously prove it?</p>
",<calculus>
"<p>The devil's staircase or <a href=""https://en.wikipedia.org/wiki/Cantor_function"" rel=""nofollow"">Cantor function</a> is an awesome function that increases value but has derivative zero everywhere (or ""almost"", whatever that means).</p>

<p>I was incredibly amazed when I found out that the standard part function also seems to have derivative zero, no matter how you take it (I'll just show forwards):</p>

<p>$$
\require{cancel}
\frac{d(\text{st}\ x)}{dx}=
\text{st}\frac{\text{st}(x+dx)-\text{st}(x)}{dx}=
\text{st}\frac{\cancel{\text{st}(x)}+\text{st}(dx)\cancel{-\text{st}(x)}}{dx}=
\text{st}\frac{0}{dx}=0
$$</p>

<p>Which is odd, since the standard part function looks exactly like the identity function, $y=x$, when plotted, well, with a normal real scale. When magnified with the infinitesimal microscope, one would see a straight horizontal line, as all nonstandard numbers would be ""rounded"" to the nearest real.</p>

<p><img src=""http://i.stack.imgur.com/zVzA0.png"" alt=""Standard part function. $\delta$ is an infinitesimal""><br>

This reminds me of the floor function, but somehow this one doesn't seem to have discontinuities (it really doesn't, for any infinitesimal $\Delta x$, there's always an infinitesimal $\Delta(\text{st}\ x)=0$).</p>

<p>I'm confused, what's wrong here? Does the standard part function really have derivative zero everywhere?</p>
",<calculus>
"<h3>Problem statement:</h3>

<p>A plane flying with constant speed of 4 km/min passes over a ground radar station at an altitude of 6 km and climbs at an angle of 35 degrees. At what rate, in km/min, is the distance from the plane to the radar station increasing 6 minutes later? </p>

<p>Hint: The law of cosines for a triangle is $c^2=a^2+b^2− 2ab \cos(\theta)$ where $\theta$ is the angle between the sides of length $a$ and $b$.</p>

<hr>

<p>Okay, so following the typical related rates algorithm:</p>

<p>Identify the problem at hand by drawing a picture. I did that. It appears to me that after the plane passes over the ground station, we get an angle between the measures a and b of 90 + 35 degrees. </p>

<p>Awesome.</p>

<p>Now, let's build a table of values:
$$
a = 6\,\mathrm{km}\\
a' = 0 \quad \text{(duh)}\\
b = b'\cdot t = 4\cdot 6 = 24\\
b' = 4\,\mathrm{km}/\mathrm{min}\\
t' = 6min\\
c = \sqrt{a^2 + b^2} = \sqrt{36+24} = 2\sqrt{15} \approx 2.78316\\
c' = ?
$$</p>

<p>Okay, let's differentiate the law of cosines, which is given to us in form of a hint:</p>

<p>$2cc' = 2aa' + 2bb' - \frac{du}{dt}$</p>

<p>$
u = 2ab\cos(\theta)$</p>

<p>$\frac{du}{dt} = \frac{ds}{dt} \cdot \cos(\theta) - \sin(\theta)\cdot(2ab)$</p>

<p>$\frac{ds}{dt} = 0\cdot ab + (a'b + b'a)$</p>

<p>And so now we rewind:
$\frac{du}{dt} = \left[ \left(0 \cdot ab + \left(a'b + b'a \right) \right) \right] \cdot \cos(\theta) - \sin(\theta) \cdot 2ab $</p>

<p>And so: </p>

<p>$2cc' = 2aa' + 2bb' - \left[ \left(0 \cdot ab + \left(a'b + b'a \right) \right) \right] \cdot \cos(\theta) - \sin(\theta) \cdot 2ab $</p>

<hr>

<p>Boom, boom. Let's substutute what we know</p>

<p>$2\left(2 \sqrt{15} \right)(c') = 2(6)(0) + 2(24)(4) - \left[ \left(0\cdot (6)(24) + \left((0)(24) +  (4)(6) \right) \right) \right] \cdot \cos(125^\circ) - \sin(125^\circ) \cdot 2(4)(24)$</p>

<hr>

<p>Now what? Well, evaluate. Not sure if that's going to yield the right answer. But I don't think that it will. </p>

<p>Please help! I don't know how to solve this kind of problem! It seems my solution/process is incorrect!</p>
",<calculus>
"<p>How many solutions are there for this equation:</p>

<p>$(n+1)x-\lfloor nx \rfloor = c$</p>

<p>I can prove some basic properties of floors and ceiling, but here I'm stumped.</p>
",<calculus>
"<p>In the following encyclopedia, <a href=""http://m.encyclopedia-of-equation.webnode.jp/including-integral/"" rel=""nofollow"">http://m.encyclopedia-of-equation.webnode.jp/including-integral/</a></p>

<p>I found the relations below</p>

<p>\begin{eqnarray}
\int_{0}^1 \frac{1}{x} \log^3{(1-x)}dx &amp;=&amp;-\frac{\pi^4}{15} \tag{1} \\
\int_{-\pi}^{\pi} \log(2\cos{\frac{x}{2}}) dx &amp;=&amp; 0 \tag{2}
\end{eqnarray}
I tried to prove these equation, but I didn't success to prove.
How do you go about evaluating those integrals to obtain the repsective values?</p>
",<calculus>
"<p>I have the following function:
\begin{equation}
y=\frac{(x-1)(x+4)}{(x-2)(x-3)}
\end{equation}
It's easy to find it's domain, which is: $\mathbb{R} - \{3,2\}$. I know how to find the range of easier functions such as parabolas and such. Is there a systematic way to find the range of more difficult functions such as this one? Thank you in advance.</p>
",<calculus>
"<p>Say I have an infinite sequence $X=(x_i)$, $i=1,2,3,\ldots$ such that it's in $\ell^2$ space, i.e. $\sum_{i=1}^\infty|x_i|^2&lt;\infty$.  Now, this function that takes this infinite sequence to a real number:</p>

<p>$$\tag{1}F(X)=\sum_{i=1}^\infty\ln \left(1+\frac{x_i}{a_i}\right)$$</p>

<p>(let's assume that $x_i\geq0$ for all $i$ and (1) converges).</p>

<p>What can I say about the Frechet derivative at $X$?  Does the fact that $\frac{\partial F}{\partial x_i}=\frac{1}{a_i+x_i}$ exist for every $i$ tell me anything?  How do I compute it here?</p>
",<calculus>
"<p>How to prove $$\lim_{n\rightarrow\infty}nx^n=0$$ without L'Hôpital's rule? where $x \in [0,1)$ and $n=1,2,3,...$.</p>

<p>I know one of way to prove this is to treat $n$ is real, and $n$ and $(\frac{1}{x})^n$ as functions of $n$. Then, apply L'Hôpital's rule and it works for discrete $n$ by the definiation of limit. </p>

<p>However, I would like to skip the step we treat $n$ is real. Is there any proof for this question only using the techniques for limit of sequences? (such as the sandwich theorem, etc.)</p>
",<calculus>
"<p>Here's my work.</p>

<p>$$\begin{align} \lim_{y\to -2} \;\dfrac{y^3+8}{y+2} &amp;= \lim_{y \to -2}\;\require{cancel}\dfrac{(\cancel{y +2})(y^2 - 2y + 4)}{\cancel{y + 2}}\\ \\
&amp; = \lim_{y \to -2}\;\; y^2 - 2y + 4 \\ \\
&amp; = 4 + 4 + 4 \\ \\
&amp; = 12\end{align}$$</p>

<p>The answer book says that the correct answer is 4</p>

<p>What did I do wrong??</p>
",<calculus>
"<p>For the function $ƒ (x)=\frac{x^2}{(x−2)^2}$</p>

<p>I know the derivative is $f'(x)=-\frac{4x}{\left(x-2\right)^3}$ and $f''(x) =\frac{8\left(x+1\right)}{\left(x-2\right)^4}$</p>

<p>Critical point is $x=-1$ which is also the min.</p>

<p>I think possible inflection point are $x=1$ and $x=0$</p>

<p>I need to find:
1)Find the vertical and horizontal asymptotes of $ƒ (x)$.</p>

<p>2)Find the intervals on which $ƒ (x)‍$ is increasing or decreasing.</p>

<p>3)Find the critical numbers and specify where local maxima and minima of $ƒ (x)$ occur.</p>

<p>4)Find the intervals of concavity and the inflection points of $ƒ (x)$.</p>

<p>sorry for the long problem. Hope you can help</p>
",<calculus>
"<p><strong>Question</strong></p>

<p>Please help me find the derivative of $\arcsin \dfrac{(e^{2x}-1)}{(e^{2x}+1)}$</p>

<p><strong>Answer</strong></p>

<p>Using the chain rule this can be written as:</p>

<p>$$\dfrac{1}{\sqrt{1- \left( \dfrac{e^{2x}-1}{e^{2x}+1}\right)^2}} \times \dfrac{2e^{2x}(e^{2x}-1)-2e^{2x}(e^{2x}+1)}{(e^{2x} + 1)^2}$$</p>

<p>This can be simplified to:</p>

<p>$$\dfrac{1}{\sqrt{1- \left( \dfrac{e^{2x}-1}{e^{2x}+1}\right)^2}} \times \dfrac{-4e^{2x}}{(e^{2x} + 1)^2}$$</p>

<p>The correct answer should be</p>

<p>$$\dfrac{2e^{x}}{e^{2x}+1}$$</p>

<p>I can't really see how they get there...</p>
",<calculus>
"<blockquote>
  <p>Evaluate $$\int \frac{x^2+3}{x^6(x^2+1)}dx .$$</p>
</blockquote>

<p>I am unable to break into partial fractions so I don't think it is the way to go. Neither is $x=\tan \theta$ substitution. Please give some hints. Thanks.</p>
",<calculus>
"<p>Evaluate $$\int\frac{dx}{(1+x^2)^4}$$ Now I did solve it, but I used the mentioned substitution and after a lot of converting into double angles, I did it. But, it doesn't look like a good approach. Is there a better way? Thanks.</p>
",<calculus>
"<p>Let's say I want to evaluate the upper and lower Riemann Sums of the function $f(x)=x^2$ for the following partition on $[-1,1]$, $P = \{-1, -\frac{1}{2}, 1\}$</p>

<p>I'll start of with the definitions of upper and lower Riemann Sums, just to show you where I'm coming from, and so that you can see where any misunderstanding on my part would lie.</p>

<p>$$L(f, P) := \sum_{i=1}^{n}m_i(t_i - t_{i-1})$$
$$U(f, P) := \sum_{i=1}^{n}M_i(t_i - t_{i-1})$$
where
$$m_i = \inf\{f(x): x \in [t_{i-1}, t_i]\}$$
$$M_i = \sup\{f(x): x \in [t_{i-1}, t_{i}]\}$$</p>

<hr>

<p>This is what I did :</p>

<p>Since $f$ is continuous on $[-1,1]$, $m_i = \min\{f(x): x \in [t_{i-1}, t_i]\}$ and $M_i = \max\{f(x): x \in [t_{i-1}, t_i]\}$</p>

<p><strong>Lower Sum</strong> (Incorrectly Evaluated)</p>

<p>$\min(f(x)) = 0$ and occurs on the sub interval $[-\frac{1}{2}]$, this implies $m_i = 0$. But this has to be incorrect as it then implies $L(x^2, \{-1, -\frac{1}{2}, 1\}) = 0$. The correct answer is $\frac{1}{8}$</p>

<p><strong>Upper Sum</strong> (Correctly Evaluated) (EDIT: As it turns out due to a special case of this question)</p>

<p>$\max(f(x)) = 1$ and occurs on the sub-intervals $[-1, -\frac{1}{2}]$ and $[-\frac{1}{2}]$ at $x=-1$ and $x=1$, thus $M_i=1$</p>

<p>$$\begin{align}U(x^2 \{-1, -\frac{1}{2}, 1\}) &amp;= \sum_{i=1}^{n} 1 \cdot (t_{i} - t_{i-1})\\
&amp;= (-\frac{1}{2}-(-1)) + (1-(-\frac{1}{2}))\\
&amp;= 2
\end{align}$$</p>

<hr>

<p>I seem to have some misunderstanding, with calculating $M_i$ and $m_i$, I'm not sure why.Perhaps it's because I've computed the Riemann Sums with $m_i$ and $M_i$ as constant throughout all sub-intervals?</p>

<p>How would you evaluate the upper and lower Riemann Sums, given this function over the specified interval? Any hints would be appreciated.</p>
",<calculus>
"<p>I'm an engineering student, currently working my way through the fundamental mathematics courses.</p>

<p>I've done reasonably well so far&mdash;mostly A's and a couple of B's in Algebra, Statistics, Pre-Calculus, and Calculus I (I'm currently struggling quite a bit in Calculus II; so only time (and sweat; no blood or tears yet) will tell if I can maintain my academic performance after this course.</p>

<p>However, although my school is good and well-ranked among community colleges, it's still a community college. None of the courses go too in-depth on any of the topics we cover. It's all about teaching us techniques and methods for solving problems (not extraordinarily difficult problems, either). It's not that the instructors aren't good - many are quite good and certainly know their math. But there just isn't time to spend on any individual topics. We covered all of the integration techniques that are taught at this level (with the exception of improper integrals) in about 2 weeks, or 8 class meetings.</p>

<p>In spite of this (or maybe because I've realized a lot of the responsibility for learning the rest falls on me), I've really developed an awe and a love for mathematics. Not enough too switch majors; I still have an overwhelming desire to build robots. ;) </p>

<p>But I really want to <em>master</em> the subjects in mathematics I'm exposed to, to really learn them thoroughly and at a deep level&mdash;not only because the better I do that, the better an engineer I'll be (I hope), but also because I'm really blown away by how cool the math is.</p>

<p>So, my question is, how can I develop more adept mathematic thinking and reasoning skills, better math intuition?</p>

<p>None of my classes have been proof-based, yet. Would starting to learn how to build proofs help my intuitive skills to grow faster?</p>

<p>For instance, I've been studying (and struggling with a <em>lot</em>) infinite sequences and series, and how to represent functions as power, taylor, and maclaurin series. </p>

<p>I have made some progress, but I'm advancing <em>very slowly.</em> When I look at a formula like:</p>

<p>$$P_0(x) = \sum_{n=0}^{\infty} \frac{(-1)^n x^{2n}}{2^{2n} n!^2}$$</p>

<p>or even a more simple one, like:</p>

<p>$$\sum_{n=1}^{\infty} \frac{(-1)^n 3^{n-1}}{n!}$$</p>

<p>I have a great deal of trouble seeing past the jumble of variables and constants to the pattern they describe. I want to reach the point at which I can see the matrix!  ;)  (the movie type, not the spreadsheet type).</p>

<p>That's a joke of course, but seriously, while a mathematician may look at a matrix and see a mathematical structure, I have to think very hard, and sometimes to sketch an actual structure, to see a matrix as anything more than a large table of numbers.</p>

<p>If learning to prove theorems isn't the answer, (or the whole answer), what are some things you can try to help increase your capability to think mathematically / logically about concepts in calculus, and mathematics in general?</p>
",<calculus>
"<p>Can cusps be considered points of inflection?</p>

<p>I'm getting conflicting information but my thought process is that cusps cannot be points of inflection?</p>

<p>Can points of inflection exist when there is a vertical tangent to the graph? Assume there is change in concavity and the function is continuous.  </p>
",<calculus>
"<p>(1) A curve is given by the function $$r(t)=(t^3 -3t^2 +2t +4)i + (13-5t)j +(t^2 -t-3)k$$</p>

<p>Determine a parameterization for the line which is tangent to the curve at $t=2$</p>

<p>I started by solving for when $t=2$, and got the vector $4i + 3j - k$</p>

<p>I don't know what this vector means or if this was even the correct approach. Do I need to find the vector perpendicular to this, or what should I be doing instead?</p>
",<calculus>
"<blockquote>
  <p>A torus (a doughnut-shaped object) is formed by revolving the circle 
  $$x^2 + y^2 = a^2$$
  about the vertical line $x = b$, where $0 &lt; a &lt; b$. Find its volume.</p>
</blockquote>

<p>I have tried to create an integral that makes sense but I keep getting it wrong. We are learning shell method and disk method if that helps. I'm just stuck at where I should even start with this problem. Any help would be appreciated!</p>
",<calculus>
"<blockquote>
  <p>Let $f(x)$ be continuous on $[0,2]$, and differentiable on $(0,2)$ such that $0&lt;f(1)&lt;f(0)&lt;f(2)$. Prove that $f'$ has a solution on $(0,2)$.</p>
</blockquote>

<p>Here's a little crappy sketch: </p>

<p><img src=""http://i.imgur.com/KnZ4V1d.png"" alt=""enter image description here""></p>

<p>My attempt:</p>

<p>From $f(1)&lt;f(0)&lt;f(2)$ and continuity, there's a point $c\in (1,2)$ such that $f(c)=f(0)$, $f$ is continuous on $[0,c](\subseteq[0,2])$, differentiable on $(0,c)(\subseteq(0,2))$ so from Rolle's we know that there's some $k\in (0,c)$ such that $f'(k)=0$.</p>

<p>Is this alright? Is there another way to do this? Maybe with Lagrange's MVT?</p>

<p>Note: no integration or Taylor's.</p>
",<calculus>
"<p>Please check my setup for this very simple $LC$ circuit. But first assume the given differential equation is correct. There is no need to open a physics or design of electric circuits book as in its very essence this is just a problem in solving a differential equation with Laplace transforms. I just need my <strong>Setup</strong> checked which comes after the section entitled <strong>Given</strong>.</p>

<p><strong>Given</strong>:</p>

<p>All elements are in series: $L=1$ $henrys$, $C=1 farads$, $v(t)=1-e^{-t}$ $volts$ for $(0&lt;t&lt;\pi)$. The emf $v(t)=0$ for all $t$ outside the given interval. We have an initial condition of $i(0)=0$ for current and zero charge on the capacitor. Our goal is to find the current $i(t)$ in the circuit once the switch is closed.</p>

<p>The following differential equation models this physical problem...</p>

<p>$$Li'+\frac{1}{C}\int i(t)dt=v(t)$$</p>

<p>Because all units are compatible with this equation we can just plug in their values to simplify this equation giving us...</p>

<p>$$i'+\int i(t)dt=v(t)$$</p>

<p>That was the ""given."" The rest is just a math problem of solving a very simple differential equation with Laplace transforms. The next section is what I need checked.</p>

<p><strong>Setup</strong>: (Please check this for me)</p>

<p>First express $v(t)=1-e^{-t}$ in terms of unit steps for $(0&lt;t&lt;\pi)$...</p>

<p>$$v(t)=(1-e^{-t})u(t)-(1-e^{-t})u(t-\pi)$$</p>

<p>$$=(1-e^{-t})u(t)+e^{-t}u(t-\pi)-u(t-\pi)$$</p>

<p>an adjustment is needed for the exponential factor in the middle term as follows...</p>

<p>$$e^{-t}=e^{-(t-\pi+\pi)}=e^{-\pi}e^{-(t-\pi)}$$</p>

<p>our short lived forcing function now looks like this...</p>

<p>$$v(t)=(1-e^{-t})u(t)+e^{-\pi}e^{-(t-\pi)}u(t-\pi)-u(t-\pi)$$</p>

<p>the following is the equation that we need to take the Laplace transform of where capital $I=\mathcal{L}\{i(t)\}$...</p>

<p>$$i'+\int i(t)dt=(1-e^{-t})u(t)+e^{-\pi}e^{-(t-\pi)}u(t-\pi)-u(t-\pi)$$</p>

<p>$$sI+\frac{1}{s}I=\frac{1}{s}-\frac{1}{s+1}+e^{-\pi(s+1)}\frac{1}{s+1}-\frac{e^{-\pi s}}{s}$$</p>

<p>$$I(s^2+1)=1-\frac{s}{s+1}+e^{-\pi(s+1)}\frac{s}{s+1}-e^{-\pi s}$$</p>

<p><strong>My Question</strong>:</p>

<p>This is actually a request. I just need the setup for this problem carefully examined more for methodical errors than simply arithmetic errors. I am especially interested in if I made the correct shift adjustments and if my resulting Laplace transforms were accurately taken. Could I have set up my unit steps better?</p>
",<calculus>
"<p>At time 1:06 of <a href=""http://www.youtube.com/watch?v=aNReoaONJdg&amp;t=1m6s"" rel=""nofollow"">this video by minutephysics</a>, there is a geometric representation of the product rule: </p>

<p><img src=""http://i.stack.imgur.com/t05ha.png"" alt=""enter image description here""></p>

<p>However, I don't understand how the sums of the areas of those thin strips represent $d(u\cdot v)$. The only way I can see it is that $d(u\cdot v)$ is a small change in the area of the square, and those thin strips do represent that; however, I'm not sure if this is correct and if it is, how formal of a proof is this? Thanks!</p>
",<calculus>
"<p>I found this in the beginning of a calculus book, so it should be solved with very basic techniques, but I really don't know how.</p>
",<calculus>
"<p>Given the function $f(x,a)=e^{\frac{-x^2}{2a^2}}$, show that $\lim_{a\to 0^+}\frac{f(x,a)}{a}$ is finite?</p>

<p>I was trying with l'Hospital rule but still got an undetermined case. Is there anyone out there who can see some sort of transformation or has an idea for a different approach how to show the limit exist and is finite.</p>

<p>Any references or hints are highly appreciated. Thank you.</p>
",<calculus>
"<p><em><strong>Problem :</em></strong></p>

<p>Let $a,b\in \mathbb{Z}$ such that $a\neq 0,b\neq 0$ ; $\left | a \right |\leq 100,\left | b \right |\leq 100$. </p>

<blockquote>
  <p>Prove that:
   $$\left | a\sqrt{2}+b\sqrt{3} \right |&gt; \frac{1}{350}$$</p>
</blockquote>

<p>Thanks :)</p>

<p>P/s : I have no ideas about this problem ! :(</p>
",<calculus>
"<p>Let $a,b&gt;0$:<br>
$$\mathop {\lim }\limits_{n \to \infty } {({a^n} + {b^n})^{\frac{1}{n}}}$$</p>

<p>At first look, it seemed simple, yet I couldn't evaluate it.<br>
Maybe Squeeze Thm?  </p>
",<calculus>
"<p>I am obliged to prove that this sequence:</p>

<p>$\large {a_n=(1+\frac{1}{3})(1+\frac{1}{9})(1+\frac{1}{27})...(1+\frac{1}{3^n})}$</p>

<p>is convergent sequence.</p>

<p>I mean I was thinking about this and I know that $\large\lim_{n \to \infty} (1+\frac{1}{3^n})=1 $
From this I know that it will be probably convergent sequence but I know that it is not well written proof, and probably does not prove anything. I would be glad for any tips how to prove this. </p>
",<calculus>
"<p>So, I need to test the following series for convergence or divergence:</p>

<p>$$\sum_{n=1}^\infty (-1)^{n+1}{n\over {2^n}}$$</p>

<p>I know that when you use the Alternating Series Test, the series must satisfy two conditions. Which are:</p>

<ol>
<li>$$b_{n+1} \le b_n $$</li>
<li>$$\lim_{n\to \infty} b_n =0$$</li>
</ol>

<p>I having a hard time with the first condition because if I use 1  for n then I have a problem.
This is my work so far:</p>

<p>$$ {(n+1)\over 2^{(n+1)}} ? {n\over 2^n}$$
$$ {(1+1)\over 2^{(1+1)}} ? {1\over 2^1}$$
$$ {(2)\over 2^{(2)}} ? {1\over 2^1}$$
$$ {2\over 4} = {1\over 2}$$
They end up equaling each other.
On the other hand, if I plug in 2, I get something that does satisfy the first condition.$$ {(n+1)\over 2^{(n+1)}} ? {n\over 2^n}$$
$$ {(2+1)\over 2^{(2+1)}} ? {2\over 2^2}$$
$$ {(3)\over 2^{(3)}} ? {2\over 2^2}$$
$$ {3\over 8} ? {2\over 4}$$
$$ {3\over 8} \le {1\over 2}$$</p>

<p>So... what do I do?
Thanks in advance</p>
",<calculus>
"<p>I have searched for a similar question on stack exchange but could not find one.</p>

<p>The definite integral: $\large\int_0^1 \frac{x^4}{\sqrt{25-x^2}}$</p>

<p>I realize that I need to use $x = 5\sinθ$ in the bottom which will get me $5\cosθ$, but after that I am at a loss for what to do. </p>
",<calculus>
"<p>We know that $\lim_{\theta\to0}\frac{\sin\theta}{\theta}=1$ but $\theta$ must be in radians. My first question is what happen when $\theta$ is not in radian? Is it only because in the proof we use radian so $\theta$ must be in radians?</p>

<p>Then we also know that $\lim_{x\to\pm\infty}\frac{\sin x}{x}=0$. We can also prove it using sandwich theorem: $0\le|\frac{\sin x}{x}|\le|\frac{1}{x}|$, but here we are not using the fact that $x$ must be in radian. My next question is does that mean that $\lim_{x\to\pm\infty}\frac{\sin x}{x}=0$ also works even when $x$ is in degree?</p>

<p>If we see its pretty strange, when $x$ approaches $0$, there will be a limit only when $x$ is in radian, but when $x$ approaches neg/pos infinity, the limit exists regardless whether $x$ is in radian or degree.</p>

<p>Many thanks for the helps!</p>
",<calculus>
"<p>The flow of water in a pipe is faster in the middle than near the outside. 
For a 2 cm diameter pipe, given that the velocity of the water as a function of 
distance from the center of the pipe is
$$v(r) = \frac {1-r^2}{\mathrm{cm}^2} \cdot 1.48 \mathrm m/\mathrm s.$$</p>

<p>Determine the total flow in the pipe in liters/second. </p>

<p><img src=""http://i.imgur.com/hk7yChc.png"" alt=""Picture of situation""></p>

<p>What I've tried: Find total flow of water if uniform flow, i.e. if all water in pipe was flowing at a rate of 1.48 m/s. </p>

<p>$$(1.48 \mathrm m/\mathrm s) \pi (0.01\mathrm{cm})^2 \xrightarrow{\text{convert to liters/sec}} \approx 0.46 \text{liters}/\text{sec}.$$ </p>

<p>Then multiply that by $\frac 2 3$ (integrate $1-x^2$ from -1 to 1) = 0.309 liters/sec.</p>
",<calculus>
"<p>Can someone please explain why $(x^n)'=n\cdot x^{n-1}$?</p>

<p>Sorry for not writing it in math characters, I'm new here.</p>
",<calculus>
"<p>How would I go about proving this?
Would I try finding a value for $x$ that will make $f(x) = 0$?</p>
",<calculus>
"<p>Given $f'(0) = f''(0) = 1$, $f^{(12)}$ exists, and $g\colon x \mapsto f(x^{10})$. Find $g^{(11)}(0)$. </p>
",<calculus>
"<p>Determine the force that the moon exerts on the earth. Note that since the 
diameter of the earth, 12,742 km, is not insignificant compared to the distance to 
the moon, 384,400 km, the gravitational field of the moon is not uniform over the 
volume of the earth. Therefore, to get an accurate result you will need to integrate 
the gravitational field ($\vec{g} = -GM_{moon}\frac{\hat{R}}{R^{2}}$) times the density of the earth over the 
volume of the earth (you may assume the earth is spherical and of uniform 
density). It may be expedient to provide arguments why certain components of the 
force can be ignored during the integration. </p>

<p>Compare you result to the approximate solution obtained by assuming that both 
the moon and earth are point masses. </p>

<p>--</p>

<p>My thoughts: I'm assuming the only component of the force that matters is the force directed radially towards the moon (if that makes any sense). </p>
",<calculus>
"<p>What is the precise mathematical definition of $\lim_{x\to \infty} f(x) = −∞$? As x approaches infinity.</p>

<p>I know what the general meaning is, but I heard my teacher talk about some precise mathematical definition.</p>
",<calculus>
"<p>How to evaluate $$\frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)}$$, where n is integer > 0?</p>

<p>I know the gamma function formula will give</p>

<p>$$ \frac{(\frac{n-2}{2})!}{(\frac{n-3}{2})!}$$ How to simplify it?</p>
",<calculus>
"<p><a href=""http://kruel.co/math/chainrule.pdf"" rel=""nofollow"">http://kruel.co/math/chainrule.pdf</a></p>

<p>I found this proof of the chain rule, and it looks very thorough and legitimate. I have a few questions concerning it though. </p>

<p>Is this in fact a legitimate proof of the chain rule? As the author mentions at the end of the reading, a lot of proofs in calculus books do the chain rule wrong. </p>

<p>On page 2, the author concludes that $f(g(x) + [g'(x) + v]h) = f(g(x)) + [f'(g(x)) + w] \cdot [g'(x) + +v]h$. I have stared at this line for the longest time now and I am not understanding how the author gets this. </p>

<p>Is the flawed proof wrong since he is assuming $g(x + h) - g(x)$ is nonzero? </p>
",<calculus>
"<p>The coordinates of the point $M(x,y)$ on $y=e^{−{∣x∣}}$ so that the area formed by the coordinates axes and the tangent at $M$ is greatest is what?</p>

<p>I tried to plot the graph but after that I'm not being able to proceed.Please help<img src=""http://i.stack.imgur.com/elEaJ.gif"" alt=""![enter image description here"">]<a href=""http://i.stack.imgur.com/elEaJ.gif"" rel=""nofollow"">1</a></p>
",<calculus>
"<blockquote>
  <p>Evaluation of $$\int\frac{x^7+2}{\left(x^2+x+1\right)^2}dx$$</p>
</blockquote>

<p>$\bf{My\; Try::}$ Let $$\displaystyle \mathop{I = \int\frac{x^7+2}{(x^2+x+1)^2}}dx = \int\frac{(x^7-1)+3}{(x^2+x+1)^2}dx$$</p>

<p>$$\mathop{\displaystyle  = \int\frac{x^7-1}{(x^2+x+1)^2}}+\displaystyle \int\frac{3}{(x^2+x+1)^2}dx$$</p>

<p>Now Using the formula $$\bullet x^7-1 = (x-1)\cdot \left[x^6+x^5+x^4+x^3+x^2+x+1\right]$$</p>

<p>So $$\bullet \; (x^7-1) = (x-1)\left[(x^4+x)\cdot (x^2+x+1)+1\right]$$</p>

<p>So we get $$\displaystyle I = \int\frac{(x-1)\cdot (x^4+x)\cdot (x^2+x+1)}{(x^2+x+1)^2}dx+\frac{1}{2}\int\frac{2x-2}{(x^2+x+1)^2}+3\int\frac{1}{(x^2+x+1)^2}dx$$</p>

<p>$$\displaystyle I = \underbrace{\int\frac{(x-1)(x^4+x)}{x^2+x+1}dx}_{J}+\underbrace{\frac{1}{2}\int\frac{(2x+1)}{(x^2+x+1)^2}dx}_{K}+\underbrace{\frac{3}{2}\int\frac{1}{(x^2+x+1)^2}dx}_{L}$$</p>

<p>Now  $$\displaystyle J = \int\frac{(x-1)(x^4-x)+2x(x-1)}{(x^2+x+1)}dx = \int(x^3-2x^2+x)dx+\int\frac{2x^2-2x}{x^2+x+1}dx$$</p>

<p>Now we can solve the integral Using the formulae.</p>

<p>My question is can we solve it any other way, If yes then plz explain here</p>

<p>Thanks</p>
",<calculus>
"<p>Suppose $a+b+c+d=k$ how to find the extremum value of any symmetric expression in a,b,c,d?For example say $abc+bcd+cda+dab$.</p>

<p>I've noticed that it usually occurs when a=b=c=d.Why does this happen?</p>
",<calculus>
"<p>Please, how do I write the following  as a combination of a sum and a product: </p>

<p>$$ (c-a_1)(c-a_2)(c-a_3)b_1 + (c-a_2)(c-a_3)b_2+(c-a_3)b_3 ?$$
Also, how can I generalize it? </p>
",<calculus>
"<p>This post was edited following a comment that rightly stated that the original question was unsensible. The edited version follows.</p>

<p>Why does the equation in the second line implicitly define the function specification of $a(x)$ which minimizes the integral in the first line? What is the formal rule that is used here?</p>

<p>$$\int_{x_1}^{x_2}{f(a(x),x) g(x) dx}$$</p>

<p>$$a(x) = \underset{a^*}{argmin} f(a^*,x)$$</p>

<p>It is indeed, as remarked by  one of the commenters, the derrivation of the bayesian mean minimum squared error estimator where $x$ is the data, $a(x)$ the estimator, $g(x)$ the marginal probability density of the data and f(a(x),x) the conditional mean squared error of the estimator. $g(x) \geq 0, \forall x\in [x_1,x_2]$</p>
",<calculus>
"<p>I found the following answer on Math.SE:</p>

<p><a href=""http://math.stackexchange.com/questions/73922/fourier-transform-of-unit-step"">Fourier transform of unit step?</a></p>

<p>However, it is still not clear to me and maybe somebody could explain it clearer.</p>

<h1>Problem</h1>

<p>I have the following in my notes of a theoretical physics course:</p>

<p>$$
\hat{\Theta}(\omega) = \int_{-\infty}^\infty \Theta (t) e^{i\omega t} \mathrm{d} t 
= \lim_{\varepsilon \to 0} \int_0^\infty e^{i\omega t - \varepsilon t} \mathrm{d}t
= \pi \delta(\omega) + \mathrm{P} \frac{i}{\omega},
$$</p>

<p>where the $\mathrm{P}$ denotes the Cauchy's principal value. </p>

<h1>Question</h1>

<p>I understand why I get a delta function in this computation, but I have no idea why I have $\mathrm{P} \frac{i}{\omega}$ instead of just $\frac{i}{\omega}$ in the resulting expression.</p>
",<calculus>
"<p>I have problem because I can't do this exercise:</p>

<blockquote>
  <p>Prove that: $\forall x&gt;1$,
  $(1+\frac{1}{x})^x&lt;e&lt;(1+\frac{1}{x-1})^x$</p>
  
  <p>and:  $\forall (p,q)&gt;0$, $\left ( 1+\frac{p}{q} \right )^q\leq e^p \leq \left ( 1+\frac{p}{q} \right )^{p+q}$</p>
</blockquote>
",<calculus>
"<p>Is $X_n = 1+\frac12+\frac{1}{2^2}+\cdots+\frac{1}{2^n}; \forall n \ge 0$ bounded?</p>

<p>I have to find an upper bound for $X_n$ and i cant figure it out, a lower bound can be 0 or 1 but does it have an upper bound?</p>
",<calculus>
"<p>What tools would you recommend me for computing the limit below?</p>

<p>$$\lim_{n\to\infty} \frac{\sqrt{n}}{4^{n}}\sum_{k=1}^{n}\frac{\displaystyle \binom{2n-1}{n-k}}{(2k-1)^2+\pi^2}$$</p>

<p>As soon as any useful idea comes to mind I'll make the proper update with the new findings.</p>
",<calculus>
"<p>A piece of wire $10 m$ long is cut into two pieces. One piece is bent into a square and the other is bent into an equilateral triangle. Find the maximum and minimum possible area that can be enclosed. by the wire.</p>

<p><strong>EDIT:</strong> If $x$ is the length of wire used for the triangle and $10-x$ for the square, I get this formula for the area:
$$A=\frac{(10−x)^2}{16}+\frac{x^2\sqrt{3}}{36}$$
<strong>EDIT:</strong>
and the endpoints are at $x=0$ and $x=10$.</p>

<p>What do I do next?</p>

<p>Critical point at $A=\dfrac{40\sqrt{3}}{9+4\sqrt{3}}$</p>
",<calculus>
"<p>I have a hard time to understand how to do it.</p>

<p>I think what blocks me, is that the Rolle's theorem is usually applied to functions, yet here there aren't any. What should I do?</p>
",<calculus>
"<p>Is there a continuous function constructed by elementary functions, or by integral formula involved only elementary functions (like Gamma function) that grows faster than any $e^{e^{e...^x}}$ ($e$ appears $n$ times)?</p>

<p>I ask for the answer with a single formula. Gluing continuous function together is too trivial.</p>

<p>The function need not to be defined on whole $\mathbb{R}$, the domain $(a, \infty)$ is acceptable.</p>
",<calculus>
"<p>I'm trying to evaluate the limit as $N\to \infty.$</p>

<p>$$\frac{  \left(\dfrac{\sin \frac{1}{N}} {\frac{1}{N}}\right)^{N}   -1 }{\frac{1}{N}}.$$</p>

<p>Note first that, using L'Hôpital, one can easily show that the numerator goes to $0.$</p>

<p>Using the Taylor series expansion for $sin$, the value of the actual limit seems to be  $-\frac{1}{6}$. But I'm not fully sure how to justify the infinite series in the numerator is $-\frac{1}{6N}+O(\frac{1}{N^2})$. You could either justify that, if I was right, or may be use some other method to tell me what the limit is?</p>

<p>Many thanks in advance!</p>
",<calculus>
"<p>What is a solution to the following integral:</p>

<p>$$\int_0^\infty \, \frac{\cos{kt}}{\pi}\,\mathrm{d}k\,?$$</p>

<p>I have tried to evaluate this in the usual way:</p>

<p>$$\begin{align}\frac{1}{\pi}\int_0^\infty\,\cos{kt}\,\mathrm{d}k &amp;= \frac{1}{\pi t} \left[\sin{k t}\right]\bigg|_{k\rightarrow 0}^{k\rightarrow \infty} \\ &amp;=\frac{1}{\pi t} \left[\lim_{k\rightarrow \infty}\sin{k t} - \sin 0\right]  \\ &amp;=\frac{1}{\pi t} \lim_{k\rightarrow \infty}\sin{k t},\end{align}$$</p>

<p>but $\lim_{k\rightarrow \infty}\sin{k t}$ doesn't converge within infinity. </p>

<p>What is the trick here? Note that this is a homework question, so I don't need the complete solution, just directions where to start.</p>
",<calculus>
"<p>I want to prove that: 
$$\lim_{n\to \infty}n(t^{\frac{1}{n}}-1)=\log(t)$$
without using L'Hôpital. So,<br/></p>

<p>let $f_n(x)=x^{\frac{1}{n}-1}\;\ \forall x\ge 1$ and let $f(x)=x^{-1}\;\ \forall x\ge 1$, then:</p>

<p><hr/>
<strong><em>Lemma1:</em></strong> $\:\ (f_n)_{n\in \Bbb N}\;\ \text{converges uniformly to}\; f$
<br/> (I'm stuck here)</p>

<p>$$\text{Let}\;\ \epsilon &gt;0, \text{lets take}\;\ N=\frac{1}{\epsilon} (\text{figuring out...}),\; \text{then for}\;\ n&gt;N
\Rightarrow\ n&gt;\frac{1}{\epsilon}&gt;\frac{x-1}{x} \frac{1}{\epsilon}
\\
$$</p>

<p>$$\big|x^{\frac{1}{n}-1}-x^{-1}\big|=x^{\frac{1}{n}-1}-x^{-1}&lt;x^{\frac{1}{n}-1}-\frac{1}{n}$$</p>

<p>and here got stuck since the only thing I can do is make $x^{\frac{1}{n}-1}\le 1...$</p>

<hr/>

<p>Now, since $\int_1^t x^{\frac{1}{n}-1}dx=nt^{\frac{1}{n}}-n=n(t^{\frac{1}{n}}-1)$, by <strong><em>Lemma1</em></strong> we get that:
$$\lim_{n\to \infty}\int_1^t x^{\frac{1}{n}-1}dx=\int_1^t \lim_{n\to \infty} x^{\frac{1}{n}-1}dx$$</p>

<p>and, thus:
$$\lim_{n\to \infty}n(t^{\frac{1}{n}}-1)=\lim_{n\to \infty}\int_1^t x^{\frac{1}{n}-1}dx=\int_1^t \lim_{n\to \infty} x^{\frac{1}{n}-1}dx=\int_1^t x^{-1}dx=\int_1^t \frac{1}{x}dx=\log(t)$$</p>

<p>I will appreciate if you could let me know if I got it right, wrong (and why) or missed something.</p>
",<calculus>
"<p>I know the first derivative does not exist at a cusp.</p>

<p>Does this statement also hold for the second derivative?</p>
",<calculus>
"<p>Find the volume of the solid that remains after a circular hole of radius a is bored through the center of a solid sphere of radius r > a. So in the picture it looks like a circle with a cylinder cut out of the middle. I am not even sure where to start with this. I know this has to do with integrals but I am not sure how to set this up to even get an integral. Any help would be greatly appreciated.</p>
",<calculus>
"<p>How can I determine whether the following improper integrals converge or diverge?</p>

<p>1) $\int_0^\pi \frac{x}{\sin(x)}dx$</p>

<p>2) $\int_0^{\infty} \frac{e^{\cos(x)}}{x}dx$</p>

<p>3) $\int_0^1(\log x)^{\frac{1}{3}}dx$</p>

<p>I have heard of the comparison test, but I am not sure how, if possible, to use it here.</p>
",<calculus>
"<p>Find the coordinates of the point on the curve $f(x)=3x^2-4x$ where the tangent is parallel to the line $y=8x$.</p>
",<calculus>
"<p>How would you prove that $x^2$ has exactly 1 root using the rolle's theorem?</p>

<p>If there's f(a)>0 and f(b)=0 then f(a) does not equal f(b) does that proof that there's one root?</p>
",<calculus>
"<p>I have yet another derivative I need help with. I have to differentiate :</p>

<p>$$\sqrt[\uproot{3}{\Large 5}]{\frac{t^3 + 1}{t + 1}}$$</p>

<p>with respect to $t$. </p>

<p>I had two thoughts about this, use the chain rule then the quotient rule and multiply out, but then I am left with a mess of:</p>

<p>$$\left(\frac{t^3+1}{t+1}\right)^{-4/5}\cdot\frac{0.6t^3+0.6t^2-0.2t^4+0.2t}{(t+1)^2}$$</p>

<p>This is turning into a real mess and the answer I should get is:</p>

<p>$$\frac{2t-1}{5(t^2-t+1)^{4/5}}$$</p>

<p>Am I going the correct way about this or should I try a different route?</p>

<p>Thanks</p>
",<calculus>
"<p>Let $f$ be a continuous function on $[a,b]$ such that $f(x) \geq 0 $
for every $x\in [a,b]$. Suppose $\int_a^b f = 0$ and show that $f (x) = 0$ for every $x\in [a,b]$.
 obv this is monotonic ( non-decreasing definitionaly) i would liek to show monotonic non increaseing.</p>

<p>We know that $\int_a^b f = 0$ 
i have a theorem that says S(P) = s(P) = $\sigma$ over a Partition P on [a,b] or my integral does not exists. </p>

<p>So we have $(S(P) = \sigma) \leq 0$  is this enough to make a statement that my function is monotonic non increaseing? </p>

<p>if so how can we combine monotonic non-increase and monotonic non-decreasing to show that $f(x) = 0, \forall x \in [a,b]$?</p>

<p>EDIT:
my only other trick / idea i can come up with is that $\int_a^b |f| = 0$ </p>
",<calculus>
"<p>Another simple question that I can't work out today, yet I would work it out two weeks ago!</p>

<p>I need to find the 1st derivative of $$\frac{2x}{\sqrt{x^2 + 1}}$$.</p>

<p>So I use the Quotient rule and I get: $$\frac{(x^2 + 1)^.5 (2) - (2x)(0.5x^2 + 0.5)^-5}{x^2+1}$$</p>

<p>Am I heading in the correct direction and do I just need to multiply and try to get rid of the exponents somehow?</p>

<p>Thanks</p>
",<calculus>
"<p>I need urgent help on this question. I have no clue how to solve it as it's very complicated to me. The question is the following:</p>

<p>Given
$y=\frac{2xy}{x^2 + y}$
find $\frac{dy}{dx}$.</p>
",<calculus>
"<blockquote>
  <p>For $ x^3+y^2= 2x^2y+2 $ Find $\frac{dy}{dx}$ and the slope of the tangent to the curve at $(-1,-1)$</p>
</blockquote>

<p>Having a little problem finding the derivative in respect to $\frac{dy}{dx}$ for $2x^2y+2.$
To my understandings I need to use implicit differentiation; thus, wherever there is a $y$, after I take the derivative of $y$, I need to attach $\frac{dy}{dx}$. However, how does one do that for $2x^2y+2$?  </p>
",<calculus>
"<p>I understand that we are finding the area of a curve given by some function f(x) over the area of another curve C.  (I've also successfully plugged and chugged my way through my homework, without understanding what I was doing)</p>

<p>These are some of the questions that I am a little fuzzy about:<br>
 1.  Could I use a line integral in only 2 dimensions? (for example: to find the area between the curves y=x^2 and y=2x from x is in [0,2]?  If so, how?)<br>
 2.  Moving to more dimensions, the formula my book gives me is:</p>

<p>&int;<sub>c</sub>F <sup><strong>.</strong></sup> Vds =&int;<sub>a</sub><sup>b</sup>  F(a(t)) <sup><strong>.</strong></sup>
    a'(t)dt</p>

<p>Where C is a smooth oriented curve, whose orientation is given by V and F is a continuous vector field. (I understand individually what each of these terms mean, but am having trouble understanding what this formula is finding.</p>

<p><strong>What, geometrically, is this formula finding?</strong>  </p>
",<calculus>
"<p>Show that </p>

<p>$$\frac{1}{z^2}=1+\sum_{n=1}^\infty (n+1)(z+1)^n$$ when $|z+1|&lt;1$</p>

<p>I'm having problems to resolve this type of exercise since my book has virtually no exercises of this type, these expressions are based on Maclaurin series?</p>
",<calculus>
"<p>Can someone give a simple explanation as to why the <a href=""http://en.wikipedia.org/wiki/Harmonic_series_(mathematics)""><em>harmonic series</em></a> </p>

<blockquote>
  <p>$$\sum_{n=1}^\infty\frac1n=\frac 1 1 + \frac 12 + \frac 13 + \cdots $$</p>
</blockquote>

<p>doesn't converge, on the other hand it grows very slowly? </p>

<p>I'd prefer an easily comprehensible explanation rather than a rigorous proof regularly found in undergraduate textbooks.</p>
",<calculus>
"<p>When I tried to approximate $$\int_{0}^{1} (1-x^7)^{1/5}-(1-x^5)^{1/7}\ dx$$ I kept getting answers that were really close to $0$, so I think it might be true. But why? When I <a href=""http://integrals.wolfram.com/index.jsp?expr=%281-x%5E7%29%5E%281%2F5%29+-+%281-x%5E5%29%5E%281%2F7%29&amp;random=false"" rel=""nofollow"">ask Mathematica</a>, I get a bunch of symbols I don't understand! </p>
",<calculus>
"<p><strong>I know this question was answered by using another theorem <a href=""http://math.stackexchange.com/questions/613900/assuming-forall-x-in-0-1fx-x-prove-forall-x-in-0-1fx-x"">here</a> but I wish I could get comments on my way of trying to prove it.</strong></p>

<p>We were asked to prove that for a function $f(x) &gt; x $ which is continuous in $[0,1]$ there exists an $\epsilon$ such that $f(x) &gt; x + \epsilon$ for every $x \in [0,1]$.</p>

<p>I tried a different method then suggested at the link above and would like to know if it's correct and if not what's wrong / missing.</p>

<p>I'll assume the opposite that $\forall \epsilon&gt;0 , f(x) - x &lt; \epsilon$. Also I know that $f(x) - x &gt; 0$ from $f(x) &gt; 0$ so I can deduce that $|f(x)-x| &lt; \epsilon$ that would mean that for any $x \in [0,1]$  the limit of $f$ at that point is $x$, since $f$ is continuous I also know that the limit of $f$ at any $x \in [0,1]$ is $f(x)$ so $f(x)= x$ which is a contradiction to $f(x) &gt; x$ hence there must be at least one $\epsilon$ for which $f(x) - x &lt; \epsilon$</p>

<p>Thanks for any help you can provide</p>
",<calculus>
"<blockquote>
  <p>Evaluate $\sum^\infty_{n=1} \frac{1}{n^4} $using Parseval's theorem (Fourier series).</p>
</blockquote>

<p>I have , somehow, to find the sum of $\sum_{n=1}^\infty \frac{1}{n^4}$ using Parseval's theorem.</p>

<p>I tried some things that didn't work so I won't post them.</p>

<p>Can you please explain me how do I find the sum of this series using Parseval's identity?</p>

<p>Thanks</p>
",<calculus>
"<p>I have no idea how to convert this to an integral(which has to be done as the answer is $\frac{\pi}{4}$)   I assume it may be equivalent to arctan(1).</p>
",<calculus>
"<p>How can I find this limit:<br>
$\displaystyle\lim_{x\rightarrow\infty} \left(\frac{x+\ln x}{x-\ln x}\right)^{\frac{x}{\ln x}}$</p>
",<calculus>
"<blockquote>
  <p>Let $\{x_n\}$, a sequence such that:<br>
  $\forall n \in \mathbb{N}: \left| {x_{n+2}-x_n} \right| &lt; {1 \over 2^n}; \quad  
\lim_{n \to \infty } ({x_{n + 1}} - {x_n}) = 0$</p>
</blockquote>

<p>I translated the above to:<br>
$$\begin{array}{l}
 {x_n} - \frac{1}{{{2^n}}} &lt; {x_{n + 2}} &lt; {x_n} + \frac{1}{{{2^n}}} \\ 
 {x_n} - \varepsilon  &lt; {x_{n + 1}} &lt; {x_n} + \varepsilon  \\ 
 \end{array}$$</p>

<p>But at this point I'm kinda stuck. I guess I need to ""glue"" the two statements somehow. I also considered, treating $X_{odd}$ and $X_{even}$. Would that be helpful?</p>
",<calculus>
"<p>If U has a $\chi^2$ distribution with v df, find E(U) and V(U).</p>

<p>By definition, $E(U)
=\int^{\infty}_{0} u\frac{1}{\gamma(\frac{v}{2})2^\frac{v}{2}}u^{\frac{v}{2}-1} e^\frac{-u}{2}\,du 
=\int^{\infty}_{0} \frac{1}{\gamma(\frac{v}{2})2^\frac{v}{2}}u^\frac{v}{2} e^\frac{-u}{2}\,du$.</p>

<p>How do I integrate this?</p>

<p>Note: This isn't a homework problem.</p>
",<calculus>
"<p>Let $m,\, k$ be fixed positive integers. Evaluate
$$
\lim_{n \to \infty}\left\{%
\left[\sum_{r = 1}^{k}{\left(n + r\right)^{m} \over n^{m - 1}}\right] -kn
\right\}.
$$</p>
",<calculus>
"<p>Let $f$ be a holomorphic function on the unit disk $D$. Suppose for any $z\in D$, $f'(z)\neq 0$. Then does $f$ have to be a conformal map from $D$ to $f(D)$?</p>
",<calculus>
"<p>I got this question:</p>

<p>Prove or disprove the following:</p>

<p>If the series $\sum_{k=1}^{\infty} a_k3^k$ diverges, Must the series $\sum_{k=1}^{\infty} a_k4^k$ diverge too?</p>

<p>I tried to find a couple of counterexamples but failed, I tried $a_k=1/k!$, $a_k=1/3^k$ and many more but wasn't able to find a counterexample.
Then I tried to prove this statement but I wasn't able to proceed too.</p>

<p>Thanks for any hints.</p>
",<calculus>
"<p>Let $g:R \rightarrow R$ be continuous and $ 2\pi$-periodic, let $m \in N$. How many solution in  class of $m$-times continuously differentiable $2\pi$-periodic functions has equation $$f^{(m)}=g ?$$</p>

<p>Edit. Obviously, if $f$ is a solution in this class and $C$ is a constant then $f+C$ is also a solution. Are there another solutions? </p>
",<calculus>
"<p>This question is related to this recent <a href=""http://math.stackexchange.com/questions/136067/prove-int-ab-fxdx-leq-frace2l-beta-12l-alpha-int-cd-fxdx"">other question</a>, where two intervals $[a,b] $ and $[c,d]$ were considered. Here I ask about a simpler version with just
one interval $[a,b]$.</p>

<p>Consider the following optimization problem : $f$ is a positive continuous function $[a,b] \to ]0,+\infty[$, satisfying the Lipschitz condition $|f(x)-f(y)| \leq L|x-y|$ for any $x,y \in [a,b]$. Given the constraint $\int_{a}^{b}\frac{dt}{f(t)}=\alpha$ (where $\alpha$ is a positive constant), the problem is then to find the maximum (or supremum) value $M$ of $\int_{a}^{b}f(t)dt$ under this constraint, and find the functions for which this maximum is attained, if any.</p>

<p>Although my evidence for this is still incomplete, I believe the maximum is attained when $f$ is a decreasing affine function with coefficient $-L$. In this case,</p>

<p>$$
f(t)=L\bigg(b-t+\frac{b-a}{e^{L\alpha}-1}\bigg), \
\int_a^{b} f(t)dt= L(b-a)^2\bigg(\frac{1}{2}+\frac{1}{e^{L\alpha}-1}\bigg)
$$</p>

<p>By the Stone-Weierstrass theorem, when looking for the maximum we may assume that $f$ is differentiable (we may even assume that $f$ is a polynomial). In this case, one may apply the methods explained in the abovementioned post : $|f'| \leq L$, so $|(f^2)'| =|2ff'| \leq 2L|f|$ and hence $f^2(x) \leq f^2(a)+2LF(x)$, where we put $F(x)=\int_a^x f(t)dt$. So $f(x)=\frac{f^2(x)}{f(x)} \leq \frac{ f^2(a)+2LF(x)}{f(x)}$. Putting $G(x)=f^2(a)+2LF(x)$, we deduce $\frac{G'(x)}{G(x)} \leq \frac{2L}{f(x)}$. integrating between $a$ and $b$, we obtain
${\sf ln}\big(\frac{G(b)}{G(a)}\big) \leq 2L\alpha$. Now $G(a)=f^2(a)$ and
$G(b)=f^2(a)+2L\int_a^b f(t)dt$, so that one finally obtains</p>

<p>$$
\int_a^b f(t)dt \leq \frac{e^{2L\alpha}-1}{2L}f^2(a)
$$</p>

<p>Any feedback appreciated on the following questions : what is the maximum/supremum, which functions attain equality.</p>
",<calculus>
"<p>This is the problem that I am having trouble with for my test review. I am completely blank and don't know what it is asking for. Can you please guide me step by step. For example: Why did constant $k$ appear all of a sudden?</p>

<blockquote>
  <p>$a$ varies directly with $b$</p>
  
  <p>Which of these equations could represent the relationship between $a$ and $b$?</p>
  
  <p>$a$ varies directly with $b$ if $a=k\cdot b$ for some constant $k$</p>
  
  <p>If you divide each side of this expression by $b$, you get $\displaystyle\frac ab=k$ for some constant $k$.</p>
  
  <p>$\displaystyle\frac ab=\frac12$ fits this pattern, with $k=\displaystyle\frac12$</p>
  
  <hr>
  
  <ul>
  <li>$a=\frac12-b$</li>
  <li>$\frac12\cdot\frac1a=b$</li>
  <li>$2\cdot\frac1a=b$</li>
  <li>$\frac12\cdot a=\frac1b$</li>
  <li>$\frac ab=\frac12$</li>
  </ul>
</blockquote>
",<calculus>
"<p>Show that $\ln\Big(|\frac{1+x}{1-x}|\Big)=2\sum_{n=0}^{\infty}\frac{x^{2n+1}}{2n+1},$ for $|x|&lt;1$. The previous excercise (which was within my limited reach) was to show that 
$\frac{1}{1-x^2}=\sum_{n=0}^{\infty}x^{2n},$ for $|x|&lt;1$. I suspect there is a (not overly subtle) connection here but, needless to say,  I can't see it. I don't know why the absolute value signs are included, but it might be because the solution includes integrating some power series. The excercise is contained in a chapter on derivation and integration of (convergent) power series; one is also assumed to be familiar with multiplication of power series. </p>

<p>Very thankful for any help.</p>
",<calculus>
"<p>Let  $x_1$, $x_2$, and $x_3$ be the roots of the equation
$$4x^3-6x^2+7x-8=0.$$
Find this value:
$$(x_1^2+x_1x_2+x_2^2)(x_2^2+x_2x_3+x_3^3)(x_3^2+x_3x_1+x_1^2)$$</p>

<p>My try:
$$x_1+x_2+x_3=\frac 3 2$$
$$x_1 x_2+x_2 x_3+x_1 x_3=\frac 7 4$$
$$x_1 x_2 x_3=2$$</p>

<p>Now I have
$$x_1^2+x_1x_2+x_2^2=(x_1+x_2)^2-x_1x_2=(\frac 3 2-x_3)^2-\frac 2 {x_3}.$$</p>

<p>But this is very ugly, and I think this problem should have a cleaner solution. Thanks.</p>
",<calculus>
"<p>I need to find the limit
$$
\displaystyle \lim_{n\rightarrow \infty}\left(\sqrt{n^2+n+1}-\big\lfloor  \sqrt{n^2+n+1} \big\rfloor \right),$$ where $n\in \mathbb{N}$.</p>

<p><strong>My attempt</strong>. As $\displaystyle \lim_{n\rightarrow \infty} (n^2+n+1)\approx n^2$, then $\displaystyle \lim_{n\rightarrow \infty}\sqrt{n^2+n+1}\approx \displaystyle \lim_{n\rightarrow \infty}\sqrt{n^2} = n$.</p>

<p>So $$\displaystyle \lim_{n\rightarrow \infty}\left(\sqrt{n^2+n+1}-n\right) = \displaystyle \lim_{n\rightarrow \infty}\frac{\left(\sqrt{n^2+n+1}-n\right).\left(\sqrt{n^2+n+1}+n\right)}{\left(\sqrt{n^2+n+1}+n\right)}.$$</p>

<p>So $$\displaystyle \lim_{n\rightarrow \infty}\frac{n\cdot\left(1+\frac{1}{n}\right)}{n \left(\sqrt{1+\frac{1}{n}+\frac{1}{n^2}}+1\right)} = \frac{1}{2}.$$</p>

<p>My Question is , Is my Process is Right OR Not ,OR Is there is any error .</p>

<p>If Not Then How can I Solve it</p>

<p>Help Required</p>

<p>Thanks</p>
",<calculus>
"<p><img src=""http://i.stack.imgur.com/vz1yh.png"" alt=""enter image description here""></p>

<p>Hi! I am currently working on some calc2 online homework problems and I am having difficulty with this problem. I was trying to use the polar coordinates (d,a)with the equation of the line thus being r=d*sec(theta-a). I tried solving for d by setting d equal to sqrt((-19)^2+(-6)^2) which then came out to be 137621/6907. I then tried solving for a by setting it equal to arctan(-6/-19) which came out to be 17.52556837. I then plugged everything into the equation of the line which i had as r=d*sec(theta-a) to get r= (137621/6907)sec(theta-17.52556837). Clearly my answer is wrong but I do not know why. If someone can help me solve this problem I would greatly appreciate it. </p>
",<calculus>
"<p>I'm learning single variable calculus right now and at current about integration with partial fraction. I'm stuck in a problem from few hours given in my book. The question is to integrate $$\frac{x^2 + 1}{x^2-5x+6}.$$</p>

<p>I know it is improper rational function and to make it proper rational fraction we have to divide
$$\frac{x^2 + 1}{x^2-5x+6}$$
I'm trying from sometime but couldn't find the right solution. 
<br> Please help! Thank you in advance.  </p>
",<calculus>
"<p>Suppose that the polynomial function $f(x)=x^n+a_{n-1}x^{n-1}+\cdots +a_0$ has $k_1$ local maximum points and $k_2$ local minimum points. Show that $k_2=k_1+1$ if $n$ is even, and $k_2=k_1$ if $n$ is odd.</p>

<p>Solution to the problem:</p>

<p>Let $l$=$k_1+k_2$ and let $a_l\lt a_{l-1}\lt \cdots a_1$ be all the local maximum and minimum points. <strong>On the intervals between these points $f$ is either decreasing or increasing.</strong> Since $\lim_{x\to \infty}f(x)=\infty$, the function $f$ must be increasing on $(a_1,\infty)$. Thus $a_1$ must be a local minimum point. Consequently, $f$ must be decreasing on $(a_2,a_1)$, which shows that $a_2$ must be a local maximum point. Continuing in this way we see that $a_k$ is a local minimum point if $k$ is odd and a local maximum point if $k$ is even. </p>

<p>Now if $n$ is even, then $a_l$ must be a local minimum point, since $\lim_{x\to -\infty}f(x)=\infty$. Thus $l$ must be odd, so $a_1,a_3,\dots,a_l$ are the local minimum points, and $a_2,\dots, a_{l-1}$ are the local maximum points. Consequently $k_2=k_1+1.$ If $n$ is odd, then $a_l$ must be a local maximum point, since $\lim_{x\to -\infty}f(x)=-\infty$. The same sort of reasoning then shows that $k_1=k_2$. QED.</p>

<p>I don't know how to prove the bold part. It's intuitively clear from the graph of a polynomial, however, I can't give a proof to this. The only information is $f'$ is zero at each $a_i$ and both $f$ and $f'$ are polynomials. I need to show that $f'$ is either positive or negative on these intervals. How can I show this? I would greatly appreciate any solutions, hints or suggestions.</p>
",<calculus>
"<p>use the formula $P_n(x) = \dfrac{1}{2^nn!}\dfrac{d^n}{dx^n}((x^2-1)^n)$ to show that $P_{2n}(0) = \dfrac{(-1)^n(2n)!}{4^n(n!)^2}$ and odd terms are 0.</p>

<p>I first subbed in 2n to the formula and got</p>

<p>$P_{2n}(x) = \dfrac{1}{4^n(2n)!} \dfrac{d^{2n}}{dx^{2n}}((x^2-1)^{2n})$ but I am not sure how to deal with differentiating that term $2n$ times. I have tried to use the binomial theorem but to no avail.</p>
",<calculus>
"<p>I was trying to solve this limit:</p>

<p>$\lim_\limits{n\to \infty} \binom {3n}{n}^{1/n} $</p>

<p>I solved it with Cesaro theorem:</p>

<p>$\lim_\limits{n\to \infty} \binom {3n}{n}^{1/n} $= $\lim_\limits{n\to \infty} \frac{((3(n+1))!}{(2(n+1))(n+1)!}\frac{2n!n!}{3n!}= \frac{27}{4}$</p>

<p>But when I have tried to use Stirling I arrived to the form $\lim_\limits{n\to\infty}(\frac{27}{4}(\frac{3}{4\pi n})^{1/2})^{1/n}=1$</p>

<p>Surely I've made a mistake. In order to arrive to the form that I have writed before I wrote the limit as $\lim\limits_{n \to \infty}(\frac{(3n)!}{n!(2n)!})$ and then I sobstituted to all the terms the Stirling approximation.</p>
",<calculus>
"<p>I'm doing a calculus project where we have to make a model of some graph rotated about the y axis. I am doing a fish bowl and I have most of it understood and ready. The only thing I'm not sure of is what the equation of the semicircle is if its center is at (0,2). I added a picture to help illustrate it. Thank you for your help. <a href=""http://i.stack.imgur.com/YQidm.jpg"" rel=""nofollow"">Image of semicircle</a></p>
",<calculus>
"<p>Showing the set $A = \{ x \in l_1 : |x_n| \leq 1/n^2  ,\forall n \}$ is closed. I had to show it is compact, and I am done showing it is relatively compact, but now I am stuck showing it is closed.</p>

<p>$l_1$ is the space of finite sequences $x = (x_1,x_2,...)$ with the norm $\|x\|_1 = \sum_k^\infty |x_k| &lt; \infty$ i.e. the set $A \subset l_1$ with sequences where each component of the sequence is bounded by $1/n^2$ for every component.</p>

<p>I done a lot of work to show it is relatively compact, so I feel as though showing it is closed should be simple - but I can't make any progress. </p>

<p>Any help please</p>
",<calculus>
"<p>I want to find numerically (the functional expression might become too complicated) the derivative of a complex function (to use it in a Newton-algorithm). Can I simply do something like
$$ \frac {df}{dz} = \lim_{h \to 0} {f(z+h)-f(z))\over h} + {f(z+ih)-f(z))\over h} 
$$
? Or how do I have to do this?</p>

<p><hr>
[update]           </p>

<p>By manual tests with $f(z) = \log(z)$, then $f'(z)=\frac1z$ and some examples it seems that I must do
$$ \frac {df}{dz} = \lim_{h \to 0} {{f(z+h)-f(z))\over h} + {f(z+ih)-f(z))\over ih} \over 2 }
$$
Is that formally correct?</p>
",<calculus>
"<p>Let $|x|&lt;1$. Define $R_n(x):=\int_{0}^{x}\frac{(x-t)^{n-1}}{(1-t)^n}dt$. How do we prove that $\lim_{n\to \infty}R_n(x)=0$? This is actually the integral remainder of the Taylor expansion of the function $f(x)=-log(1-x)$. Once I show that $R_{n-1}(x)\to 0$, then I can say that $f(x)=\sum_{k=1}^{\infty}\frac{x^k}{k}$. </p>

<p>By the way, I am using the following formula: $$f(x)=\sum_{k=0}^{n-1}\frac{f^{(k)}(0)}{k!} x^k+ R_{n-1}(x)$$ where $R_{n-1}(x)=\frac{1}{(n-1)!}\int_{0}^{x}(x-t)^{n-1} f^{(n)}(t)dt$</p>
",<calculus>
"<p>If $\lim h\to 0$, when finding the derivative of the function, why do you plug in the limit that is being approached. Like why would you plug in $0$ in the function $4x+2h$ (which is the derivative of $\frac{2 (x+h)^2-2x^2}{h}$</p>
",<calculus>
"<p>Be the equation $$\frac{dx}{dt} = a(t)*x^2+b(t)*x+c(t)$$ where $$a,b,c : I =[α,β] ⊂ R → R $$ are continuous function.</p>

<p>a) How can I found the solutions to the equation knowing a particular one $ρ_0$ ?</p>

<p>b) If $a(t)= 1, b(t)= -2e^{2t}, c(t)=e^{4t}+2e^{2t}$ and the equation admits the particular solution $ρ_0(t)= αe^2t$ what is the general solution to the equation?</p>

<p>Any help is appreciated. Thank you!</p>
",<calculus>
"<p>So you have the integral:
$$\int\frac{3v}{200 - 4v} dv$$
I tried to do $u$-substitution at first with $u = 200 - 4v$, but I could not get the correct answer which is:
$$-\frac{3}{4}v - \frac{150}{4}ln(200-4v) + C$$
In the worked solution, they did not use a $u$-substitution. The first integral becomes:
$$\int -\frac{3}{4} + \frac{150}{200 - 4v} dv$$
And I cannot see what technique they used to get that.  I worked out that if you actually add the 2 fractions you end up back at the first integral, but I do not see how they worked out that is the way it should be re-arranged.  I also don't understand why my $u$-substitution didn't work.  Should a $u$-substitution have worked?  I'm still trying to get my head around this integrating of fractions.</p>
",<calculus>
"<p>$\frac{2}{(x^2+3)^3}$.</p>

<p>I have ${dy}/{dx}$ x 2 x ${x^2+3^3}$ - 2 x ${dy}/{dx}$  x ${x^2+3^3}$ over $({x^2+3)^6}$</p>

<p>And then simplifying to $-12x^5 + 36x^2$  over $({x^2+3)^6}$</p>

<p>I'm not sure if this is right. </p>
",<calculus>
"<p>So the equation looks a bit complicated, but the derivation itself should be straightforward. But I'm evidently getting mixed up somewhere, because my answer is wrong.
$$ \frac{\partial ({-k_{b}T \ln(2\cosh(\frac{\epsilon}{k_{b}T}})))}{\partial T} $$
(where V is kept constant, hence the partial derivative)</p>

<p>So according to the product rule:
$$ {-k_b T} \frac{\partial ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}})))}{\partial T} + {-k_b} ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}}))) $$</p>

<p>Then the chain rule:
$$\frac{\partial ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}})))}{\partial T} 
= \frac{\partial {(2\cosh(\frac{\epsilon}{k_{b}T}}))}{\partial T} \frac {1} {(2\cosh(\frac{\epsilon}{k_{b}T}))}
= \frac{\partial {(\frac{\epsilon}{k_{b}T}})}{\partial T} 2\sinh(\frac{\epsilon}{k_b T}) \frac {1} {(2\cosh(\frac{\epsilon}{k_{b}T}))}
= \frac {\epsilon} {k_b} 2\sinh(\frac{\epsilon}{k_b T}) \frac {1} {(2\cosh(\frac{\epsilon}{k_{b}T}))}
= \frac {\epsilon} {k_b} \tanh(\frac{\epsilon}{k_b T})$$  </p>

<p>So the final answer I'm getting is: 
$$ {-k_b T} \frac {\epsilon} {k_b} \tanh(\frac{\epsilon}{k_b T}) + {-k_b} ({\ln(2\cosh(\frac{\epsilon}{k_{b}T}})))
=  -{\epsilon}T \tanh(\frac{\epsilon}{k_b T})-{k_b} {\ln(2\cosh(\frac{\epsilon}{k_{b}T}})) $$</p>

<p>But apparently this is incorrect, and the correct answer is: 
$$ \frac {\epsilon}{T} \tanh(\frac{\epsilon}{k_b T})-{k_b} {\ln(2\cosh(\frac{\epsilon}{k_{b}T}})) $$</p>

<p>I'm probably making a stupid mistake somewhere, but I can't seem to spot it.</p>
",<calculus>
"<p>I'm not sure where to even begin with this...</p>

<p>Let $f(x) = \sin x$. The polynomial $p(x)= Ax^3 + Bx^2 + Cx +D$ is a function such that $p^{k} (0) = f^{k} (0)$ for $k=0,1,2,3$</p>

<p>a) Determine $p(x)$ by computing the values of $A,B,C$ and $D$ </p>
",<calculus>
"<p>I am reading Chapter 5 of <em>Spivaks</em>. One of his examples is the function $f$ defined as 0 if $x$ is irrational and between 0 and 1,  or $\frac{1}{q}$ if $\frac{p}{q}$ is irreducible and between $0 &lt; x &lt; 1$. Here is a copy of his proof from a previous question which I looked at before asking this question: Pedro Tamaroff (<a href=""http://math.stackexchange.com/users/23350/pedro-tamaroff"">http://math.stackexchange.com/users/23350/pedro-tamaroff</a>), What exactly is going on when we're finding a limit?, URL (version: 2013-01-10): <a href=""http://math.stackexchange.com/q/195969"">http://math.stackexchange.com/q/195969</a>. One of the things that I found confusing about the proof <em>Spivak</em> gave was how he was able to conclude that the points at which the limit could be false are finite. Spivak states, ""Let $n$ be a natural number so large that $\frac{1}{n} \leq e$. Notice that the only numbers $x$ for which the limit could be false are $1/2;1/3;2/3;1/4;3/4;...;1/n,...\frac{n-1}{n}$"" </p>

<p>Let's say the epsilon I choose is $1/10$, and my $n = 11$ (both of which satisfy his statement), then the numbers I would have to check would be: $1/2;...;1/11...10/11$. In my mind, however, there are more numbers then the ones stated by Spivak that can be used to show that the limit is false such as $1/12,...11/12;1/13...12/13$, and I don't understand why this is not the case. I recognize the other parts of the proof, except for this.Furthermore, in an example prior to this one, Spivak shows the function $f(x) = 1$ if $x$ is rational and 0 if $x$ is irrational, and states that the limit can not be found which I think I understand, but this ""new"" function has the limit of 0 at all a from $(0,1)$. I think this has to do with the $\frac{1}{q}$ aspect of this ""new"" function, but I don't really understand how or why. Any help from just helping me gain an intuition for the two functions, to links to other questions would be appreciated. Thank you.   </p>
",<calculus>
"<p>(a) Find the work required to pump the water out of the spout. (Use 9.8$ \frac{m}{s^2}$ for $g$. Use 1000 $\frac{kg}{m^3}$ as the weight density of water. Assume that $a$ = 1.)</p>

<p>and </p>

<p>(b)</p>

<p>(b) Suppose that the pump breaks down after $4.7 × 10^5$J of work has been done. What is the depth of the water remaining in the tank? (Round your answer to the nearest tenth.)<a href=""http://i.stack.imgur.com/vUOpw.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vUOpw.jpg"" alt=""enter image description here""></a></p>

<p>Here is my work. But the program says it is wrong. <a href=""http://i.stack.imgur.com/9EDA9.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/9EDA9.jpg"" alt=""enter image description here""></a></p>
",<calculus>
"<p>Determine the equations of the tangent lines to the graph of $f(x)=3x(5x^2+1)$
that are parallel to the line $y=8x+9.$</p>

<p>I really don't know how to do this question; please help me out.</p>
",<calculus>
"<p>How do you sovle the equation: $\int_{-3}^{2} ( e^{-t+1} + sin (\frac{2\pi}{3}t) )  \delta(t- \frac{3}{2}) dt$</p>

<p>Because of the $\delta(t- \frac{3}{2})$ this is only non-zero at $t=\frac{3}{2}$</p>

<p>But I don't get why I can't do this: $\int_{-3}^{2} ( e^{-\frac{3}{2}+1} + sin (\frac{2\pi}{3} \times \frac{3}{2}) )  dt$</p>

<p>Why you try to solve the indefinite integral, it supposes to be:
$\int( e^{-t+1} + sin (\frac{2\pi}{3}t) ) \delta(t- \frac{3}{2}) dt = \frac{u(2t-3)-1}{\sqrt{e}} + C $. In which $u$ is the unit step function.
But I don't know how to solve this indefinite integral step-by-step.</p>
",<calculus>
"<p>Find the laplace transform of</p>

<p>$f(t) = t^2cos^2(t)$.</p>

<p>I'm a bit stuck on this one. I know how to do this function if both components aren't squared due to using the laplace transform table.</p>

<p>Could someone give me a hint to start off this question and put me in the right direction? Thanks in advance.</p>
",<calculus>
"<p>Let $B_{i,M}(x)$ be the $i^{th}$ part of the $M^{th}$ B-spline for the data-points $\{t_1,...,t_N\}$.  </p>

<p>Is there a known expression for the integral:
$$
\int_{0 \vee t_i}^{t \wedge t_{i+1}} B_{i,M}(s) \, ds?
$$</p>

<p>That is if $S_M$ is a $B$-spline curve of degree $M$ then what is the integral:
$$
\int_0^t S_M(s) \, ds
$$
equal to?</p>
",<calculus>
"<p>This is (hopefully) the final step in a larger proof. I would like to show that:</p>

<p>$$\vec{v}\cdot\frac{d||\vec{v}||}{dt}\hat{v} = 0\implies \frac{d||\vec{v}||}{dt} = 0$$</p>

<p>Where: $\vec{v}$ is a velocity vector and $\hat{v}$ is the the unit velocity vector, i.e. has a magnitude of 1.</p>

<p>I don't know that $||\vec{v}||$ is a constant as this is what I'm trying to show so I can't simply take it out of the dot product. However, it seems apparent that it must be a constant as the only way (that I can see) to make the dot product equal to zero is for $\frac{d||\vec{v}||}{dt} = 0$.</p>

<p>I'm hoping to find a more mathematically rigorous way of showing the desired outcome.</p>

<p>Thank you for your time and assistance.</p>

<p>Best,
Eric</p>
",<calculus>
"<p><strong>Question:</strong> Define $p(x) = 2x^3-7x$. Give an $\epsilon - \delta$ proof that $\lim_{x \to a} p(x)$ exists for every $a \in R$.</p>

<p><strong>My attempt:</strong>  Claim that the $\lim_{x \to a} 2x^3-7x = 2a^3 - 7a$.</p>

<p>Given an arbitrary $\epsilon &gt; 0$, $\exists\delta&gt;0$ such that $0&lt;|x-a|&lt;\delta \implies |f(x)-L|&gt;\epsilon$.</p>

<p>(i.e) $0&lt;|x-a|&lt;\delta \implies |2x^3-7x-(2a^3-7a)|&lt;\epsilon$ </p>

<p>Consider, </p>

<p>$$|2x^3-7x-(2a^3-7a)| = |2x^3-2a^3+7a-7x|$$
$$ \le |2||x^3-a^3|+|7||x-a|$$ by triangle inequality.</p>

<p>So,</p>

<p>$$|x-a|&lt; \frac{\epsilon - |2||x^3-a^3|}{|7|}$$</p>

<p>Because $|x^3-a^3|$ is a problem point, I need to control it, so the way I tried to do that is as follows:</p>

<p>If $|x-a|&lt;1$ then $$|x-a|&lt;1 \implies |x^3-a^3|&lt;|a^2+ax+x^2| (*)$$</p>

<p>Here's where I'm stuck. I'm not sure what to do with this: $|a^2+ax+x^2|$. Usually for proof like this I end up with something like $\delta = \min\left\{1, \frac{\epsilon-2(*)}{7}\right\}$ but in this case the result from $(*)$ wasn't in some form of a number, so I am not sure what to do next. If someone can help me with this that would be really appreciated.</p>
",<calculus>
"<p>This is my second question on the exchange. I've been wrestling with this particular problem for several weeks now, ever since I discovered a certain function, which I call the 'grum' function. I don't know if it's ever been discovered before. I'm just doing street math, pretty much.</p>

<p>In any case, the grum function gives the answer to the following question:</p>

<p>Given any right triangle, and any line segment connecting:</p>

<ul>
<li>The vertice of the angle opposite the hypotenuse, and:</li>
<li>Any point p resting on the hypotenuse, where the distance between the point p and the adjacent vertices of the triangle is given as the following ratio:</li>
<li>$d_{1} = \frac{r}{n}$</li>
<li>$d_{2} = r - \frac{r}{n}$</li>
</ul>

<p>The distance of said line segment can be found via the following function:</p>

<p>$$grum_{n}(\theta) \equiv n^{-1}\sqrt{(n-1)^{2}\cos^{2}(\theta) + sin^{2}(\theta)}$$</p>

<p>pronounced ""grum with index 'n' and evaluated at 'theta'"". The proof is fairly simple, and involves a geometric argument involving right triangles inscribed within right triangles, but that's not relevant to the question at the moment. Although, if you guys do want the proof, then I'll provide it.</p>

<p>In any case, however, my problem revolves around taking the indefinite integral of this function. I can take the derivative of grum with respect to theta like so:</p>

<p>$$\frac{d}{d\theta} n^{-1}\sqrt{(n-1)^{2}\cos^{2}(\theta) + \sin^{2}(\theta)}$$
$$\frac{-2(n-1)^{2}\cos(\theta)\sin(\theta) + 2\sin(\theta)\cos(\theta)}{2n\sqrt{(n-1)^{2}\cos^{2}(\theta) + \sin^{2}(\theta)}}$$</p>

<p>Which, if we multiply the denominator by 'n' and then simplify the expression, we get the nifty equation of:</p>

<p>$$\frac{d}{d\theta} grum_{n}(\theta) \equiv \frac{\sin(2\theta)(1 - (n-1)^{2})}{2n^{2}grum_{n}(\theta)}$$</p>

<p>So far, so good! However, I got stuck when I tried to take the indefinite integral of this function. I really want to learn not just what the indefinite integral is, but how to get it, and why it works. In any case, here's what I've gotten so far.</p>

<p>First, I draw out the constant in front of the radical, and rewrite the exponents.</p>

<p>$$n^{-1}\lmoustache ((n-1)^{2}\cos^{2}(\theta) + \sin^{2}(\theta))^{\frac{1}{2}}d\theta$$</p>

<p>Next, I rewrite $\sin^{2}(\theta)$ in the form of $1 - \cos^{2}$ and simplify the expression, which yields:</p>

<p>$$n^{-1}\lmoustache((n-1)^{2}\cos^{2}(\theta) - \cos^{2}(\theta) + 1)^{\frac{1}{2}}d\theta$$
$$n^{-1}\lmoustache(\cos^{2}(\theta)((n-1)^{2} - 1)) + 1)^{\frac{1}{2}}d\theta$$</p>

<p>Then I use the trig substitution identity $\sqrt{a^{2} + b^{2}x^{2}} = \frac{a}{b}\tan(x)$ to get the following form:</p>

<p>$$\frac{1}{n\sqrt{((n-1)^{2}-1) + 1)}}\lmoustache\tan(cos^{2}(\theta))d\theta$$</p>

<p>And... here's where I get stuck. How in hell does one calculate the indefinite integral of $\tan(cos^{2}(\theta))$? Am I even doing this the right way? In using the trigonometric substitution, I simply inserted $\cos^{2}(\theta)$ where x would have gone, but there's no guarantee that that's a valid operation.</p>

<p>If I could get some guidance, or even just a push in the right direction, I would be grateful. In the meantime, I'll keep working on it, and let you guys know if I make any progress on it.</p>

<p>Sorry for the long question, haha. I hope it was at least interesting in some small way, and I thank you very much for your time.</p>

<p>Thank you very much,</p>

<p>sincerely,</p>

<p>Druid.</p>
",<calculus>
"<p>I know there is such a subsequence for $b_n = \sin(n)$. What about $a_n = n\sin(n)$?</p>
",<calculus>
"<p>I need to solve the to following integral:
$$\int_{-1}^1\frac{1}{\sqrt{1-x^2}}\arctan\frac{11-6\,x}{4\,\sqrt{21}}\mathrm dx.$$</p>

<p>I tried this integral in <a href=""http://www.wolfram.com/mathematica"">Mathematica</a>, but it was not able to solve it. An approximate numeric integration gives $1.6449340668482264364724151666460251892189...$ that is close to $\frac{\pi^2}6$. But when I tried to increase the precision above 60 decimal digits, I began to see a tiny difference, which could be interpreted either as a numerical algorithm glitch, or as $\frac{\pi^2}6$ being just an accidentally close value and not the exact answer. Indeed, $\frac{\pi^2}6$ would be a suspiciously nice result for this integral. Anyway, I need your help with this.</p>
",<calculus>
"<p>I have the following task: Check whether given function series is pointwise convergent, uniformly convergent or ""almost"" uniformly convergent (<em>id est</em> $f, f_{n} : I \rightarrow \mathbb{R} $ and $ \forall_{a, b \in I} (f_{n} \downarrow _{[a;b]}) $ uniformly convergent to $ (f \downarrow _{[a;b]}) $.</p>

<p><em>I use $ \downarrow _{[a;b]} $ to say that function is cut to range $[a;b]$.</em></p>

<p>Function series: $ \sum^{+\infty}_{n=1} \frac{(-1)^n}{n+x}, x \in [0;+\infty) $</p>

<p>I have found that limit of sum argument ($n \rightarrow +\infty$) is equal 0. Is it enough to say yes, yes and yes?</p>
",<calculus>
"<p>How to evaluate the $\displaystyle\lim\limits_{x\to 0}\frac {2\sin(x)-\arctan(x)-x\cos(x^2)}{x^5}$, using power series? </p>

<p>It made sense to first try and build the numerator using power series that are commonly used: </p>

<p>$\displaystyle2\sin(x)=\sum_{k=0}^\infty \dfrac{2(-1)^kx^{2k+1}}{2k+1!} = 2x -\frac{x^3}{3}+\frac{x^6}{60} + \dotsb$</p>

<p>$\displaystyle-\arctan(x)=\sum_{k=0}^\infty \dfrac{(-1)^{k+1}x^{2k+1}}{2k+1} = -x +\frac{x^3}{3}-\frac{x^6}{6} + \dotsb$</p>

<p>$\displaystyle-x\cos(x^2)=\sum_{k=0}^\infty \dfrac{(-1)^{k+1}x^{4k+1}}{2k!} = -x +\frac{x^5}{2}+ \dotsb$</p>

<p>Hence, </p>

<p>$\displaystyle\lim\limits_{x\to 0}\frac {2\sin(x)-\arctan(x)-x\cos(x^2)}{x^5} =
\lim\limits_{x\to 0} \dfrac{[2x -\frac{x^3}{3}+\frac{x^6}{60} + \dotsb] + [-x +\frac{x^3}{3}-\frac{x^6}{6} + \dotsb] + [x +\frac{x^5}{2}+ \dotsb]} {x^5}$</p>

<p>In similar problems, the there is an easy way to take out a common factor that would cancel out with the denominator, resulting in an easy-to-calculate limit. Here, however, if we were to take a common factor from the numerator, say, $x^6$, then we would end up with an extra $x$</p>

<p>What possible strategies are there to solve this question? </p>
",<calculus>
"<p>I have this integral to evaluate</p>

<p>$$\int^x_1 \sqrt{1+ t^4}\, dt$$</p>

<p>I have tried substitution, trig identity and integration by parts, I don't have any answer.</p>

<p>Can anyone explain the method I need to work this out?</p>

<hr>

<p>Edit: I copied the following text from a now deleted answer by the OP in the hope that having it here may clarify and otherwise improve the question, JL.</p>

<p>It really is a problem that is asked to show that the function y = f (x) is a solution of the differential equation in this case would be</p>

<p>\begin{array}{rc}
\frac{1}{\sqrt{1+x^{4}} } \int ^{x}_{1}\sqrt{1+} t^{4}&amp; dt
\end{array}</p>

<p>And the ec. Dif is $$y'+\frac{2x^{3}}{1+x^{4}} y=1$$</p>
",<calculus>
"<p>I am starting out with the following:</p>

<p>$$
\frac{d^n}{dx^n}[g(x)^{f(x)}] = \sum_{c=0}^n g(x)^{f(x)-c}\lambda_{n,c}(x)
$$</p>

<p>Therefore:</p>

<p>$$
\frac{d^{n+1}}{dx^{n+1}}[g(x)^{f(x)}] = \sum_{c=0}^{n+1}g(x)^{f(x)-c}\lambda_{n+1,c}(x) = \frac{d}{dx}\sum_{c=0}^n g(x)^{f(x)-c}\lambda_{n,c}(x)
$$</p>

<p>$\lambda_{n,c}(x)$ is defined like so:</p>

<p>$$
\lambda_{n,c}(x) = \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j} \frac{d^j}{df^j}[f(x)_c] B_{n,k}^{(f \diamond g)^c}(x)
$$</p>

<p>My goal is to find a recurrence relation for $B_{n,k}^{(f \diamond g)^c}(x)$ by setting the two expressions equal to eachother. This is my work so far:</p>

<p>$$
\frac{d}{dx}[g(x)^{f(x)-c} \lambda_{n,c}(x)] = \left((f(x)-c)\frac{g'(x)}{g(x)} + \ln(g(x)) f'(x)\right)g(x)^{f(x)-c} \lambda_{n,c}(x) + g(x)^{f(x)-c} \frac{d}{dx}[\lambda_{n,c}(x)]
$$
Note from now on i will denote $\frac{d^j}{df^j}[f(x)_c] = f_c^{(j)}$
$$
\frac{d}{dx}[\lambda_{n,c}(x)] = \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \left(\frac{g'(x)}{g(x)}(k-c-j) \ln(g(x))^{k-c-j-1} f_c^{(j)} B_{n,k}^{(f \diamond g)^c} + \ln(g(x))^{k-c-j} \frac{d}{dx}[f_c^{(j}B_{n,k}^{(f \diamond g)^c}]\right)
$$</p>

<p>Now, for me to find an expression that will result in a recurrence relation i am going to attempt to group all the $\ln(g(x))^{k-c-j}$ together and set all these terms equal to:</p>

<p>$$
\sum_{c=0}^{n+1} g(x)^{f(x)-c} \lambda_{n+1,c}(x)
$$</p>

<p>By doing this i will have found a way to isolate the $g(x)^{f(x)-c}$ terms as well as the $\ln(g(x))^{k-c-j}$ terms.</p>

<p>To do this i will seperate each individual term and attempt to manipulate it in order to fit these conditions:</p>

<p>$$
A = f'(x) \ln(g(x)) \lambda_{n,c}(x) = f'(x) \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j+1} f_c^{(j)} B_{n,k}^{(f \diamond g)^c}(x) = f'(x) \sum_{k=c+1}^{n+1} \sum_{j=0}^{k-c-1} {k-c-1 \choose j} \ln(g(x))^{k-c-j} f_c^{(j)} B_{n,k-1}^{(f \diamond g)^c}(x)
$$
Now,for $B$ i will shift over a step backwards so that instead of $c$ we will be dealing with $c-1$, this is because of the differentiation of the natural log which in turn results in $\frac{g'(x)}{g(x)}$. When we multiply $\frac{g'(x)}{g(x)}$ with $g(x)^{f(x)-c}$ we will get $g'(x) g(x)^{f(x)-c-1}$ therefore by evaluating the expression at $c-1$ we will be evaluating the part of the summation that is dealing with $g(x)^{f(x)-c}$ instead of dealing with the summation that deals with $g(x)^{f(x)-c-1}$. If there is any questions about this please do not hesitate to ask in the comments.
$$
B = g'(x) (f(x)-c+1) \lambda_{n,c-1}(x) = g'(x) (f(x)-c+1) \sum_{k=c-1}^n \sum_{j=0}^{k-c+1} {k-c+1 \choose j} \ln(g(x))^{k-c-j+1} f_{c-1}^j B_{n,k}^{(f \diamond g)^{c-1}}(x) = g'(x) (f(x)-c+1) \sum_{k=c}^{n+1} \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j} f_{c-1}^j B_{n,k-1}^{(f \diamond g)^{c-1}}(x)
$$
Now, for $C$ and $D$ i will split up the two parts in the part where i differentiated the $\lambda_{n,c}(x)$, in variable $C$ we will be using the same logic as i used for ""shifting"" the $c$ variable to $c-1$.</p>

<p>$$
C = \lambda_{n,c}'(x)_{part \space 1} = g'(x)\sum_{k=c-1}^{n} \sum_{j=0}^{k-c+1} {k-c+1 \choose j} (k-c-j+1) \ln(g(x))^{k-c-j} f_{c-1}^{(j)} B_{n,k}^{(f \diamond g)^{c-1}}(x) = g'(x)\sum_{k=c}^{n+1} \sum_{j=0}^{k-c} {k-c \choose j} (k-c-j) \ln(g(x))^{k-c-j} f_{c-1}^{(j)} B_{n,k}^{(f \diamond g)^{c-1}}(x)
$$
Now for $C$ i did a little bit of trickery, first of all, when $k=c-1$ the term is equal to zero due to the $(k-c+1)$ term and when $j = (k-c+1)$ the term is equal to zero due to the $(k-c-j+1)$ term.
$$
D = \lambda_{n,c}'(x)_{part \space 2} = \sum_{k=c}^n \sum_{j=0}^{k-c} {k-c \choose j} \ln(g(x))^{k-c-j} \frac{d}{dx}[f_c^{(j)} B_{n,k}^{(f \diamond g)^c}(x)]
$$</p>

<p>Now the problem arises when i try to add $A+B+C+D$ and set it equal to $\lambda_{n+1,c}(x)$. i have attempted to do this many times but i have hit some points where it becomes troubling or that the identity does now work at all. If someone can please help me with the issue it would be alot of help to me. Thank you all for reading this if you have gotten this far, i appreciate it a lot.</p>
",<calculus>
"<p>This demand is a part of a proof. It must be easy, I'm just failing showing it rigorously.  </p>

<p>Let: $\left| {x_{n+1} -x_n} \right| &lt; {1\over 2^n}$  </p>

<p>We want to prove it's a Cauchy's sequence:  </p>

<p>Without the lose of generality, Let us assume $m&gt;n$:  </p>

<p>$$\left| {{x_m} - {x_n}} \right| = \left| {{x_m} - {x_{m - 1}} + {x_{m - 1}} - {x_{m - 2}} + ... + {x_{n + 1}} - {x_n}} \right| \le \left| {{x_m} - {x_{m - 1}}} \right| + ... + \left| {{x_{n + 1}} - {x_n}} \right| \le \frac{1}{{{2^m}}} + ... + \frac{1}{{{2^n}}}$$</p>

<p>I feel like I'm on the right path. Can you help from here?  </p>
",<calculus>
"<p>Prove that
$$\sum\limits_{k=1}^{\infty} \frac {1}{(p+k)^2} = -\int_0^1 \frac{x^p \log x}{1-x}\,dx$$
for $p&gt;0$.</p>

<p>I tried to transform LHS as Riemann sum form but failed.</p>

<p>Can anyone give some idea? Many Thanks!</p>
",<calculus>
"<blockquote>
  <p>Let a sequence, $\{x_n\}$ such that: $x_{n+1}=x_n-x_n^3$ and $0&lt;x_1&lt;1$.<br>
  1) Prove $\mathop {\lim }\limits_{n \to \infty } {x_n} = 0$<br>
  2) Calculate $\mathop {\lim }\limits_{n \to \infty } n{x_n}^2$  </p>
</blockquote>

<p>So, section (1) is very easy. I didn't really bother to write it down - just show the sequence is monotonically decreasing and bounded bellow by zero.  </p>

<p>Section (2) is the real fun, I do familiar with the Lemma says: ""If $a_n$ limit is $0$ and $b_n$ is bounded then the limit of $a_nb_n$ is also zero"" - But I don't think it can work here. </p>

<p>I tried separating the limit using limits-arithmetic into two limits, but then I got:<br>
$$\mathop {\lim }\limits_{n \to \infty } n{x_n} \cdot \mathop {\lim }\limits_{n \to \infty } {x_n}$$</p>
",<calculus>
"<p>So far I've substituted $x=\sin t$ ; $dx = \cos t\; dt$, leaving me to integrate $\sin^2t\cos^2t\;dt$.</p>

<p>I'm stuck here. I thought to use the identity $\sin 2t = 2 \sin t \cos t$ but it looks like it doesn't lead anywhere.</p>

<p>Any tips would be greatly appreciated!</p>
",<calculus>
"<p>Let a>b>c>0.</p>

<p>How may one find the limit $\lim_{x \to \infty} (a^x+b^x-c^x)^{\frac{1}{x}}$?</p>

<p>It's obvious that it's bounded from below by c, so I tried to show that it's also bounded from above by c and then use sandwich (I factored out c and then tried to show that the limit is 1).</p>

<p>I also tried to use the method of $e^{ln}$, but also got nowhere... L'Hospital also didn't help.</p>

<p>Please help, thank you!  </p>
",<calculus>
"<p>I'm having a little difficulty understanding how to do the .05 using differentials. I'm just hoping someone can walk me through, step by step, and explain why they are during it that way.</p>

<p>$$\sqrt[3] {27.05}$$</p>

<p><em>Edit</em>
Apologies to all, I wasn't very clear on what I was asking. </p>

<p>I was looking for someone to explain how to find an approximate answer to the equation above using differentials. (such as @Arturo has done)</p>

<p>@Sasha @Ross  I apologize for wasting your time when answering the question using series. </p>
",<calculus>
"<p>$x = 0, x = 9 - y^2$ rotated about $x = -1$</p>

<p>I'm having a lot of trouble deciding whether to use the disc method or the shell method. Intuitively, it makes sense that the shell method would be simpler when you are rotating horizontally, like around the y-axis or x = -1.</p>

<p>I know that the shell method is:</p>

<p>$[circumference][height][width]$, where $C = 2\pi r$, $h = f(x)$, and $w =
 \Delta x$</p>

<p>I must find the radius of the solid about the line $x = -1$.</p>

<p>So, I see that the radius must be $y - (-1) \rightarrow r = y + 1$</p>

<p>Therefore, $C = 2\pi (y + 1)$, right?</p>

<p>The height varies with $f(y) = 9 - y^2$.</p>

<p>Here is where things get really muddled for me.</p>
",<calculus>
"<p>I sometimes see Cauchy's Mean Value Theorem stated as follows:</p>

<blockquote>
  <p>Let $f,\ g:\mathbb{R}\rightarrow\mathbb{R}$ be continuous on $[a,\ b]$ and differentiable on $(a,\ b)$. Suppose that $g(b) \neq g(a)$. <strong>Then there exists $c\in(a,\ b)$ such that $g'(c)\neq 0$</strong> and such that $$\frac{f(b) - f(a)}{g(b) - g(a)} = \frac{f'(c)}{g'(c)}$$</p>
</blockquote>

<p>I have never once seen a proper proof of the bolded fact and I'm beginning to wonder about the validity of it. Is the assumption $g(b) \neq g(a)$ really enough to prove the existence of such a $c$?</p>

<p>Edit: I think my question is being misunderstood. I am <strong>not</strong> asking for a standard proof of the Cauchy Mean Value Theorem. The proofs I see assume that $g'(x) \neq 0\ \forall\ x\in(a,\ b)$. This version also claims $g'(c) \neq 0$ when $g(b) \neq g(a)$ (along with the standard continuity/differentiably conditions of course). How can we guarentee there exists such a $c$?</p>
",<calculus>
"<p>I need to prove the following:</p>

<blockquote>
  <p>Let $\delta &gt; 0$. Then $$
\sin \pi x \geq \frac{\pi}{2}\delta\;.
$$ holds for $x \in [\delta, 1-\delta]$.</p>
</blockquote>

<p>I tried to deduce the inequality using the definition of the sine as a power series, however, to no avail. Is there any quick way or hint to deduce this inequality?</p>

<p>Thanks a lot!</p>
",<calculus>
"<ol>
<li><p>Is there some general method for finding such curves? Let's say I have a planar curve, how can I project it onto a sphere?</p></li>
<li><p>I am interested in a curve that starts at the south pole of a sphere, then wraps it in spiral motion and ends at the north pole. Is it possible to construct?</p></li>
</ol>
",<calculus>
"<p>One of the first things ever taught in a differential calculus class:</p>

<ul>
<li>The derivative of $\sin x$ is $\cos x$.</li>
<li>The derivative of $\cos x$ is $-\sin x$.</li>
</ul>

<p>This leads to a rather neat (and convenient?) chain of derivatives:</p>

<pre>
sin(x)
cos(x)
-sin(x)
-cos(x)
sin(x)
...
</pre>

<p>An analysis of the shape of their graphs confirms <em>some</em> points; for example, when $\sin x$ is at a maximum, $\cos x$ is zero and moving downwards; when $\cos x$  is at a maximum, $\sin x$ is zero and moving upwards.  But these ""matching points"" only work for multiples of $\pi/4$.</p>

<p>Let us move back towards the original definition(s) of sine and cosine:</p>

<p>At the most basic level, $\sin x$ is defined as -- for a right triangle with internal angle $x$ -- the length of the side opposite of the angle divided by the hypotenuse of the triangle.</p>

<p>To generalize this to the domain of all real numbers, $\sin x$ was then defined as the Y-coordinate of a point on the unit circle that is an angle $x$ from the positive X-axis.</p>

<p>The definition of $\cos x$ was then made the same way, but with adj/hyp and the X-coordinate, as we all know.</p>

<p>Is there anything about this <strong>basic</strong> definition that allows someone to look at these definitions, alone, and think, ""Hey, the derivative of the sine function with respect to angle is the cosine function!""</p>

<p>That is, from <strong>the unit circle definition alone</strong>.  Or, even more amazingly, the <strong>right triangle definition alone</strong>.  Ignoring graphical analysis of their plot.</p>

<p>In essence, I am asking, essentially, ""Intuitively <em>why</em> is the derivative of the sine the cosine?""</p>
",<calculus>
"<p>I've sort of gotten a grasp on the Chain rule with one variable.  If you hike up a mountain at 2 feet an hour, and the temperature decreases at 2 degrees per feet, the temperature would be decreasing for you at $2\times 2 = 4$ degrees per hour.</p>

<p>But I'm having a bit more trouble understanding the Chain Rule as applied to multiple variables.  Even the case of 2 dimensions </p>

<p>$$z = f(x,y),$$ </p>

<p>where $x = g(t)$ and $y = h(t)$, so</p>

<p>$$\frac{dz}{dt} = \frac{\partial z}{dx} \frac{dx}{dt} + \frac{\partial z}{dy} \frac{dy}{dt}.$$</p>

<p>Now, this is easy enough to <em>""calculate""</em> (and figure out what goes where).  My teacher taught me a neat tree-based graphical method for figuring out partial derivatives using chain rule.  All-in-all, it was rather hand-wavey.  However, I'm not sure exactly how this works, intuitively.</p>

<p>Why, intuitively, is the equation above true?  Why <strong>addition</strong>?  Why not multiplication, like the other chain rule?  Why are some multiplied and some added?</p>
",<calculus>
"<blockquote>
  <p>$$\lim\limits_{ x \rightarrow 0}{f(g(x))}=f(\lim\limits_{ x
 \rightarrow 0}g(x))$$</p>
</blockquote>

<p>I have seen this step in a derivation of a result which is not the point of interest here.</p>

<p>The book wrote the reason for it was that it is when $f$ is continuous.</p>

<p>I wonder how one can write so. Does there exist any proof? Any hint to the proof is more appreciated.</p>
",<calculus>
"<p>Why does it converge conditionally?
$$\sum_{k=1}^{\infty} \frac {(-1)^{k-1}}{k}$$</p>
",<calculus>
"<p>I am trying to find $$\lim_{x\to0}\frac{\sin5x}{\sin4x}$$</p>

<p>My approach is to break up the numerator into $4x+x$. So,</p>

<p>$$\begin{equation*}
\lim_{x\to0}\frac{\sin(4x+x)}{\sin4x}=\lim_{x\to0}\frac{\sin4x\cos x+\cos4x\sin x}{\sin4x}\\
=\lim_{x\to0}(\cos x +\cos4x\cdot\frac{\sin x}{\sin4x})\end{equation*}$$</p>

<p>Now the problem is with $\frac{\sin x}{\sin4x}$. If I use the double angle formula twice, it is going to complicate the problem.</p>

<p>The hint says that you can use $\lim_{\theta\to0}\frac{\sin\theta}{\theta}=1$.</p>

<p>I have little clue how can I make use of the hint.</p>

<p>Any helps are greatly appreciated. Thanks!</p>
",<calculus>
"<p>can someone give me an example of Differentiable function at x=4 and funcstions who dont Differentiable function at x=4?</p>

<p>$f(x) = 2x-7$</p>

<p>$k(x) = 100x^7-55x^5+10000x^2$</p>

<p>$g(x) = 23$</p>

<p>Those are Differentiable function at x-4, right?</p>

<p>$q(x) = x/(x-4)$</p>

<p>$y(x) = 78x^2/(x^2-8x+16)$</p>

<p>$p(x) = 2/(x^2+16)$</p>

<p>and those are not Differentiable function?</p>

<p>Am I right?</p>

<p>Thanks for help</p>
",<calculus>
"<p>I have downloaded a book about Calculus from <a href=""http://ocw.mit.edu"" rel=""nofollow"">MIT OCW</a>. In that book, there is a section ""A Thousand points of Light"". (You can download the relevant section from <a href=""https://drive.google.com/file/d/0BytxARWilliKdFpVeDRGcVZ1dFU/edit?usp=sharing"" rel=""nofollow"">here</a>.)</p>

<p>In that section, it is written that the graph of $y=\sin x$ is different from the graph of $y=\sin n$.</p>

<p><img src=""http://i.stack.imgur.com/NKArE.png"" alt=""Book Image""></p>

<p>However, I cannot understand this thing.  Why will the graph of $y=\sin n$ be different from the graph of $y=\sin x$? How can you change a graph by changing a variable in the function? And how have they plotted the graph in the book?</p>
",<calculus>
"<p>Does there exist a sequence $\left(a_n\right)_{n\ge1}$ with $a_n &lt; a_{n+1}+a_{n^2}, \forall n=1,2,3,\ldots$ such that the series $\displaystyle{\sum_{n=1}^{\infty}a_n}$ converges? </p>

<p>This is the <strong>first</strong> part of <a href=""http://math.stackexchange.com/questions/242190/does-there-exist-a-sequence-a-n-n-ge1-with-a-n-a-n1a-n2-such"">this</a> question which has an (accepted) answer for its <strong>second</strong> part only:<br>
The last sentence of the <a href=""http://math.stackexchange.com/a/245596/39722"">answer</a> is: <br> ""Now we note that $\sum_{i=1}^{\infty}a_i\geq\sum_{k=0}^{\infty}\sum_{i\in J_k}a_i&gt;\sum_{k=0}^{\infty}a_n$, so the sum diverges.""<br>
For the inequality $\sum_{i=1}^{\infty}a_i\geq\sum_{k=0}^{\infty}\sum_{i\in J_k}a_i$ to be valid, we have to assume the positivity of $(a_n)_{n\in\mathbb N}$ since $\displaystyle{\bigcup_{k\in\mathbb N}J_k\neq\mathbb N}$.</p>

<p>According to the <a href=""http://math.stackexchange.com/questions/242190/does-there-exist-a-sequence-a-n-n-ge1-with-a-n-a-n1a-n2-such?lq=1#comment556592_242190"">comments</a> the first part is a difficult question.</p>
",<calculus>
"<p>Is there a continuous function on R such that $f(f(x))=e^{-x}$? I have tried to take derivative of the two sides,but I can't get anything I want.what can I do?</p>
",<calculus>
"<p>Express the volume $V$ of a regular tetragonal pyramid as a  function of its altitude $x$ and the edge of a lateral face (lateral edge) $y$ </p>

<p>The answer given by the book is $\frac{2}{3} (y^2 - x^2) x $. But,I've found the lateral edge is $2 \sqrt{y^2 - x^2 } $ and I thought that the area of the basis is $ 4(y^2 - x^2)$. What am I doing wrong?</p>

<p>Thanks for your help!</p>
",<calculus>
"<p>So I tried this out and got stuck with this:</p>

<p>$$0 = 3x^{(7/6)} + 2x - 10$$</p>

<p>I didn't think I could use a quadratic for this since its to the power of $7/6$</p>

<p>Here is the working I did:</p>

<p>We know its a critical point when f'(a) = 0</p>

<p>So I found the derivative of f(x) which is $$2*(5-x)/3x^{1/2} - x ^{2/3}$$ </p>

<p>So I set this equal to 0</p>

<p>$$2*(5-x)/3x^{1/2} - x ^{2/3} = 0$$ 
$$2*(5-x)/3x^{1/2}=x ^{2/3}$$ 
$$2*(5-x)=x ^{2/3}\times3x^{1/2}$$ 
$$10-2x=3x ^{2/3 +1/2}$$ 
$$10=3x ^{7/6} + 2x$$ </p>

<p>But this would be such a messy answer, so I think I have done something wrong with my working. Do you have any ideas?</p>
",<calculus>
"<p>I just read <a href=""http://www.npr.org/2014/04/20/303716795/far-from-infinitesimal-a-mathematical-paradoxs-role-in-history"" rel=""nofollow"">this article</a> on npr, which mentioned the following question:</p>

<blockquote>
  <p>You can keep on dividing forever, so every line has an infinite amount
  of parts. But how long are those parts? If they're anything greater
  than zero, then the line would seem to be infinitely long. And if
  they're zero, well, then no matter how many parts there are, the
  length of the line would still be zero.</p>
</blockquote>

<p>It further mentions that</p>

<blockquote>
  <p>Today, mathematicians have found ways to answer that question so that
  modern calculus is rigorous and reliable.</p>
</blockquote>

<p>Can anyone elaborate on the modern answers to this question? </p>
",<calculus>
"<blockquote>
<p> Let $ \zeta(s) $be the riemann zeta function, then

$$ \prod_{n=2}^{\infty}n^{\zeta(n)-1} &lt;1+\frac{\pi^2}{6}$$
</p>
</blockquote>

<p>The problem is difficult, I don't know how to go started</p>

<p>Thank you very much!</p>
",<calculus>
"<p>Can anyone help me with finding the volume of a solid of revolution of f(x) about the x axis for the interval [1,6].  It's supposed to be able to be done without needing calculus but I am having trouble figuring it out.</p>

<p>$f(x) =
\begin{cases}
1  &amp;  1 \leq x&lt; 2\\ 
1/2 &amp;  2 \leq x&lt; 3\\ 
. &amp;          .\\ 
. &amp;          .\\
1/n &amp;  n\leq x&lt; n+1\\ 
\end{cases}$</p>

<p>I know the volume would be found like this $\pi$ $\int_{1}^{6}(f(x))^2dx$ but I am unsure about how to go about it with this function.</p>

<p>Any help is appreciated.
Thanks</p>
",<calculus>
"<p>I have the following series which gives me Pi.</p>

<p><img src=""http://i.stack.imgur.com/qEjCP.png"" alt=""Pi series""></p>

<p>I need to figure out how many terms of the series I need to be accurate (with respect to Pi) up to 4 decimals.</p>

<p>I also need a formula to figure out how many terms of the series I will need to be accurate to n decimal places of accuracy. If I can find this formula, I should be able to answer the above question easily.</p>

<p>I believe I should be using the error bounds to determine this.</p>

<p><img src=""http://i.stack.imgur.com/HHfPp.png"" alt=""Error bounds equation""></p>

<p>However, I don't understand exactly how this will tell me how many terms I need to be accurate to n decimals of Pi.</p>

<p>For example, how many terms would I need to be accurate to 100 decimal places using the equations above?</p>
",<calculus>
"<p>I have a question here $\frac{d}{dx}\left(\frac{8}{e^{1-4x}}\right)$</p>

<p>I simplify this  $8\left(\frac{1}{\left(e^{1-4x}\right)^2}\right)\left(-4e^{1-4x}\right)$ to  $\left(\frac{32}{e^{1-4x}}\right)$ </p>

<p>but I am being told this is wrong am I missing out a step ?</p>
",<calculus>
"<p>$$
\begin{align}
DFS[x^*(-n)] &amp;= \frac{1}{N}\sum^{N-1}_{n=0}x^*(-n)e^{-j2\pi kn/N}\\
&amp;= \left[\frac{1}{N}\sum^{N-1}_{n=0}x(-n)e^{j2\pi kn/N}\right]^*\\
&amp;= \left[\frac{1}{N}\sum^{N-1}_{p=0}x(p)e^{-j2\pi kp/N}\right]^*\\
&amp;= c^*_k
\end{align}
$$</p>

<p>(<a href=""http://i.stack.imgur.com/2P5nK.png"" rel=""nofollow"">Source</a>)</p>

<p>In the given series we have substituted $p=-n$ and limit for $n$ is from $0$ to $N-1$. Therefore, the limit for $p$ should be from $0$ to $1-N$, but limit is still the same. Can anyone explain why?</p>
",<calculus>
"<p>the real analysis book says that 
$$f:\mathbb{R}_+ \rightarrow \mathbb{R}_+$$ where $f$ is strictly increasing and concave function. it has the following property
$$f(ax+(1-a)y) \le f(ax) + f((1-a)y)$$
where $a \in [0,1]$.</p>

<p>This property seems wrong. As far as I know, that property is for convex function, not concave function. I do not think the textbook is wrong. Can you please explain it?</p>

<p>The textbook used it to show the function $d(x,y)$ is a matric in $\mathbb{R}$</p>
",<calculus>
"<p>While I was <a href=""http://math.stackexchange.com/a/1402835/153012"">working</a> on <a href=""http://math.stackexchange.com/q/879854/153012"">this question</a> by @Vladimir Reshetnikov, I've found the following relations between Gaussian hypergeometric function values and the Baxter constant:</p>

<blockquote>
  <p>$$\begin{align}{_2F_1}\left(\begin{array}c\tfrac13,\tfrac13\\1\end{array}\middle|\,-1\right) &amp;\stackrel{?}{=} \frac{1}{2^{\small2/3}}\,C^2_\text{B4CC},\\
{_2F_1}\left(\begin{array}c\tfrac23,\tfrac23\\1\end{array}\middle|\,-1\right) &amp;\stackrel{?}{=} \frac{1}{2}\,C^2_\text{B4CC},\\
{_2F_1}\left(\begin{array}c\tfrac13,\tfrac13\\1\end{array}\middle|\,\frac19\right) &amp;\stackrel{?}{=} \frac{1}{\sqrt[3]{3}}\,C^2_\text{B4CC},\\
{_2F_1}\left(\begin{array}c\tfrac23,\tfrac23\\1\end{array}\middle|\,\frac19\right) &amp;\stackrel{?}{=} \frac{\sqrt[3]{3}}{2}\,C^2_\text{B4CC},\\
{_2F_1}\left(\begin{array}c\tfrac13,\tfrac13\\1\end{array}\middle|\,9\right) &amp;\stackrel{?}{=} \frac{3-i\sqrt3}{6}\,C^2_\text{B4CC},\\
{_2F_1}\left(\begin{array}c\tfrac23,\tfrac23\\1\end{array}\middle|\,9\right) &amp;\stackrel{?}{=} -\frac{i}{2\sqrt3}\,C^2_\text{B4CC},\\
\end{align}$$</p>
</blockquote>

<p>where ${_2F_1}$ is the <a href=""http://mathworld.wolfram.com/HypergeometricFunction.html"" rel=""nofollow"">Gaussian hypergeometric function</a>, and</p>

<blockquote>
  <p>$$
C^2_\text{B4CC} = \frac{3}{4\pi^2}\Gamma^3\left(\tfrac{1}{3}\right) \approx 1.460998486206318358158873117846059697\dots
$$</p>
</blockquote>

<p>is <a href=""http://mathworld.wolfram.com/BaxtersFour-ColoringConstant.html"" rel=""nofollow"">Baxter's four-coloring constant</a>.</p>

<p>The first two identity are known, but with the last four relations I've never met before.</p>

<blockquote>
  <p>How could we prove these identities?</p>
</blockquote>

<p>In this <a href=""http://arxiv.org/pdf/1203.4498.pdf"" rel=""nofollow"">paper</a>, there is another connection between a hypergeometric value and Baxter constant.</p>
",<calculus>
"<h2><strong>Derivation/equation for solid angle factor correction</strong></h2>

<p><strong><em>Summary</em></strong>: I want to determine a correction for the Solid Angle Factor (SAF) due to partially overlapping 'outer' spheres (of different sizes), as perceived/viewed from the center of a 'central' sphere. The distances of these spheres from the 'central sphere' are not equal, i.e. they are not equidistant. The coordinates, distances and relatives angles of all the spheres can be determined via software, i.e. they are known.</p>

<hr>

<p>“The Solid Angle Factor (SAF) is defined as the solid angle of the ligand cone comprising the metal at the apex and the primary coordinating atom or group (the first-order SAF) or the whole ligand (the second-order SAF) divided by 4π. <strong>Geometrically, it refers to the ratio of the projected area to 4π, i.e. the area of the sphere surface.</strong> It actually represents the size of the ligand as viewed from the metal centre towards the ligand. The sum of the values of SAF of all the ligands coordinated to the metal centre represents the total occupancy of the ligands in the coordination sphere. It is apparent that this occupancy should not reach unity because there are gaps and holes among the ligands.” (Polyhedron Vol. 6, No. 5, pp. 104-1048, 1987) (See <a href=""https://www.dropbox.com/s/g2f97bn97yq56vu/xi-zhang1987.pdf?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/g2f97bn97yq56vu/xi-zhang1987.pdf?dl=0</a>)</p>

<p>In other words the solid angle is related to the projection of an 'outer' 'ligand' (sphere of known radius) onto the surface of a central ('metal' atom) sphere, as viewed from the center of the central sphere.
<strong>Solid angle factor (SAF) equation</strong>:</p>

<p>$$SAF=\frac{2\pi (1-\cos \Theta )}{4\pi } = \frac{1}{2}(1-\cos \Theta)$$</p>

<p>$$\theta =\sin^{-1}(\frac{\nu }{\iota })$$</p>

<p>, where ν = radius of the sphere of ligand/coordinating atom (This is known),
l = distance between the center of the 'ligand' spheres to the center of the metal atom/ 'sphere'.</p>

<p>A correction (approximation) has been supplied for the case where 2 identical  'ligand' spheres (i.e. the spheres have identical radii) are partially overlapping and are equidistant from the metal center:</p>

<p>$$\Delta SAF = \frac{2(\frac{2\varphi (\pi \nu ^{2})}{360}-d\nu \sin \varphi )}{4\pi (\iota \cos \eta )^{^{2}}}$$</p>

<p>(See link above and the following link:
<a href=""https://www.dropbox.com/s/7w3kyvf1xq9e320/Solid%20angle%20factor_details_1_2%20%281%29.pdf?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/7w3kyvf1xq9e320/Solid%20angle%20factor_details_1_2%20%281%29.pdf?dl=0</a> . These diagrams/equations illustrate what I understand of the the given correction equation, as well as the definition of the above symbols)</p>

<hr>

<p><strong>Derivation/equation for solid angle factor correction</strong>:
However, since there are multiple 'ligand' spheres around the metal several may partially overlap (or be perceived to overlap as viewed from the central 'metal' sphere) due to their close proximity to the metal, thereby resulting in the ‘observed’ solid angle factor being different from the calculated solid angle factor (without correction) from the metal.
The correction must consider:</p>

<ol>
<li>The distance from the central 'metal' sphere to the center of each 'ligand' sphere. (They are not necessarily equidistant)</li>
<li>The radius of each 'ligand' sphere (since they may have different (known) radii).</li>
</ol>

<p><strong>How would such an equation be derived and what would the equation be?</strong> Perhaps by considering 2 'ligand' spheres at a time? I am interested in the corrected (i.e. accurate) sum of all the SAF (known as the <strong>Solid Angle Sum (SAS)</strong>) of all the surrounding 'ligand' spheres, which should be &lt; or = 1.0.</p>

<p>I look forward to any helpful responses.
$$Thank you$$</p>
",<calculus>
"<p>According to <em>Mathematica</em>, we have that </p>

<p>$$\int_0^{\infty } \frac{\arctan(x)}{x \left(x^2+1\right)^5} \, dx=\pi  \left(\frac{\log (2)}{2}-\frac{1321}{6144}\right)$$
that frankly speaking looks pretty nice.</p>

<p>However <em>Mathematica</em> shows that </p>

<p>$$\int \frac{\arctan(x)}{x \left(x^2+1\right)^5} \, dx$$
$$=-\frac{1}{2} i \text{Li}_2\left(e^{2 i \tan ^{-1}(x)}\right)-\frac{1}{2} i \tan ^{-1}(x)^2+\tan ^{-1}(x) \log \left(1-e^{2 i \tan ^{-1}(x)}\right)-\frac{65}{256} \sin \left(2 \tan ^{-1}(x)\right)-\frac{23 \sin \left(4 \tan ^{-1}(x)\right)}{1024}-\frac{5 \sin \left(6 \tan ^{-1}(x)\right)}{2304}-\frac{\sin \left(8 \tan ^{-1}(x)\right)}{8192}+\frac{65}{128} \tan ^{-1}(x) \cos \left(2 \tan ^{-1}(x)\right)+\frac{23}{256} \tan ^{-1}(x) \cos \left(4 \tan ^{-1}(x)\right)+\frac{5}{384} \tan ^{-1}(x) \cos \left(6 \tan ^{-1}(x)\right)+\frac{\tan ^{-1}(x) \cos \left(8 \tan ^{-1}(x)\right)}{1024}$$</p>

<p>and this form doesn't look that nice. </p>

<p>Having given the nice form of the closed form I wonder if we can find a <strong>very nice and simple way</strong> of   getting the answer. What do you think?</p>

<p><em>A supplementary question</em>:</p>

<p>$$\int_0^{\infty } \frac{\arctan^2(x)}{x \left(x^2+1\right)^5} \, dx=\frac{55}{108}-\frac{1321}{12288}\pi^2+\frac{\pi^2}{4} \log (2)-\frac{7 }{8}\zeta (3)$$</p>
",<calculus>
"<p>I'm trying to find the integral of $$\int\frac{1}{x* (\sqrt{4x^4 - 9})}$$</p>

<p>Attempt:</p>

<p>I assumed that the integral would be some sort of inverse trigonometric function. Because of this, I did the following</p>

<p>Let $$u = 2x^2$$
$$du = 4x$$
$$\frac14 du = xdx$$</p>

<p>The reason why I did this was because I wanted to make it into the form of 
$$\int \frac{1}{\sqrt{a^2 - u^2}}$$ since this would be equal to $$\sin(\frac ua) +c$$</p>

<p>The problem I have encountered is that I have a - in between of my a and u values instead of a positive. I know that if there was a u variable in front of my radical in the bottom then the answer would be some sort of inverse secant. </p>

<p>This is my first time asking a question so I hope I'm being clear enough about the question and what I've said I think the answer would be like.</p>

<p>Thank you in advance!</p>
",<calculus>
"<p>I am a beginner in calculus and I want to know what is the difference between sum and integral. More specifically I came across this example:</p>

<blockquote>
  <p>Compare $$\sum^\infty_1\frac1x\space \text{and} \space \int_1^\infty\frac1xdx$$</p>
</blockquote>

<p>It would be really helpful if someone explains this to me. I want to know the difference between the two.</p>

<p>Thanks for any help!!</p>
",<calculus>
"<p>I'm trying to solve this equation
$$
\left \lfloor{x +\frac{1}{100}}\right \rfloor + \left \lfloor{x +\frac{2}{100}}\right \rfloor + ... + \left \lfloor{x +\frac{223}{100}}\right \rfloor = 521
$$</p>

<p>I haven't faced until now problems like this one... How can I find $x$? 
Rather than a specific solution, I'm looking for the intuition to use in order to set up a solution to problems like this one.</p>
",<calculus>
"<p>Given a function $f:\mathbb{R} \to \mathbb{R}$ such that 
$$f(x)=x+\int_{0}^{x}f(t)\,dt$$ then what is the relation between $f(x+y)$, $f(x)$ and $f(y)$</p>

<p>My Try: we have $$f(x+y)=x+y+\int_{0}^{x+y}f(t)dt$$ $\implies$</p>

<p>$$f(x+y)=x+y+\int_{0}^{x}f(t)dt+\int_{x}^{x+y}f(t)\,dt$$ $\implies$</p>

<p>$$f(x+y)=f(x)+y+\int_{x}^{x+y}f(t)\,dt$$ $\implies$</p>

<p>$$f(x+y)=f(x)+y+\int_{0}^{y}f(x+t)\,dt$$ Unable to proceed further.Help required</p>
",<calculus>
"<p>I have a set $S\subset\mathbb {R}^2$ with the following property</p>

<p>(P) $\forall x,y\in S$, $\forall\mathscr{C}$ a convex set that contains $x$ in its interior, $bd\mathscr{C}\cap [x,y]\subset \overline{bd\mathscr{C}\cap S}$.</p>

<p>Here $[x,y]$ denotes the segment with end-ponts $x$, $y$, $bd\mathscr{C}$ denotes the boundary of $\mathscr{C}$, and ""$\overline{\ \ \ \ \ }$"" stands for closure. </p>

<p>In other words the inclusion says </p>

<p>$\forall z\in bd\mathscr{C},\ z=tx+(1-t)y$, for some $0&lt;t&lt;1$, there is $(z_n)_n\subset bd\mathscr{C}\cap S,\ z_n\to z$.</p>

<p>This feels like $S$ is dense in an orderly fashion in the segment $[x,y]$ (on every boundary of a convex set). </p>

<p>Conjecture: If $S$ has (P) then $S$ is pathwise connected (meaning there is a continuous path within $S$ that connects any two points in $S$). </p>

<p>My try: For fixed $x,y\in S$ define the multi-function $F:[0,1]\rightrightarrows\mathbb{R}^2$, $$F(t):=\cup\{bd\mathscr{C}\cap S\mid tx+(1-t)y\in bd\mathscr{C}\ {\rm and}\ \mathscr{C}\ {\rm is\ a\ convex\ set\ that\ contains\ } x \ {\rm in\ its\ interior}\}$$</p>

<p>If one could use a continuous selection theorem to get a continuous selection of $F$ then it would be done. Unfortunately, I cannot show that, for example, $F$ has convex values to use Michael's selection theorem. </p>

<p>P.S. Of course evey locally pathwise connected set of $\mathbb {R}^2$ has (P). The current question is exactly the converse of that fact.</p>

<p>Any remark is greatly appreciated.</p>
",<calculus>
"<p>Given $f$ is function with continous derivatives, how do I obtain $f(x)$ in terms of $x$ from the equation below? Thanks in advance. </p>

<p>$$
f(x)=\lim_{t\to 0} \frac{1}{2t} \int_{x-t}^{x+t} s f'(s) ds
$$</p>
",<calculus>
"<p>Taylor expansion about $(x,y)$ of $f(x + a,\; y + k\; f(x + b,\; y + c))$</p>

<p>I do not understand what happens to the second $f$ inside. The inspiration for this question is Runge-Kutta methods.</p>
",<calculus>
"<blockquote>
  <p>If $x^2+ax-3x-(a+2)=0\;,$ Then $\displaystyle \min\left(\frac{a^2+1}{a^2+2}\right)$</p>
</blockquote>

<p>$\bf{My\; Try::}$ Given $x^2+ax-3x-(a+2)=0\Leftrightarrow ax-a = -(x^2-3x-2)$</p>

<p>So we get $$a=\frac{x^2-3x-2}{1-x} = \frac{x^2-2x+1+1-x-4}{1-x} = \left[1-x-\frac{4}{1-x}+1\right]$$</p>

<p>Now $$f(a) = \frac{a^2+1}{a^2+2} = \frac{a^2+2-1}{a^2+2} = 1-\frac{1}{a^2+2}$$</p>

<p>So $$f(x) = 1-\frac{1}{\left[(1-x)-\frac{4}{1-x}+1\right]^2+2}$$</p>

<p>Now put $1-x=t\;,$ Then we get $$f(t) =1- \frac{1}{\left(t-\frac{4}{t}+1\right)^2+2}$$</p>

<p>Now How can I maximize $\displaystyle \frac{1}{\left[(1-x)-\frac{4}{1-x}+1\right]^2+2, }\;,$ Help Required, Thanks</p>
",<calculus>
"<p>I'm trying to determine whether or not </p>

<blockquote>
  <p>$$\sum_{k=1}^\infty \frac{2+\cos k}{\sqrt{k+1}}$$ </p>
</blockquote>

<p>converges or not. </p>

<p>I have tried using the ratio test but this isn't getting me very far. Is this a sensible way to go about it or should I be doing something else?</p>
",<calculus>
"<blockquote>
  <p>Show, for $x_0=0$, that $\ln(\frac{1-x}{1+x})=-2\big[x+\frac{x^3}{3}+\dots+\frac{x^{2n-1}}{2n-1}+R_{2n}(f,0)(x)\big]$, with $$R_{2n}(f,0)(x)=-\frac{x^{2n+1}}{2n+1}\bigg(\frac{1}{(1+\theta x)^{2n+1}}+\frac{1}{(1-\theta x)^{2n+1}}\bigg)$$
  with $\theta\in(0,1)$</p>
</blockquote>

<p>I did already show the $-2\sum_{k=1}^n -\frac{x^{2k-1}}{2k-1}$ part, but I have struggle finding the Residual. We know that $$R_n=\frac{f^{n+1}(\theta)}{(n+1)!}(x-x_0)^{n+1}, \theta\in(x_0, x)$$</p>

<p>So, using the fact that $\ln(\frac{1-x}{1+x})=\ln(1-x)-\ln(1+x)$
and $\frac{d^n}{dx^n}\ln(1-x)=-\frac{(n-1)!}{(1-x_0)^n}$, $\frac{d^n}{dx^n}\ln(1+x)= (-1)^{n+1}\frac{(n-1)!}{(x+1)^n}$,</p>

<p>we obtain
$$R_{2n}(f,0)(x)=-\frac{x^{2n+1}}{2n+1}\bigg(\frac{1}{(1+\theta )^{2n+1}}+\frac{1}{(1-\theta)^{2n+1}}\bigg)\neq -\frac{x^{2n+1}}{2n+1}\bigg(\frac{1}{(1+\theta x)^{2n+1}}+\frac{1}{(1-\theta x)^{2n+1}}\bigg)$$</p>

<p>Could anyone explain me where I made a mistake?</p>
",<calculus>
"<p>Good morning, i have a problem when i go to calculate the partial sum of this series:</p>

<p>$S = 2+\frac{2}{3}+\frac{2}{9}+\frac{2}{27}+...+\frac{2}{3^{n-1}}$</p>

<p>I make this:</p>

<p>If this an geometric series then $a=2$ and $r=\frac{1}{3}$</p>

<p>then</p>

<p>$S={\displaystyle \sum_{i=1}^{n}2(\frac{1}{3}})^{i-1}$</p>

<p>but, i cannot calculate the partial sum, please help me!</p>
",<calculus>
"<blockquote>
  <p>I would like to know how to show that :
  $$\lim_{X \to -k}\:\prod_{j\neq k}\left(X+j \right)=(-1)^{k}k!\left(n-k \right).$$</p>
</blockquote>

<p>This is came from solution of exercise that he said :</p>

<ul>
<li>what is Partial fraction decomposition of :</li>
</ul>

<p>$$F(X)=\dfrac{n!}{\prod_{k=0}^{n}\left(X+k \right)}$$</p>

<p>indeed,</p>

<p>PFD of F:</p>

<p>$$F(X)=\sum_{k=0}^{n}\dfrac{a_k}{X+k} $$</p>

<p>$$\left(X+k\right)F(X)=\dfrac{n!}{\prod_{j\neq k}\left(X+j \right)}$$
and 
$$\prod_{j\neq k}\left(X+j \right)=(-1)^{k}k!\left(n-k \right)$$ then :</p>

<p>$$a_k=(-1)^{k}{n \choose k} $$</p>

<p>Finaly:</p>

<p>$$F(X)=\sum_{k=0}^{n}(-1)^{k}{n \choose k}\dfrac{1}{X+k} $$ </p>
",<calculus>
"<p>I am struggling to understand the derivation of an equation in a paper (<a href=""http://robots.stanford.edu/papers/diebel.surface.pdf"">A Bayesian Method for Probable Surface Reconstruction and Decimation</a>, specifically Eqn. 16). </p>

<p>Basically they define three vertices of a facet: $x_k, x_{k'},x_{k''}$ 
The normalized facet normal is defined as: $n_i = \dfrac{(x_{k'}-x_k) \times (x_{k''}-x_k)}{|(x_{k'}-x_k) \times (x_{k''}-x_k)|}$</p>

<p>So far so good. The problem is then that they need to compute $\frac{\partial n_i}{\partial x_k}$. Firstly ${n_i}$ and ${x_k}$ are both vectors, hence I'd expect that this partial derivative notation means in effect the Jacobian of ${n_i}$ wrt. ${x_k}$? In that case that is a 3x3 matrix. However the formula below (see Eqn 16) implies that the result is a 3x1 vector?? (confused!)</p>

<p>$\frac{\partial n_i}{\partial x_k} = \frac{I - n_in_i^T}{|(x_{k'} - x_k) \times (x_{k''} - x_k)|} (x_{k''} - x_{k'}) \times x_k$</p>

<p>I was hoping someone could shed some light on the dimensionallity confusion and also how that formula was derived, or if incorrect what is the correct forumation for $\frac{\partial n_i}{\partial x_k}$?  Thanks for the help!</p>
",<calculus>
"<p>What's the best way to evaluate an antiderivative like this one $$\int \frac{\sqrt{x-2}}{x+1}dx\ ?$$</p>

<p>I tried a $u$ substitution with $x-2$ and $x+1$ and neither got me a nicer looking integrand.  There are no squared terms so a trig sub doesn't leap out to me.  What's the way to do this?</p>
",<calculus>
"<p>I need advice on my studies of mathematics... I'm really depressed because it's impossible for me to understand many important parts of books such as Tenenbaum &amp; Pollard ""Ordinary Differential Equations"" or Kreyszig's ""Differential Geometry"", even after having got an A at a rigorous course in calculus (construction of the reals, limits with epsilon-delta arguments, proofs of almost all theorems presented...).</p>

<p>The reason is that these books on DE and DG use thinks like multiply the two sides of the equation by dx, or integrate dt, consider an infinitesimal displacement, etc, to arrive at conclusions... I really can't understand this reasonings... And I'm now looking at books on mechanics (for engineers) and it's even worse, because they talk about ""virtual work"", and other impossible-to-understand (for me) things...</p>

<p><strong>What should I do? Relearn calculus from some textbook that teach these things or maybe search other books for learning differential equations, mechanics, and differential geometry? I'm feeling really dumb.</strong></p>

<p>Thanks in advance.</p>
",<calculus>
"<p>I need to find the max of $$f(x)=\sqrt{(x^2-4)^2+(x-5)^2}-\sqrt{(x^2-2)^2+(x-1)^2}$$</p>

<p>When $x$ is a real number.</p>

<p>What i did is to simplify: $$f(x)=\sqrt{x^4-7x^2-10x+41}-\sqrt{x^4-3x^2-2x+5}$$.</p>

<p>Then i compute: $$f'(x)=\frac{-5-7x+2x^3}{\sqrt{41-10 x-7 x^2+x^4}}+\frac{1+3x-2x^3}{\sqrt{5-2 x-3 x^2+x^4}}$$.</p>

<p>But failed to solve $f'(x)=0$ for finding $f(x)_{max}$.</p>

<p>I would be glad for your help.</p>

<p>Thanks.</p>
",<calculus>
"<p>I am having a hell of time trying to differentiate the following function with respect to x. Do you have any suggestions</p>

<p>$f(x) = \frac{ w(i)^x}{  \sum\limits_{j} w(j)^x }$</p>

<p>where $w$ is a vector
Basically I don't get how to handle the vector in the denominator. Any help would be appreciated.</p>

<p>Thanks!</p>

<p>Also follow up: </p>

<p>$g(\hat{x}) = \sum\limits_{i} a* \hat{x}(i)$</p>

<p>what would be the derivative with respect to $\hat{x}$.</p>

<p>Again, thanks so much, I come from a CS background so still trying to wrap my head around the calculus of neural networks.</p>
",<calculus>
"<p>I'm having a bit of trouble with this problem. I tried writing down the first few terms explicitly but that doesn't seem to be working.</p>

<p>This is the sequence for when n is approaching infinity:</p>

<p>$\frac{25}{20^{n}} + 16\mathrm{arctan}(n^{6})$</p>
",<calculus>
"<p>The reason why I'm having trouble with this problem is because it involves natural log (ln) and I need to find the limit.</p>

<p>I need to find $\lim_{n\to\infty} \ln(3n+7)-\ln(n)$.</p>

<p>I noticed that as $n$ approaches infinity, $-\ln(n)$ should be approaching $-\infty$ but I'm having trouble finding the limit since $\ln(3n+7)$ is in the sequence.</p>
",<calculus>
"<p>In definition of limits why can't we have "" there exist delta for all epsilon"" instead of  "" for all epsilon there exist delta""</p>
",<calculus>
"<p>Normally I would just divide both sides by the number $4$ because it's not good in there, but I can't do it for </p>

<p>$$4x^2+y^2=1$$</p>

<p>I must have $$\frac{x^2}{a^2}+\frac{y^2}{b^2}=1$$</p>

<p>So what's the easiest way?</p>
",<calculus>
"<p>How to prove whether there does exist a differentiable map $f: \mathbb{R}^{2} \rightarrow \mathbb{R}^{2}$ so that it maps the $X$-axis to the $ S = \{ (x, y): y=|x| \}$?</p>

<p>For example, i got an attempt to build something, which should looks like an example: 
$f(x) = \begin{cases} x &amp;\mbox{if } |x|&gt;\frac{1}{n} \\ 
x^{2}+\frac{n-1}{n^{2}}&amp; \mbox{if } |x| \le \frac{1}{n} \end{cases} $
Does this seem to be an appropriate one?</p>

<p>Any help would be much appreciated.</p>
",<calculus>
"<p>Compute the limit $\lim\limits_{n\to\infty}a_n$ for the following sequences:</p>

<p>(a) $a_n=e^{5\cos((\pi/6)^n)}$</p>

<p>(b) $a_n=\frac{n!}{n^n}$</p>

<p>For part (a) do I just take the limit of the exponent part and then the answer would be $e$ raised to whatever the limit is?</p>

<p>And would the limit be $1$ or $-1$? because $\cos$ goes between those two.</p>

<p>For part $b$ it is in the form of infinity over infinity but how do you take the derivative of $n!$? Will it ever break out of infinity over infinity?</p>
",<calculus>
"<p><img src=""http://i.stack.imgur.com/H0aEv.png"" alt=""enter image description here""></p>

<p>how do you determine if a series converges or diverges? Do you just look at their behavior?</p>
",<calculus>
"<p><img src=""http://i.stack.imgur.com/T6fxv.png"" alt=""enter image description here""></p>

<p>Compute the limit of the series $$\sum\limits_{n=4}^\infty 3\frac{2^{n+1}}{5^{n-2}}$$</p>

<p>How do you approach these types of problems?</p>

<p>I'm thinking that this one is in indeterminate form, is that correct?</p>
",<calculus>
"<p>Hey guys this was given to me as an exercise question and its really confusing. I'm not really sure where to start with this one, and I am assuming that the derivative isn't just $e^{-t^2}  dt$. Anyways, any help is appreciated, thank you!.</p>

<p>Find the derivative of $$\int \limits_x^{x^2} e^{-t^2}dt $$</p>
",<calculus>
"<p>I've got a complex equation with 4 roots that I am solving. In my calculations it seems like I am going through hell and back to find these roots (and I'm not even sure I am doing it right) but if I let a computer calculate it, it just seems like it finds the form and then multiplies by $i$ and negative $i$. Have a look: <a href=""http://www.wolframalpha.com/input/?i=%288%2asqrt%283%29%29/%28z%5E4%2b8%29=i"" rel=""nofollow"">http://www.wolframalpha.com/input/?i=%288*sqrt%283%29%29%2F%28z%5E4%2B8%29%3Di</a></p>

<p>Here's me going bald: <img src=""http://i.stack.imgur.com/oFE1P.jpg"" alt=""enter image description here""></p>
",<calculus>
"<p>So I'm trying to get this:</p>

<p><a href=""http://www.wolframalpha.com/input/?i=%288%2asqrt%283%29%29/%28z%5E4%2b8%29=i"" rel=""nofollow"">http://www.wolframalpha.com/input/?i=%288*sqrt%283%29%29%2F%28z%5E4%2B8%29%3Di</a></p>

<p>And I've calculated $z^4=16 \left( \cos (\frac{- \pi}{3})+ \sin ( \frac{- \pi}{3}) \right)$</p>

<p>So I'm trying to find the roots using the formula:</p>

<p>$ r^{1/n} = \left( \cos ( \frac { \theta + 2 \pi \cdot k}{n}) + i \cdot \sin ( \frac{ \theta + 2 \pi \cdot k}{n}) \right) $</p>

<p>But my result does not equal. Take a look:</p>

<p><a href=""http://www.wolframalpha.com/input/?i=16%5E%281/4%29%2a%28cos%28%28%28-pi/12%29%29/4%29%2bi%2asin%28%28%28-pi/12%29%29/4%29%29"" rel=""nofollow"">http://www.wolframalpha.com/input/?i=16%5E%281%2F4%29*%28cos%28%28%28-pi%2F12%29%29%2F4%29%2Bi*sin%28%28%28-pi%2F12%29%29%2F4%29%29</a></p>

<p>This is for $k=0$.</p>

<p>What am I doing wrong?</p>
",<calculus>
"<p>$$∂u/∂x + ∂u/∂y = 1,$$</p>

<p>$$u(x,0) = \mathrm{e}^x$$</p>

<p>My prof hasn't explained how to solve these very well.  I think it has something to do with the method of characteristics, but I'm not entirely sure what that is or how to employ it.  I'd really appreciate any help.  Thanks in advance.</p>
",<calculus>
"<p>I am working on the following calculus problem. Would you guys help me how to integrate the following function:</p>

<p>$$
\int^1_0 x^2\sin\left(\frac{\pi}{2}x^2\right)dx
$$</p>

<p>I was struggling to compute this. I really appreciate your help in advance.</p>
",<calculus>
"<p>I'm pretty sure the sum converges by Abel or Dirichelet, I just have no idea how to tackle the numerator. Any tips would be appreciated!</p>
",<calculus>
"<p>I have the following problem:</p>

<p>$$(t+2)dx=2x^2dt$$</p>

<p>First I divide both sides by $t+2$ to get:
$$dx = \frac {2x^2}{t+2}\,dt $$
Then, divide by $2x^2$ to gey:
$$\frac{dx}{2x^2}=\frac{dt}{t+2}$$
This will end up to:
$$\int \frac1{2x^2}dx=\int\frac{dt}{t+2}$$</p>

<p>From now on I am not sure how to continue! I ended up having this equation:
$$\frac 1 5 x^3 = \ln (t+2)+c$$</p>

<p>I need to find $x(t)$ now. Can somone help please?</p>

<p><strong>update</strong> 
This is how I got $\frac 1{5} x^3$:
I said because $\int \frac 1{2x^2}dx$ is $\frac 12 \int x^-2$</p>

<p>isnt it right?</p>
",<calculus>
"<p>Suppose that we have following second order differential equation
$$\frac{d^2y}{dx^2}-\frac{dy}{dx} = 2(1-x).$$
When I saw this  equation in the book, it was said that solution is of the form
$$y(x)=x(a_1+a_2 x).$$
My assumption about this is that because the right-hand side of the differential equation is the linear function $2(1-x)=2-2x$, it means  that a function $y(x)$ whose second and first derivative is linear must be  quadratic, or in other words
$$y(x)=ax^2+bx+c.$$
Is this right? I have posted this question because I wanted to be sure that my assumption is correct. Thanks guys.</p>
",<calculus>
"<p>A rectangular page is to have a printed area of 62 square inches. If the border is to be 1 inch wide on top and bottom and only 1/2 inch wide on each side find the dimensions of the page that will use the least amount of paper</p>

<p>Can someone explain how to do this?</p>

<p>I started with:</p>

<p>$$A = (x + 2)(y + 1) $$</p>

<p>Then I isolate y and come up with my new equation:</p>

<p>$$A = (x+2)\left(\frac{62}{x + 2}{-1}\right)$$    </p>

<p>Then I think my next step is to create my derivative, but wouldn't it come out to -1?</p>

<p>Anyways, I would appreciate if someone could give me a nudge in the right direction.</p>

<p><strong>EDIT</strong> </p>

<p>How does this look for a derivative?</p>

<p>$$A = \left(\frac{x^2-124}{x^2}\right)$$ </p>

<p>Then to solve:
$$ {x} = 11.1 $$ </p>

<p>$$ y = 98 / 11.1  $$</p>

<p>Does that seem about right?</p>

<p>If not, the only thing I would have left is setting it to 0 and solving.</p>
",<calculus>
"<blockquote>
  <p><strong>Proposition 3 (<a href=""http://www.ssc.wisc.edu/~bhansen/718/Anderson2003.pdf"" rel=""nofollow"">ABDL03</a>):</strong>
  If a special semimartingale process $X$ is square integrable with respect to the natural filtration of a standard Brownian motion $W$, then one can write</p>
  
  <p>$X_t - X_0 = \int_0^t \! \mu_u \, \mathrm{d}u + \int_0^t \! \sigma_u \, \mathrm{d}W_u$</p>
  
  <p>where $\mu,\sigma$ are predictable processes. </p>
</blockquote>

<p>(Note: I modified the statement of the proposition quite a bit.)</p>

<p>I know that if $X$ is a local martingale, this proposition holds with $\mu \equiv 0$. This is just the martingale representation theorem. </p>

<p><strong>Q:</strong> What if $X$ is a finite variation process? Does this proposition hold with $\sigma\equiv 0$? If so, can the sufficient condition of square integrability be relaxed? Is there some standard set of necessary and sufficient conditions for when one can write a finite variation process as $\int_0^t \! \mu_u \, \mathrm{d}u$ with $\mu$ predictable?</p>

<hr>

<p><strong>UPDATE #1:</strong></p>

<p><strong>Partial Answer:</strong> So, if the paths of $X$ are continuously differentiable and bounded on compacts (almost surely?) then by the fundamental theorem of calculus we can write </p>

<p>$X_t - X_0 = \int_0^t \mu_u \, \mathrm{d}u $</p>

<p>where $\mu$ is the continuous, bounded <strong>derivative</strong> of $X$ (almost surely?) with respect to $t$. </p>

<p><strong>Remaining Q:</strong> Have I done this right? Is the existence of a bounded, continuous derivative for the paths of $X$ (a finite variation process) necessary and sufficient for writing it as $\int_0^t \mu_u \, \mathrm{d}u$ with $\mu$ predictable?</p>

<hr>

<p><strong>UPDATE #2:</strong> We might be able to relax this to just <em>differentiable</em> (rather than <em>continuously differentiable</em>) since $X$ already has finite variation. </p>
",<calculus>
"<p>Akhil showed that the <a href=""http://math.stackexchange.com/questions/477/cardinality-of-set-of-real-continuous-functions/479#479"">Cardinality of set of real continuous functions</a> is the same as the continuum, using as a step the observation that continuous functions that agree at rational points must agree everywhere, since the rationals are dense in the reals.</p>

<p>This isn't an obvious step, so why is it true?</p>
",<calculus>
"<p>I'm looking for hints on how to efficiently solve this inequality:  $$\left( \frac {|x|-|1-x|}{|x|}  \right)^{2x-1}   \gt   \left(\frac {|x|-|1-x|}{|x|} \right)^{8-x} $$</p>
",<calculus>
"<p>A cylindrical tank with radius 5 cm is being filled with water at rate of 3 cm^3 per min. how fast is the height of the water increasing?</p>

<p>I dont want this question solved, but please help me correct my working out:</p>

<ul>
<li>radius = 5</li>
<li>dv/dt =  3</li>
<li>dh/dt = dh/dv * dv/dt</li>
<li>v=(pi)(r^2)(h)</li>
</ul>

<p>because r is constant you could write: V=(pi)(5^2)(h) and then find the derivative...</p>

<p>but is there an alternate method where we can derive dv/dh without first substituting r=5?? </p>
",<calculus>
"<p><img src=""http://i.stack.imgur.com/oK6Wy.png"" alt=""enter image description here""></p>

<p>Is there a simpler way of solving this then calculating</p>

<p>x1(h)+x2(h)+x3(h)+x4(h) by using the given y values (in this case h, the height is one, because the length of each rectangle is one) </p>

<p>because it could take a while if the heights were all different, and there were many more rectangles... is there a CAS (calculator/graphing) method... something more efficient.</p>

<p>Can you calculate L/R area approximation using a formula, without drawing the graph.. so imagine the graph wasn't part of the question... could you solve this alternatively with a formula?</p>

<p>Does anyone have an efficient method to solve L/R area approximation </p>
",<calculus>
"<p><img src=""http://i.stack.imgur.com/BMapu.png"" alt=""enter image description here""></p>

<p>long method: Determine an equation for each and solve using average value formula</p>

<p>alternative methods? </p>

<p>How could you prove the average value to be C over an interval [a,b] if you are given a graph.... looking for most efficient/unique methods. </p>
",<calculus>
"<p>If $f''(x)$ exists on $[a,b]$ and $f'(a)=f'(b)$, then :</p>

<p>$$f(\frac{a+b}{2})=\frac 1 2[f(a)+f(b)]+\frac{(b-a)^2}{8}f''(c)$$</p>

<p>for some $c\in(a,b)$.</p>

<p>I tried but was unable to think of a function and was unable to use the given condition except for Rolle's Theorem which does not yield anything useful(yet). </p>

<p>Any hints or help will be appreciated.</p>
",<calculus>
"<p>Applying the <a href=""http://mathworld.wolfram.com/CopsonsInequality.html"" rel=""nofollow"">Copson's inequality</a>, I found:
$$S=\displaystyle\sum_{k=1}^{\infty }\left(\Psi^{(1)}(k)\right)^2\lt\dfrac{2}{3}\pi^2$$ where
$\Psi^{(1)}(k)$ is the polygamma function.
Is it known any sharper bound for the sum $S$?
Thanks.</p>
",<calculus>
"<p>According to the fundamental theorem of calculus, if $f$ is continuous and $F$ is defined as $F(x)=\int_a^x f(t) dt$ then $F'=f$. But what happens if $x$ appears inside the integral? I'm trying to find the derivattive of</p>

<p>$$f(x)=\int_0^x \frac{e^{xy}}{y}dy$$</p>

<p>I read about the Libniz integral rule, but when I try to use it I get</p>

<p>$$\frac{df(x)}{x}=\int_0^x e^{xy} dy + \frac{e^{x^2}}{x}-\frac{1}{0}\cdot0$$</p>

<p>Is this because $\frac{e^{xy}}{y}$ is not defined when $y=0$? Also, is the Leibniz rule the only way to solve problems like this one?</p>
",<calculus>
"<p>Solve: $ 5&lt; \left\vert\dfrac{x+10}{x-10}\right\vert&lt;6$</p>

<p>attempt at a solution: </p>

<p>Dividing into two: </p>

<p>$5&lt;\left\vert \dfrac{x+10}{x-10}\right\vert  $     And  $\left\vert \dfrac{x+10}{x-10}\right\vert&lt;6  $ </p>

<p>For first we solve: 
 $ \dfrac{x+10}{x-10}&lt;-5  $  or $ \dfrac{x+10}{x-10} &gt;5  $  which yields $( -∞,15)$</p>

<p>For Second: </p>

<p>$  -6&lt;\dfrac{x+10}{x-10}  $  and $ \dfrac{x+10}{x-10} &lt;6  $  which yields $(14, ∞)$</p>

<p>intersecting the two we get $(14,15)$</p>

<p>This solution was deemed wrong by the text book. 
Is there any other way of solving this? Is there a mistake in this method of solution? </p>
",<calculus>
"<p>So there is a stool, the legs are apart from each other in a triangle fashion (isosceles triangle). The length between the points is 5 , 5 and 6.</p>

<p>We want to stabilise this shaky stool by putting wire between the legs, in an upside down Y fashion. (see image for clear example)<img src=""http://i.stack.imgur.com/0XXst.png"" alt=""enter image description here""></p>

<p>So This is how I tired to start:</p>

<p>1 said lets label the wire a,b,c.</p>

<p>a is the bit between the 5'st</p>

<p>$$height = \sqrt(34)$$ which is not useful.</p>

<p>$$area = 17.48$$</p>

<p>Can i assume that the wires are isosceles triangle since the legs form an isosceles triangle or is that a bit of a stretch? </p>
",<calculus>
"<p>Could anyone guide me step by step how to solve this problem or give me some pointers. I recently came across it and I can't seem to solve it. </p>

<p>Here's the entire problem: </p>

<p>Suppose that the function $f\colon\mathbb{R}\to\mathbb{R}$ is continuous at all real numbers $x,x\neq 0$, and satisfies the condition $\displaystyle \left\vert f(x)\right\vert \leq \frac{1}{\left\vert x\right\vert}$ for all $-1\leq x\leq 1$. Show that the function $g\colon\mathbb{R}\to\mathbb{R}$, given by $g(x)=x^2f(x)$, is continuous on $\mathbb{R}$.</p>

<p>Thank you. </p>
",<calculus>
"<p>Ok guys, I'm reading a book and I'm not getting quite well a concept.
If I have to expand $U'(Y_0(1+r_i))$ around $Y_0(1+r_f)$, why I get this:</p>

<p>$\mathbb{E}[U'(Y_0(1+r_i))(r_i-r_f)]=U'[Y_0(1+r_f)]\mathbb{E}(r_i-r_f)+U''[Y_0(1+r_f)]\mathbb{E}(r_i-r_f)^2Y_0$</p>

<p>Any step-by-step explanation would be greatly appreciated, since I can't get the same result by applying Taylor. Thanks in advance!</p>
",<calculus>
"<p>I know the answer is $-10$ but I don't know where the negative sign is coming from.</p>

<p>This is what I ended up with.    $$\frac{(x-25)(\sqrt{x}+5)}{x-25}   =  (1)\sqrt x+5 = 10
$$                                   </p>

<p>Like I said I'm not sure where the negative sign comes from.</p>
",<calculus>
"<p><a href=""http://i.stack.imgur.com/pyAeN.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/pyAeN.jpg"" alt=""enter image description here""></a></p>

<p>Not getting the right answer for this, can someone point me to where I'm going wrong? </p>
",<calculus>
"<p>Once upon a time, I memorized the following formula out of laziness. </p>

<p>Let $k(x)=\frac{f(x)^{g(x)}h(x)+i(x)}{j(x)}$. Then $k'(x)$ is as follows. </p>

<p>$k'(x)=\frac{j(x)(g(x)h(x)f(x)^{g(x)-1}f'(x)+f(x)^{g(x)}(h(x)log(f(x))g'(x)+h'(x)+i'(x))-j'(x)(h(x)f(x)^{g(x)}+i(x))}{j(x)^2}$</p>

<p>(as confirmed by <a href=""http://www.wolframalpha.com/input/?i=(f(x)%5Eg(x)h(x)%2Bi(x))%2Fj(x)"" rel=""nofollow"">wolframalpha</a>)</p>

<p>This was because I did not want to bother with logarithmic functions or the chain rule to find the derivatives of functions such as $x^{sin(x)}$</p>

<p>After some considerable time and effort, I had managed to memorize the formula. </p>

<p>However, I had trouble actually applying this formula to tests for $f'(x)$, mostly because the formula is too long and complicated, and started to wonder if I had wasted my time and effort. </p>

<p>This suspicions were heightened when I made several mistakes while using this formula. </p>

<p>Would memorizing such a formula actually prove useful for tests?</p>

<p>Any advice would be appreciated. </p>
",<calculus>
"<p>I have a question where I am asked to find the amount of terms required in a Maclaurin polynomial to estimate $\cos(1)$ to be correct to two decimal places.</p>

<p>So far what I have done is used Taylor's Theorem to get the follow:</p>

<p>$$|R_n(x)| = (|f^{(n)}*x^n|)/n! &lt; (x^n)/n! &lt; 0.005$$</p>

<p>I think so far this is my best attempt but I am not really sure how to proceed from this point to calculate a value of $n$. I did write out a Maclaurin polynomial for $f(x) = \cos(x)$ and attempt to see if I plugged numbers into that to see what came out and compare that to $\cos(1)$ but was unable to make any sense of my answers there.</p>

<p>I am not sure if I am on the right track here and any feedback would be greatly appreciated, </p>
",<calculus>
"<p>Given the following definite integral</p>

<p>$$\int_0^4 \left[\left(1/2x^2 - 2x +8\right)-\left(1/4x^2+x\right)\right]\;\mathrm dx$$
I have done in the following process.</p>

<p>$$\int_0^4 \left[\left(1/2x^2 - 2x +8\right)-\left(1/4x^2+x\right)\right]\;\mathrm dx$$
$$\implies \int_0^4 [1/4x^2 -3x+8)]\;\mathrm dx\\ \implies {{{{1\over 4}x^3}\over 3} - {3x^2\over 2} + 8x}\\ \implies {{16\over 3} - 24 + 16} \\ \implies {2{2\over 3}}$$
I didn't get the right answer. Is there any mistake in the process I have done.</p>
",<calculus>
"<p>I need help on problem based on integration calculus.</p>

<p>Q: how to integrate
$$\int\frac{dx}{1+\sin(x)\tan(x)}$$
Wolfram and integrate calculator does not help me.</p>
",<calculus>
"<p><img src=""http://i.stack.imgur.com/RfiTj.png"" alt=""enter image description here""></p>

<p>Hi! I am currently working on some calc2 online homework problems and I am having difficulty with this particular question. To be completely honest I am not sure how to even approach this problem, so if someone would be kind enough to help me solve this one I would really appreciate it, Thank you! </p>
",<calculus>
"<blockquote>
  <p>$$\lim_{x \to 0}x^x$$</p>
</blockquote>

<p>I know the answer is one but I have no idea how to get there.  I tried taking a natural log and I think I need lhopitals rule but I keep going In circles.</p>
",<calculus>
"<p>If $g$ is of rapid decrease, that is $\displaystyle\sup_{x\in\mathbb{R}}|x|^{l\geq 0}|g^{(k\geq 0)}(x)|&lt;\infty$, then we have: $$\displaystyle\sup_{x\in\mathbb{R}}|x|^{l\geq 0}|g^{(k\geq 0)}(x-y)|\leq A_{l,k}(1+|y|)^{l}$$
where $A_{l,k}\geq 0$ is a constant dependent on $l,k$.</p>

<p>How can I induce the above inequality? </p>
",<calculus>
"<p>I was working through a physics problem related to magnetic flux, but was confused at the math the solution uses. I understand up till the last line:</p>

<p>$
c=1.65-.12t\\
A=c^2/4\pi\\
\Phi_B=BA=(\frac{B}{4\pi})c^2\\
|\varepsilon|=|\frac{d\Phi_B}{dt}|=(\frac{B}{2\pi})c|\frac{dc}{dt}|
$</p>

<p>I'm unclear of the exact steps that allow this to go from $\frac{B}{4\pi}\frac{dc^2}{dt}$ to $(\frac{B}{2\pi})c|\frac{dc}{dt}|$</p>
",<calculus>
"<p>Suppose somebody is modeling a solution (or set of solutions) to a particular partial differential equation (Navier-Stokes maybe) via some software that makes use of some numerical method(s) to solve the partial differential equation. That software will 'converge' to a particular solution to the differential equation.</p>

<p>Shouldn't that be pretty much sufficient to show existence and uniqueness to that differential equation, given that the partial differential equation itself didn't involve any bad operations (dividing by zero maybe)? </p>

<p>I'm sure the answer is no, so what I'm really asking for is why not?</p>
",<calculus>
"<p>I want to show that $f(x)=e^{-x^2/4k}$ (where $k&gt;0$ is fixed) is continuous using an $\epsilon$,
$\delta$ argument. I've been trying to choose $\delta$ using $\ln$ somehow and I've also been trying to write $|f(x)-f(y)|$ in a form with $|x-y|$ in the exponent so that I can complete the proof, but I have gotten stuck. </p>
",<calculus>
"<p>The problem said:</p>

<blockquote>
  <p>A caterer must supply 110 napkins on Monday, 90 on Tuesday, 130 on
  Wednesday, and 170 on Thursday. The caterer initially has no napkins
  on hand. New napkins can be bought for 7 cents each. Used napkins can
  be laundered for use the next day at 4 cents/napkin or laundered for
  use in 2 days or more at 2 cents/napkin. At the end of the week, all
  used napkins have no value. How can the caterer meet these demands at
  minimal cost? (Hint> consider this as a transportation problem with
  four sources-the new-napkin outlet and the first 3 days' collections
  of used napkins.)</p>
</blockquote>

<p>I try to setup the tableu but I can't apply the algoritm to find the correct distribution due, I can not be able to figure out the correct supply and demand in each extrem of the tableau.</p>

<p>Below, is the my tableu so far:</p>

<p><a href=""http://i.stack.imgur.com/2YCNv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/2YCNv.png"" alt=""enter image description here""></a></p>

<p>7 -cost of new napking
4- fast laundry
2- slow laundry</p>

<p>The minimal cost (show in book said): $22.4</p>

<p>I really apreciate any help, in set up this tablaeu.</p>
",<calculus>
"<p><a href=""http://i.stack.imgur.com/royGT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/royGT.png"" alt=""enter image description here""></a></p>

<p>So the clinician I spoke to told me it should be a simple operation like 
$$(0.05)^n/n - 0.25^n &lt; .0001$$</p>

<p>Which gave me some the value .996 which is obviously a non-nonsensical answer for a  question asking how many terms of a series are necessary to essentially be accurate to the true value to 3 decimal places. How do I restart my effort of this problem? </p>
",<calculus>
"<p>The following is the graph of $y=\cos10x+\cos21x$.<a href=""http://i.stack.imgur.com/FJkSY.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FJkSY.jpg"" alt=""enter image description here""></a></p>

<p>You can see that there seems to be four curves that can touch this graph. I tried $y=\cos(x/2+\pi/2\pm\pi)+1$ and $y=-\cos(x/2\pm\pi/2)-1$:
<a href=""http://i.stack.imgur.com/brVkZ.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/brVkZ.jpg"" alt=""enter image description here""></a></p>

<p>But unfortunately, they cut the graph. What actually are that four curves touching the graph? Thanks.</p>
",<calculus>
"<p>I am trying to show explicitly that the partial sums (for the series $\sum \frac{1}{j(j+1)}$ from j=1 to $\infty$) converge. Would it be sufficient to say that by looking at $\sum \frac{1}{j(j+1)}$ = $\frac{1}{j}-\frac{1}{j+1}$ and $\frac{1}{j}-\frac{1}{j+1} \rightarrow 0$ as $j \rightarrow \infty$? </p>

<p>There is a theorem in the book that says that if $\sum a_j$ converges, then $a_j \rightarrow 0$ as $j \rightarrow \infty$, but I dont know if this is an iff condition that holds the other way.</p>
",<calculus>
"<p>Show that if $\int_0^1 h(x)\cos(n \pi x)dx=0$, $n=1,2,\ldots$, then $h(x)$ is constant.</p>

<p>I know that if the index $n$ starts with zero then $h(x)$ is a zero function since in that case the system of function $\{\cos(n \pi x)\}_{n=0}^\infty$ is an complete orthormal system in $L_2$. However, in the question the system of function is $\{\cos(n \pi x)\}_{n=1}^\infty$ and not a complete system. </p>
",<calculus>
"<p><strong>Somos's quadratic recurrence constant</strong></p>

<p>The Somos's Quadratic recurrence constant is defined by the sequence $g_n=ng_{n-1}$ with initial value of $ g_0= 1$</p>

<p>The value of $\sigma=1.661687...$</p>

<p>An infinite product from maths world $\sigma=\prod_{k=1}^{\infty}k^{\frac{1}{2^k}}$</p>

<p>We found another infinite product involving the factorial numbers by experiments on a sum calculator.</p>

<p>$$\sigma=\prod_{n=1}^{\infty}(n!)^{\frac{1}{2^{n+1}}}$$
Where n! is valid for non-negative integers and defined by</p>

<p>$n!=n(n-1)(n-2)\cdots2\cdot1$</p>

<p>Can somebody help us to prove this </p>
",<calculus>
"<p>In <a href=""http://math.stackexchange.com/a/1776286/72031"">this answer</a> I established the following characterization of $\exp(x)$:</p>

<p><em>If $f:\mathbb{R} \to \mathbb{R}$ is a function such that</em></p>

<ul>
<li><em>$f(x) \geq 1 + x$ for all $x \in \mathbb{R}$</em></li>
<li><em>$f(x + y) = f(x)f(y)$ for all $x, y \in \mathbb{R}$</em></li>
</ul>

<p><em>then $f(x) = \exp(x)$.</em></p>

<p>Actually I established in the linked answer that $f(0) = 1, f'(0) = 1$ and this implies $f'(x) = f(x)$ so that the characterization of $f$ as the exponential function is complete.</p>

<p>Similarly it can be proved that the following characterization for $\log x$ holds:</p>

<p><em>If $g: \mathbb{R}^{+} \to \mathbb{R}$ is a function such that</em></p>

<ul>
<li><em>$g(x) \leq x - 1$ for all $x \in \mathbb{R}^{+}$</em></li>
<li><em>$g(xy) = g(x) + g(y)$ for all $x, y \in \mathbb{R}^{+}$</em></li>
</ul>

<p><em>then $g(x) = \log x$.</em></p>

<p>These characterizations don't mention anything about continuity or differentiability and instead rely on inequalities and the functional equation. </p>

<blockquote>
  <p>Do we have any other characterizations of exponential and logarithmic functions which don't rely on analytic properties (continuity, derivatives etc) and instead rely on properties which are purely algebraic in nature?</p>
</blockquote>

<p>I am thinking of monotone nature. If we augment the functional relation with the requirement that the function is strictly increasing on its domain, will it be sufficient to determine the functions $f, g$ uniquely? I guess it does not work. The function $f(x) = a^{x}$ with $a &gt; 1$ is strictly monotone and satisfies the functional equation, so we still don't get uniqueness. My bad!</p>
",<calculus>
"<p>I need to find the sum of an alternating series correct to 4 decimal places. The series I am working with is: $$\sum_{n=1}^\infty \frac{(-1)^n}{4^nn!}$$ So far I have started by setting up the inequality: $$\frac{1}{4^nn!}&lt;.0001$$Eventually I arrived at $$n=6$$ giving the correct approximation, which is approximately equal to $$\frac{1}{2,949,120}$$ But this is not the answer WolframAlpha gets.</p>
",<calculus>
"<p>The problem states:
Let $f(x),g(x)$ be Riemann integrable over $[a,b]$.
Define $h:[a,b]\rightarrow\mathbb{R}$ as:
$$h(x)=\max(f(x),g(x))$$</p>

<p>Prove that $h(x)$ is Riemann integrable over [a,b]</p>

<p>-I know that because $f$ and $g$ are integrable, then:</p>

<p>$\forall \epsilon&gt;0$, there exists a partition P so that: $U(f,P)-L(f,P)&lt;\epsilon$</p>

<p>$\forall \epsilon\prime&gt;0$, there exists a partition P$\prime$ so that: $U(g,P\prime)-L(g,P\prime)&lt;\epsilon\prime$</p>

<p>I also know that: $maxf(x)\le\sup f(x)$ and $maxg(x)\le\sup g(x)$ where both functions are defined.</p>

<p>Any hints on how should Iaproach this proof? Thanks</p>
",<calculus>
"<p>Use implicit differentiation to determine $\frac{\partial z}{\partial x}$ in $yz=ln(x+z)$ and $ \sin(xyz)=x+2y+3z$.</p>

<p>Here is my answer:</p>

<p>$$ yz=ln(x+z) $$
$$ yz'=(1+z')\frac{1}{x+z} $$
$$ z' = \frac{1}{yx+yz-1} $$</p>

<p>and</p>

<p>$$ \sin(xyz)=x+2y+3z $$
$$ y(z+xz')\cos(xyz)=1+3z' $$
$$ yz\cos(xyz)+xyz'\cos(xyz) = 1+3z' $$
$$ yz\cos(xyz) - 1 = z'(3-xy\cos(xyz)) $$
$$ z' = \frac{yz\cos(xyz) - 1}{3-xy\cos(xyz)} $$</p>

<p>is that right?</p>
",<calculus>
"<p>three questions about integrals for you.</p>

<p>1)'Given that f(x) is even on $R$, show that $F(x) = \int^x_0f(t)dt $ is odd.'</p>

<p>Was I right in doing the following:
 Given that $f(x)$ exists, this implies that $f(-x)$ exists (symmetry along $y$ axis). Due to this, $$\int^x_0 f(t) dt = \int^0_{-x}f(t)\, dt$$  </p>

<p>which implies</p>

<p>$$\int^x_0 f(t) \, dt = -\int^{-x}_0 f(t) \,dt$$</p>

<p>Thus,</p>

<p>$$F(x) = -F(-x)$$</p>

<p>which shows an odd function.</p>

<p>2) 'Given that $f(x) \le g(x)$ on $[a,b]$, show that $\int_a^b f(x)\,dx \le \int_a^bg(x)\,dx$ using the Riemann definition of a definite integral.'</p>

<p>First I stated the definition: </p>

<p>$$\lim_{x\to \infty} \sum_{i=1}^n f(x^*)\,\Delta x_i$$</p>

<p>At this point I sorta bs'd my way through it. Since $f(x) \le g(x)$ on $[a,b]$, the height of the rectangles (of $f(x)$) forming the area under $f(x)$ will always be less than or equal to the height of the rectangles (of $g(x)$) forming the area under $g(x)$. Therefore, this will ensure that the sum of the areas of the rectangles of $f(x)$ will always be less than or equal to the sum of the areas of the rectangles of $g(x)$, thus making the entire area under $f(x)$ $\le$ to that of $g(x)$.</p>

<p>3) 'Differentiate $\int^x_a\sec^2t \, dt$'</p>

<p>I used the FTC1 which states that $F'(x) = f(x)$ when $F(x)$ is an integral function. After some differentiating I got $\sec^2x$ I believe. </p>

<p>I'm not sure that's correct, but it's my best guess. </p>

<p>Thanks for your help.</p>
",<calculus>
"<p>Evaluate all first order partial derivative of $f(x,y,z)=x^\frac{y}{z}$</p>

<p>Here is my attempt:</p>

<p>$y$ and $z$ are constant, so we use the power rule to evaluate $f_x$:
$$ f_x = \frac{y}{z}x^{\frac{y-z}{z}}$$</p>

<p>since for any positive constant $c$ the derivative of $f(x)=c^x$ is $f'(x)=c^x \ln c$, we have:</p>

<p>$$ f(x,y,z) = (x^\frac{1}{z})^y \implies f_y = x^\frac{y}{z}\ln x^\frac{1}{z} $$</p>

<p>if we write $g(x)=\frac{1}{x}$ and $h(x)=c^x$ we have that $(h(g(x)))'=g'(x)h'(g(x)) = \frac{-1}{x^2}c^\frac{1}{x}\ln (c)$ by the chain rule. Combining this argument with the previous one we have:</p>

<p>$$f_z = \frac{-1}{z^2}(x^y)^\frac{1}{z}\ln(x^y)$$</p>

<p>of course we can simplify these expressions.</p>

<p>Is my solution right?</p>
",<calculus>
"<p>SO I am trying to figure out the points of intersection from the interval <strong>0 to pi/2</strong> and mathematically, I set both equations equal to one another and solved for x.</p>

<p>I did $$cosx = sin2x$$ and then got $$1/2 = sinx$$ which gave me $$pi/6$$
This is not completely right, since in the book, it says there are two points of intersection and the one I am missing is $$pi/2$$. How do I get that point without graphing?</p>
",<calculus>
"<p>I need help finding the length of the curve represented by the following relation:
$$x = 5\,cos^3\theta; y = 5\,sin^3 \theta$$</p>

<p>Here is what I've tried:
$$s = \int_0^{2\pi} \sqrt{(\frac{d\theta}{d\theta})^2 + (\frac{d\theta}{d\theta})^2}\,d\theta$$
$$\frac{d\theta}{d\theta} = -15\,sin\theta\,cos^2\,\theta$$
$$\frac{dy}{d\theta} = 15\,sin^2\theta\,cos\,\theta$$</p>

<p>Plugging in,</p>

<p>$$s = \int_0^{2\pi} \sqrt{(-15\,sin\theta\,cos^2\,\theta)^2 + (15\,sin^2\theta\,cos\,\theta)^2}\,d\theta$$
$$s = \int_0^{2\pi} \sqrt{225\,sin^2\,\theta\,cos^4\,\theta + 225\,sin^4\,\theta\,cos^2\,\theta}\,d\theta$$
$$s = \int_0^{2\pi} \sqrt{225\,sin^2\,\theta\,cos^2\,\theta\,(cos^2\,\theta + sin^2\,\theta)}\,d\theta$$
$$s = \int_0^{2\pi} \sqrt{225\,sin^2\,\theta\,cos^2\,\theta}\,d\theta$$
$$s = \int_0^{2\pi} 15\,sin\,\theta\,cos\,\theta\,d\theta$$
$$s = 15\,\int_0^{2\pi} \,sin\,\theta\,cos\,\theta\,d\theta$$</p>

<p>And there I am stumped... The textbook says that the answer should be 30, and my graphing application corroborates that number. I know I can't get to 30 from here. What am I doing wrong?</p>
",<calculus>
"<p>I have this scenario:</p>

<blockquote>
  <p>1 animal with 30% probability of be moved to Japan. <br> 1 animal with
  30% probability of be moved to Japan. <br> 1 animal with 30%
  probability of be moved to Japan. <br> 1 animal with 30% probability
  of be moved to Japan. <br> 1 animal with 30% probability of be moved
  to Japan. <br> 1 animal with 30% probability of be moved to Japan.
  <br> 1 animal with 30% probability of be moved to China. <br> 1 animal
  with 30% probability of be moved to Japan. <br> 1 animal with 80%
  probability of be moved to Brazil. <br> 1 animal with 30% probability
  of be moved to Japan. <br> 1 animal with 20% probability of be moved
  to Brazil. <br> 1 animal with 30% probability of be moved to Japan.
  <br> 1 animal with 50% probability of be moved to Mexico. <br> 1
  animal with 30% probability of be moved to Japan. <br> (...)</p>
</blockquote>

<p>Resuming, 10 animals with 30% of probability of being moved to Japan.</p>

<p>Is that ""right"" to expect that 3 animals gonna be moved to Japan?</p>

<p>The formula is:
30/100 * 10 = 3</p>

<p>Can I use <strong>Binomial Distribution</strong> for this scenario?
If yes, how to elaborate the formula?</p>

<p>Thanks a lot!</p>
",<calculus>
"<p>The velocity $v$ of blood that flows in a blood vessel with radius $R$ and length $L$ at a distance $r$ from the central axis is
$$v(r) = \frac{P}{4\eta L}(R^2 − r^2)$$
where $P$ is the pressure difference between the ends of the vessel and $\eta$ is the viscosity of the blood (See Example.). Find the average velocity (with respect to $r$) over the interval $0 \leq r \leq R$. </p>

<p>$v_{\text{ave}} =$______</p>

<p>Compare the average velocity $v_{\text{ave}}$ with the maximum velocity $v_{\text{max}}$.</p>

<p>$\dfrac{v_{\text{ave}}}{v_{\text{max}}}=$_____</p>

<p>This problem is driving me crazy, I understand average value and have no problem with solving average value problems.  However I do not know what the question is asking of me, I have tried many different answers on web assign with no luck.</p>

<p>I am given a link to an explanation of blood velocity with an example in it giving me numbers $R=.008~\text{cm}$, $L= 2~\text{cm}$, $\eta =.027$ and $P =4000~\frac{\text{dynes}}{\text{cm}^3}$ and an example radius of $r =.002~\text{cm}$.  </p>

<p>Thank you for any help.</p>
",<calculus>
"<p>The problem is an alternating series, that looks like this:
<img src=""http://i.stack.imgur.com/PgGWG.png"" alt=""Alternating series""> </p>

<p>I am given the series:
<img src=""http://i.stack.imgur.com/jk5KX.png"" alt=""Given series""></p>

<p>The book mentions the Alternating Series Estimation Theory, however it seems like there is a definite answer by the wording of the question.</p>
",<calculus>
"<blockquote>
  <p>Assume that the sequence ${a_n}$ is defined recursively by $a_{n+1} = \sqrt{3a_n + 1}$ for all $n \in \mathbb N$, with $a_1 = 1$. Use mathematical induction to prove that $a_n \leq a_{n+1}$ for all $n \in \mathbb N$.</p>
</blockquote>

<p>I've gotten most of the way, but I need help with the last bit. I've proven the base case, and gotten as far as:</p>

<p>Assume $P(k)$ is true. That is, $a_k \leq a_{k+1}$ for any $k \in \mathbb N$.
Prove $P(k+1)$. That is, $a_{k+1} \leq a_{k+1+1}$.</p>

<p>And now I'm stuck.  </p>
",<calculus>
"<blockquote>
  <p>I'm asked to find the equation of plane satisfying the given conditions:</p>
  
  <ul>
  <li>Passing through the line given by:
  \begin{cases}
x+y=2 \\
y-z=3
\end{cases}</li>
  <li>Perpendicular to the plane:
  $$
2 x+3 y+4 z=5
$$
  Knowing that the normal to the plane is 
  $2 i+3 j+4 k$</li>
  </ul>
</blockquote>

<p>I would have hade no problems finding this out if I was given the point. However I am not able to figure it out.
My first tought was to find the point where these lines intersect and then use this point to create the plane with these coinditions, 
$$
x+y-2=y-z-3\Rightarrow z=-x-1
$$</p>

<p>Which I could have expected since I am dealing with tree variables. </p>

<p>Now how could I solve this?</p>

<p>Answer should be $x+6 y-5 z=17$</p>
",<calculus>
"<p>$\displaystyle \lim_{x\to 7^-} \frac{\left|x-7\right|}{x-7} = $</p>

<p>Writing absolute value as:</p>

<p>$x-7 &gt; 0$</p>

<p>$x &gt; 7$</p>

<p>which means</p>

<p>$x - 7$ when $x &gt; 7$</p>

<p>then:</p>

<p>$ -(x - 7) &lt; 0$</p>

<p>$-x + 7 &lt; 0$</p>

<p>$-x &lt; - 7$</p>

<p>$x &gt; 7$</p>

<p>which means</p>

<p>$-x + 7 $ when $x &gt; 7$</p>

<p>So when $x &gt; 7$ what equation should I use?</p>

<p>$x - 7$ </p>

<p>or </p>

<p>$-x + 7 $</p>
",<calculus>
"<p>Please calculate $$I=\int_0^1 dx \int_0^x dy \int_0^y \frac{\sin z}{(1-z)^2}dz$$</p>

<p>Any hints? Thank you!</p>
",<calculus>
"<blockquote>
  <p>$$ \ln(Y) = \ln(A) + \frac{\ln[\alpha K^\gamma + (1-\alpha) L^\gamma]}{\gamma}$$</p>
  
  <p>can be taken to the limit by applying l'Hôpital's rule:</p>
  
  <p>$$\lim_{\gamma\rightarrow 0} \ln(Y) = \ln(A) + \alpha \ln(K) + (1-\alpha) \ln(L).$$</p>
</blockquote>

<p>I am not sure how l'Hopital's rule was used - differentiating by $\gamma$ produces some weird results.</p>
",<calculus>
"<p>prove that
$$\int_{0}^{\infty}\sin{x}\sin{\sqrt{x}}dx=\dfrac{\sqrt{\pi}}{2}\sin{\left(\dfrac{3\pi-1}{4}\right)}$$</p>

<p>I have some question,use the <a href=""http://www.wolframalpha.com/input/?i=%5Cint_%7B0%7D%5E%7B%5Cinfty%7Dsinxsin%28sqrt%28x%29%29dx"" rel=""nofollow"">http://www.wolframalpha.com/input/?i=%5Cint_%7B0%7D%5E%7B%5Cinfty%7Dsinxsin%28sqrt%28x%29%29dx</a></p>

<p>find this integral is not converge,I'm wrong?
Thank you everyone</p>
",<calculus>
"<p>I need to evaluate:<br>
$$\lim_{x \to 0} \left( \frac{\ln (\cos x)}{x\sqrt {1 + x}  - x} \right)$$</p>

<p>Now, it looked to me like a classic <a href=""http://en.wikipedia.org/wiki/L&#39;H%C3%B4pital&#39;s_rule"" rel=""nofollow"">L'Hôpital's rule</a> case. Indeed, I used it (twice), but then things became messy and complicated.  </p>

<p>Am I missing the point of this exercise? I mean, there must be a ""nicer"" way.
Or should I stick with this road?</p>

<h2>EDIT:</h2>

<p>Regarding Yiorgos's answer: Why is the following true? 
$$\ln\left(1- {x^2 \over 2}\right) \approx -{x^2 \over 2}$$</p>
",<calculus>
"<p>I make a big fuss that my calculus students provide a ""continuity argument"" to evaluate limits such as $\lim_{x \rightarrow 0} 2x + 1$, by which I mean they should tell me that $2x+1$ is a polynomial, polynomials are continuous on $(-\infty, \infty)$, and therefore $\lim_{x \rightarrow 0} 2x + 1 = 2 \cdot 0 + 1 = 1$.</p>

<p>All the examples they encounter where it is <em>not</em> correct to simply evaluate at $a$ when $x \rightarrow a$ fall into one of two categories:</p>

<ul>
<li>The function is not defined at $a$.</li>
<li>The function is piecewise and expressly constructed to have a discontinuity at $a$.</li>
</ul>

<blockquote>
  <p>I'd like to find a function $f$ with the following properties:</p>
  
  <ul>
  <li>$f(a)$ exists</li>
  <li>$f(a)$ is not (obviously) piecewise defined</li>
  <li>$f(x)$ is not continuous at $a$</li>
  <li>$f$ is reasonably familiar to a Calculus I student - trigonometry would be admissible, but power series would not (though they might
  still make for interesting reading)</li>
  </ul>
</blockquote>

<p>The best example I know is $f(x) = \frac{|x|}{x}$, but the natural definition of $|x|$ is essentially piecewise ($\sqrt{x^2}$ is cheating).</p>
",<calculus>
"<p>You are standing on a cliff at a height $h$ above the sea. You are capable of throwing a stone with velocity $v$ at any angle $a$ between horizontal and vertical. What is the value of $a$ when the horizontal distance travelled $d$ is at a maximum?</p>

<p>On level ground, when $h$ is zero, it's easy to show that $a$ needs to be midway between horizontal and vertical, and thus $\large\frac{\pi}{4}$ or $45°$. As $h$ increases, however, we can see by heuristic reasoning that $a$ decreases to zero, because you can put more of the velocity into the horizontal component as the height of the cliff begins to make up for the loss in the vertical component. For small negative values of $h$ (throwing up onto a platform), $a$ will actually be greater than $45°$.</p>

<p>Is there a fully-solved, closed-form expression for the value of $a$ when $h$ is not zero?</p>
",<calculus>
"<p>How do we know if a particular function can be represented as a power series? And once we have come up with a power series representation, how does one figure out its radius of convergence ?</p>
",<calculus>
"<p>When differentiated with respect to $r$, the derivative of $\pi r^2$ is $2 \pi r$, which is the circumference of a circle.</p>

<p>Similarly, when the formula for a sphere's volume $\frac{4}{3} \pi r^3$ is differentiated with respect to $r$, we get $4 \pi r^2$.</p>

<p>Is this just a coincidence, or is there some deep explanation for why we should expect this?</p>
",<calculus>
"<p>I am working on computing phase diagrams for alloys.  These are
blueprints for a material that show what phase, or combination of
phases, a material will exist in for a range of concentrations and
temperatures (see <a
href=""http://web.cos.gmu.edu/~tstephe3/talks/SIAMMaterialsScience2010.pdf"" rel=""nofollow"">this
pdf presentation</a>).  </p>

<p>The crucial step in drawing the boundaries that separate one phase
from another on these diagrams involves minimizing a free energy
function subject to basic physical conservation constraints.  I am
going to leave out the chemistry/physics and hope that we can move forward
with the minimization using Lagrange multipliers. </p>

<p>The free energy that is to be minimized is this:</p>

<p>$\widetilde{G}(x_1, x_2) = f^{(1)}G_{1}(x_1) + f^{(2)}G_{2}(x_2),$</p>

<p>subject to:</p>

<p>$f^{(1)}x_1 + f^{(2)}x_2 = c_1,$</p>

<p>$f^{(1)} + f^{(2)} = 1. $</p>

<p>(and also that the $x_{i} > 0$ and $f^{(i)} > 0$, for $i=1,2$.)</p>

<p>The Lagrange formulation is:</p>

<p>$L(x_1,x_2,f^{(1)},f^{(2)},\lambda_1, \lambda_2, \lambda_3) =
f^{(1)}G_{1}(x_1) + f^{(2)}G_{2}(x_2)$  </p>

<p>$- \lambda_{1}(f^{(1)}x_1 + f^{(2)}x_2 - c_1)$</p>

<p>$- \lambda_{2}(f^{(1)} + f^{(2)} - 1) $</p>

<p>The minimization of $\widetilde{G}$ follows from finding the $x_{i}$'s  that satisfy $\nabla L = 0:$</p>

<p>$\frac{\partial L}{\partial x_{1}}   = f^{(1)}G_{1}'(x_1) - \lambda_{1}f^{(1)} = 0$</p>

<p>$\frac{\partial L}{\partial x_2}     = f^{(2)}G_{2}'(x_2) - \lambda_{1}f^{(2)} = 0$</p>

<p>$\frac{\partial L}{\partial f^{(1)}} = G_{1}(x_1) - \lambda_{1}x_{1} - \lambda_2 = 0$</p>

<p>$\frac{\partial L}{\partial f^{(2)}} = G_{2}(x_2) - \lambda_{1}x_{2} - \lambda_2 = 0$</p>

<p>which yields:</p>

<p>$(*) f^{(1)}\left[G_{1}'(x_1) - \lambda_1 \right] = 0$       </p>

<p>$(**) f^{(2)}\left[G_{2}'(x_2) - \lambda_1 \right]= 0 $       </p>

<p>$(***) G_{1}(x_1) - G_{2}(x_2) = \lambda_1 \left[ x_1 - x_2\right]$ </p>

<p>Because $f^{(1)}$ and $f^{(2)}$ are not to be zero, from (*) and (**) we have that </p>

<p>$G_{1}'(x_1) = G_{2}'(x_2) = \lambda_{1}.$</p>

<p>And, a manipulation of equation (***) looks like </p>

<p>$\frac{G_{1}(x_1) -G_{2}(x_2)}{x_1 - x_2} = \lambda_{1}.$</p>

<p>Now, think of $G_{i}$ as an even degree polynomial (which it isn't, but
it's graph sometimes resembles one) in the plane.  Let the points $x_1$
and $x_2$ be locations along the x-axis that lie roughly below the
minima of this curve.  The constraints (*),(*<em>), and (*</em>*) describe the
condition that the line drawn between $(x_1,G_{1}(x_1))$ and $(x_2,G_{2}(x_2))$ form a common tangent
to the ""wells"" of the curve.  It is these points $x_1$ and $x_2$,
which represent concentrations of pure components in our alloy, that
become mapped onto a phase diagram.  It is essentially by repeating this procedure for many
temperatures that we can trace out the boundaries in the desired phase diagram.</p>

<p><strong>The question is:</strong>  Looking at this from a purely analytic geometry
perspective, <strong>how would one derive the ""variational"" approach to find a common tangent line that we seem to have found using the above Lagrangian?</strong>  (warning: I don't really know how to
model things using variational methods.)  </p>

<p><strong>And, secondly:</strong> I have presented a model of a binary alloy, meaning
two variables to keep track of representing concentrations.  I have
been working on ternary alloys, where this free energy $\widetilde{G}$
is a function of three variables (two independent: $x_1,x_2,x_3$,
where $x_3 = 1- x_1 - x_2$) and is therefore a surface over a Gibbs
triangle.  Then $\nabla L = 0$ produces partial derivatives that no
longer ""speak geometry"" to me, although the solution is a common tangent
plane.  (I have attempted to characterize a common tangent plane
based purely in analytic geometry - completely disregarding the
Lagrangian - and have come up with several relations between
directional derivatives... <strong>How might directional derivatives relate
to the optimality conditions set forth by the Lagrangian?</strong>)</p>

<p><strong>EDIT:</strong>  Thank you Greg Graviton for wading through this sub-optimal notation and pointing out several mistakes in the statement of the problem.  (Also, thank you for the <a href=""http://math.stackexchange.com/questions/632/validating-a-mathematical-model-lagrange-formulation-and-geometry/1245#1245"">excellent discussion below</a>.)</p>
",<calculus>
"<p>I just came back from my Introduction to Rotational Kinematics class, and one of the important concepts they described was <em>Rotational Inertia</em>, or <em>Moment of Inertia</em>.</p>

<p>It's basically the equivalent of mass in Netwon's $F = m a$ in linear motion.  The equivalent rotational equation is $\tau = I \alpha$, where $\tau$ is rotational force, $\alpha$ is rotational acceleration, and $I$ is rotational inertia.</p>

<p>For a point about an axis, $I$ is $m r^2$, where $r$ is the distance from the point to the axis of rotation.</p>

<p>For a continuous body, this is an integral -- $I = \int r^2 \,dm$.</p>

<p>This really doesn't make any sense to me...you have two independent variables?  I am only used to having one independent variable and one constant.  So I would solve this, using my experience with calculus (which encompasses a read through the Sparks Notes packet) as $ I = m r^2 $</p>

<p>But obviously, this is wrong?  $r$ is not a constant!  How do I deal with it?  Do I need to replace $r$ with an expression that varies with $m$?  But how could $r$ possibly vary with $m$?  Isn't it more likely the other way around?  But how can $m$ vary with $r$?  It's all rather confusing me.</p>

<p>Could someone help me figure out what to do with all these substitutions for, example, figuring out the Moment of Inertia of a hoop with no thickness and width $w$, with the axis of rotation running through its center orthogonal to its plane?</p>
",<calculus>
"<p>For equation below:</p>

<p>$$(t+1) \, dx=4(x+4) \, dt$$</p>

<p>After separation  I ended up with:</p>

<p>$$(x+4)dx = \frac 4{t+1}dt $$</p>

<p>Resulting in:</p>

<p>$$\int x+4 \,dx = 4 \int \frac 1{t+1} \,dt$$</p>

<p>So:</p>

<p>$$\frac 12 x^2  + 4x + C = 4\ln(t+1) + C$$</p>

<p>Now I have to express this as $x(t)$ and I have no clue how to. Also I am not sure if I did the above steps correctly. Any help will be appriciated!</p>

<p><strong>UPDATE</strong></p>

<p>As gerry pointed my mistake now I have:</p>

<p>$$ \int \frac {1}{x+4}\,dx = 4\int \frac{1}{t+1}\,dt  $$</p>

<p>Then:</p>

<p>$$ \ln(x+4) = 4 \ln(t+1) + C$$</p>

<p>Still not able to express this as x(t)...how to?!</p>
",<calculus>
"<p>Find the point on the line $y = x + 2$ that is nearest to the point $(1,1)$. The shortest distance from point to point.</p>

<p>I honestly don't even know where to begin with this one.</p>
",<calculus>
"<p>I am scratching my head to figure out a way to separate variables of the following equation:</p>

<p>$$(t+3)(t-2)dx = (t+tx^2)dt$$</p>

<p>Doesn't matter how many times I divide and multiply, I always get $x$ and $t$ on one side. Is there a trick applicable here?!</p>
",<calculus>
"<p>I'm trying to find the area in the curve $r^2=2\cos \theta$ and out of $r=2(1-\cos \theta)$</p>

<p>The intersections are at $\theta=\frac{\pi}{3}$ and $\theta=\frac{-\pi}{3}$, then, the integral to find the area is:</p>

<p>$$A=\frac{1}{2} \int_{\frac{-\pi}{3}}^{\frac{\pi}{3}} (\sqrt{2 \cos{\theta}})^2-(2-2\cos{\theta})^2  d\theta=9\sqrt{3}-4\pi$$</p>

<p>Using the result that the area of ​​a region in polar coordinates is given by:</p>

<p>$$\frac{1}{2} \int_{\theta_1}^{\theta_2} (f(\theta))^2 d\theta$$</p>

<p>Is this correct?</p>

<p>Thanks for your help.</p>
",<calculus>
"<p>I don't understand why the following is true, explanation would be greatly appreciated!
Suppose $$E(x,y)=\gamma x^{2n}+{y^2\over a}$$ where $\gamma &gt;0, a&gt;0,n\in \mathbb Z^+$.</p>

<p>And we define $$\alpha = {1\over 2\pi}\oint y\,\,\,dx$$ where the path is where $E, \gamma$ are constants.</p>

<p>Why then does it follow that $$a^{n\over n+1}E=\gamma^{1\over n+1}\left({n\pi\alpha\over f}\right)^{2n\over n+1}$$ where $f=\int_0^1(1-u)^{1\over 2}u^{1-2n\over 2n}\,\,du$?</p>

<hr>

<p>I first tried making $y$ the subject then substitute its expression in terms of $E$ and $x$ into the integral of $\alpha$. Here I can take $E, \gamma$ to be constants, since they are so on the path. This gives $$\alpha = {\sqrt a\over 2\pi}\oint (E-\gamma x^{2n})^{1\over 2}dx$$. But what next? Or perhaps there is a different approach to begin with?</p>
",<calculus>
"<p>Suppose $f = \frac{(1/2)^n}{1+(1/2)^n}$ where $n \geq 1 $ I wanted to give an upper bound the function.</p>

<p>So I did</p>

<p>$f = \frac{(1/2)^n}{1+(1/2)^n} \leq \frac{(1/2)^n}{(1/2)^n} = 1$</p>

<p>Which is right, but then I also did</p>

<p>$f = \frac{(1/2)^n}{1+(1/2)^n} \leq \frac{(1/2)^n}{(1)} = (1/2)^n$ and as $n\to \infty$, the function is bounded by $0$ and this makes no sense at all. I have no idea what I am doing wrong in my algebra, but the solution makes no sense ot me, I couldn't interpret the answer at all</p>
",<calculus>
"<p>$$\displaystyle \begin{align*}
  &amp; \int_{0}^{+\infty }{\frac{\text{d}x}{1+{{x}^{n}}}} \\ 
 &amp; \int_{-\infty }^{+\infty }{\frac{{{x}^{2m}}}{1+{{x}^{2n}}}\text{d}x} \\ 
 &amp; \int_{0}^{+\infty }{\frac{{{x}^{s-1}}}{1+x}\text{d}x} \\ 
\end{align*}$$</p>
",<calculus>
"<p>Suppose that the function $f(x)$ satisfies $f(0)=0$, $f'(0)=0$ and $f''(0)&gt;0$.</p>

<p>(a) Show that there exists $d&gt;0$ such that $\frac{f'(x)-f'(0)}{x-0}&gt;0$ for every non-zero $x$. </p>

<p>(b) Use Rolle's Theorem to show that $f(x)$ is not equal to $0$ for all non-zero x in $(-d,d)$.</p>

<hr>

<p>I'm okay with part (a). </p>

<p>My problem with part (b) is that using  Rolle's Theorem here assumes continuity on some closed intervals, say $[a,0]$ and $[0,b]$, and differentiability on the corresponding open intervals $(a,0)$ and $(0,b)$. Obviously $f(x)$ is continuous at $x=0$, but how does that guarantee the conditions needed for the application of Rolle's theorem? I have completed part (b) by assuming the required conditions but would like to know if/why I am justified in doing so.</p>
",<calculus>
"<p>The original problem is: </p>

<p>""Find the volume of the solid obtained by rotating about the x axis the region enclosed by the curves $y = \frac{9}{x^2 + 9},y=0,x=0,\,$and $x = 3$""</p>

<p>I set up the following integral $$81\pi\int_0^3\frac{1}{(x^2 + 9)^2}dx$$ using the cylinder method (I believe it's called like that) and when I calculated it using a computer I obtained the correct answer but I have been having difficulties in solving it manually.  I tried the shell method as well and I didn't see it any easier to solve but I may be wrong of course. </p>
",<calculus>
"<p>When integrating over a certain variable $x$, we make sure to end the integral with $dx$, like so:</p>

<p>$$\int_{1}^{\infty}\frac{1}{x^2}dx$$ The reason for this of course becomes more clear as one goes deeper into single- and especially multivariable calculus, where one discovers that it does't just signify which variable to integrate.</p>

<p>But is there <strong>no</strong> valid reason to write, for example, the sum $1+1/4+1/9+\dots$ in this fashion:</p>

<p>$$\sum_{1}^{\infty} \frac{dn}{n^2}$$</p>

<p>Instead of the usual:</p>

<p>$$\sum_{n=1}^{\infty} \frac{1}{n^2}$$</p>

<p>Has it ever been done?</p>
",<calculus>
"<p><a href=""http://math.stackexchange.com/questions/7892/comparing-pie-and-e-pi"">Comparing $\pi^{e}$ and $e^{\pi}$</a></p>

<p>I read the answer there but I didn't understand one thing. How I should know to put $\dfrac{π}e-1$ instead of $x$? If I had this question on a test, I had no idea what to put instead of $x$. I mean, why the first thing I need to think about is to calculte when $x=\dfrac{π}e-11$ .</p>

<p>I hope you understand my question.</p>

<p><strong>Note :</strong></p>

<p>This is not a duplicate - I'm not asking what is bigger - I don't understand the answer, that's all!</p>
",<calculus>
"<p>Evaluate the derivative of $x^3 - 3x +1$ using the $\lim_{x \to a} \frac{f(x) - f(a)}{x - a}$ definition to find the tangent of the curve at the point $(2, 3)$.</p>

<p>I already calculated this derivative using $ x = a + h$ for the above mentioned definition and this is what I got, for what it's worth:</p>

<p>$$\lim_{h \to 0} \frac{((2+h)^3 - 3(2+h) +1) - 3)}{h}$$</p>

<p>$$=\lim_{h \to 0} \frac{h(9 + 6h)}{h} = 9$$</p>

<p>The problem I encounter when using the $\lim_{x \to a} \frac{f(x) - f(a)}{x - a}$ definition is that I can't factor the numerator of $\lim_{x \to 2} \frac{x^3 - 3x +1 - 3}{x - 2}$ to get rid of that $(x-2)$ in the denominator.</p>

<p>Can anybody give me a hint as to what trickery I can use to factor that numerator?</p>
",<calculus>
"<p>How can one find this limit:
$$\lim_{n \to \infty} \frac{n^3}{(3n)!^\frac{1}{n}}$$</p>

<p>Hospitals is out of the question, in this case, because n! is not a differentiable function.</p>
",<calculus>
"<p>My professor said that </p>

<p>$$\lim_{\delta \to 0}(1-\lambda \delta)^{t/\delta}=e^{-\lambda t}$$</p>

<p>can be shown with L'Hospital's rule. I don't know what he meant. What is the best way to show this (or, more simply, $\lim_{\delta \to 0}(1-\lambda \delta)^{1/\delta} = e^{-\lambda}$)?</p>

<p>If I try as follows </p>

<p>$$\lim_{\delta \to 0}\left(1-\lambda\delta \right)^{1/\delta} = \lim_{\eta \to \infty} \frac{(\eta-\lambda)^\eta}{\eta^\eta},$$</p>

<p>then I'm getting led into confusion trying LHR on the last one.</p>
",<calculus>
"<p>What is the equation of the tangent and normal lines to this function at the point p</p>

<p>$f(x)$ = $x^3$ at the point $p = (2,8)$</p>
",<calculus>
"<p>When a mortar shell is ﬁred with an initial
velocity of v0 ft/sec at an angle α above the
horizontal, then its position after t seconds is
given by the parametric equations
$x = (v0 \cos \alpha)t$ , $y = (v0 \sin \alpha)t − 16t^2$</p>

<p>If the mortar shell hits the ground 4900 feet
from the mortar when α = 75◦, determine v0.</p>

<p>So I've tried various forms of:
\begin{align*}
t = {} &amp; 4900/(v0 \cos 75) \\
0 = {} &amp; (v0 \sin 75)(4900/(v0 \cos 75)) - 16(4900/(v0 \cos 75))^2 \\
4900(v0 \sin 75)/(v0 \cos 75) = {} &amp; 384160000/(v0 \cos 75)^2 \\
v0 \sin 75 = {} &amp; 78400/(v0 \cos 75) \\
v0 = {} &amp; 78400/\sin 75 * v0 * \cos 75 \\
v0^2 = {} &amp; 78400/\sin 75 * \cos 75 \\
v0 = {} &amp; 468.33...i
\end{align*}</p>

<p>which doesn't seem right. And the answer choices are:</p>

<ol>
<li>v0 = 530 ft/sec</li>
<li>v0 = 560 ft/sec</li>
<li>v0 = 520 ft/sec</li>
<li>v0 = 550 ft/sec</li>
<li>v0 = 540 ft/sec</li>
</ol>
",<calculus>
"<p>Suppose we have a smooth surface in 3D, called $S$. ${\bf n}$ is the unit normal vector. Suppose locally, we have curvilinear coordinates $s,t$ such that ${\bf s}={\bf r}_s$, ${\bf t}={\bf r}_t$ and $\{{\bf s},{\bf t},{\bf n}\}$ forms an orthonormal basis. Without loss of generality, assume ${\bf r}(0,0)={\bf 0}$.
Consider a smooth extension of ${\bf n}$ into a ball $B({\bf 0},\epsilon)$ for some $\epsilon&gt;0$ and $|{\bf n}|=1$. One possible example is ${\bf n}=\nabla\varphi/|\nabla\varphi|$ where $\varphi$ is the signed distance function.</p>

<p>$({\bf t}\cdot\nabla{\bf n})\cdot{\bf s}$ and $({\bf s}\cdot\nabla{\bf n})\cdot{\bf t}$(evaluated at $s=0,t=0$) are independent of the extension, where ${\bf t}\cdot\nabla{\bf n}$ is defined to be $t_i\partial_i{\bf n}$ with Einstein summation convention used.<br>
Actually, one can simply consider ${\bf t}\cdot\nabla n_1=\frac{\partial}{\partial t}n_1({\bf r}(s,t))|_{s=0,t=0}$. This is only determined by the values of $n_1$ on the surface. Then, ${\bf t}\cdot\nabla{\bf n}$ is independent of the extension.</p>

<p>Then, let's consider $g=({\bf t}\cdot\nabla{\bf n})\cdot{\bf s}-({\bf s}\cdot\nabla{\bf n})\cdot{\bf t}=[{\bf t}\cdot(\nabla{\bf n}-\nabla{\bf n}^T)]\cdot{\bf s}
=-{\bf t}\cdot[(\nabla\times{\bf n})\times{\bf s}]=-(\nabla\times{\bf n})\cdot{\bf n}$ at ${\bf 0}$, which should be independent of the extension as well.</p>

<p>Let's use the extension ${\bf n}=\nabla\varphi/|\nabla\varphi|$. Then, $\nabla\times{\bf n}=\nabla(1/|\nabla\varphi|)\times\nabla\varphi$ which is perpendicular with ${\bf n}$. This means $g=0$ or ${\bf n}\cdot(\nabla\times{\bf n})=0$ on the surface for any extension.</p>

<p>However, consider ${\bf v}=(x-z,y-z,-2z)$. At point, $(1,0,0)$, in a neighborhood, we can find a smooth surface such that ${\bf v}/|{\bf v}|$ is the normal vector of that surface. This seems to suggest ${\bf v}\cdot(\nabla\times({\bf v}/|{\bf v}|))=0$ at $(1,0,0)$. This means
$\frac{1}{|{\bf v}|}{\bf v}\cdot(\nabla\times{\bf v})=0$. One can verify directly that this is not true. Where does the argument fail?</p>
",<calculus>
"<p>Here we $D_2 f(1,y)$ means we have to calculate the partial derivative w.r.t $y$, so I have applied one short tricks that I have put $x=1$ in the equation then $f(1,y)= 1+0=1$ so the $D_2(f(1,y)=0$. Now my question is that the way I have gone is it correct or not. Please comment and give solution if I am wrong.</p>
",<calculus>
"<p>I know $\frac{dy}{dx}\frac{dx}{dy} = 1$ because the chain rule says $1 = \frac{dy}{dy} = \frac{dy}{dx}\frac{dx}{dy}$.  But does $\frac{d^2 y}{dx^2} \frac{d^2 x}{dy^2} = 1$?  Or would that be too good to be true?</p>
",<calculus>
"<p>Let:</p>

<p>$$ A = \int_0^1 \frac{e^t}{1+t} dt$$</p>

<p>Then what is the value of:</p>

<p>$$ \int_{a-1}^a \frac{e^{-t}}{t-a-1} dt$$</p>

<p>I tried using the property:</p>

<p>$$ \int_a^b f(x) dx =\int_a^b f(a+b-x) dx$$ </p>

<p>But that was of no help</p>
",<calculus>
"<p>Is $f(x,y)=ax^2+by^2$ a bijection between $\mathbb R^2 \to \mathbb R$ ?</p>

<p>How about $f(x,y,z)=\frac{x^2}{a^2} + \frac{y^2}{b^2}+ \frac{z^2}{c^2}? ( \mathbb R^3 \to \mathbb R )$</p>

<blockquote>
  <p>What confuses me now is this: My professor defined the function
  $f(x,y)=x^2+y^2$ then stating : $f^{-1}((1,2))=\{(x,y)\in \mathbb R^2:
 1 &lt; f(x,y)&lt;2\}$ Then, what I can assume the logic: since $(1,2)$ is
   open in $\mathbb R $ then $f^{-1}((1,2)) $ is open in $\mathbb R^2.$
  What are your thoughts on this?</p>
</blockquote>

<p>And also, if a have a bijective between two topologies, are the following statements correct:</p>

<p>1.)If a subset in one topology is open/closed its map is  open/closed as well in the respected topology.</p>

<p>2.)If a subset is nor open nor closed in one topology its map is nor open nor closed in the respected topology.</p>

<p>3.) If a subset is open and closed in one topology then it's map is open and closed in the respected other topology. </p>
",<calculus>
"<p>Consider the function $x : \mathbb{R} \to \mathbb{R}$ given by 
$$x(t) = \frac{1}{30000} \frac{1}{\mathrm{e}^t}+ \frac{2}{30000} \mathrm{e}^{\frac{t}{2}} \cos \left(\frac{\sqrt{3}}{2}t\right), \quad t\in \mathbb{R}.$$
Arrange the roots of this function $x$ in increasing order. Let $t_n$ denote the $n$th root of the function. The sequence $\{t_n\}_{n = 1}^\infty$ begins with (rounded to one decimal place)
$$1.8, 5.4, 9.1, 12.7, 16.3, \ldots$$</p>

<p>Based on plotting some graphs using Wolfram Alpha, the function $x$ appears to have infinitely many roots. Does there exist a positive integer $N$ such that, for each positive integer $n \ge N$, 
$$t_{n + 1} \le t_n + 5?$$ </p>
",<calculus>
"<p>$\lim_{(x,y)\to(0,0)}\frac{x^2y^2}{\sin(x)\cos(y)}$  is it allowed to split a multi-variable limit into its component variables as in the next step?</p>

<p>$= (\lim_{x\to0}\frac{x^2}{\sin(x)})(\lim_{x\to0}\frac{y^2}{\sin(y)})$ this is an indeterminate form and now I use L'Hopital</p>

<p>$=(\lim_{x\to0}\frac{2x}{\cos(x)})(\lim_{x\to0}\frac{2y}{\cos(y)})$</p>

<p>$=(\frac{0}{1})(\frac{0}{1})=0$</p>
",<calculus>
"<p>I know that this is a pretty basic limit, I found this limit in this forum but not the way I did, so I need to know if this is right:</p>

<p>We know that </p>

<p>$$\lim_{x \to \infty}\left(1 + \frac{1}{x} \right)^x = e$$</p>

<p>ans I was wondering how to calculate </p>

<p>$$\lim_{x \to \infty}\left(1 + \frac{a}{x} \right)^x$$</p>

<p>The way I tough was like this:</p>

<p>$$\lim_{x \to \infty}\left(1 + \frac{a}{x} \right)^x = \lim_{ax \to \infty}\left(1 + \frac{a}{ax} \right)^{ax} = \left[\lim_{ax \to \infty}\left(1 + \frac{1}{x} \right)^x\right]^a = e^a$$</p>

<p>Can I make this $ax$ substitution in the limit? I was wondering about this, and for me is ok, because I'm considering the entire $ax$ thing going to infinity. </p>
",<calculus>
"<p>Let $S=\sum\limits_{n=1}^\infty a_n$ be an infinite series such that $S_N=4-\frac{2}{N^2}$.</p>

<p>(a) Find a general formula for $a_n$. </p>

<p>(b) Find the sum $\sum\limits_{n=1}^\infty a_n$. </p>

<p>Can you explain to me how I can convert the partial sum to the general equation?</p>

<p>(a) What are the values of
$\sum\limits_{n=1}^{10} a_n$ and $\sum\limits_{n=4}^{16} a_n$?</p>

<p>$\sum\limits_{n=1}^{10} a_n=23433271/635040$</p>

<p>$\sum\limits_{n=4}^{16} a_n= 15799025474051/259718659200 $</p>

<p>(b) What is the value of $a_3$?
$167/18$</p>

<p>Why aren't these values correct as well?</p>
",<calculus>
"<p>I have recently read about convergence and divergence. However, I am having trouble understanding how something can converge/diverge ""slowly"" or ""fast"". If you sum up two series (that converge to the same number) infinitely, they will converge, not at any particular rate, but just to the number- this is how I see it.</p>

<p>So what does it <strong>mean</strong> for a series to converge (or diverge) slowly?</p>
",<calculus>
"<p>What approach would be ideal in solving for a number $k$ when the area about the $x$ axis and under the graph of the function $f(x) = \frac1x$  from interval $x = [2, k]$ is equal to $\ln(4)$?</p>
",<calculus>
"<p>How to integrate $\frac{\sqrt{x}}{1-\sqrt{x}}$?</p>

<p>I tried by using integration by parts, but always got sucked. Should be very easy...</p>
",<calculus>
"<p>I have a function, 
  $$F(r) = \int_0^r |c x^2 + {(2 a + b - 4 a r - 3 b r - 2 c r) x^2\over2 r} + b x^3 +
  a x^4| dx$$</p>

<p>a, b and c are constants. I want to determine r such that $f=F'(r) = k$. Integrating with an absolute value is nasty, so my first thought was to use the First Fundamental Theorem of Calculus, which states:</p>

<p>if $F(x) = \int_a^x f(t) dt$ then $F'(x) = f(x)$</p>

<p>But what I have is more like $F(x) = \int_a^x f(t, x)dt$ and so I'm not quite sure if/how the theorem applies. Is there any way to compute $F'(r)$ without first solving for $F(r)$ (which requires breaking the integrand apart into a piecewise function to remove the absolute value)?</p>
",<calculus>
"<p>I'm taking calculus and we're up to areas between curves. Thing is that unless I do a table of values and graph, or I'm given an easy transformation, its really hard to figure out which graph is the top and bottom so I can do</p>

<p>$A=\int_a^b \! F_{top} - F_{bottom} \, \textrm{d}x$ or $A=\int_c^d \! F_{right} - F_{left} \, \textrm{d}x$</p>

<p>Also, what's a quick way of determining a problem in which I'll have to add two integrals and when I only need to solve one?</p>
",<calculus>
"<p>I have a vector of size $n$ x $1$ named $\alpha$. Let $f(\alpha) = u\cdot\mathbf 1^{\!\top}ln(\alpha)$ where $u$ is scalar.</p>

<p>What is the $f'(\alpha)$ and $f''(\alpha)$ and equivalent <strong>Matlab</strong> code?</p>

<p>According to me the first derivative is</p>

<p>$$f'(\alpha) = u/\alpha$$</p>

<p>and equivalent MATLAB code is --</p>

<p><code>f_a_1 = u ./ a</code></p>

<p>and for the second derivative</p>

<p>$$f''(\alpha) = u\cdot(Diag(\alpha)*Diag(\alpha))^{-1}$$</p>

<p>Equivalent MATLAB code is</p>

<p><code>f_a_2 = u*inv(diag(a)*diag(a))</code></p>

<p>Is my inference correct?</p>
",<calculus>
"<p><a href=""http://i.stack.imgur.com/j5OHj.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/j5OHj.jpg"" alt=""enter image description here""></a></p>

<p>What's the thought process when confronted with a question like this.
After a year this is the first time I've seen a question like this, in C1.</p>

<p>I've worked out $$y'' =  \frac{3}{2x^{1/2}}$$</p>

<p>How do I then go to the equation including k, where does the k even come from?</p>
",<calculus>
"<p>I've found the following exercise in one of my courses. How can I solve the 
following Cauchy equation?</p>

<p>$$\frac{dx}{dt} = x^2 -2e^{2t}x + e^{4t} + 2e^{2t}, (t,x) ∈ R^2$$
$$x(0)=2$$</p>

<p>Any help is appreciated!</p>
",<calculus>
"<p>$$\int_{0}^{1}\frac{x}{\sqrt{4-x}}dx=$$
I used a substitution like so: $t=x^{2}$, and got to 
$$\frac{1}{2}\int_{0}^{1}\frac{dt}{\sqrt{4-\sqrt{t}}}=\frac{1}{2}\left (\arcsin\frac{\sqrt[4]{1}}{2}-\arcsin\frac{\sqrt[4]{0}}{2}  \right )=\frac{\pi }{12}$$
But the correct answer is doing a different substitution: $t=4-x$  and it ends up with this value- $$-6\sqrt{3}+10\tfrac{2}{3}$$</p>

<p>Notice the two answers are very close - mine is $\sim 0.26$ and the other is $\sim 0.27$.</p>

<p>Was my way wrong? If so, why?</p>
",<calculus>
"<p>I have to prove that the bound of the following relation is $\theta(n^2)$ by induction-</p>

<p>$$T(n) = T(n-1) +  n$$</p>

<ol>
<li>should i seprate my induction into two sections - 
to claim  that $T(n) = O(n^2)$ and $T(n) = \Omega(n^2)$ and prove each case, or should i expand the relation and then formulate my claims ?</li>
<li>should my two equations be the same , but with diffrent sign  -->   $\leq$ and $\geq$</li>
</ol>

<p>Thanks!</p>
",<calculus>
"<p>Let  $f:[0,1]\rightarrow R$, a continuous function such that $f(0)=f(1)=0$.</p>

<p>Assume that there exists an M such that:
$$
\max \{ f(x) \mid x \in [0,1] \} = M &gt; 0
$$
Prove that for every $ 0 \lt \lambda \lt M $ there exists $x_1,x_2 \in [0,1]$ that fulfill the following requirements:</p>

<p>1.) $x_1 \ne x_2$</p>

<p>2.) $f(x_1)=f(x_2)=\lambda $</p>

<p>I'm leaning towards picking $a,b \in [0,1]$ such that $ -1\lt a \lt b \lt1$. f is then also continuous in [a,b] because it is a subgroup of [-1,1]</p>

<p>Here I'm having difficulties figuring out where to continue with this train of thought (if it's even correct.), so I'd appreciate any hints.</p>
",<calculus>
"<p>Recall the definitions of the <a href=""http://mathworld.wolfram.com/SineIntegral.html"" rel=""nofollow"">sine</a> and <a href=""http://mathworld.wolfram.com/CosineIntegral.html"" rel=""nofollow"">cosine integrals</a>:
$$\operatorname{Si}(x)=\int_0^x\frac{\sin t}t dt,\quad\operatorname{Ci}(x)=-\int_x^\infty\frac{\cos t}t dt.$$
Both functions are oscillating, with a countably infinite number of minima and maxima.</p>

<p>Note that
$$\lim_{x\to\infty}\operatorname{Si}(x)=\frac\pi2,\quad\lim_{x\to\infty}\operatorname{Ci}(x)=0.$$
Consider the following function:
$$f(x)=\sqrt{\left(\operatorname{Si}(x)-\frac\pi2\right)^2+\operatorname{Ci}(x)^2}.$$
<a href=""http://i.stack.imgur.com/NClxo.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NClxo.png"" alt=""Function graph""></a>
It appears that the function $f(x)$ and all its derivatives are monotonic  for $x&gt;0$. Specifically, the function itself and all its derivatives of an even order are strictly decreasing, and all its derivative of an odd order are strictly increasing. </p>

<p>Is it actually true? If so, then how can we prove it?</p>
",<calculus>
"<p>How to solve the definite integration as showed in the title.
And $m$ is an arbitrary natural number, $a$ is a non-negative number.
Many thanks in advance.</p>
",<calculus>
"<blockquote>
  <p>Prove that $$\lim\limits_{h\to 0}\frac{f(x+h)-2f(x)+f(x-h)}{h^2}=f''(x)$$</p>
</blockquote>

<p>Is the following a correct proof:</p>

<blockquote>
  <p>$f''(x)=$$\lim\limits_{h\to 0}\frac{f'(x)-f'(x-h)}{h}=\lim\limits_{h\to 0}\frac{\frac{f(x+h)-f(x)}{h}-\frac{f(x)-f(x-h)}{h}}{h}=\lim\limits_{h\to 0}\frac{f(x+h)-2f(x)+f(x-h)}{h^2}$</p>
</blockquote>

<p>I would really love input on this proof. The book ""Berkeley Problems in Mathematics"" solves it differently.</p>
",<calculus>
"<p>This is taken from ""Undestanding Analysis""- Abbot,</p>

<p><strong>Exercise 4.5.8</strong>: Imagine a clock where the hour hand and the minute hand are indistinguishable from each other. Assuming the hands move continuously around the face of the clock, and assuming their positions can be measured with perfect accuracy, is it always possible to determine the time?</p>

<p>From my point of view, I think we CAN tell the time because if you observe for, say, 5 minutes, you can see that the minute hand moves faster than the hour hand. Having that, you can determine which one is the hour hand and which one is the minute hand, and hence, you can easily deduct the exact time. However, this is not very mathematical, and although there is a solution for this question, I do not want to look at it so it would be better that I come up with my own original solution. </p>

<p>So, please help me, did I answer this question correctly? How do I move up my argument to a solid proof statement? I thank you very much for your help.</p>
",<calculus>
"<blockquote>
  <p>Definition: A partition of the interval $[a, b]$ is a <strong>finite</strong> collection of points in $[a, b]$, one of which is $a$ and one of which is $b$. The points in a partition can be numbered $t_0, ...., t_n$ such that $a = t_0 &lt;t_1 &lt; ... &lt; t_{n-1} &lt; t_n =b$.</p>
</blockquote>

<p>I've quoted the above definition of a partition for a reason, as you will see soon. From the definition of a partition, with $P = \{t_0, t_1, ... , t_n{-1}, t_n \}$ we can define the lower and upper Riemann Sums like so:</p>

<p>$$L(f, P) := \sum_{i=1}^{n}m_i(t_i - t_{i-1})$$
$$U(f, P) := \sum_{i=1}^{n}M_i(t_i - t_{i-1})$$
where
$$m_i = \inf\{f(x): t_{i-1} \leq x \leq t_i\}$$
$$M_i = \sup\{f(x): t_{i-1} \leq x \leq t_i\}$$</p>

<p>So essentially what the partition is doing, is selecting <em>sampling points</em> to break up the real field over the interval $[a, b]$ into $n$ sub-intervals, $[t_0, t_1],\  [t_1, t_2],\  ...\ , \ [t_{n-2}, t_{n-1}], \ [t_{n-1}, t_{n}]$, correct?</p>

<p>Is is then mathematically correct to rewrite $m_i$ and $M_i$ as the following:</p>

<p>$$m_i = \inf\{f(x): x \in [t_{i-1}, t_i]\}$$
$$M_i = \sup\{f(x): x \in [t_{i-1}, t_i]\}$$</p>

<p>where $[t_{i-1}, t_i] \subset \mathbb{R}$. (and where $[t_{i-1}, t_i]$ is a continuous interval)? </p>

<p>And if $f$ is assumed to be continuous, then $m_i = \min\{f(x) : x \in [t_{i-1}, t_{i}]\}$, and $M_i = \max\{f(x) : x \in [t_{i-1}, t_{i}]\}$ (in words: $m_i$ will be the minimum value $f$ takes on over the $n$ sub-intervals and $M_i$ will be the maximum value $f$ takes on over the $n$ sub-intervals)?</p>

<p>If what I've said above is correct, just out of curiosity why is the $a \leq x \leq b$ notation preferred over the interval notation $x \in [a, b]$?</p>

<hr>

<p>I really do apologize if what I'm writing is blatantly obvious, I asked a question earlier, and I think only now I realized that what the partition was doing, was just selecting sampling points to break up the real field into sub-intervals.</p>
",<calculus>
"<p>I'm signing up for University soon (Compsci program) as a mature student. It's been a long time since I've done any math, and I went as far as grade 11 in high school. So, I'm looking for a book that will review the essentials of high school math, starting from the basics, and prepare me for Uni.</p>

<p>Any recommendations? I know many of you recommended ""Mathematics: Its Content, Methods and Meaning"" in the past, but I'm not sure if it's beginner-friendly. Perhaps I should buy one of those GED preparation books?</p>
",<calculus>
"<p>Let $g : [a, b] → \mathbb{R}$ be a continuous function on $[a, b]$. Given any $n \in \mathbb{N}$ and
$x_1, . . . , x_n ∈ [a, b]$, show that there exists $x_0 ∈ [a, b]$ such that
$g(x_0) =(g(x_1) + · · · + g(x_n))$.</p>

<p>I am very confused on how to start here. I know that there is the basic epsilon delta proofs for continuity, but I don't how how or even if that should be applied here. </p>
",<calculus>
"<p>I am used to seeing integral notation like this, which means the integral over the domain from a to b.</p>

<p>$$
\int_{a}^{b}
$$</p>

<p>But now I am looking at a statistics book that says ""let A be an event"" and then shows the probability of that event like this</p>

<p>$$
\int_{A} fx(X)dx = P(X ∈ A)
$$</p>

<p>How do I interpret the notation when only the bottom symbol is given? </p>
",<calculus>
"<p>want to evaluate $$\int\frac{1}{\sqrt{x^2+y^2+z^2}}dxdydz$$ over entire $\mathbb{R}^3$ except $(0,0,0)$.</p>

<p>I did this using polar coordinate and got $$\lim\limits_{a\rightarrow0}\int\limits_a^\infty\int\limits_0^{\pi}\int\limits_0^{2\pi}r\sin\phi d\theta d\phi dr$$ but I think this diverges.</p>

<p>where am I wrong? help me please</p>
",<calculus>
"<p>I am looking for all real-valued continuous functions f, on R, which satisfy</p>

<p>$$ f(x)*f(y) = f(x_1)f(y_1) $$ 
for all x,y, $x_1$, $y_1$,  such that $$x^2 + y^2 = (x_1)^2 +(y_1)^2.$$</p>

<p>I don't have much idea on how to solve this problem.  The only thing that comes to mind, which doesn't help very much, is the fact that if I let g(x,y) = f(x)*f(y), then since the function g factorizes into two functions of a single variable, we have that the integral of g is the product of the single-variable integrals of f(x)dx and f(y)dy.</p>

<p>Thanks,</p>

<p>Edit:  This is not a homework problem.  It is a problem that dates back to 2007, as far as I know, and there is a not-so-good solution to it that basically says ""guess that the function is Guassian and let's force it to be Guassian.""  I am looking for another solution to this problem.  Thanks.</p>
",<calculus>
"<p>When it comes to taking a derivative, what does $\displaystyle \frac{d^2 u}{dt^2}$ mean ? Does it mean taking derivative of the function twice with respect to $t$. If yes, why is then $d^2 u$ squared? Thanks in advance!</p>
",<calculus>
"<p>Prove that:</p>

<p>$\displaystyle \sum_{k=1}^{\infty} \frac{H_k}{k^q} = (1 + \frac{q}{2})\zeta(q + 1) - \frac{1}{2}\cdot \sum_{n=1}^{q-2}\zeta(k+1)\zeta(q-k)$</p>

<p>It looks tough just to start off with. </p>

<p>Any ideas on approach?</p>
",<calculus>
"<p>I know the answer but I don't understand the steps to integrate.</p>

<p>$$  \int \frac{\cos x\,d x}{1 + (\sin x)^{2}} $$</p>
",<calculus>
"<p>Let $f(x)=x^4+x^3-x-1$. Use Rolle's theorem to prove $4x^3+3x^2-1=0$ has at least one real root in [-1,1].</p>

<p>Do I have to let $f(x)=\sin(x)$ and continue to do it? I have no idea how to use the Rolle's theorem as this can be easily mixed up with LaGrange theorem.</p>
",<calculus>
"<p>In my notes I have a step that I don't understand:</p>

<p>$$x\sqrt{1+{x^\prime}^2}=\text{constant}$$
$$x' = \tan s\quad\quad\text{?????}$$</p>

<p>Firstly, I don't get clarification for what $s$ is. Second, how did they deduce this?</p>
",<calculus>
"<p>Integrate the following :</p>

<p>$ \int _{ 0 }^{ 1 }{ { sec }^{ 2 } } \frac { \pi x }{ 4 } \quad dx $</p>
",<calculus>
"<p>Can someone explain to me how I find the real and the imaginary part of $e^{\theta i}$?</p>

<p>I'm learning complex numbers but I don't quite understand how $e$ is intertwined in all this.</p>
",<calculus>
"<p>Consider the function $f(x,y,z)=2y^{2}+2xy+2xz+2yz$, Find a symmetric matrix $A$ such that $f(x,y,z)$ can be written in the form $(Ax)x=(Ax)^{T}x$, where $x^T = [x y z]$. </p>
",<calculus>
"<p>Assume $f : [0, 1] \to \mathbb{R}$ is continuous and arbitrarily often differentiable on $(0, 1)$ (i.e.
$f$ is smooth). Denote by $f^{m}$ the $m\text{-th}$ derivative of $f$ with $m∈\mathbb{N}$ and set $f^{0}:=f$.
Prove via induction that the following formula holds for arbitrary $m∈\mathbb{N}$,</p>

<p>$$\frac{1}{n!}\int_0^1x^nf(x)dx =\sum_{r=1}^{m}\frac{(-1)^{r-1}f^{r-1}(1)}{(n+r)!}+(-1)^m\int_0^1 
\frac{x^{n+m} f^m(x)}{(n+m)!}dx$$</p>

<p>where $n∈\mathbb{N}$ is fixed.</p>
",<calculus>
"<p>I need your help</p>

<p>Find an enumeration of positive rational numbers, $a_n$ say, such that $\lim\limits_n a_n^{\frac{1}{n}}=1$</p>
",<calculus>
"<p>I have been using Lagrange multipliers in constrained optimization problems, but I don't see <em>how</em> they actually work to simultaneously satisfy the constraint <em>and</em> find the lowest possible value of an objective function.</p>
",<calculus>
"<p>I read <a href=""http://math.stackexchange.com/questions/625/why-is-the-derivative-of-a-circles-area-its-perimeter-and-similarly-for-spheres"" rel=""nofollow"">this question</a> the other day and it got me thinking: the area of a circle is $\pi r^2$, which differentiates to $2 \pi r$, which is just the perimeter of the circle. </p>

<blockquote>
  <p>Why doesn't the same thing happen for squares? </p>
</blockquote>

<p>If we start with the area formula for squares, $l^2$, this differentiates to $2l$ which is sort of right but only <em>half</em> the perimeter. I asked my calculus teacher and he couldn't tell me why. Can anyone explain???</p>
",<calculus>
"<p>I was coming back from my Driver's Education class, and something mathsy really stuck out to me.</p>

<p>One of the essential properties of a car is its current speed.  Or speed at a current time.  For example, at a given point in time in my drive, I could be traveling 40 mph.  But what does that <em>mean</em>?</p>

<p>From my basic algebra classes, I've learned that speed = distance/time.  So if I travel ten miles in half an hour, my average speed would be $20$ mph ($\frac{10 mi}{25 h}$).</p>

<p>But instantaneous velocity...you aren't measuring average speed for a given amount of time.  You're measuring instantaneous speed over an...instantaneous amount of time.</p>

<p>That would be something like (miles) / (time), where time = $0$?  Isn't that infinite?</p>

<p>And perhaps, in a difference of time = $0$, then I'd be travelling $0$ miles.  So would I be said to be going $0$ mph at an instantaneous moment in time?  I'd like to be able to tell that to any cops pull me over for ""speeding""!</p>

<p>But then if miles = $0$ and time = $0$, then you have $\frac00$?</p>

<p>This is all rather confusing.  What does it <strong>mean</strong> to be going $40$ mph at a given moment in time, exactly?</p>

<p>I've heard this explained using this strange art called ""calculus"" before, and it's all gone over my head.  Can anyone explain this using terms I (a High School Algebra and Geometry and Driving student) will understand?</p>

<p>(I figured that my problem had numbers in it, and therefore has to do with Maths.)</p>
",<calculus>
"<p>What are some particularly well-known functions that exhibit pathological behavior at or near at least one value and are particularly useful as examples?</p>

<p>For instance, if $f&#39;(a) = b$, then $f(a)$ exists, $f$ is continuous at $a$, $f$ is differentiable at $a$, but $f&#39;$ need not be continuous at $a$.  A function for which this is true is $f(x) = x^2 \sin(1/x)$ at $x=0$.</p>
",<calculus>
"<h1>Problem</h1>

<p>Consider the trigonometric equation:
$$
a\sin x+b\cos x-\cos x\sin x=0\qquad(0\le x&lt;2\pi)\tag{*}
$$
try to analyze the number of solutions to equation (*) with parameters $a,b$, i.e, let $A=a^{2/3}+b^{2/3}-1$, we have:</p>

<ol>
<li>$A&lt;0$, there are four distinct solutions.</li>
<li>$A&gt;0$, there are two distinct solutions.</li>
</ol>

<h1>Endeavors</h1>

<p>Let $f(x)=a\sin x+b\cos x-\cos x\sin x$, we have $f^\prime(x)=a\cos x-b\sin x-\cos2x$. It seems no advance to calculate the derivative, because $f^\prime$ is as hard as $f$.</p>

<p>Let $u=\cos x$ and $v=\sin x$, we have $u^2+v^2=1$ and $av+bu=uv$. We can work on these equations, but I prefer the trigonometric way, i.e, analyze the properties of $f(x)$.</p>

<p>I want to illustrate some details about $f(x)$, which might be useful. Let $a=r\cos\phi$ and $b=r\sin\phi$, where $r=\sqrt{a^2+b^2}$, we have
$f(x)=r\sin(x+\phi)-\frac12\sin2x$. It's a linear combination of $\sin(x+\phi)$ and $\sin2x$. I don't know whether there's a systematical way to deal with it.</p>

<p>Any idea? Thanks!</p>
",<calculus>
"<p>$$ \ X_n=\frac{1}{\sqrt{n^3+1}}+\frac{2}{\sqrt{n^3+2}}+\cdots+\frac{n}{\sqrt{n^3+n}}$$ Find $\displaystyle\lim_{n\to\infty} X_n$ using the squeeze theorem</p>

<p>I tried this approach:<br>
$$
\frac{1}{\sqrt{n^3+1}}\le\frac{1}{\sqrt{n^3+1}}&lt;\frac{n}{\sqrt{n^3+1}}
$$
$$
\frac{1}{\sqrt{n^3+1}}&lt;\frac{2}{\sqrt{n^3+2}}&lt;\frac{n}{\sqrt{n^3+1}}$$
$$\vdots$$
$$\frac{1}{\sqrt{n^3+1}}&lt;\frac{n}{\sqrt{n^3+n}}&lt;\frac{n}{\sqrt{n^3+1}}$$</p>

<p>Adding this inequalities:</p>

<p>$$\frac{n}{\sqrt{n^3+1}}\leq X_n&lt;\frac{n^2}{\sqrt{n^3+1}}$$</p>

<p>And this doesn't help me much. How should i proced?</p>
",<calculus>
"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://math.stackexchange.com/questions/115501/sqrtc-sqrtc-sqrtc-cdots-or-the-limit-of-the-sequence-x-n1-sq"">$\sqrt{c+\sqrt{c+\sqrt{c+\cdots}}}$, or the limit of the sequence $x_{n+1} = \sqrt{c+x_n}$</a>  </p>
</blockquote>



<p>Some time ago I was playing with a calculator and I found the following relation
$$2 = \sqrt{2 + \sqrt{2 + \sqrt{2 + \sqrt{2 + \cdots}}}}$$
In fact I found more, I found that
$$r = \sqrt{r(r - 1) + \sqrt{r(r - 1) + \sqrt{r(r - 1) + \sqrt{r(r - 1) + \cdots}}}}$$
if $r &gt; 1$, but I couldn't give a formal proof and I still can't.</p>

<p><strong>Note</strong>: If you solve $r(r - 1) = 1$ then you'll find an interesting property of the golden number.</p>
",<calculus>
"<p>I have a system of equations $xy=3$ and $4^{x^2}+2^{y^2}=72$ whose solution I know is $x=y=\sqrt 3$, but what are the steps to solve it?</p>
",<calculus>
"<p>Evaluate $$\lim_{x\to0} \frac{\sin(x^2)}{\sin^2(x)}.$$</p>

<p>Using L'Hospital twice, I found this limit to be $1$. However, since the Taylor series expansions of $\sin(x^2)$ and $\sin^2(x)$ tell us that both of these approach $0$ like $x^2$, I'm wondering if we can argue that the limit must be 1 via the Taylor series formal way?</p>
",<calculus>
"<p>I want to know a rigorous method to prove that </p>

<p>If $a&gt;1$, $\displaystyle\lim_{n \rightarrow \infty } a^n = \infty$</p>
",<calculus>
"<p>The vector field is obviously conservative on every closed domain that doesn't encompass the point $(0,0)$, so there must be a potential function.</p>

<p>I've got $\arctan(\frac{x}{y})$ for $x$ unequal to zero and $\arctan(\frac{y}{x})$ for $y$ unequal to zero.</p>

<p>However, when I try to find the line integral of the given field from point $(1,0)$ to point $(0,1)$ I get $\frac{\pi}{2}$, but when I try to find the result by using the potential function I get $0$.</p>

<p>What am I doing wrong?</p>

<p>Thanks in advance.</p>
",<calculus>
"<p>I am trying to integrate:
sin^5(3x)
What I did is as follows:</p>

<p>sin^6(3x)/(-cos3x*3*6)</p>

<p>is this the right way to do it?</p>
",<calculus>
"<p>The square of the side opposite a MUNDANGLE in a triangle is equal to the sum of the squares of the other two sides added to the product of these two sides multiplied by $\sqrt{3}$. What is MUNDANGLE?</p>

<p>This was an extra credit problem on my test, but my teacher said we had to do it for homework.</p>

<p>If anyone could support me and lead me to through the problem that would be great</p>
",<calculus>
"<p>If $c≠0$ and $\lim_{x→c}⁡〖f(x)〗=L$, prove that $\lim_{x→1/c}⁡〖f(1/x)〗=L$</p>

<p>I know that meaning for all $ε&gt;0$, there exist a $δ&gt;0$ such that $0&lt;|x-c|&lt;δ$  implies $|f(x)-L|&lt;ε$. How can I use these to prove the conclusion.</p>
",<calculus>
"<p>I have this function. I noticed that it can be written as: $ $x$ $y$^2 +$y$(1-$x$^2) + ($x$ - 2) = 0 $, so this is a quadratic in y.
Thus
\begin{equation}
y=\frac{(x^2-1)\pm \sqrt[2]{(x^2-1)^2 -4x(x-2)}}{2x}
\end{equation}
So i notice that this is not actually a function since it maps two $y$ values given one $x$ value. Anyway, the domain is $x$ $\neq$ 0 and from the expression inside the square root i find: $ $x$^4 -6$x$^2 +8$x$ + 1 \geqslant 0 $ which i can't solve. I don't know how to find the range either. From graphing this on WolframAlpha it <a href=""http://www.wolframalpha.com/widget/widgetPopup.jsp?p=v&amp;id=b0160688b805d84769cebe1afb71895&amp;title=Domain%20and%20Range%20calculator&amp;theme=blue&amp;i0=(x%5E2-1%2B((x%5E2%20-%201)%5E2%20-4x(x-2))%5E(1%2F2)%20))%2F(2x)&amp;podSelect="" rel=""nofollow"">gives</a> the domain but not the range. 
How did it find the domain? And what about the range?
Can somebody help? Thanks in advance.</p>
",<calculus>
"<blockquote>
  <p>Find a power series representation for $\displaystyle\left(\frac{x}{2-x}\right)^3$</p>
</blockquote>

<p>My approach is in finding something similar to $\displaystyle\left(\frac{x}{2-x}\right)^3$ to which I can easily find the power series representation of. </p>

<p>I use $\displaystyle\frac{1}{2-x}$, noting that $\displaystyle\left(\frac{1}{2-x}\right)'=\frac{1}{(2-x)^2} \text { and } \left(\frac{1}{2-x}\right)''=\frac{2}{(2-x)^3}$.</p>

<p>So 
$$\displaystyle \frac{1}{2-x}=\int\frac{1}{(2-x)^2}dx \iff \frac{1}{2}\sum^{\infty}_{n=0}\left(\frac{x}{2}\right)^n=\int\frac{1}{(2-x)^2}dx$$</p>

<p>and differentiating both sides, I get the power series representation of the first derivative</p>

<p>$$\frac{1}{2}\sum^{\infty}_{n=1}n\left(\frac{x}{2}\right)^{n-1}=\frac{1}{(2-x)^2}$$</p>

<p>for the second derivative, </p>

<p>$$\frac{1}{(2-x)^2}=\int\frac{2}{(2-x)^3} \iff \frac{1}{2}\sum^{\infty}_{n=1}n\left(\frac{x}{2}\right)^{n-1}=\frac{1}{2}\sum^{\infty}_{n=0}(n+1)\left(\frac{x}{2}\right)^n=\int\frac{2}{(2-x)^3}dx$$</p>

<p>differentiating both sides, I get the power series representation of the second derivative</p>

<p>$$\displaystyle \frac{1}{2}\sum^{\infty}_{n=1}(n+1)n\left(\frac{x}{2}\right)^{n-1}=\frac{2}{(2-x)^3}$$</p>

<p>Is this so far correct? If it is, in the end I would multiply the power series representation of $\displaystyle\frac{2}{(2-x)^3}$ by $\displaystyle\frac{x^3}{2}$ to cancel out the $2$ and get the power series for $\displaystyle\left(\frac{x}{2-x}\right)^3$.</p>
",<calculus>
"<p>I have been trying to understand this proof for the product rule of sequences, where the author makes use of some properties for infinitesimals, to prove this theorem. This is quite a long question, but please answer it as explicitly as you possibly can.</p>

<p>""A sequence ($y_nz_n$) is convergent to $ab$ if sequences ($y_n$) and($z_n$) are convergent to $a$ and $b$, respectively.""</p>

<ul>
<li>First of all how would <strong>you</strong> prove this.</li>
<li>The author uses an important property described earlier in the book:
That for any convergent sequence ($y_n$) there corresponds an infinitesimal    sequence ($\alpha_n$) where $\alpha_n$ = $y_n$- $a$. <strong>Why is this true, is there any intuition/ a precise reason behind this?</strong> Explain this property please.</li>
<li>Lastly after initial steps are taken we get:<br>
($y_nz_n$) = ab + $\gamma_n$ where $\gamma_n$ = $b\alpha_n$+$a\beta_n $+ $\alpha_n\beta_n$</li>
</ul>

<p>The author then states:</p>

<p>the sequences 
($b\alpha_n$) , ($a\beta_n $) , ($\alpha_n\beta_n$) are infinitesimal as well.</p>

<ul>
<li><strong>Why?</strong> Is it true that if we multiply a <strong>limit</strong> with a <strong>infinitesimal</strong>, we get another infinitesimal as $n\to\infty$? Explain please.</li>
</ul>
",<calculus>
"<p>I'm having a hard time finding the arc length of one section of the polar equation $r=\sin(3\theta)$. </p>

<p>I thought I had a good understanding but the integration seems to be a nightmare. Unless I'm not doing something correctly.</p>
",<calculus>
"<p>Here are basically my two problems, which I have the answer from WolframAlpha:
$$
\lim_{n\to\infty}(1-\sqrt 2-\sqrt{n+1}+\sqrt{n+2})=1-\sqrt 2
$$
$$
\lim_{n\to\infty}(\sqrt n-2\sqrt{n+1}+\sqrt{n+2})=0
$$</p>

<p>I have no idea how to actually solve them on my own though. At this moment it's gonna be inf - inf (and inf - 2 inf + inf for the second one) which I can't do nothing with (am I remembering correctly that you can't subtract infinity from infinity?). I need to simplify them somehow.</p>
",<calculus>
"<p>What is the value of the following sum?  </p>

<blockquote>
  <p>$$\sum_{i=1}^{2000}\gcd(i,2000)\cos\left(\frac{2\pi\ i}{2000}\right)$$  </p>
  
  <p>where </p>
  
  <ul>
  <li>$\gcd$ is the greatest common divisor.</li>
  </ul>
</blockquote>
",<calculus>
"<p>Let $g(x) = f(x)/(x+1)$, where $f(x)$ is differentiable on $x\in[0,5]$, such that $f(0)=4$ and $f(5)=-1$. What is the range of values $g'(c)$ for a $c$ belonging to $[0,5]$?</p>

<p>Considering values of $f(x_i)$, $f(x)$ must decrease at least once from $0$ to $5$. But that is all the information I can use here. Is there anything I am missing?</p>
",<calculus>
"<p>I'm solving a couple of integration problems using the method of changing variables, and would like assistance with two particular problems that I can't seem to solve. I completed rest of the problems in this problem set without much effort, but these two seem impossible.</p>

<p>I've tried changing a few different variables in both problems, and I tried to calculate the solution with Wolfram Alpha, but neither of those had any avail.</p>

<p>$$\int x^{e^{x^{2}}}dx$$</p>

<p>and</p>

<p>$$\int \frac{dx}{x+ ln^2x}$$
are the problems that I'm trying to solve. Any help is much appreciated.</p>
",<calculus>
"<p>I need to solve $$\frac{\partial u^2}{\partial x\partial y}=0$$ with the boundary conditions: $u(x,y=x^3)=\sin(x^6)$ and $\frac{\partial u}{\partial x}(x,y=x^3)=0$.</p>

<p>I got a particular solution, I thing which is $u_p=A\sin(y^2)$, where $A\in\mathbb{R}$, that satisfies the two boundary conditions, but it is rather a guess.</p>
",<calculus>
"<p>This is the question: <a href=""http://i.stack.imgur.com/wqjGC.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/wqjGC.jpg"" alt=""enter image description here""></a></p>

<p>I first found $\frac{dv}{ds}=6s^2+5$, then I tried to find $\frac{ds}{dt}$ by messing about a little with implicit differentiation, but I had no luck and I therefore couldn't apply the chain rule (i.e. $a=\frac{dv}{dt}=\frac{ds}{dt}\frac{dv}{ds}$) to find acceleration. The back of my book tells me the answer is $(6s^2+5)(2s^3+5s)$, but I fail to see how this is true as it would imply that $\frac{ds}{dt}=v$, which I can't exactly understand. Can anyone tell me what I am doing wrong?</p>
",<calculus>
"<blockquote>
  <p>Compute the indefinite integral
  $$
\int\ln(\tan x)\,dx
$$</p>
</blockquote>

<p><strong>My Attempt:</strong></p>

<p>Using $\sin x = \frac{e^{ix}-e^{-ix}}{2i}$ and $\cos x = \frac{e^{ix}-e^{-ix}}{2}$ and remembering that $\ln(\tan x) = \ln(\sin x) - \ln(\cos x)$, we have</p>

<p>$$
\begin{align}
\int\ln(\tan x)dx &amp;= \int \ln(\sin x)\,dx - \int \ln (\cos x)\,dx\\
&amp;= \int \ln \left(\frac{e^{ix}-e^{-ix}}{2i}\right)\,dx - \int \ln \left(\frac{e^{ix}-e^{-ix}}{2}\right)\,dx\\
&amp;= \int (e^{ix}-e^{-ix})\,dx-\int \ln(2i)\,dx-\int \ln \left(e^{ix}-e^{-ix}\right)\,dx+\int \ln(2)\,dx
\end{align}
$$</p>

<p>What should I do next to get to the solution?</p>
",<calculus>
"<p>What approach would be ideal in finding $\frac{dy}{dx}$ for $\sqrt{xy} = 1$?</p>
",<calculus>
"<p>A large vase has a square base of side length $6 \text{ cm}$, and flat sides slopingoutwards at an angle of $120^{\circ}$ with the base. Water is flowing in at $12 \text{ cm}^3/\text{s}$. Find, to three significant figures, the rate at which the height of water is rising when the water has been flowing in for $3$ seconds. </p>

<p>Spent around an hour trying to do it, but I keep getting the answer wrong. I think I'm not getting the right volume function.</p>
",<calculus>
"<p>I seem to be stuck trying to prove the following integral
$$
\int\frac{\cos^mx}{\sin^nx}dx = -\frac{\cos^{m+1}x}{(n-1)\sin^{n-1}x}-\frac{m-n+2}{n-1}\int\frac{\cos^mx}{\sin^{n-2}x} dx + C\,\,(n \neq 1)
$$
My thinking so far has been that if I take
$$
I = \int\frac{\cos^mx}{\sin^nx}dx
$$
I have been able to prove that
$$
I = -\frac{\cos^{m-1}x}{(n-1)\sin^{n-1}x}  - \frac{m-1}{n-1}\int\frac{\cos^{m-2}x}{\sin^{n-2}x}\,dx+C\,\,\,\,\,(1)
$$
and
$$
I = \frac{\cos^{m-1}x}{(m-n)\sin^{n-1}x} + \frac{m-1}{m-n}\int\frac{\cos^{m-2}x}{\sin^nx}\,dx+C\,\,\,\,\,(2)
$$
but showing that 
$$
I = -\frac{\cos^{m+1}x}{(n-1)\sin^{n-1}x}-\frac{m-n+2}{n-1}\int\frac{\cos^mx}{\sin^{n-2}x} dx + C
$$
seems to be eluding me. I attempted to apply a similar technique what I used on $(1)$ to get $(2)$ to try to obtain this integral, but it didn't seem to work.<br/>
I can also show that
$$
I = -\frac{\cos^{m+1}x}{(m+1)\sin^{n+1}x} - \frac{n+1}{m+1}\int\frac{\cos^{m+2}x}{\sin^{n+2}x}\, dx + C\,\,\,\,\,(3)
$$
but there's obviously more to it.</p>

<p>Any broad hints would be more than welcome.</p>
",<calculus>
"<p>What would be the ideal approach in finding the integral for:</p>

<p>$$
\int 2^{\sin{x}}\cos{x}\;\mathrm{d}x
$$</p>
",<calculus>
"<p>My integral is
$$I=\int\sqrt[3] {\frac{1}{(x+1)^2(x-1)^4}} dx$$
and hence
$$I=\int\frac{1}{(x-1)(x+1)}\sqrt[3] {\frac{x+1}{x-1}}dx $$
$\cos2\theta$ substitution wont be helpful here because of the cube root. Should I apply by parts ?</p>
",<calculus>
"<p>Do these series coverage or diverge? What test would you use to show this? Find the sum of the series when possible.
I am stuck with this one and I don't know how to go about it.</p>

<p>$$\sum_{i=1}^\infty [cos(\frac{1}{n+1})-cos(\frac{1}{n})]$$</p>
",<calculus>
"<p>I have a function $ f(x,y) = \begin{cases}
(x^2+y^2)\sin(\frac{1}{x^2+y^2}),  &amp; (x,y)\neq(0,0) \\
0, &amp; (x,y)=(0,0)
\end{cases}$</p>

<p>and I need to show that $f(x,y)$ is differentiable, even though it doesn't have partial derivatives at the point $(0,0)$. </p>

<p>I have proven that the function is continuous in $(0,0)$, and even though I understand that the function may have discontinuity in parital derivatives, it can be differentiable, but I am unable to prove it. </p>
",<calculus>
"<blockquote>
  <p>The acceleration as a function of time $a(t)$ (in m/s$^2$) and the initial velocity $v(0)$ are given for a particle moving along a line:
  $$a(t) = 2t + 4, \hspace{4mm}v(0) = −5, \hspace{8mm} 0 \leq  t \leq 4.$$
  (a) Find the velocity at time $t$. ($v(t) =  t^2+4t−5$)</p>
  
  <p>(b) Find the distance traveled during the given time interval.</p>
</blockquote>

<p>I was able to solve part (a) but have been having issues with part (b). It's a webassign assignment and the ""master it"" section only gives me the first portion and it hasn't allowed me to see other problems so I could try and figure it out by example. Is anyone able to explain to me how to do this?</p>
",<calculus>
"<p>I've been at this problem for days. </p>

<p>Forgot most of my series from calculus, so I started to review series and sequence for numerical methods and approximations. </p>

<p>$$\sum_{n=5}^{\infty} \frac{12}{16n^2+40n+21}$$</p>

<p>How can I evaluate this series using telescoping method?</p>
",<calculus>
"<p>Consider the parametric curve: $x(t)=1+4t-t^2$,  $y(t)=2-t^3$ for $t\in\mathbb{R}$. </p>

<p>What is the equation of the tangent line at the point $(x(1),y(1))$?</p>

<p>I got the slope by using the equation $\frac{\frac{dy}{dt}}{\frac{dx}{dt}}$ and I found that $y = -\frac{3}{2}x + 7$. Is it correct? </p>
",<calculus>
"<p>Could someone please give an intuition about the usage of the Hessian Matrix in multivariate optimization?</p>

<p>I know that it consists of all second order partial derivatives of a multivariate function and that it's used, for example, in the Newton-Raphson-Method. This method is intuitive for a function with a single variable but it's confusing to see the inverted Hessian in the expression for multiple variables.</p>
",<calculus>
"<blockquote>
  <p><strong>Problem:</strong> Consider partitioning $f(x)=x^2$ on $[-1,1]$ into $n$equal sub-intervals. We seek to derive an expression for $L(f, P)$ and $U(f, P)$ in terms of $n$, where $n$ is even.</p>
</blockquote>

<p>Now the question seemed a bit vague to me, but non the less this is what I did and arrived at:</p>

<hr>

<p>Given $P = \{x_0, x_1, ..., x_n \}$</p>

<p>$$L(f, P) := \sum_{i=1}^{n}m_i(x_i - x_{i-1})$$
$$U(f, P) := \sum_{i=1}^{n}M_i(x_i - x_{i-1})$$
where
$$m_i = \inf\{f(x): x \in [x_{x-1}, x_i]\}$$
$$M_i = \sup\{f(x): x \in [x_{i-1}, x_{i}]\}$$</p>

<p>Now since we wish to partition $f$ over $[-1, 1]$ into $n$ <strong>equal</strong> sub-intervals</p>

<p>$$\implies L(f, P) = (x_n - x_{n-1})\sum_{i=1}^{n}m_i$$
$$\implies U(f, P) = (x_n - x_{n-1})\sum_{i=1}^{n}M_i$$</p>

<p>And since we know $n$ is even, $\exists k \ni 2k =n$. This allows us to break our partition $P$ into two separate partitions $P_1$ and $P_2$ over two intervals $[-1, 0]$ and $[0, 1]$. Thus</p>

<p>$$\begin{align}P_1 &amp;= \{x_1, x_2, ...., x_k\} \ &amp; \text{where} &amp;&amp; \ x_1 = -1 &lt; x_2 &lt; ... &lt; x_k = 0 \\
P_2 &amp;= \{x_{k}, x_{k+1}, ...., x_{2k}\} \ &amp; \text{where} &amp;&amp; \ x_k = 0 &lt; x_{k+1} &lt; ... &lt; x_{2k} = 0
\end{align}$$</p>

<p>Since $f$ is continuous on $[-1,1]$, $m_i = \min(f : [x_{n-1}, x_n] \to \mathbb{R})$ and $M_i = \max(f : [x_{n-1}, x_n] \to \mathbb{R})$. </p>

<p>Therefore for $P_1$: $m_i = f(x_n)$ and $M_i = f(x_{n-1})$, and for $P_2$: $m_i = f(x_{n-1})$ and $M_i = f(x_n)$. </p>

<p>Finally putting all this together allows us to rewrite $L(f, P)$ and $U(f, P)$ as follows:</p>

<p>$$\implies L(f, P) = (x_n - x_{n-1})\left(\sum_{n=1}^{k}f(x_n) + \sum_{n=k}^{2k}f(x_{n-1})\right)$$</p>

<h2>$$\implies U(f, P) = (x_n - x_{n-1})\left(\sum_{n=1}^{n}f(x_{n-1}) + \sum_{n=k}^{2k}f(x_{n})\right)$$</h2>

<p>But I'm assuming when they say 'derive an expression' they want a closed form solution for the Riemann Sum,and I'm not sure how to convert what I've arrived at into a closed form, or if it is even possible to convert into closed form as the summand is varying.</p>

<p>Can a closed form solution for these lower and upper Riemann Sums be found?</p>
",<calculus>
"<blockquote>
  <p><strong>Problem:</strong></p>
  
  <p>Let $f(x,y)=x-\ln x - y +\ln y$ for $x,y&gt;0$. Prove that there exists a $\delta&gt;0$ and a function $\varphi : (-\delta, \delta)\to \mathbb{R}:\ \varphi \in C^1,\ \varphi(0)&lt;0,\ \forall_{x\in (1-\delta, 1+\delta)}\ f(x,1+(x-1)\varphi(x-1))=0$</p>
</blockquote>

<p>I came up with a bit of a solution but there must be an easier way. Also, I'd like find out if mine is correct and how to finish it.</p>

<p>Ok, it is easy to see that IFT won't work for the function $f$ because its derivative is zero on the line $y=x$. My idea is to modify the function.</p>

<p>Let $g(x) = x-\ln x$ and $F(x,y)=\frac{g(x)-g(y)}{x-y}$. We have $F=0 \Rightarrow f=0$. By Lagrange Theorem $F(x,y)=g'(c)$ where $c\in [x,y]$, thus by taking limit with $(x,y)\to (a,a)$ we have $F(a,a)=g'(a) = 1-\frac{1}{a}$. Hence $F$ is continuous on $\mathbb{R}^2_+$.</p>

<p>Now let's show that it's $C^1$ class. We only need to show that derivative is continuous on the line $x=y$. We have 
\begin{eqnarray*}
&amp;\frac{\partial F}{\partial x}(x,y) = \frac{(x-y)g'(x) -(g(x)-g(y))}{(x-y)^2}
\end{eqnarray*}
Thus
\begin{eqnarray*}
&amp;\lim _{h\to 0}\frac{\partial F}{\partial x}(x+h,x) = 
\lim_{h\to 0} \frac{hg'(x+h) -(g(x+h)-g(x))}{h^2} =\\
&amp;\lim_{h\to 0} \frac{hg'(x) + h^2g''(x)- hg'(x) - \frac{1}{2}h^2g''(x) +o(h^2)}{h^2}=\frac{1}{2}g''(x)= \frac{1}{2x^2}
\end{eqnarray*}
We can do the same for $\frac{\partial F}{\partial y} \Rightarrow$ F has continuous paritial derivatives, hence is of class $C^1$. Moreover $DF(x,y)=(0,0) \Rightarrow \frac{\partial F}{\partial x} + \frac{\partial F}{\partial y}=0 \Rightarrow \frac{g'(x)-g'(y)}{x-y}=0 \Rightarrow x=y$ but $DF(x,x)\neq (0,0)$ for $x&gt;0$ (which was proven above). So, $DF(x,y)\neq (0,0)$ for all $(x,y)$.</p>

<p>Hence we can apply IFT: there exists a $\delta &gt;0$ and $C^1$ class function $h:(1-\delta,1+ \delta):\ F(1+x,h(1+x))=0 \Rightarrow f(1+x,h(1+x))=0$. Now we can define $\varphi(x) := \frac{h(x+1)-1}{x}$. All we need o do is to check that $\varphi$ satisfies task conditions. We can write Taylor series for $h$ around the point $(x,y)=(1,1)$ (since we know that $DF(1,1)=(\frac{1}{2},\frac{1}{2})$) and obtain: $\frac{h(x+1)-1}{x} = \frac{1+2x+o(x)-1}{x} \to 2$ as $x\to 0$. </p>

<p>Here the trouble begins:
$\varphi$ doesn't satisfy the conditions but it would suffice to take $\bar{F}=-F(x,y)$ instead of $F(x,y)$ and $\bar{h}:\bar{F}(x,\bar{h}(x))=0$, then $\phi := \frac{\bar{h}(x+1)-1}{x} \to -2$ as $x\to 0$ so we can handle this problem. But how to show that $\varphi$ has continuous derivative at $x=0$? Do we have to compute the second Taylor polynomial for $h$? It's possible but seems like a lot of calculations..</p>

<p>I came across this problem on my calculus exam two days ago and it was only a part of a bigger one, so I don't believe the answer is that convoluted. It is the only thing I couldn't figure out so I would appreciate any help. </p>

<p><strong>EDIT:</strong> BTW: It should be true for any locally non-constant $C^1$ function $f$, that around it's local maxima/minima there exists a non-identity $C^1$ function $g$ that $f(a+x)=f(a+xg(x))$ for $x$ from neighbourhood of that maxima/minima - $a$. So the task is only a special case of this theorem, am I right?</p>
",<calculus>
"<p>A few days back I was asked this question in my class.</p>

<blockquote>
  <p><strong>Is $f:\mathbb{R}^2\to\mathbb{R}$ differentiable?</strong></p>
</blockquote>

<p>$f(x,y)=x+y$</p>

<p>My course of action was </p>

<p>$$\lim_{(\xi,\eta)\to(0,0)}\frac{f(x+\xi,y+\eta)-f(x,y)}{\sqrt{\xi^2+\eta^2}}=\lim_{(\xi,\eta)\to(0,0)}\frac{\xi+\eta}{\sqrt{\xi^2+\eta^2}}$$ which does not exist. </p>

<blockquote>
  <p>It is equal to $1$ if we take one of the variables $\xi$ or $\eta$ zero. (i.e. directional derivatives along the axes). If we take derivative along $x=y$, we get $\sqrt{2}$</p>
</blockquote>

<p>But someone told me that I was wrong and differentiability in $\mathbb{R}^2\to\mathbb{R}$ is determined only by the directional derivatives along the axes. Is it true? If yes, could someone explain? I am fairly new to the subject. Thanks!</p>
",<calculus>
"<p>I try with a simple example </p>

<p>I put $$f(x)=2x$$ </p>

<p>and $$f(2x)=4x$$</p>

<p>so </p>

<p>$$f(x)-f(2x)=-2x$$</p>

<p>and I try to solve the last equation </p>

<p>$$f(x)-f(2x)=-2x$$</p>

<p>by put $$f(x)=e^{mx}$$</p>

<p><a href=""http://www.wolframalpha.com/input/?i=solve%20e%5E%28mx%29-e%5E%282mx%29=-2x%20for%20m"" rel=""nofollow"">then the solution</a> $$e^{mx}=\frac{1}{2}\pm \sqrt{2x+\frac{1}{4}}\neq 2x$$</p>

<p>I know that $$e^{mx}-e^{2mx}=\frac{1}{2}\pm \sqrt{2x+\frac{1}{4}}-(\frac{1}{2}\pm \sqrt{2x+\frac{1}{4}})^{2}=-2x$$</p>

<p>but $$\frac{1}{2}\pm \sqrt{2x+\frac{1}{4}}\neq 2x$$</p>

<p>where is the wrong </p>

<p>and any way how to solve equation like $$f(x)\pm f(g(x))=m(x)$$</p>
",<calculus>
"<p>how to solve it answer is $0$, but $\frac 1{\infty + \infty}$ is indeterminate form</p>

<p>$$\lim_{x \to \pi/2} \frac 1{\sec x + \tan x}$$</p>
",<calculus>
"<p>I know that $x^x$ for all $x&gt;0$ </p>

<p>but what is negative values for that function which give a real number</p>

<p>for example  $$f(-1)=(-1)^{-1}=-1\in R$$</p>

<p>I try to put sequence for that but i faild </p>

<p>is there any help </p>

<p>thanks for all</p>
",<calculus>
"<p>I am trying to optimize the output of a given neural network with a single hidden layer.  To accomplish this, I intend to find solve for all combinations of inputs where the derivative of the neural network = 0 and select the input vector with the highest (or lowest, depending on the problem) neural network output.  It uses the activation function</p>

<p>$$
H_i,_j = \frac{1}{(1 + e^{-t})}
$$</p>

<p>where </p>

<p>$$
t = X_i\theta_j
$$</p>

<p>for a given input vector i and hidden node j. </p>

<p>The activation values of each hidden node are multiplied by a separate weight matrix to produce the outputs.  The output k of a given input vector i is the product of the hidden node activation values i and the weight vector k.</p>

<p>$$
O_i,_k = H_iW_k
$$</p>

<p>Could someone please explain the steps I would use to create the derivative formula for an input vector of arbitrary length, an arbitrary number of hidden nodes, a single hidden node layer, and a given output k?  Thank you so much.</p>
",<calculus>
"<p>I figured out that the top is (2x-1) and that the difference between the denominator ends up being (2x-1), just not sure how to figure out what the series is.</p>

<p><img src=""http://i.stack.imgur.com/WmZdl.png"" alt=""alternating series""></p>
",<calculus>
"<p>It's too much hassle to post it here as latex, to so here's <a href=""http://i.imgur.com/oa6RswP.png"" rel=""nofollow"">the screenshot</a>.</p>

<p>I don't understand why |cos(c)| = 1</p>

<p>Why 1? Why not $\frac {\sqrt{3}}{2}$? Why absolute value assumes the max value a function can take?
Shouldn't it be like:</p>

<p>$\cos(c) &gt; 0$</p>

<p>and $-\cos(c) &lt; 0$</p>

<p>?</p>
",<calculus>
"<p>I have a field of measured vectors, see example of four vectors in image below. If there was no noise they would all point outward exactly from one ""central point"". i.e. there would be a circle whose tangent is perpendicular to all vectors. Unfortunately there is some noise in the measurement, I am looking for the best approximation for the center of this circle.</p>

<p>Thanks for your ideas!</p>

<p><img src=""http://i.stack.imgur.com/M6Ym2.png"" alt=""Field of measured vectors""></p>
",<calculus>
"<p>$$\quad\quad \lim_{ x \to 0} \frac {\sin 5 x } {\sin 2 x } $$</p>

<p>I don't know how to start, should I multiply by something... to simplify the expression or ...?</p>
",<calculus>
"<p>I'm having trouble understanding the following progression of equalities.</p>

<p>$\begin{align*}
\ddot{x} &amp;= \frac{dv}{dt}\\
&amp;= \frac{dv}{dx} \frac{dx}{dt}\\
&amp;= v \frac{dv}{dx} \tag{1}\\
&amp;= \frac{1}{2} \frac{dv^2}{dx} \tag{2}
\end{align*}$</p>

<p>I understand up to $(1)$. In the previous line, I realize that $v=\frac{dx}{dt}$, but shouldn't $(1)$ have then been $\frac{dv}{dx} v$? Why is it acceptable to move $v$ to the left side of the differential; $v$ depends on $x$ and so will be operated on by it, won't it?</p>

<p>Accepting $(1)$, I don't understand how to get to $(2)$. I expect it has something to do with integrating $v$ so it can be moved to the right side; that would account for the $\frac{1}{2}$ and $v^2$, but I don't know the principle that makes $\frac{dv}{dx} v^2 = \frac{dv^2}{dx}$.</p>

<p>Trying to reach conceptual understanding by substituting in an example term, I do this, where $v=x^2$
$\begin{align*}
v \frac{d}{dx}v &amp;= \frac{1}{2} \frac{d}{dx} v^2\\
(x^2)(2x) &amp;= \frac{1}{2} (4x^3)\\
2x^3 &amp;= 2x^3
\end{align*}
$</p>

<p>I see that they are equal, but I don't understand. With my reasoning about the integration, this would have happened</p>

<p>$
\begin{align*}
v \frac{d}{dx} v &amp;= \frac{d}{dx}v \frac{1}{2}v^2\\
&amp;= \frac{1}{2} \frac{d}{dx} v^3
\end{align*}
$</p>

<p>which does not give equal answers.</p>

<p>Hopefully I've made my deficit of knowledge obvious and someone can prod me helpfully in the right direction. :)</p>
",<calculus>
"<p>Prove the identity:</p>

<p>$$n(n-1)2^{n-2}=\sum_{k=1}^n {k(k-1) {n \choose k}}$$</p>

<p>I tried using the binomial coefficients identity $2^n = \sum_{k=1}^n {n \choose k}$  but got stuck along the way.</p>
",<calculus>
"<p>In physics I ran into some nasty integrals involving characteristic functions $\chi$.</p>

<p>The ones are given by </p>

<p>$$\int_{\mathbb{R}^2} \left(E - \frac{p^2}{2m}-\frac{q^2 \omega^2}{2} \right)^{n-1} \chi_{[0,E]} \left( \frac{p^2}{2m}+\frac{q^2 \omega^2}{2} \right) \frac{p^2}{2m} dq dp,$$</p>

<p>$$\int_{\mathbb{R}^2} \left(E - \frac{p^2}{2m}-\frac{q^2 \omega^2}{2} \right)^{n-1} \chi_{[0,E]} \left( \frac{p^2}{2m}+\frac{q^2 \omega^2}{2} \right) \frac{q^2 \omega^2}{2}  dq dp,$$</p>

<p>where all constants are positive(!).</p>

<p>If anything is unclear, please let me know.</p>
",<calculus>
"<p>Since $\infty&gt;0$, so $1/\infty &gt;0$, thus I think $1/\infty$ should be infinitesimal, but the calculus book says 
$$\lim_{x \to \infty} \frac{1}{x}= 0$$</p>

<p>So is $1/\infty$ zero or infinitesimal ?</p>

<p>P.S. I mean are $1/\infty$ and $\lim_{x \to \infty} 1/x$ the same thing here?</p>
",<calculus>
"<p>I am suppose to differentiate </p>

<p>$y=(\sin x)^{\ln x}$</p>

<p>I have absolutely no idea, this was asked on a test and I just do not know how to do this I have forgotten the tricks I was suppose to memorize for the test.</p>
",<calculus>
"<p>=Could anyone help me show that:</p>

<p>$$
f(x) = -x^2 + 2x
$$</p>

<p>using</p>

<p>$$
f(ax + (1-a)y) \geq af(x) + (1-a)f(y)
$$
is CONCAVE in $(0,1)$? I am trying to solve it by directly substituting to the general theorem but I sem to prove just the opposite.</p>

<p>Update:</p>

<p>I managed to get:</p>

<p>$$
-(ax + (1-a)y)^2 \geq -ax^2 - y^2 + ay^2
$$</p>

<p>Anyone?</p>
",<calculus>
"<p>I have a $3$-D sphere of radius $R$, centered at the origin. </p>

<p>$(x_1,y_1,z_1)$ and<br>
$(x_2,y_2,z_2)$ are two points on the sphere. </p>

<p>The Euclidean distance is easy to calculate, but what if I were to restrict myself to traveling on the surface of the sphere?  </p>

<p>Two approaches come to mind: use <a href=""http://en.wikipedia.org/wiki/Arc_length"" rel=""nofollow"">arc-length</a> in some way, or simply use trigonometry: calculate the angle between the two points and get a distance from that.  </p>

<p>Will both/either of these methods work? Which would be easier?</p>

<p>Somewhat related to <a href=""http://math.stackexchange.com/questions/720/how-to-calculate-a-heading-vector-on-the-earths-surface"">this question</a>. Maybe it will inspire someone to go answer it!</p>
",<calculus>
"<p>Wind resistance -- upwards acceleration, typically varies either linearly or quadratically by the current velocity.</p>

<p>There is a constant downward acceleration due to gravity.</p>

<p>How can we model the velocity over time of a falling object, subject only to wind resistance and downwards gravity?</p>

<p>I don't have much experience with differential equations, but I do know that this answer necessarily involves it, so could you possibly explain every step?</p>

<p>Thank you.</p>
",<calculus>
"<p>What is the volume of intersection of the three cylinders with axes of length $1$ in $x, y, z$ directions starting from the origin, and with radius $1$?</p>
",<calculus>
"<p>There is a  theorem of Riemann to that effect. How to prove it?</p>

<p>Note: This was asked by Kenny in the beta for ""calculus"".</p>
",<calculus>
"<p>What class of Partial Differential Equations can be solved using the method of separation of variables?</p>
",<calculus>
"<p>There are so many available bases. Why is the strange number $e$ preferred over all else?</p>

<p>Of course one could integrate $\frac{1}x$ and see this. But is there more to the story?</p>
",<calculus>
"<p>There are quite simple, intuitive and straightforward expressions for evaluating the area or volume of a figure. But why is the expression for the length of a curve so complicated?</p>
",<calculus>
"<p>""Find the derivative of $y=x\sqrt{9-x}$.""</p>

<p>So this is what I have and now I'm stuck.</p>

<p>\begin{align}
y' &amp;= x \frac{d}{dx}\left[(9-x)^{1/2}\right] + (9-x)^{1/2} \frac{d}{dx}(x)\\
   &amp;= x \left[\frac{1}{2}(9-x)^{-1/2}\right] + (9-x)^{1/2} (1)
\end{align}</p>

<p>So I now that I need to multiply and simplify but I don't know where to start. Help!</p>

<p>This problem is actually part of a homework question where I have to analyze a graph and find critical points and min and max.</p>
",<calculus>
"<blockquote>
  <p>Does the graph of the function $f$ have tangent line at the given points? If yes, what is the tangent line?</p>
  
  <p>$f(x)=(x+2)^{3/5}$ at $x=-2$</p>
</blockquote>

<p>solution: yes, $x=-2$</p>

<p>The derivative I found:</p>

<p>$$f'(x)=\frac 3{5(x+2)^\frac 25}$$</p>

<p>and then I get $f'(-2)=\frac 3{5\cdot 0}=\frac 30$ which is undefined.</p>

<p>anyone know how to go about this problem? I got the slope or whatever to be undefined or something, any tips/solution appreciated!</p>
",<calculus>
"<p>The function has 2 parts: </p>

<p>$$f(x) = \begin{cases} -\sin x &amp; x \le 0 \\ 2x &amp; x &gt; 0\end{cases}$$</p>

<p>I need to calculate the integral between $-\pi$ and $2$.
So is the answer is an integral bewteen $-\pi$ and $0$ of $f(x)$ and then and $0$ to $2$.
but why the calculation of the first part of $-\pi$ and $2$ aire on $-\sin x$ and the second part of the intgral is on $x^2$, which is part of the $F(x)$.</p>

<p>I'd like to get some help over here, I'm lost </p>
",<calculus>
"<p>$$\int_{1}^{\infty} \frac{\ln{(2x-1)}}{x^2} dx$$</p>

<p>My approach is to calc
$$\int_{1}^{X} \frac{\ln{(2x-1)}}{x^2} dx$$ and then take the limit for the answer when $X \rightarrow \infty$</p>

<p>However, I must do something wrong. The correct answer should be $2\ln(2)$.</p>

<p>$$\int_{1}^{X} \frac{\ln{(2x-1)}}{x^2} dx = \left[-\frac{1}{x} \ln (2x-1) \right]_{1}^{X} + \int_{1}^{X} \frac{1}{x} \times \frac{2}{2x-1} dx = -\frac{1}{X}\ln(2X-1) + 2\int_{1}^{X} -\frac{1}{x} + \frac{2}{2x-1} dx = -1\frac{1}{X}\ln(2X-1)-2\ln X+2\ln(2X-1) $$</p>

<p>Am I wrong? If I'm not, how to proceed? </p>

<p>=== EDIT ===</p>

<p>After the edit I wonder if this is the correct way to proceed:</p>

<p>$$ - \frac{1}{X}\ln(2X-1)-2\ln X+2\ln(2X-1) $$ The first part will do to zero because of $\frac{1}{X} $ so we ignore that one, the second and third part: </p>

<p>$$ -2\ln X+2\ln(2X-1) = 2\ln \left( \frac{2X-1}{X}\right) = 2\ln \left( 2-\frac{1}{X}  \right) = 2\ln (2)$$</p>
",<calculus>
"<p>Which of the following statements are ??</p>

<p>a. Let $\phi$ be a non-negative and continuously differentiable function on $(0,\infty)$ such that $\phi'(x)\le\phi(x)$ for all $x$ $\in (0,\infty)$. Then </p>

<p>$$lim_{x\to \infty}\phi(x)=0$$ </p>

<p>b. Let $\phi$ be a non-negative function continuous on $[0,\infty)$ and differentiable on     $(0,\infty)$ such that $\phi(0)=0$ and such that   $\phi'(x)\le\phi(x)$ for all $x$ $\in   (0,\infty)$. Then $\phi=0$.</p>

<p>c. Let $\phi$ be a non-negative function continuous on $[0,\infty)$ and such that 
      $$\phi(x) \le \int_{0}^{x}\phi(t) dt$$ for all $x \in [0,\infty)$. Then $\phi=0$.</p>
",<calculus>
"<p>Give an $\epsilon$-$\delta$ proof to show that for any positive integer $n$ we have $\begin{align} \lim \frac{1}{x^n} =0 \end{align}$ as $x→\infty$.</p>

<p>How would you incorporate $\epsilon$-$\delta$ formalism to prove this limit?</p>

<p>Also on a side note, if one part of a limit of the function is approaching negative infinity, and on the right side of the limit it's a point, would it be considered as a infinite discontinuity?</p>
",<calculus>
"<p>You are in a directed weighted graph with $N$ $(63 \le N \le 10^6)$ vertices and $M$ $(1 \le N \le 10^6)$ edges and you want to get from $63^{rd}$ to $4^{th}$ vertex. Going through $i^{th}$ edge takes you $t_i$ hours $(1 \le t_i \le 24)$. A big monster wants to kill you. He appears in the $i^{th}$ edge $f_i$ times per 24 hours randomly. What is the lowest expected value of you meeting with monster?</p>

<p>Input: In the first line we've got $N$ and $M$. In the $M$ following lines there are descriptions of the edges. In each line there is 4 numbers - $a_i, b_i, t_i, f_i$. It means that $i^{th}$ edge is directed from $a_i$ to $b_i$.</p>

<p>Output: You should give one number. The lowest possible expected value of you meeting with monster.</p>

<p>Example:</p>

<p>Input:</p>

<pre><code>64 3
63 4 24 24
63 2 5 1
2 4 5 1
</code></pre>

<p>Output: </p>

<pre><code>0.416667
</code></pre>
",<graph-theory>
"<p>What are anonymous graphs, what is graph embeddedness, and how do they relate to each other? Very confused - I could not find short answer. Thanks.</p>
",<graph-theory>
"<p>I need to a way to express a change in the structure of a given graph <code>G</code>, such that the original graph is <code>G</code> and the changed graph is <code>G'</code>. A change in the graph can be the addition or removal of any number of vertices and/or edges.</p>

<p>Currently, my thought is to express the change as two percentages: the number of vertices in <code>G'</code> as a percentage of the number of vertices in <code>G</code>, and the same for edges. However, it occurs to me that removing a vertex also means removing all edges connected to it.</p>

<p>Are there any standard ways of quantifying the change in a graphs structure?</p>
",<graph-theory>
"<p>Define the Revision Tracking Graph (RTG), which is an oriented graph (without circles) where each node x has a set C(x) associated with it, which contains all edges leading into it on all paths from a node 0 (node with empty set). Each edge can be in a set exactly once!</p>

<p>You can also describe this data structure by the rules for its growth:</p>

<ol>
<li>Start with node 0 with associated empty set <code>C(0) = {}</code></li>
<li>For any node x create a new node y where (x,y) is oriented edge from x to y and 
<code>C(y) = C(x) union { (x,y) }</code></li>
<li>For any nodes f and t, create node r, where (f,r) and (t,r) are oriented edges and 
<code>C(r) = C(f) union C(t) union { (f,r), (t,r) }</code></li>
</ol>

<p>Those rules can be described by words as 1) creating new object 2) branching and versioning 3) merging. You can see that the only difference between branching and versioning is whether there already is an edge leading from a node or not. </p>

<p>You can further differentiate the graph by naming branches (paths in the graph).</p>

<p>Base node is defined as a node b, such as <code>C(b) = C(f) intersect C(t)</code>. For short: <code>Result=(From-Base)+To</code>. In other words, if you remove all edges in C(b) from C(f) and add edges from C(t), the resulting set would have every edge exactly once and all edges from C(f) and C(t) would be present in C(r).</p>

<p>I have actually several questions pertaining to this data structure:</p>

<blockquote>
  <ol>
  <li>Prove there are no circles in the graph</li>
  <li>Prove that for certain graphs there are such nodes F,T for which base cannot be found as specified in simple equation.</li>
  <li>Prove that for each graph and each pair of nodes F and T, there is a set of n pairs of nodes Fi,Bi where Fn = F and <code>R = T + Sum(i=1..n) of (Fi-Bi)</code>.</li>
  <li>Create algorithm to find B in simple case where n = 1.</li>
  <li>Create algorithm to find Fi,Bi for i=1..n where n > 1.</li>
  </ol>
</blockquote>

<p>I know the answers to 1,2 and 4, but I put them here to get you in the mood of working with this data structure. Have fun with it, pose additional problems and find answers. Any new answers could significantly advance the theory behind revision control systems.</p>
",<graph-theory>
"<p>Let $A_1, A_2, \ldots, A_{20}$ be twenty sets each of size $20$ such that $|A_i \cap A_j | \le 2$. Prove that they have a system of distinct representatives.</p>
",<graph-theory>
"<p>Prove that if $G$ is connected, not the complete graph and $\Delta(G) &gt; 2$ then the chromatic number of G is at most $\Delta(G)$.</p>
",<graph-theory>
"<p>An excercise says ""deduce Konig's Theorem on bipartite graphs from Dilworth's theorem on posets"". </p>

<p>Let G be a bipartite graph, G=(A,B). Order the edges left to right. A maximum antichain is a largest independent set in the graph. </p>

<p>I can see a maximum antichain must have every vertex in G incident with it. But it doesn't follow that every edge is incident with it. That may not be true. </p>

<p>So how does one proceed? </p>
",<graph-theory>
"<p>I am working on the following problem:</p>

<p>Suppose that $T$ is a spanning tree of a graph $G$, with an edge cost function $c$.  Let $T$ have the <em>cycle property</em> if for any edge $e' \not \in T, c(e') \geq c(e)$ for all $e$ in the cycle generated by adding $e'$ to $T$.  Let  $T$ have the <em>cut property</em> if for any edge $e \in T$, $c(e) \leq c(e')$ for all $e'$ in the cut defined by $e$</p>

<p>Show that (i), (ii), and (iii) are equivalent, where: (i) T has the cycle property, (ii) T has the cut property, and (iii) T is a minimum cost spanning tree.</p>

<p>I believe that to show that (iii) implies (i), we suppose otherwise, and then show that this would give a cycle with an edge that can replace another edge in T and that is cheaper, whence we have a contradiction.  Similarly, I believe to show that (iii) implies (ii), we similarly suppose otherwise, and then show that this would give a cut with an edge that can replace another edge in T and that is cheaper, whence a contradiction.</p>

<p>However, I am not sure how to prove the other implications needed for this problem.  My feeling is to somehow use a similar argument to what I listed, but ""in reverse"".</p>

<p>Any help with this problem would be greatly appreciated.  Thank you very much.</p>
",<graph-theory>
"<p>I'm looking for a simple (or better yet, minimal) example of a planar triangulation of which dual graph (that of the faces) would be ""obviously"" non-Hamiltonian.</p>

<p>(NB: I once asked the same question for planar triangulations, but I'm now interested in their dual graphs.)</p>

<p>Thanks in advance!</p>
",<graph-theory>
"<p>I know that the answer to this question is given using the following regression formula:
$$f(n)=(n-1)f(n-1)+\binom {n-1}{2}f(n-3)$$</p>

<p>The first part of the right-hand side is true when a node is added somewhere between two nodes, however I'm having trouble understanding why the second part is true.</p>
",<graph-theory>
"<p>If two graphs are isomorphic, and one has a simple circuit of a particular length, must the other graph also have a circuit of the same length?</p>

<p>Do they also have to have the same number of such circuits? </p>

<p>In the book that I use it is claimed that the following two graphs are isomorphic:</p>

<p><img src=""http://i.stack.imgur.com/MoFPc.png"" alt=""enter image description here""></p>

<p>Graph G has two circuits of length 5, but no circuits of length 4. Graph H has one circuit of length 5 and one circuit of length 4. Doesn't this mean that they are not isomorphic?</p>

<p>Edit: The book shows their isomorphism with the function f, where f(u1)=v6, f(u2)=v3, f(u3)=v4, f(u4)=v5, f(u5)=v1, f(u6)=v2. Doesn't this prove that they are isomorphic?</p>
",<graph-theory>
"<p>Is there a natural to other than 0 such that in the diagram of Hasse of natural with divisibility there is a natural between a and 0?</p>
",<graph-theory>
"<p>I'm looking for a way to generate random connected directed acyclic graphs, where I can specify the number of vertices that have no outgoing edges (leaf vertices).</p>

<p>Anyone ever seen such a thing, or know how to generate such graphs?</p>

<p>Thanks.</p>
",<graph-theory>
"<p>Let $d_1$, $d_2$, ..., $d_n$ be positive integers. Let $B$ be the $n \times n$ matrix
$$\begin{pmatrix}
d_1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 \\
1 &amp; d_2 &amp; 1 &amp; \cdots &amp; 1 \\
1 &amp; 1 &amp; d_3 &amp; \cdots &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 1 &amp; 1 &amp; \cdots &amp; d_n \end{pmatrix}.$$
When does $B$ have a square root in $\mathrm{Mat}_n(\mathbb{Z})$?</p>

<p>Motivation: The <a href=""http://en.wikipedia.org/wiki/Friendship_graph"" rel=""nofollow"">Friendship Theorem</a> states that the only graph in which every pair of vertices is joined by a path of length $2$ is the ""Friendship Graph"", which you can see at the linked article. If $A$ is the adjacency matrix of such a graph, with degree sequence $(d_1, d_2, \ldots, d_n)$, then $A^2=B$. So this contributes the solution $(d_1, d_2, \ldots, d_n) = (2,2,2,\ldots,2,2m)$, with $n=2m+1$.</p>

<p>I was preparing notes on the friendship theorem and got distracted by trying to figure out when this matrix has an integer square root at all. It seemed like it might make a nice challenge for here.</p>
",<graph-theory>
"<p>I am having trouble understanding and producing integer linear programming formulations for combinatorial optimisation problems.</p>

<p>I can understand basic ones like the knapsack problem:</p>

<p>$min \quad \sum_{i \in I} v_i x_i$<br>
$s.t.$<br>
$\qquad \sum_{i \in I} w_i x_i \leq K$<br>
$\qquad x_i \in \{0, 1\} \qquad (i \in I)$</p>

<p>however when it comes to something like this formulation for the steiner minimal tree problem:</p>

<p>$min \quad \sum_{[i,j]\in E} d_{ij}$<br>
$s.t.$<br>
$\qquad d_{ij} \leq \| a^i-x^j\| - M(1-y_{ij}), \quad [i,j] \in E_1,$<br>
$\qquad d_{ij} \leq \| x^i-x^j\| - M(1-y_{ij}), \quad [i,j] \in E_2,$<br>
$\qquad d_{ij} \leq 0, \hspace{105pt}  [i,j] \in E_3,$<br>
$\qquad \sum_{j\in S} y_{ij} = 1, \hspace{82pt} i \in P,$<br>
$\qquad \sum_{i &lt; j,i \in S} y_{ij} = 1, \hspace{68pt} j \in S - \{p+1\},$<br>
$\qquad y_{ij} \in \{0,1\}, \hspace{87pt} [i,j] \in E,$<br>
$\qquad d_{ij} \in \mathbb{R}, \hspace{104pt} [i,j] \in E,$<br>
$\qquad x^i \in \mathbb{R}, \hspace{105pt} i \in S.$  </p>

<p>I have trouble working out what each of the constraints represent.</p>

<p>I am not asking for someone to explicitly explain that particular problem (although that would be a good help) I am more asking for advice on good ways to increase my understanding ILP formulations for combinatorial optimisation problems - especially for graph theory related problems.</p>

<p>Are there any good resources that people could recommend that explain the basics well with some good worked examples?</p>

<p>I'm sure I will get it eventually, it feels like the sort of thing that takes a while to understand and then it just ""clicks"" all of a sudden; I just need some help getting it to ""click""!</p>

<p>Any help would be greatly appreciated.</p>
",<graph-theory>
"<p>How many paths <em>starting from a given node</em> touch each node a given number of times?</p>

<p>We have a complete graph with vertices $1,2,3…j$. We want to know the number of paths of length $N$, starting from vertex $1$, such that vertex $i$ appears $k_i$ times for $1≤i≤j$. $N$ is equal to the sum of the $k_i$'s.</p>

<p>For the case with arbitrary initial vertex, the answer has been provided elsewhere on this forum (see <a href=""http://math.stackexchange.com/questions/129451/find-the-number-of-arrangements-of-k-mbox-1s-k-mbox-2s-cdots/129802#129802"">Find the number of arrangements of $k \mbox{  }1&#39;$s, $k \mbox{  }2&#39;$s, $\cdots, k \mbox{  }n&#39;$s - total $kn$ cards.</a>) </p>
",<graph-theory>
"<p><em><a href=""http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.4863"" rel=""nofollow"">Cai-Furer-Immerman </a> showed that the W-L(Weisfeiler-Lehman )
hierarchy cannot distinguish general graphs except at linear dimension.</em></p>

<p>Even besides CFI's result, there is good reason to believe that purely
combinatorial methods cannot work for general graphs. Even for the
""bounded parameter"" families for which we have polynomial-time
algorithms, the group-theory method is generally required. </p>

<p>Furthermore,
the ""algebraic structure"" of the GI problem seems essential to its
potential tractability, since every generalization of GI that destroys
the algebraic structure is NP-complete.</p>

<p><strong>But I can not find any error in <a href=""https://www.researchgate.net/publication/296704876_An_Ofracnlog_2n2log_2n2_Graph_Isomorphism_Algorithm_for_a_Restricted_Class_of_Graph"" rel=""nofollow"">this quasi-polynomial claim</a>, which proposition is wrong?</strong></p>

<p>The algorithm does not depend on W-L method entirely though .</p>

<p>This post is motivated by <a href=""http://cstheory.stackexchange.com/questions/32237/is-anyone-aware-of-a-counter-example-to-the-dharwadker-tevet-graph-isomorphism-a"">this query</a> .</p>

<p>Link of the claim :<a href=""https://www.researchgate.net/publication/296704876_An_Ofracnlog_2n2log_2n2_Graph_Isomorphism_Algorithm_for_a_Restricted_Class_of_Graph"" rel=""nofollow"">'Quasi polynomial claim for a restricted class Graph Isomorphism'</a></p>
",<graph-theory>
"<p>suppose that $G$,$H$,$K$ are connected graphs with at least two vertices,prove that the only solution of the equation $L(G)=H \square K$ is $K=K_n$ ,$H=K_m$ and $G=K_{m,n}$.</p>

<p>because the eigenvalue of any line graph $L(G)$ is not fewer than -2 (as a theorem),if we suppose that $K$ and $H$ will be some thing else,some how I must make a contradiction. but I can't make it.</p>

<p>I don't what to do,any hint or guidance me to be in right way will be great ,thanks.</p>
",<graph-theory>
"<p>So while going through the <a href=""https://www.dropbox.com/s/428pyq6lek1tfax/WeightedGraphMatchingApproach.pdf?dl=0"" rel=""nofollow"">paper</a> H. A. Almohamad and S. O. Duffuaa: <em><a href=""https://www.google.com/search?q=%22linear%20programming%20approach%22%20%22weighted%20graph%20matching%22"" rel=""nofollow"">A Linear Programming Approach for the Weighted Graph Matching Problem</a></em>, <a href=""http://dx.doi.org/10.1109/34.211474"" rel=""nofollow"">DOI:10.1109/34.211474</a>. I came across the equation
<a href=""http://i.stack.imgur.com/N7LZX.png"" rel=""nofollow"">Equation (7) being transformed to Equation (10).</a> I don't seem to know how they achieved that and what it means.</p>

<p>$$R=A_GP-PA_H \tag{7}$$</p>

<p>$$VEC(R)=A_{GH} VEC(P) \tag{10}$$</p>

<p></p>

<p>P- is an orthogonal matrix $n \times n$
<br/>A<sub>G</sub> and A<sub>H</sub> are Adjacency matrices</p>
",<graph-theory>
"<p>I've got this problem on my Graph algorithms exam and I still can't solve it!Here is the problem:
At first there are 100 people sitting at a round table and neither one is  enemies with their neighbor. Than first night comes and each person becomes an enemy with one of his neighbor.How many nights have to pass so that there is no more way that one can be sat down so one doesn't sit beside his neighbor?</p>
",<graph-theory>
"<p>If we use the standard definition of a path in a graph, is it possible that there exists an infinite path in a finite graph?</p>
",<graph-theory>
"<p>I've read an article about a comparison between human brain and artificial neural networks. It's said that human brain contains $\approx 10^{11}$ neurons and each neuron is connected to $\approx 10^4$ others! Is it possible? What kind of graph would it be?</p>
",<graph-theory>
"<p>I feel like it is, but couldn't find anything online to support it - suppose the set A={a,b,c} and the relation set R={(a,b),(b,c),(c,a)}, would the transitive closure be reflexive (ie contain (a,a), (b,b) and (c,c)) since you can go from a back to a going all the way around?</p>

<p>Thanks in advance.</p>
",<graph-theory>
"<p>If G is a simple graph containing exactly two components H and H', show that $$|E(G)| \le \frac{(|V(G)|-1)(|V(G)|-2)}{2}$$</p>

<p>Here is my (incomplete) proof that I need help with:<br>
 1. Since H and H' are components of G, $E(H) \cap E(H') = \emptyset$ and $V(H) \cap V(H') = \emptyset$.<br>
 2. Therefore $|E(G)| = |E(H)| + |E(H')|$ and $|V(G)| = |V(H)| + |V(H')|$.<br>
 3. Suppose H and H' are complete subgraphs. Then $|E(H)| = C(|V(H)|, 2) = \frac{(|V(H)|)(|V(H)|-1)}{2}$.<br>
4. Similarly, $|E(H')| = \frac{(|V(H')|)(|V(H')|-1)}{2}$.<br>
5. Let $|V(H)| = a$ and $|V(H')| = b$. Then, $|E(H)| = \frac{a(a-1)}{2}$ and $|E(H')| = \frac{b(b-1)}{2}$ and $|V(G)| = a + b$.<br>
6. $Max |E(G)| = |E(H)| + |E(H')| = \frac{a(a-1) + b(b-1)}{2}$.<br>
7. Therefore, $|E(G)| \le \frac{a(a-1) + b(b-1)}{2}$.<br>
8. ...</p>

<p>How do I show that $\frac{a(a-1) + b(b-1)}{2} \le \frac{(a+b-1)(a+b-2)}{2}$ for all $a, b \ge 1$? If I can do that, I can show that $|E(G)| \le \frac{(a+b-1)(a+b-2)}{2}$ and thus $|E(G)| \le \frac{(|V(G)|-1)(|V(G)|-2)}{2}$.</p>

<p>EDIT: Is my approach correct? I am also open to any alternative methods of proof for this question. </p>
",<graph-theory>
"<p>Find a connected graph whose automorphism group has size 3.</p>

<p>Note: I know such graph must be non-simple.</p>
",<graph-theory>
"<p>Let p1, p2, . . . , pn be n points in the plane such that the distance between any two points is
at least one. Let G = (V, E) be the graph such that V = {p1, p2, . . . , pn} and E = {pipj
|
distance between pi and pj
is exactly one}. Show that ∆(G) = 6.</p>
",<graph-theory>
"<p>What is the method of proof to show that a graph has a certain topological minor? I am in a rigorous second discrete math class where we were given this question:</p>

<p><strong>Prove or disprove: If G is bipartite and does not have K3,3 as
a topological minor, then G is planar.</strong></p>

<p>My idea was to show that G does not have K5 as a topological minor, then invoke Kuratowski's Theorem. The problem was I could not 100% think of a way to show that G has the K5 topological minor since we never went over a way of proof in lecture. Any help would be great!</p>
",<graph-theory>
"<p>How many induced graphs has a graph with $n$ vertices?</p>

<p>I think that there are $2^n=\sum_{k=0}^n \binom{n}{k}$.Is this correct?</p>
",<graph-theory>
"<p>Arrange $2^{n-1}-1$ zeroes and $2^{n-1}$ ones in a balanced full binary tree of depth $n$. If we want the number of edges that connect the same (and respectively different) digits are the same, then one claims that the root of this tree has to be a one. Why is that?</p>

<p>For example, if $n=2$, then we need to arrange 1 zero and 2 ones. One arrangement that makes the number of edges that connect the same digits (which in this case is only one: the edge with a ""+"" who connects 2 ones,) and the number of edges that connect two different digits (which in this case is also one: the edge with a ""-"") are the same. Note that the root of this tree is unexceptionally 1.</p>

<pre><code>      1
    /   \
 + /     \ -
  /       \
 1         0
</code></pre>

<p>And here is a case where $n=3$.</p>

<pre><code>          1
        /   \
     + /     \ -
      /       \
     1         0
  + / \ -   + / \ -
   /   \     /   \
  1     0   0     1
</code></pre>

<p>Again, we see that the root is a one. If we change the root to a zero, however, then we can never find an arrangement that makes the number of ""+"" edges equals to the number of ""-"" edges. Why is that?</p>
",<graph-theory>
"<p>I am trying to show that a graph is planar. Possibly the simplest method I have found is to show the graph can be drawn on a page (i.e. in the plane) without any edges crossing. So, can I assume that if I am given a graph that has edges crossing I can simple move the vertices around to obtain a version such that the edges are not crossing (if such an arrangement exists)?</p>

<p>It may be better to show an example of what I am thinking. Perhaps someone can confirm that what I have done is permutable. Refer to the figure below were I start with the left graph and end with the right graph.</p>

<p><a href=""http://i.stack.imgur.com/EnE2d.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/EnE2d.jpg"" alt=""enter image description here""></a></p>

<p>Alternatively, it seems as though one can show a graph is planar if it can be embedded in a disk. I believe the following figure shows such an embedding in a disk</p>

<p><a href=""http://i.stack.imgur.com/euukC.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/euukC.png"" alt=""enter image description here""></a></p>
",<graph-theory>
"<p>If I take a triangle-free graph, By <a href=""http://en.wikipedia.org/wiki/Gr%C3%B6tzsch%27s_theorem"" rel=""nofollow"">Grötzsch's theorem</a>, it should be 3-colorable, and I have already had a coloring. If I change the color of one vertex, and I want to check how many vertices (at least) do I need to change, and I cannot find an idea on writing an algorithm to find out. I know that by symmetry, I can exchange two colors, I will get a new coloring method, it gives an upper bound. But it cannot give me any idea about the algorithm. </p>

<p>Is there any hint or literature to help me? Thanks for your attention!</p>
",<graph-theory>
"<p>I have read proofs and descriptions stating that a planar connected graph have the Euler characteristic 2. I'm not sure if that statement is equivalent to ""a connected graph with the Euler characteristic 2 have a planar embedding""?</p>

<p>I'm studying a math course where I'm supposed to show that a certain graph have a planar embedding, and I'm wondering if I can show it by using the Euler characteristics.</p>
",<graph-theory>
"<p>Consider a graph that we know is completely connected and has a Hamiltonian cycle. Can we say that it is two colorable because we visit each vertex once and alternate colors as we walk the cycle? Or is this there something that I'm overlooking?</p>
",<graph-theory>
"<p>Suppose we have a finite simple graph $G$. Call $\mathcal O$ the set of graphs without isolated vertices up to isomorphism whose line graph is isomorphic to $G$. Can $\mathcal O$ contain more than one element?</p>
",<graph-theory>
"<p>How to prove inductively the total number of paths from the root to all leaves in a given tree? </p>

<p>From what I understand, one should show how to find the number of paths to a specific leaf, then use induction to find the total number of paths to all leaves. However, I am not quite sure how to show the this step mathematically. </p>
",<graph-theory>
"<p>From an array of u rows and v columns ,we can derive a directed graph G = (N,A), where each node i elementof(N) is a pair [h(i), k(i)] where h(i) indicates the row and k(i) indicates the column of the element represented by i. The number of nodes is n = uv. An arc (i,j) elementof(A) exists if k(j) = k(i)+1 and h(j) =h(i) + alpha, where alpha = −1; 0; 1 but the extreme cases in which h(i) = 1 or = u; the cost of arc (i, j) is set to the entry associated to the pair [h(i), k(i)], i.e. all the arcs leaving node i have the same cost, and it is the value at position [h(i), k(i)]. With this transformation the shortest path problem on the grid is mapped into a shortest path problem on a classical graph. By using this transformation, we can see that our problem has another special characteristic: the graph G is a stable acyclic sequential layered graph. </p>

<p>this is taken from Circular Shortest Path on Regular Grids by Changming Sun Stefano Pallottino</p>

<p>what is alpha here? how to represent the adjacency matrix of grid graph?</p>
",<graph-theory>
"<p>Would you do it by showing that elementary matrix operations can be used to get from one matrix to the other? If not, how would you show that 2 adjacency matrices are isomorphic?</p>
",<graph-theory>
"<p>I'm struggling with the following question:</p>

<blockquote>
  <p>Prove that every Eulerian graph of odd order has three vertices of the same degree.</p>
</blockquote>

<p>I'm not sure how to proceed with this. If someone could give me a boost it would be appreciated.</p>
",<graph-theory>
"<p>I have been recently reading about Cayley graphs and character theory. It is evident that Cayley graphs are very useful tool in theoretical computer science. In physics, Cayley graphs seem do appear in the study of quantum walks. I wonder however, if they have been used anywhere else in physics, specially in the study of the spectral properties of physical systems. Any references will be helpful.</p>

<p>Thanks in advance.</p>
",<graph-theory>
"<p>I have a research question that involves human subjects being sorted into groups before playing a social game. </p>

<p>Before sorting, each person decides on their preferred group size, from 1 to n; where n is the total number of players.</p>

<p>I want to be able to sort people into groups of their preferred size, allowing a given level of tolerance for being in a group size that differs from the preferred group size. </p>

<p>For example, in the simplest case we allow no tolerance.
Consider the set of group size preferences for a population of 25 players</p>

<blockquote>
  <p>G = [1 1 1 1 2 2 2 3 3 3 3 3 3 3 3 4 4 4 4 4 4 5 5 5 5].</p>
</blockquote>

<p>This will result in the following sorting:</p>

<blockquote>
  <p>Group a: [ 1 ]</p>
  
  <p>Group b: [ 1 ] </p>
  
  <p>Group c: [ 1 ]</p>
  
  <p>Group d: [ 1 ]</p>
  
  <p>Group e: [ 2 2 ]</p>
  
  <p>Group f: [ 3 3 3 ]</p>
  
  <p>Group g: [ 3 3 3 ]</p>
  
  <p>Group h: [ 4 4 4 4 ]</p>
  
  <p>and the group of remainders: [ 2 3 3 4 4 5 5 5 5 ].</p>
</blockquote>

<p>Note how group size equals group size preference and the remainders are all those individuals that could not be put into a group of preferred size.</p>

<p>I want to be able to extend this by allowing some level of tolerance in acceptable group size compared to the preference. For instance, an individual that prefers to be in a group of 3, with tolerance of 1, would be willing to be in any group of size 2-4, but would otherwise be a remainder. However, being in a group of preferred size should have more weight than being assigned to a different group.</p>

<p>Can this problem be solved to minimise:
a.) the number of individuals that are left as a remainder
b.) the number of people that are put into groups different to their preference
c.) the level of tolerance?</p>

<p>Even pointing to similar problems would be much appreciated. </p>

<p>Thank you for any assistance!</p>
",<graph-theory>
"<p>A non-closed path is chosen at random on the complete graph K9. All
paths are equally likely. What is the probability that the path contains
the edges {23} and {34} given that it is length 6? Given that it has
the edge {89}?</p>
",<graph-theory>
"<p>I need to prove that in a planar graph with more than two vertexes at least a vertex with a maximum grade of five exists.</p>

<p>I know that planar graphs fulfill the relation $\#E \le 3\#V-6$, and relation $\#E \le 2\#V-4$ if there is no subgraph that is isomorphic to $K_{3}$. $\#E$ being the number of edges and $\#V = n$ being the number of vertexes.</p>

<p>Based on the statement that says $\sum_{i=1}^{n}gr(v_{i}) = 2\#E$ I have come up that for complete graphs $K_{n}$ the next relation must be fulfilled: $$n^{2}-4n+6\le 0$$ But I can not figure how to prove what it is asked to prove </p>
",<graph-theory>
"<p>All the citizens of a country,of which the alphabet has $10$ letters have as a name a word of length exactly $6$.
The number of the citizens is $10^6$ and all of them have different names.If $G$ is the graph with vertices the citizens and $2$ citizens are connected with an edge if and only if their names differ exactly at one position,which is the diameter of $G$ ? </p>
",<graph-theory>
"<p>Hi I saw in an R forum the <a href=""https://stat.ethz.ch/pipermail/r-help/2011-February/268569.html"" rel=""nofollow"">answer</a>: </p>

<p>“If the graph has n nodes and is represented by an adjacency matrix, you can square the matrix (log_2 n)+1 times. Then you can multiply the matrix element-wise by its transpose. The positive entries in the 7th row will tell you all nodes sharing a cycle with node 7.  This assumes all edge weights are positive.""</p>

<p>I’m a PhD student working on my research and I need to check for cycles in a directed graph to make sure it is a DAG. The answer given is extremely useful but I need the theorem statement, or a reference. Does anyone has a book reference where this is stated or a paper?</p>

<p>Thanks!</p>

<p>Daniela.</p>

<p>PS Unfortunately the people from the R forum didn't let me to ask the question there</p>
",<graph-theory>
"<p>Let $S_1$ and $S_2$ be two minimal separators of a graph $G$ such that $S_1\cap S_2 \neq \phi$. Then is it true that $S_1 \cap S_2$ is also a minimal seperator. </p>
",<graph-theory>
"<p>So I was given this question that asks: Given a simple graph with $n = 4k + 2$ vertices. Can the vertices of
this graph have distinct degrees?</p>

<p>I was wondering how I would go about this. I am usually provided a graph as a visual but I am confused with this question</p>
",<graph-theory>
"<p>In advance I apologise for my lack of Mathematical knowledge and low quality question: I'm not trained as a mathematician, nor do I have a substantial knowledge of mathematical terminology. However, I'm eager to put my programming lines into a mathematical framework, so that's why I ended up here. </p>

<p>My problem is the following:
Consider a semi-random network of $N$ nodes $i$ and $M$ connections, the structure of the network does not really matter here. Now I have defined the degree $k$ of a node $i$ as
$$
    k_i=\sum\limits_j A_{ij}
$$
with $A_{ij}$ the adjacency matrix defining the connections between nodes in the network and summing over all nodes $N$.  </p>

<p>Now I want to define hubs in the network using a hub threshold $k_h$. I actually want to <em>count</em> the number of hubs in my network with respect to the total number of nodes $N$ (let's call this $H$), so far I've come up with this:
$$
H=\frac{1}{N}\sum\limits_i \sum\limits_{k_i&gt;k_h} i
$$
But I don't think this gives me the right answer, since say that node 10 and 11 are hub nodes, the answer would be $21/N$ instead of $2/N$. I feel like I'm very close, but I can't see it. How should I state it?</p>

<p>Many thanks!</p>
",<graph-theory>
"<p>Let $X_1, \ldots, X_n$ be a collection of random variables. Consider the directed graph with vertex set $\{ 1, 2, \ldots, n \}$ where there is a directed edge $i \to j$ if $\mathbb{P}(X_i &gt; X_j) &gt; \frac{1}{2}$. </p>

<p><strong>Question 1:</strong> What directed graphs can arise in this way? Certainly they must be simple and have no loops. Is that the only restriction? Alternatively, $\mathbb{P}(X_i &gt; X_j) &gt; \frac{1}{2}$ defines an irreflexive antisymmetric relation on $\{ 1, 2, ... n \}$. Which such relations arise in this way? </p>

<p><strong>Question 2:</strong> Does the answer change if we require the $X_i$ to all be defined on a finite sample space? </p>

<p><strong>Question 3:</strong> What if we require the $X_i$ to be independent? </p>

<p>It is known (see <a href=""http://en.wikipedia.org/wiki/Nontransitive_dice"">nontransitive dice</a>) that this graph can have directed cycles even if the $X_i$ are independent; in particular, the corresponding relation need not be transitive. </p>
",<graph-theory>
"<p>I have the following graph theory question that I am stuck on:</p>

<p>Prove or disprove:
For every graph G and every integer $r \geq \text{max} \{\text{deg}v: v \in V(G) \}$ , there is an r-regular graph H containing G as an induced subgraph. Thanks for any help.</p>
",<graph-theory>
"<p>Can somebody explain why there cannot be any triangles or squares in a Moore graph with diameter 2? This was stated without proof in my class. </p>
",<graph-theory>
"<p>Like everybody on this website it seems, I have a traveling salesman problem. But the traveler wants to visit tunnels, so his exit points are not the entry points, he has to visit all of them, and his travel distance between tunnels has to be minimized. The tunnels are one way, he doesn't get to choose the travel direction.</p>

<p>Is there an academical name for this variant? some google keywords? </p>
",<graph-theory>
"<p>I'm currently reading a proof of the following claim from the notes <a href=""http://www.cs.berkeley.edu/~sinclair/cs271/n5.pdf"" rel=""nofollow"">http://www.cs.berkeley.edu/~sinclair/cs271/n5.pdf</a> which can be found on the bottom of page 6. I'd like to point out i'm interested in the random algorithmic aspects of the notes and have never studied Boolean functions before.</p>

<p>Claim 5.4: Almost all $n$-input Boolean functions require circuits of size at least $2^{n}/n$</p>

<p>The proof goes as follows: </p>

<p>Proof: (sketch) Let $N(S)$ be the number of circuits of size $S$. To bound $N(S)$, note that each gate has two
possible inputs and $16$ functions, so there are $16\times S^{2}$
choices for a single gate, so the number of circuits is
$N(S) \leq (16 ∗ S^{2})^{S}$. There are $2^{2^{n}}$
different functions with $n$ inputs. A calculation shows that if $S &lt; 2
^{n}/n$,
then $N(S)/2^{2^{n}} \rightarrow 0$
as $n \rightarrow \infty$.</p>

<p>Questions:</p>

<p>1) (NOT REGARDING PROOF) Firstly they mention briefly what a gate is but can every boolean function be represented using a series of gates? On page 6 they seem to mention a boolean function has $n$ inputs ($x_{1},...,x_{n}$) and $1$ output, and then just jump to a notion of a gate without really linking them together.  </p>

<p>2) (REGARDING PROOF) I understand since each gate has two inputs for example $x_{2},x_{5}$ and $|\{0,1\}^{2}|=4$ each of these pairs can be assigned two values $0$ or $1$ and so there are 16 possible functions. How did they get $16 \times S^{2}$ possible choices for each gate and more importantly what do they mean here by choice?</p>

<p>3) (REGARDING PROOF) How does the deduction that if $S&lt;2^{n}/n$ then then $N(S)/2^{2^{n}} \rightarrow 0$
as $n \rightarrow \infty$ complete the proof?</p>

<p>I really appreciate any help. </p>
",<graph-theory>
"<p>Prove or disprove: There exists an integer k such that every k-connected graph is hamiltonian.</p>
",<graph-theory>
"<p>$G$ is a connected graph. Show that if $A,B$ are bonds, and $e \in A \cap B$, there is a bond $C \subseteq A \cup B \setminus \{e\}$</p>

<p>A bond is a set of edges $X$ so that $G \setminus X$ has two connectivity components.</p>

<p>Any hints and assistance would be much appreciated!  </p>
",<graph-theory>
"<blockquote>
  <p>If $G$ is a connected finite graph which has no triangles, and $G$ has the property that if two vertices have a common neighbour then they have exactly two common neighbours, does $G$ have to be strongly regular?</p>
</blockquote>

<p><strong>Note:</strong> I showed it is regular, but how to prove/disprove it's strongly regular?</p>
",<graph-theory>
"<p>There is a transitive tournament (T) with $n &gt; 1$  vertices and the  score sequence is defined as $s_1, s_2, \ldots,s_n$</p>

<p>Prove that T is transitive if and only if the score of the $k^{th}$ vertex ($s_k$) = '$k-1$' for $k =1,2,\ldots. n $ </p>

<p>i.e $s_k = k-1$ for all $k= 1,2,3,\ldots, n$</p>

<p>I have been researching and noticed that this is similar to the Landau's Theorem (1953), however i am completely lost with the proof. I know that thew score is the out degree, and i know that the out degree is the total number of edges coming out of the vertex (n-1) minus the in-degree , which is of course also (0,1,2,3...n-1). </p>

<p>I know that a tournament must have a unique ranking, and i also know that at transitive  tournament must have at least one in degree on 0 .</p>

<p>I also know that a transitive tournament must have an in degree sequence of $(0,1,2,3,...n-1)$, and obviously the out degree is the number of edges minus that. </p>

<p>I have tried going along the lines of proof by contradiction, but i am completely stuck </p>

<p>I would really like to find a proof for this, please help! </p>
",<graph-theory>
"<p>Is there an elementary (no consideration of root systems involved) proof of the fact that the graph of an finite coxeter system doesn't entail any cycle? I got as far as this: If there were any cycle $s_1,\dots,s_p$, consider the element $s_1\dots s_p$. It's probably going to be of infinite order (but I can't say why) and therefore the coxeter group isn't finite. Alternatively, $(s_1\dots s_p)^r$ could yield to elements of increasing length contradicting the fact that finite coxeter groups entail an element of maximal length.</p>
",<graph-theory>
"<p>Prove that in a group of 60 people one can always find two people with even number of common acquaintances. </p>

<p>I just want a small hint to this problem , not a full solution .</p>
",<graph-theory>
"<p>A coherent graph with at least $3$ vertices is called doubled coherent,if it remains coherent even if we delete any edge.How many,at least,edges do such a graph with $n$ vertices has to have?</p>

<p>I think that it should have at least $n$ edges,or am I wrong?</p>
",<graph-theory>
"<p>We say that a graph $G$ is distributed with $\mathcal{G}_{n,p}$ if it is a graph on $n$ vertices, and for which each of the ${n\choose 2}$ possible edges is chosen independently of the other edges and with probability $p$.</p>

<p>A <em>monotone property</em> $P$ of a graph is a set of graphs (on $n$ vertices) that is closed from above (that is, if $G\in P$ and $G\subseteq H$ then $H\in P$.</p>

<p>A function $f(n)$ is said to be a <em>threshold</em> for a property $P$ if for any $p(n)=\omega(f(n))$, $G\sim\mathcal{G}_{n,p}$ has $P$ asymptotically almost surely (a.a.s.), and for any $p(n)=o(f(n))$, $G\sim\mathcal{G}_{n,p}$ does not have $P$ a.a.s.</p>

<p>For example, if $P$ is ""has a triangle as a subgraph"", then $P$ is clearly monotone, and $f(n)=n^{-1}$ is a threshold for $P$. $f(n)=\frac{\ln{n}+\ln\ln{n}}{n}$ is a threshold for the Hamiltonicity property (in a stronger sense).</p>

<p><strong>My question is this:</strong> what are the thresholds for the properties of having ""quite short"" paths or cycles? By ""quite short"" I mean of length $\Theta(n^\varepsilon)$ for some $0&lt;\varepsilon&lt;1$, or of length $\Theta(\ln{n})$.</p>
",<graph-theory>
"<blockquote>
  <p>Thomassen ($1983$): Prove that there exists a function $f : N → N$ such
  that every graph of minimum degree at least $3$ and girth at least
  <em>f(r)</em> has a K$_{r}$ minor, for every $r ∈ N$.</p>
</blockquote>

<p>So my book says that the proof results from theses two Lemmas (<strong>BOLDED</strong>), but I am having a hard time seeing the connections. </p>

<p>I just need to know/understand how it results from the two lemmas (I want to say this problem might just be some simple algebraic manipulation of the equations to find the functions, but I am having a hard time figuring it out. Maybe solve for a $k$ in terms of $r$ then plug it in somewhere?)</p>

<p>This is from the book:</p>

<blockquote>
  <p>""There exists a function $f: N → N$ such that every graph of minimum
  degree at least $3$ and girth at least $f(r)$ has a $K_r$ minor, for all
  <em>r∈ N</em>. </p>
  
  <p>Proof. </p>
  
  <p>We prove the theorem with $f(r) := 8 log r + 4 log(log r)+ c $, for some constant $c∈R$. Let $k = k(r)∈N$ be minimal with $3·2k$
  $≥ c′r\sqrt{logr}$, where $c′∈R$ is the constant from Theorem $7.2.3.$ 
  Then for a suitable constant $c∈R$ we have $8k + 3 ≥ 8 log r +$
  $4log(log &gt; r)+ c$, 
  and the <strong>result follows by Lemma $1$ and $2$</strong> (below).""</p>
  
  <p>$1.$ <em>(Kostochka $1982$)</em> There exists a constant $c∈R$ such that for every $r ∈ N$, every graph $G$ of average degree $d(G) ≥ c ·r\sqrt{logr}$ contains a K$_r$ as a minor. Up to the value of $c$, this bound is best
  possible as a function of $r$.</p>
  
  <p>$2.$ Let $d, k ∈ N$ with $d≥3$ and let $G$ be a graph of minimum degree $\delta(G)≥d$ and girth $g(G)≥ 8k+3$. Then $G$ has a minor $H$ of minimum
  $\delta(H) ≥ \{d(d − 1)\}^k$.</p>
</blockquote>
",<graph-theory>
"<p>Given two points $X,Y$ on two sides of square $[0,1]\times [0,1]$ ($X:(0,1/2),Y:(1,1/2)$ (PS: My original question is $X,Y$ on opposite of a square, but I think that's not the real case) )and $n$ points distributed uniformly(i.i.d) in the square (where $n$ is large, and $A$ denotes the set of $n$ points), can I caluculate the asymptotic behavior of the value $M(n)$, where $M$ is defined as
$$M(n)=E\left[\min_{B\subset A} \sum_{k=1}^{|B|+1} d(B_k,B_{k-1})^2\right]$$
where $B_k$ is the $k$th element of $B$,$B_0=X,B_{m+1}=Y$(We let $m=|B|$), and the expected value is taken over all the possible $A$ . That is to say, I would like to compute the expected value of the minimal weight defined as sum of the square of distance.</p>

<p>I know that when $n\to\infty,M(n)\to 0$. And in the $1$-dim case, this is easy, since it is only a Poisson process, and the distance between two consecutive points are surely exponential distribution.(Calculation suggests it's about $(n+3)/((n+1)(n+2))$,where $n$ is number of points added) But in the two dimensional case, I got stuck and don't know how to tackle it. This is a problem arouse from the calculation of the cost of a network. Any hint or reference are welcomed, Thanks!</p>

<p>(Some computer experiment suggests that the weight is about $\approx 1.1/\sqrt{n}=O(1/\sqrt{n})$. I also wonder if there are some similar results?)</p>
",<graph-theory>
"<p>There is a proof in my textbook for the following claim, which doesn't make a whole lot of sense to me.  My annotations are in <strong>bold</strong>.  Could someone perhaps elaborate on what's going on?</p>

<p>Claim.  If there does not exist a cycle containing edges $e$ and $g$ then there exists a vertex $u \in V (G)$ such that every path in $G$ sharing one end with e and another with $g$ contains $u$.</p>

<p>Proof: The claim trivially holds if $e$ or $g$ is a loop <strong>OK</strong>, so we assume that neither is. Let $P$ with vertex set $v_1, v_2, . . . , v_k,$ in order, be a path with $e$ joining $v_1$ to $v_2$ and $g$ joining $v_{k−1}$ and $v_k$. Let $f_i \in E(P_i)$ be the edge with ends $v_i$ and $v_{i+1}$. Let $j$ be chosen minimum so that no cycle in $G$ contains $e$ and $f_j$ <strong>We can do this because we know that at least the edge $g$ will not create a cycle by assumption, right?</strong>. We will show that $u = v_j$ satisfies the claim.</p>

<p>Suppose not. Let $C$ be a cycle containing $e$ and $f_{j−1}$ <strong>What if $f_j = e$?  then what cycle? a single vertex?</strong> and let $P′$ be a
path from an end of $e$ to an end of $f$ <strong>Not sure what the book meant by $f$ here, any guesses?</strong> avoiding $u$. Choose a subpath $Q$ of $P′$ with one end in $V (C)$ and another in ${v_{j+1}, v_{j+2}, . . . , v_k}$ as short as possible. Then $C \cup Q \cup P$ contains a cycle containing both $e$ and $f_j$, a contradiction. (The last statement requires some case checking.) <strong>This ending seems a bit abrupt and non-obvious to me</strong></p>

<p>Thanks for the help</p>
",<graph-theory>
"<p>Let $G$ be a graph. </p>

<p><strong>Is the following implication true ?</strong></p>

<p>$G$ is graceful $\Rightarrow G$ is connected </p>

<p><strong>Definition</strong>: Let $G$ be a graph with $m$ edges. $G$ is <strong>graceful</strong> if there exists an injection $\Phi: V(G) \mapsto \{0,\ldots, m\}$ and $\{\mid \Phi(u) - \Phi(v) \mid : uv \in E(G) \} = \{1, \ldots, m\}$ </p>
",<graph-theory>
"<p>Naturally we can describe graphs via tables of ""yes there is an edge"" or ""no there is not"" between each pair of vertices, so the definition of an adjacency matrix is easily understood.  Thinking of these tables as <em>matrices</em>, however, adds structure - specifically, an interpretation as a linear operator.  Why do we look at them in this light?  Is it just for application - for example, efficiently obtaining a lot of data about a graph by computing its spectrum?  Or is there also an intuitive geometric (or algebraic) motivation behind the adjacency matrix?</p>

<p>For example, the $2$-path <img src=""http://i.stack.imgur.com/bSdoS.png"" alt=""2-path""> has adjacency matrix
 $$\mathcal{A}(P_2)=\left(\begin{array}{cc} 0 &amp; 1\\1 &amp; 0\end{array}\right)$$ which acts on a $2$-dimensional vector space by flipping the coordinates, $(x,y)\mapsto (y,x)$.  Can we somehow intuitively connect this action to the $2$-path?  What about for other simple graphs?</p>
",<graph-theory>
"<p>Can a graph <em>g</em> that satisfies the necessary condition for having an Eulerian trail have two or more distinct Eulerian trails given a fixed starting vertex? </p>

<p>Or, if a graph has an Eulerian trail, is that the only Eulerian trail in that graph?</p>
",<graph-theory>
"<p>I was reading a paper named <a href=""http://www.dli.gov.in/data_copy/upload/INSA/INSA_2/20005a23_585.pdf"" rel=""nofollow""><strong><em>Decompositions of the Kronecker product of a cycle and a path into long cycles and long paths</em></strong></a> by P. K. Jha (<em>Indian J. pure appl. Math.</em> <strong>23</strong>(8): 585-606, August 1992).</p>

<p>In one theorem I have a doubt. I am not getting how the proof of <strong><em>Lemma 1.3</em></strong> is done. I am not getting any idea what is the need of taking a graph like $G$ here. I will be very thankful if anybody can explain that. Here is that lemma :</p>

<p><img src=""http://i.stack.imgur.com/MTtyQ.jpg"" alt=""enter image description here""></p>
",<graph-theory>
"<p>Im starting to learn graph theory and i want to learn which approaches to take when it comes to finding the chromatic polynomial to different graphs. </p>

<p>Say we have these graphs,</p>

<p><a href=""http://i.stack.imgur.com/HvtWe.png"" rel=""nofollow"">Graphs</a></p>

<p>I can see that G1 is similar to k7 so using deletion-contraction to get subgraphes would be the best approach for that graph i think. But what about the G2 and G3? Does the deletion-contraction method work on those to? Or are there other approaches you can use and more importantly, how do you you know when you should use them?</p>
",<graph-theory>
"<p>The knight's tour is a sequence of 64 squares on a chess board, where each square is visted once, and each subsequent square can be reached from the previous by a knight's move.  Tours can be cyclic, if the last square is a knight's move away from the first, and acyclic otherwise.</p>

<p>There are several symmetries among knight's tours.  Both acyclic and cyclic tours have eight reflectional symmetries, and cyclic tours additionally have symmetries arising from starting at any square in the cycle, and from running the sequence backwards.</p>

<p>Is it known how many knight's tours there are, up to all the symmetries?</p>
",<graph-theory>
"<p>This is a practice question (not HW)</p>

<p>Prove that any circuit in a graph must contain a cycle AND that any circuit that is not a cycle contains at least two cycles.</p>

<p>Note : This is for a  first course in Graph theory</p>

<p>I have answer to this question but the answer raises more questions.
ANSWER: Suppose the vertices of the circuit are $v_0,v_1,...v_n,v_0$. Consider all subcircuits of the form $v_i,...v_n,v_i$. The subcircuit that uses the fewest number of vertices is a cycle (my Q: BUT WHY).</p>
",<graph-theory>
"<p>I am a novice when it comes to graph theory. Now i'm solving different questions where you get a graph and should determine the chromatic number and chromatic polynomial for that graph. I'm stuck at this particular graph:</p>

<p><a href=""http://i.stack.imgur.com/BSnrT.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/BSnrT.png"" alt=""enter image description here""></a></p>

<p>I started with isolating the ""trickiest"" sub graph ""g"" which happened to also be a wheel graph with an even amount of vertices n. And according to the internet, a wheel graph with this property has a chromatic number of 4, yes? </p>

<p>My big issue is when i check the correct answer of the question it says that the chromatic number of the entire graph G is 3. Did i incorrectly assume that the chromatic number of G couldn't be less than the chromatic number of g?</p>
",<graph-theory>
"<p>If $G$ is an outerplanar graph of order $n \geq 2$ and size $m$, show that $m \leq 2n -3$</p>

<p>[I can show the result for Hamiltonian outerplanar graphs, and I think its posible to extend the result, but my proof is very inelegant and tedious. Any ideas?]</p>
",<graph-theory>
"<blockquote>
  <p>Let $G = (V,E)$ be a connected graph. Suppose $e$ is an edge in a circuit of $G$. Show that the new graph $(V,E-\{e\})$ is still connected. </p>
</blockquote>

<p>Attempt: Let $v,w \in V$ be vertices. Then inside $G$, $v$ and $w$ are connected. Suppose the circuit starts and ends at vertex $a \in V$. Then the only problem that could occur is that the deleted edge is part of the connection between $v$ and $w$. So let's suppose this were the case. </p>

<p>I want to use the transitivity of the connected relation to say that $v$ is connected to $a$ which is connected to $v_i$, the vertex of the disconnect, which is connected to $w$. I can do the latter by symmetry of the relation $v$ connected to $w$. However, there is a concern whether $v$ is connected to $a$. Namely that the deleted edge occurring at $v_i$ might also appear in the connection between $v$ and $a$. Can this happen? If so, how can I change my walk to bypass it. </p>
",<graph-theory>
"<p>The proof of this is everywhere skipped and said to be collorary of Ford-Fulkerson theorem.
It's usually something like:</p>

<p>Let $A$ and $B$ be low cuts of a flow chart. Then $A \cup B$ and $A \cap B$ are also min cuts.</p>

<p>Can anyone tell me how exactly this is proved or link me a proof please?</p>

<p>Regards, Raxel.</p>
",<graph-theory>
"<p>Is there a relation between Clique size $\omega(G)$ and genus $g(G)$ of a graph? That is does $$\omega(G)^c\geq g(G)\geq \omega(G)^{\frac{1}d}$$ hold with constants $c,d\geq1$?</p>
",<graph-theory>
"<p>It is known that if a graph is connected, cubic, simple and $t$-transitive, then $t \le 5$. A proof is given in [Biggs, Algebraic Graph Theory, Chapter 18], and this result is due to [Tutte, ``A family of cubical graphs,'' Proc. Cambridge Philosophical Society, 45, 459-474].  </p>

<p>My question is: Is the proof given in Biggs' text the same as the one in Tutte's paper?  I was unable to obtain Tutte's paper.  I would appreciate if someone could electronically post or mail to me his paper.  </p>
",<graph-theory>
"<p>Is this true that graph consisting of $n$ edges and $n$ vertices has only one circuit. </p>

<p>I drew some graphs on paper and I believe that it is true. But how to prove that? I will be glad for any help.</p>
",<graph-theory>
"<p>Which of the following graphs have Euler circuits, Euler trails, or neither?</p>

<p><img src=""http://i.stack.imgur.com/WngtA.png"" alt=""enter image description here""></p>

<p>I tried :Euler Trails [A,B,C,A,D,B,C]</p>

<p><img src=""http://i.stack.imgur.com/1IB7h.png"" alt=""enter image description here""></p>

<p>I tried :Euler Trails [A,B,D,E,G,F,D,C,A,D,G]</p>

<p>but I am confused about Euler circuits.</p>
",<graph-theory>
"<p>The degree of every vertex of a graph $G$ of order $2n+1\geq5$ is either $n+1$ or $n+2$. Prove that $G$ contains at least $n+1$ vertices of degree $n+2$ or at least $n+2$ vertices of degree $n+1$. </p>
",<graph-theory>
"<p>I don't know what is the way to check this:  </p>

<p>Check whether the graph having degree sequence $\{3,3,1,1\}$ is a simple graph or not?  </p>

<p>Please help explaining the strategy I must follow to check this...</p>
",<graph-theory>
"<p>Suppose we model traffic flow between two points with a directed graph. Each route has either a constant travel time or one that linearly increases with traffic. We assume that each driver wishes to minimise their own travel time and we assume that the drivers form a Nash equilibria. Can removing a route ever decrease the average travelling time?</p>

<p>Note that the existence of multiple Nash equilibria makes this question a bit complicated. To clarify, I am looking for a route removal that will guarantee a decrease in the average traveling time regardless of the Nash equilibria that are chosen before and after.</p>
",<graph-theory>
"<p>Suppose that $G$ and $H$ are infinite graphs and that $G$ is isomorphic to a subgraph of $H$ and $H$ is isomorphic to a subgraph of $G$. Must $G$ and $H$ be isomorphic?</p>

<p>I've only just started on graph theory and this is one of the bonus questions on the assignment sheet. I don't see anything in my notes regarding infinite graphs and so I don't think this can quite classify as homework.</p>

<p>My intuition tells me that this might be a good definition for isomorphism for infinite graphs and hence Yes to the question but I don't quite seem to have any ideas on how to prove it.</p>

<p>Any insight or literature on this (because I can't seem to find much on this even on the internet!) will certainly be helpful.</p>
",<graph-theory>
"<p>Lovasz theta number: (It is said to be polynomial, but I do not know how to computer it)</p>

<p><a href=""https://en.wikipedia.org/wiki/Lov%C3%A1sz_number"" rel=""nofollow"">https://en.wikipedia.org/wiki/Lov%C3%A1sz_number</a></p>

<p>Grötzsch graph:<a href=""http://i.stack.imgur.com/jidJt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/jidJt.png"" alt=""enter image description here""></a></p>

<p><a href=""https://en.wikipedia.org/wiki/Gr%C3%B6tzsch_graph"" rel=""nofollow"">https://en.wikipedia.org/wiki/Gr%C3%B6tzsch_graph</a></p>

<p>I would like to know the Lovasz number of the complement Groetzsch graph.</p>
",<graph-theory>
"<p>$S$ is the string of characters:<code>TACGCGGT$</code></p>

<p>For string S and each of the positions $i=1,2,\dots,9$ write down the suffix string starting at position $i$.</p>

<blockquote>
  <p>What is the above question asking? For the following question I need to create the suffix tree, which I have done. But the above question seems to be asking me to simply write:</p>
</blockquote>

<p><code>TACGCGGT$</code>,<code>ACGCGGT$</code>,<code>CGCGGT$</code>,<code>GCGGT$</code>,<code>CGGT$</code>,<code>GGT$</code>,<code>GT$</code>,<code>T$</code>,<code>$</code>.</p>

<p>Surely that isn't the answer to the problem. What does it mean?</p>

<hr>

<p>Alternatively: It could be asking me for </p>

<p>T: <code>ACGCGGT$</code>,<code>$</code>
A: <code>CGCGGT$</code>
C: <code>GCGGT$</code>,<code>GGT$</code>
G: <code>CGGT$</code>,<code>GT$</code>,<code>T$</code>
C: <code>GGT$</code>
$: ``</p>

<p>But that seems absurd, since it doesn't really feel like $i=1,2,\dots,9$ since we don't really do $i=6,7,8$, as they are already done.</p>

<hr>

<p>The first way seems wrong since it would be too easy for a second year math course, especially for $8\%$ of the test's value...</p>
",<graph-theory>
"<p>I am learning martingale and Hoeffding-Azuma inequality recently but do not how to apply the those inequality or theorem here.</p>

<p>Let $G=(V,E)$ be a graph with chromatic number 600,i.e. $\chi(G)=600$. Let $S$ be a random subset uniformly chosen from $V$. Denote $G|_S$ the induced subgraph of $G$ on $S$. Prove that 
$$P(\chi(G|_S)\leq 200 )\leq 2^{-10}.$$</p>

<p>I am not sure how to approach ones, especially for the condition $\chi(G)=600$. I am thinking that for a 600 vertices complete graph, the probability to be computed is just the ratio $$\frac{\sum_{i=0}^{200}C_i^{600} }{2^{600}},$$</p>

<p>meaning the ratio btween the number of all subgraph with vertices number less than 200 and the total number of subset of $V$. But is it enough?  Even this ratio is hard to compute.</p>
",<graph-theory>
"<p>I am looking for some <em>well-known</em> algorithms in which sparse matrix elements are accessed in a <em>non-structured</em> way, i.e. row/column depends on a value of another (sparse) matrix/vector element or some variable that is changing throughout the course of the algorithm.</p>

<p>So, if one was to execute an algorithm on a computer or even by hand on a paper, there should <em>not</em> be many visible patterns in which <em>adjacent non-zero elements</em> in a single row/column are accessed consecutively.</p>

<p>For example, consider a sparse matrix
$$
M = \begin{bmatrix}
0 &amp; 1 &amp; 0 &amp; 0 &amp; 2 \\
0 &amp; 3 &amp; 4 &amp; 0 &amp; 5 \\
6 &amp; 0 &amp; 0 &amp; 7 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 8 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 9
 \end{bmatrix}
$$</p>

<p>I am not interested in algorithms which contain fairly regular element access patterns like:</p>

<ul>
<li><code>3 4 5</code> - accesses the non-zero elements in row 2, in order</li>
<li><code>9 8 0 5 2</code> - accesses last column in reverse order</li>
<li><code>1 4 7 8</code> - accesses the superdiagonal, still too structured/regular</li>
</ul>

<p>but those which involve a lot of ""random"" patterns like:</p>

<ul>
<li><code>6 8 1 9 4 3</code> - accesses only non-zeroes, very interesting</li>
<li><code>3 0 8 5 1 0 0 6</code> - accessing some zero elements is also interesting</li>
<li><code>8 0 2 5 9</code> - permutation of a row/column is still interesting enough</li>
</ul>

<p>(Of course, the exact access pattern depends on the implementation of an algorithm, but I am looking for those algorithms in which there is no obvious way to access, for instance, an entire row/column of a matrix in (reverse) order.)</p>

<p>I would prefer a deterministic algorithm, but randomized ones are also fine.</p>
",<graph-theory>
"<p>I have the following paragraph in my notes:</p>

<blockquote>
  <p>If $G=(V,E)$ is a general graph . Let $U\subseteq V$ and let $F$ be a subset of $E$ such that the vertices of each edge in $F$ are in $U$ ,<br>
  then $H=(U,F)$ is also a general graph and $H$ is a subgraph of $G$ .  </p>
  
  <p>If $F$ consists of all edges of $G$ which have endpoints in $U$ ,then $H$ is called induced subgraph of $G$ and is denoted by $G_U. $  </p>
</blockquote>

<p>From here both the definition of a subgraph and a induced subgraph seem same to me..<br>
I can't understand what is the difference between them...<br>
Please help with this..</p>
",<graph-theory>
"<p>For any digraph $G = (V,E)$, consider the digraph $H = (E,F)$ with $F = \{ee' \in E \times E\ |\ \exists u,v,w \in V: e = uv \wedge e' = vw\}$, that is, the digraph $H$ whose nodes are the edges of the digraph $G$ and whose edges connect any two edges of $G$ which, together, form a directed path.</p>

<p>Which term is used in the literature for the digraph $H$ wrt. the digraph $G$?</p>
",<graph-theory>
"<p>I was wondering if someone can help me understand how prove that this graph is connected.</p>

<p>Given a graph with n vertices, prove that if the degree of each vertex is at least $(n − 1)/2$ then the graph is connected.</p>

<p>So far I know that about connected graphs:</p>

<p>An undirected graph is called <strong>connected</strong> if there is a path between
every pair of distinct vertices of the graph</p>

<p>The <strong>distance</strong> between two vertices in a graph is the length of
the shortest path between them.</p>

<p>The <strong>diameter</strong> of a graph is the distance between the two vertices
that are farthest apart.</p>

<hr>
",<graph-theory>
"<p>Let $T$ be a tree in which the largest degree of a node equals to $t$. Let $n_1$ denote the number of nodes of degree $1$ in $G$. Prove that $n_1 ≥ t$</p>

<p>I understand why this works but I am not sure how to prove it mathematically. It makes sense, because vertices of degree one are those at the end of each leaf (let their number be n) and/or the vertex in the beginning of the tree that doesn't branch into more than one edge. And the vertex with highest degree is gonna have at max n edges connected to it. Am I making sense? any help in the formal proof?</p>
",<graph-theory>
"<p>If I have the following graph :  <img src=""http://i.stack.imgur.com/4eVLO.png"" alt=""enter image description here""></p>

<p>Should the degree of vertex $v_2$ be 1 or 2...I'm asking this because I'm not sure whether loop should be counted while considering degree...</p>

<p>(In my notes the definition of degree of a vertex $a$ is defined to be : the no. of edges it belongs to). </p>
",<graph-theory>
"<p>Suppose there is a square binary matrix (Adjacency matrix of a graph), $A$.</p>

<p>I got that, the matrices, $A^2$ and $A^3$ are distinct but the set of eigenvalues are same for $A^2$ and $A^3$. It is to be noted that the set of eigenvalues of $A$ is different from the same of $A^2$ and $A^3$. Other powers of $A$ are same as $A^3$. </p>

<p>What does the above result interpret?</p>

<p>Please let me know. </p>

<p>Thanks in advance!  </p>
",<graph-theory>
"<blockquote>
  <p><strong>Serf definition</strong>: A vertex $z$ in a nontrivial tournament is called a <strong>serf</strong> if for every vertex $x$ distinct from $z$, either $x$ adjacent to $z$ or $x$ is adjacent to a vertex that is adjacent to $z$. </p>
</blockquote>

<p>Prove that every nontrivial tournament has at least one serf.</p>

<p>I'm not sure if I understand this correctly, but a tournament is  an oriented complete graph, and in a complete graph of order $n$ every vertex un-oriented adjacent to $n-1$ other vertex, so the only way for a vertex to not be a serf is that vertex has to be a source.</p>

<p>Assume the contrary that there exists a tournament that doesn't have any serf, then that tournament has every vertex is a source, which is impossible, thus every tournament must have at least one serf.</p>

<p>is my argument acceptable?</p>
",<graph-theory>
"<p><img src=""http://i.stack.imgur.com/cbjy9.png"" alt=""maxflow-mincut theorem"">
The proof is taken from course <em>Algorithm II, Princeton, coursera</em>. In the proof of iii => i, Why/How iii implies the existence of cut (A, B)?</p>
",<graph-theory>
"<p>A standard proof of the existence of Eulerian circuits proves the following are equivalent for a connected graph $G$: </p>

<blockquote>
  <p>(i) Every vertex in $G$ has even degree</p>
  
  <p>(ii) The edges of $G$ can be partitioned into disjoint cycles</p>
  
  <p>(iii) $G$ is Eulerian</p>
</blockquote>

<p>I'm interested in $(i) \implies (ii)$. The proof I've seen is by induction. However, the claim is very much about the edge space of $G$. Is there a linear algebraic proof of that implication?</p>
",<graph-theory>
"<p>a)  Show that for $2$ vertices $u$ and $v$ have the same score in a tournament $T$ then $u$ and $v $ belong to the same strong component of order $k$</p>

<p>b)  Prove that every regular tournament is strong</p>

<blockquote>
  <p>Define the relation on $V(T)$ by $u$ is related to $v$ if there is both $u-v$ path and $v-u$ path in $T$. This is an equivalent relation so this relation partition $V(T)$ into equivalent classes $V_1, V_2,..., V_k$  $(k\geq 1)$. Let $S_i= T[V_i]$ for $1 \leq i \leq k$, then $S_i$ are called <strong>strong components</strong> of $T$</p>
</blockquote>

<p>a)</p>

<p>From $u$ and $v$ have the same score I can see that $od(u)=od(v)$. So there is a path from $u$ and $v$ to other $k$ vertices, but how do I know there is a path from $u$ to $v$ and from $v$ to $u$  from these info?</p>

<p>b)</p>

<p>Let $T$ be a regular tournament then for every $v$ in $T$ , $id(v)=od(v)$. A tournament is a directed graph, and a directed graph such that for every $v$ in $T$ , $id(v)=od(v)$ is Eulerian. So a regular tournament is strong because it contain an Eulerian cycle.</p>
",<graph-theory>
"<p>Are there any class of graphs where distance between every two vertices is $\geq$2. 
I was wondering about the existence of such graphs. Because for counter examples I have Paths $P_n$. </p>

<p>Thank you very much.</p>
",<graph-theory>
"<p>Consider directed graph which has $N + 2$ layers numbered from left to right by integers from $0$ up to $N + 1$.</p>

<p>The leftmost ($0$) and the rightmost ($N + 1$) layers both contain only one vertex while every other layer contains exactly $M$ vertices. Vertices are numbered independently in each layer by integers from 0 to M - 1. For each pair of vertices which are in the adjacent layers ($i$ and $i + 1$ for any $i$ ($0 &lt;= i &lt;= n$)), there exists an edge. The vertex which is in the layer with smaller number is the initial vertex for such edge and the other one is the terminal vertex.</p>

<p>So the graph initially is something like this :</p>

<p>Let us assume N = 4  And M=3 </p>

<p><img src=""http://i.stack.imgur.com/sGvod.png"" alt=""enter image description here""></p>

<p>Here A node is in layer 0 and N th node is in layer N+1 that is 5</p>

<pre><code>Layer 1 has nodes = {B,C,D}
Layer 2 has nodes = {E,F,G}
Layer 3 has nodes = {H,I,J}
Layer 4 has nodes = {K,L,M}
</code></pre>

<p>Now we know that number of paths to go from A(that is first layer) to N(that is last layer) is $M^N$</p>

<p>Now suppose we add K more edges. Each edge connects two vertices which are in the different layers, no matter the adjacent layers or not. Also, each edge is directed from left to right (as well as all previously existing edges).</p>

<p>Like say we add an edge between Layer 1 Node 3 that is D to Layer 3 Node 3 that is J then graph look like this :</p>

<p><img src=""http://i.stack.imgur.com/1RmFA.png"" alt=""enter image description here""></p>

<p><strong>How many ways are there to reach from leftmost layer(0) to the rightmost layer(N+1) after adding these K edges ?</strong> </p>

<p><strong>Note : Two paths are considered different if there is, at least, one edge which belongs to exactly one path. However, we are allowed to traverse the same set of vertices. In that case, there should be a multiple edge in the graph. It is also possible if some edge added connects two adjacent layers.</strong></p>

<p>For example : It can also be like this :</p>

<p><img src=""http://i.stack.imgur.com/FroCw.png"" alt=""enter image description here""></p>

<p>In this case both edges are considered as different . So we need to count these total ways.</p>

<p>My Attempt : I know that if suppose we add a single edge between layers at A from start and B from end then it introduces M^(A-1)*M^(N-B) new paths. But problem arise when their are other edges added after that edge.</p>

<p>Example : Let N=4 , M=2 and K=2</p>

<p>There are 16 ways to get from the layer #0 to the layer #5. Now we have added edges. </p>

<p>Let first edge added is between (Layer 2,Node 1) to (Layer 5,Node 0) then there are 2 ways to get from the layer #0 to the layer #5 using this edge (0, 0 -> 1, 0 -> 2, 1 -> 5, 0 and 0, 0 -> 1, 1 -> 2, 1 -> 5, 0) </p>

<p>Let second edge added is between (Layer 0,Node 0) to (Layer 4,Node 0) then there is 1 way to get from the layer #0 to the layer #5 using this edge (0, 0 -> 4, 0 -> 5, 0)</p>

<p>So total is 16+2+1=19 ways</p>

<p><strong>Edit :</strong> To make question precise we can assume that we are given N , M and K </p>

<p>Also then we are given K extra edges. Each edge is of form Layer1 , Node1 , Layer2 , Node 2 which shows that a edge between Node 1 of Layer 1 is directed toward Layer 2 Node 2.</p>

<p>How many paths are their now to reach last layer from starting layer ?</p>
",<graph-theory>
"<p>I am currently in the process of reading an article by D.Bundy <a href=""http://www.sciencedirect.com/science/article/pii/S0097316505001834"" rel=""nofollow"">The connectivity of commuting graphs</a>. In section 3 (in the Preliminary Results) Bundy gives the following result:</p>

<p>$\mathbf{(3.1)}$ Let $G=\operatorname{Sym}(n)$ and $H$ be the stabilizer in $G$ of a system of imprimitivity with blocks of size $s$, for $1&lt;s&lt;n$. Then $H$ is a maximal subgroup of $G$.</p>

<p><strong>Proof.</strong> Elementary. $\Box$ </p>

<p>I'm afraid I fail to see how to prove this. Indeed, suppose that $1&lt;s&lt;n$ and that $st=n$ for some $1&lt;t&lt;n$. Then we have that $H\cong \operatorname{Sym}(s)\wr\operatorname{Sym}(t)$, so the result is equivalent to proving that if $1&lt;t,s&lt;n$ with $ts=n$, then the copy of $\operatorname{Sym}(s)\wr\operatorname{Sym}(t)$ contained in $\operatorname{Sym}(n)$ is maximal in $\operatorname{Sym}(n)$. Any help on seeing why this is true would be greatly appreciated.</p>
",<graph-theory>
"<p>For any flow network N, add an edge $e_{ts}$ and color it black. Color all other edges in N with black, red or green, then at least one of the following two cases is true:</p>

<p>1) There exists a cycle C which includes $e_{ts}$ and is composed only with <strong>black and red</strong> edges, in which all <strong>black</strong> edges points to the same direction(all clockwise/anticlockwise in the cycle)</p>

<p>2) There exists a edge set A which includes $e_{ts}$ and is composed only with <strong>black and green</strong> edges, where the vertices in G-A can be divided into two sets $V_1$ and $V_2$ (suppose t is in $V_1$), such that all black edges point from $V_1$ to $V_2$ </p>

<p>I guess I may need to construct a feasible flow from s to t and $e_{ts}$ joins the sink and the source. But I'm not sure how to do that.</p>
",<graph-theory>
"<p>There would be 34 edges.</p>

<p>If we increase the vertices then we would decrease the Degree. </p>

<p>Hence 10 Vertices with degree of 3 and one with degree of 4. </p>

<p>So I think in total it would be 11 vertices am I right.</p>
",<graph-theory>
"<p>I'm trying to prove that given an undirected non-trivial graph $G, G$ is the periphery of some other graph $H$, if and only if:  </p>

<p>a)for each vertex $ v \in V(G)$ , $ecc(v)=1 $<br>
 or<br>
 b)for each vertex  $ v \in V(G)$ , $ecc(v)\neq1 $  </p>

<p>$V(G)$ being the set of vertices of $G$ , $ecc(v)$ being the eccentricity of vertex $v$<br>
The periphery of a graph is the set of vertices with eccentricity equal to the diameter of the graph (diameter being the maximum eccentricity).</p>

<p>The first case seems a bit easier, as from what i can tell, graphs with eccentricity 1 are the complete graphs $K_n$ , so maybe i can construct some graph for every $n$ number of vertices that has G as the perimeter.
For the other case i don't really know where to start.</p>
",<graph-theory>
"<p>I have a homework problem where I have a graph $G$ and I am tasked with proving that at least one of $G$ and $G$ complement is connected. However, I am unclear on the exact meaning of $G$ complement. </p>

<p>For example, let's imagine I have a disconnected graph with four vertices $(V_1, V_2, V_3, \text{and } V_4)$. 
If the edges form a sort of box where the bottom edge is left disconnected, would $G$ complement have that edge filled in along with the cross edges as well? Furthermore, does $G$ complement contain all of the edges in $G$ or just the edges not contained in $G$? Thanks for taking the time to read.</p>
",<graph-theory>
"<p>If $A$ is an adjacency matrix of a graph $G$ and it can be diagonalized to get it in the form $A=PDP^{-1}$, with $D$ diagonal, is there any graph-theoretic interpretation to the matrices $P$ and $D$?</p>
",<graph-theory>
"<p>I read the following statement :</p>

<blockquote>
  <p>If the graphs $G$ and $G'$  are isomorphic then following is true:  </p>
  
  <ul>
  <li>If $G$ is connected, so is $G'$. More generally, $G$ and $G' $ have same number of connected components.   </li>
  </ul>
</blockquote>

<p>I don't understand what connected components mean.  Can anyone explain this to me, please?</p>
",<graph-theory>
"<p>I am trying to do Mathematics for CS course( 6.042) from MIT opencourseware. Could anyone please help me with this problem( from problem set 6. Problem 6).</p>

<p>Let G be a graph. In this problem we show every vertex of odd
degree is connected to at least one other vertex of odd degree in G.
<br><br>
(a) [6 pts] Let v be an odd degree node. Consider the longest walk starting at v that does
not repeat any edges (though it may omit some). Let w be the final node of that walk. Show
that w is not equal to v.<br><br>
(b) [4 pts] Show that w must also have odd degree.</p>
",<graph-theory>
"<p>The set of the vertices of the graph $G$ is $V=\{ 0,1,2,3,4,5,6\}$. The vertices i and j are connected with an edge if and only if $|i-j| \mod 3 \in \{0,1 \}$. </p>

<p>Does $G$ have an Euler circuit?? </p>

<p>I drawed the graph:</p>

<p><img src=""http://i.stack.imgur.com/rhkuF.png"" alt=""enter image description here""></p>
",<graph-theory>
"<p>So, Gallai is mentioned in e.g. Wikipedia page about perfect graphs <em>and</em> Berge's article at some point. </p>

<p>My understanding is that Gallai was the first who properly talked about bipartite graphs and line graphs of bipartite graphs and comparability graphs being ``clique colourable"" (+complements). </p>

<p>He also was the first to talk about odd holes, but Berge was the first to realise about odd antiholes and hence the perfect graph theorem and the strong perfect graph theorem. </p>

<p>Is this correct? I want to say something about the history of perfect graphs without it being guff. </p>
",<graph-theory>
"<blockquote>
  <p>For a simple undirected graph $G$, suppose we have two vertices $v_1$ and $v_2$ such that $G-v_1 \simeq G-v_2$. Does this necessarily mean that there is an automorphism of $G$ that maps $v_1$ to $v_2$?</p>
</blockquote>

<p>This is just a condition that I've assumed to be true for awhile now without thinking too hard about whether or not it's actually true. Using the condition that $G-v_1 \simeq G-v_2$ has been a useful way to characterize two vertices as being ""the same,"" but I was wondering if this actually corresponds to automorphisms of the graph.</p>
",<graph-theory>
"<p>A standard result in graph minor theory is that a graph is series-parallel if and only if it is $K_4$-minor free.</p>

<p>I'm looking for a good source for a proof of this that is understandable to undergraduates.</p>
",<graph-theory>
"<p>We have a data set which is comprised of Connectors and Segments.  Each segment has exactly two connectors, but each connector can belong to zero or more segments (i.e. connector 'A' in the left image below has no segments, while connector 'M' has three, M-R, M-L and M-N.)</p>

<blockquote>
  <p>It is understood that wherever any lines meet or intersect, there will be a connector so we don't have to worry about even/odd rules, overlapping or partially-enclosed polygons, etc. as they don't apply.</p>
</blockquote>

<p>In short, we're trying to identify all of the created polygons (the colored shapes in the right image.)  I believe this can be completed in two steps.</p>

<p><a href=""http://i.stack.imgur.com/3AENJ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/3AENJ.png"" alt=""Polygons""></a></p>

<p><strong>Part 1: Removing superfluous items</strong></p>

<p>Stand-alone connectors (connector 'A' here) can simply be removed since they can't be part of a shape's outline.</p>

<p>Floating end-points referencing a single segment (connectors 'B' and 'E') can also be removed as they too can't be part of a shape's outline.  This will also remove their referenced segments (B-C and E-D).</p>

<p>Performing the above recursively will next identify 'C' as an endpoint (since 'B' and B-C were already removed) so it and it's remaining segment C-D can also be removed.  On the next recursive pass, connector 'D' and segment D-F will also be removed, etc.</p>

<p>However, I haven't found a good way to identify segment G-H. That said, I think that can be achieved during polygon detection since such segments would only be the result of compound paths and would be traced in both directions during one shape's detection. (More on that below.)</p>

<p><strong>Step 2: Polygon Detection</strong></p>

<p>Each segment can be traced in two directions. For instance, the segment connecting 'O' and 'P' can be either O-P or P-O.  Picking a trace-direction of clockwise, O-P would belong to the polygon O-P-Q-N whereas P-O would belong to the polygon P-O-I.</p>

<p>The following logic assumes a trace-direction of clockwise.</p>

<p>Starting from any segment, when tracing around, if you get back to your starting point, you have identified a potential polygon.  By keeping a running delta of your heading's angle as you trace around (this is how much your heading turns and is not to be confused with simply adding the angles between segments), when done, if that angle is positive, you've detected a valid polygon.  If it's negative, you've detected a 'containing' polygon, meaning one that contains one or more 'valid' polygons.  The outer perimeter of the entire shape (or shapes) are all containing polygons.</p>

<p>Consider the case of a square, diagonally divided into two triangles.  Tracing each segment twice--once in each direction--you will end up with three potentially-valid polygons: a square and two triangles.  The triangles will have a positive angle delta telling you they're valid, but the square's angle delta will be negative telling you that's the containing polygon.</p>

<blockquote>
  <p>Note: A containing polygon can be equal to a valid polygon too. It will just be 'wound' in the opposite direction.</p>
</blockquote>

<p>Consider a simple triangle.  The clockwise trace will yield the valid polygon.  The second attempt to trace clockwise will actually yield a counter-clockwise trace which will give you a negative angle delta, telling you that's actually the outline of the shape.</p>

<blockquote>
  <p>Note: You also have to test for other polygons encountered along the way by also testing each point for any previously-encountered point during that shape detection.  If you find you've revisited the same point, save off the polygon created since the first encounter of that point, check it's angle. If it's positive, it's a valid polygon (and you're actually currently tracing a containing polygon.) If it's negative, you've detected a containing polygon (in which case you're currently tracing a valid polygon.) Finally, remove all segments on your accumulation stack back to the first instance that point was last encountered and continue on with your detection.</p>
</blockquote>

<p>For instance, if you started at 'J' and traced around counter-clockwise, you would go through 'I', 'H', then 'G', then 'F' then you'd be back at 'H'. You just found a polygon H-G-F which has a negative angle so you know it's a containing polygon. Remove those three segments from your stack and continue on.  Now you'll again hit 'I'.  In this case, you already visited that same segment during this pass, but in the other direction, so simply remove that segment completely from your stack and continue on, next to 'O' then 'N', etc. You'll eventually be back at 'J'.</p>

<p>When a segment has been traced in both directions, it can be considered 'used' and no further processing of that segment is needed. Continue processing all non-used segments.  Once all segments have been traced in both directions, you can be sure all polygons--valid and containing--have been found.</p>

<p>Finally, check each containing polygon to see if it falls within any valid polygon. If so, exclude it from that valid polygon creating a compound path.  In the example here, containing polygon H-G-F is contained by the valid cyan polygon so it should be excluded.  Note there is also a valid H-F-G polygon which is marked in red here.</p>

<p>Anyway, that's what I've come up with, but I'm wondering if there's a better/simpler way. Thoughts?</p>
",<graph-theory>
"<p>I'm having trouble making progress on this. I'm trying to use contradiction and I'm really not seeing anything.  Any help would be greatly appreciated!</p>
",<graph-theory>
"<p>Let G1, G2, G3 be three (possibly overlapping) graphs on the same vertex set, and suppose that G1 can be properly colored with 2 colors, G2 can be properly colored with 3 colors, and G3 can be properly colored with 4 colors. Let G be the graph on the same vertex set, formed by taking the union of the edges appearing in G1, G2, G3. Prove that G can be properly colored with 24 colors.</p>
",<graph-theory>
"<p>Is it possible to create a graph ( represented in the form of Adjacency Matrix), when the number of nodes and the count of neighbors for each node is given?</p>
",<graph-theory>
"<p>Say I have an image, with pixels that can be either $0$ or $1$. For simplicity, assume it's a $2D$ image (though I'd be interested in a $3D$ solution as well). </p>

<p>A pixel has $8$ neighbors (if that's too complicated, we can drop to $4$-connectedness). Two neighboring pixels with value $1$ are considered to be connected. </p>

<p>If I know the probability $p$ that an individual pixel is $1$, and if I can assume that all pixels are independent, how many groups of at least $k$ connected pixels should I expect to find in an image of size $n\times n$?</p>

<p>What I really need is a good way of calculating the probability of $k$ pixels being connected given the individual pixel probabilities. I have started to write down a tree to cover all the possibilities up to $k=3$, but even then, it becomes really ugly really fast. Is there a more clever way to go about this?</p>
",<graph-theory>
"<p>I'm looking for a reference rather than an answer. I think I'm just not Googling the right combination of terms. I imagine that there is a class of graphs which is equivalent to some class of languages via some transformation (acceptance?). I'd like to know more about this, but can't seem to find much.</p>
",<graph-theory>
"<p>What is the smallest $n$ such that every 2-coloring of edges of $K_n$ contains a red or blue 4-cycle (not $K_4$)? I am given that $R(4,4) \le 18$ and $R(3,5) \le 14$</p>

<p>Any help is greatly appreciated!</p>
",<graph-theory>
"<p>Proposition. If $G$ is a bipartite graph with at least one edge, then its spectrum is symmetrical with respect to $0$, i.e. if a number $\lambda$ is an eigenvalue of $G$ then $- \lambda$ is also an eigenvalue of $G$.</p>

<p>Proof. Let $G \in B(m,n)$. Then $A(G)$ is an $(m+n) \times (m+n)$ matrix. Suppose that $\lambda$ is an eigenvalue of $G$ and $x=(x_{1},x_{2},...,x_{m+n})$ is a corresponding eigenvector. Consider the vector $y=(y_{1},y_{2},...,y_{m+n})$ where $y_{j}=x_{j}$ if $1 \le j \le m$ and $y_{j}=-x_{j}$ if $m+1 \le j \le m+n$. 
Then </p>

<p>$ \sum_{j=1}^{m+n}a_{ij}y_{j}=\sum_{j=m+1}^{m+n}a_{ij}y_{j}=-\sum_{j=m+1}^{m+n}a_{ij}x_{j}=- \lambda x_{i} = -\lambda y_{i}$, if $1 \le i \le m$, 
and $ \sum_{j=1}^{m+n}a_{ij}y_{j}=\sum_{j=1}^{m}a_{ij}y_{j}=\sum_{j=1}^{m}a_{ij}x_{j}=\lambda x_{i} = - \lambda y_{i}$, if $m+1 \le i \le m+n$.</p>

<p>So $y$ satisfies $A(G)y=- \lambda y$, and $- \lambda$ is an eigenvalue of $G$</p>

<p><strong>My question</strong>: How to proof that laplacian spectrum is symmetric for bipartite graphs. Proof above is the proof in one way: if $G$ is bipartite then condition, but my task is to proof that: if condition then $G$ is bipartite. I would like to get a seed of an idea, how can I do that.</p>
",<graph-theory>
"<p>Let $\binom{n}{2}$ be the set of all subsets of $\{1,2,3, \ldots, n\}$ of size $2$ and let $C_n$ be the set of $E \subseteq C_n$ so that the graph $G$ with vertex set $\{1,2, 3, \ldots, n\}$ and edge set $E$ is connected.  Using generating function methods one can show that $$\sum_{E \in C_n} (-1)^{|E|} = (-1)^{n-1}(n-1)!.$$</p>

<p>For example, if $n=3$ then $$C_n = \{\{12,23\}, \{12, 13\}, \{13, 23\}, \{12, 13, 23\} \}$$ and then $$(-1)^2 + (-1)^2 + (-1)^2 + (-1)^3 = 2!.$$
Is there a more direct proof?  For example, a sign-reversing involution argument.</p>
",<graph-theory>
"<p>Consider a 'game' played on a subset $S$ of an $n^2$ square grid as follows. There are 3 types of pieces, each occupying a square, 1 green, some red and the rest are blue, a move consists of shuffling the green piece with any of its 4 adjacent pieces (if they are within $S$). $S$ consists of squares, squares not in $S$ are static, $S$ can be any subset of the $n^2$ square.</p>

<p>If two board configurations are reachable from eachother, is it possible to obtain an upper bound on the number of moves needed, given only the board size $n$, is it polynomial in $n$?</p>
",<graph-theory>
"<p>suppose $P_n$ and $P_m$ are paths with $n$ and $m$ edges respectively.consider $A_n$ and $A_m$ as adjacency matrix of them.now I want to calculate the number of perfect matching of $P_n \square P_m$ (it is cartesian product <a href=""http://en.wikipedia.org/wiki/Cartesian_product_of_graphs"" rel=""nofollow"">http://en.wikipedia.org/wiki/Cartesian_product_of_graphs</a> )</p>

<p>now I consider $A_{m,n}=A_m \otimes I_n + I_m \otimes A_n$ which is adjacency matrix for $P_n \square P_m$. ($\otimes$ is Kronecker product <a href=""http://en.wikipedia.org/wiki/Kronecker_product"" rel=""nofollow"">http://en.wikipedia.org/wiki/Kronecker_product</a>) </p>

<p>now because $P_n \square P_m$ is bipartite graph then the number of perfect matching is equal to $\sqrt{per(A_{m,n})}$.</p>

<p>now I know that if I define $A^{*}_{m,n}=A_m \otimes I_n + iI_m \otimes A_n$ which $i^2=-1$ it is enough to show that $ det(A^{*}_{m,n})=per(A_{m,n})$</p>

<p>my problem is to show this later equation which is so great relation.</p>

<p>please give me the way to prove that,any help will be great,thanks.</p>
",<graph-theory>
"<p>I am wondering if there is any existing algorithm for the following routing problem. </p>

<p>Let's suppose that you are given a directed graph where the edges are labeled with a weight indicating a cost. Each node belongs to a POI (point-of-interest) such as restaurants, grocery stores, etc. Given a set of POIs you have to visit, what is the shortest route from a node X to Y, visiting all the POIs given you?  (e.g., ""I have to go to my office from my house. On the way, I have to visit a grocery store, a department store, and a gas station. What is the shortest route?)</p>

<p>I tried to find if there was any literature on this problem but could not find any. Does anybody know this type of problem and any solution?</p>

<p>Thanks</p>
",<graph-theory>
"<p>The maximum number of points in a plane such that the distance of any of these points from  a given point in the plane is less than the distance of it from any other point is five.</p>
",<graph-theory>
"<p>I am trying to find a polynomial time reduction from the colored graph isomorphism to the regular graph isomorphism.
Doing a search on this problem, I found <a href=""http://pages.cs.wisc.edu/~dieter/Papers/3gi.pdf"" rel=""nofollow"">this article</a> and it seems like theorem 1 is the solution I am looking for, however I don't fully understand it.
Could someone please explain how the reduction works?</p>
",<graph-theory>
"<p>So i asked this question and was given a hint</p>

<p>The degrees would have to be the integers 0,1,…,4k+1: why?</p>

<p>This was my solution to it</p>

<p>Given a simple graph with n = 4k + 2 vertices. Can the vertices of
this graph have distinct degrees?</p>

<p>Since n is isomorphic to its complement, we know that graph have
the same number of edges. Also, if we look at the union of the edges of both
graphs we know we get all possible edges. If n has n vertices, there are $n(n-1)\over 2$ possible edges, so n must have exactly half of them. Therefore,</p>

<p>$|E(n)| = |E(n)| =$$n(n-1)\over4$</p>

<p>Now we can see that since |E(n)| must be an integer, 4 must divide evenly
into n or n − 1. So, we conclude that n = 4k + 1 for some nonnegative
integer k.</p>

<p>From this can i conclude that n is isomorphic to its complement and n = 4k + 1 for some nonnegative integer k, then it has distinct degrees.</p>
",<graph-theory>
"<p>Let G be a graph of order n. Prove that if deg u + deg v ≥ n - 2 for every pair u, v of nonadjacent vertices of G, then G has at most two components.</p>
",<graph-theory>
"<p>There is this know formula for determining the automorphism group of a graph $G$: let the connected components of $G$ consist of $n_1$ copies of $G_1$, $\dots$, $n_r$ copies of $G_r$, where $G_1, \dots, G_r$ are pairwise non-isomorphic. Then $${\rm Aut}(G) = ({\rm Aut}(G_1) \wr S_{n_1}) \times \dots \times ({\rm Aut}(G_r) \wr S_{n_r}).$$</p>

<p>I know intuitively what the formula does, but I am not able to prove the formula formally, probably because I don't understand the definition of the wreath product properly.</p>

<p>I would also appreciate if someone would describe the automorphism group of a graph which is a disjoint union of three paths of length one (or any other simple example).</p>

<p>If you are aware of a text or a website, where this is explained, it would be nice if you gave me a link, I wasn't able to find anything.</p>

<p>Thank you for your help!</p>
",<graph-theory>
"<p>So i was given this question</p>

<p>Draw the graph whose vertex set is the set of integers from 1 to 7,
and two vertices x and y are adjacent if $|x − y| ≡ 0(mod 2)$. Is the graph simple, count the degree of each vertex, By adding some edges is it possible to transform it into Eulerian one?</p>

<p>What is really throwing me off is the $|x − y| ≡ 0(mod 2)$ part. </p>

<p>In a simple graph the edges form a set and each edge is a unordered pair of distinct vertices. In a simple graph with n vertices, the degree of every vertex is at most n − 1. So since the graph would be adjacent to $|x − y| ≡ 0(mod 2)$ it would be simple because it can evenly divide.</p>
",<graph-theory>
"<p>In a university, the secretariat plans the examination period. There are $6$ subjects, $A,B,C,D,E,Z$ and $9$ students($1, \dots , 9$). At the subject $A$ the students $1,2,3$ are subscribed, at the subject $B$ the students $1,2,9$, at the subject $C$ the students $1,7,8$, at the subject $D$ the students $3,5,7,9$, at the subject $E$ the students $4,5,8$ and at the subject $Z$ the students $4,6,8$. Each examination lasts $2$ hours, and it can only be during the morning hours $10-12$. The only restriction at the planning is that it is not allowed that $2$ subjects, ,at which the same student is subscribed , get examinated simultaneously.</p>

<p>Which is the minimum number of days that are required,so that all the exams are taken?</p>

<p>I tried to solve the exerise,with the chromatic number,using the following graph:</p>

<p><img src=""http://i.stack.imgur.com/BCqgN.png"" alt=""enter image description here""></p>

<p>So,the minimum number of days is $4$..or am I wrong??</p>
",<graph-theory>
"<p>Consider a tournament with $n$ contestants - that is, a complete graph directed graph $K_n$ where each edge is pointed one way or the other. We call a subset $\{a,b,c\}$ a ""cyclic triplet"" if each of the three wins one of the two games against the other two. It is not hard to find a maximum number of cyclic triplets. </p>

<p>We can argue by considering the triplets involving a particular contestant that the maximum number of ""cyclic triplets"" among such triplets occurs when he beats  half of the remaining contestants. Hence the global maximum occurs when everyone beats half the remaining ones.</p>

<p>Now, define a subset $\{a,b,c,d\}$ to be a ""cyclic quadruplet"" if two contestants win two games against the other two, while the remaining two win one game. What is the maximum number of cyclic quadruplets?</p>
",<graph-theory>
"<p>if the graph G can be embedded on a torus can we say:</p>

<p>if</p>

<p>$\chi(G)\ge r\Rightarrow K_r\prec G$.</p>

<p>Kr is a minor of G?</p>
",<graph-theory>
"<p>I am reading the following paper on Rectangular Cartograms - <a href=""http://ac.els-cdn.com/S0925772106000770/1-s2.0-S0925772106000770-main.pdf?_tid=ce3aa43e-cef1-11e3-b342-00000aacb361&amp;acdnat=1398702556_1adee439a6b55c82efa20917178fb0aa"" rel=""nofollow"">http://ac.els-cdn.com/S0925772106000770/1-s2.0-S0925772106000770-main.pdf?_tid=ce3aa43e-cef1-11e3-b342-00000aacb361&amp;acdnat=1398702556_1adee439a6b55c82efa20917178fb0aa</a> and I am totally confused as to what this statement says on Page-177 in the Algorithmic Outline section of the paper</p>

<blockquote>
  <p>""Assume that we have an administrative subdivision into a set of
  regions. The regions and adjacencies can be represented by nodes and
  arcs of a graph F, which is the face graph of the subdivision.""</p>
</blockquote>

<p>Now there are a couple of questions I would like to ask :</p>

<p>1) What is meant by administrative subdivision into a set of regions?</p>

<p>2) What is meant by a Face Graph?</p>
",<graph-theory>
"<p>Let's say $G$ is the graph for the legal moves of the rook in a chess board where the nodes corresponds to squares in the board; thus, there are 64 nodes present. I am trying to figure out #edges in $G$.</p>

<p>I think, there should be $ \binom{8}{2} $ edges for each column and $ \binom{8}{2} $ for each row. Since there are 8 rows and 8 columns in the chess board the solution is: $$ 8 * \binom{8}{2} + 8 * \binom{8}{2} $$</p>

<p>Is it correct? If not, why not?</p>

<p>Note: You can see the movements of the rook at <a href=""http://en.wikipedia.org/wiki/Chess#Movement"" rel=""nofollow"">http://en.wikipedia.org/wiki/Chess#Movement</a>.</p>
",<graph-theory>
"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://math.stackexchange.com/questions/227681/how-to-find-chromatic-number-of-the-hypercube-q-n"">How to find chromatic number of the hypercube $Q_n$?</a>  </p>
</blockquote>



<p>Let $G$ be the graph whose vertex set is the set of $k$-tuples with coordinates in $\{0,1\}$ with $X$ adjacent to $Y$ when $X$ and $Y$ differ in exactly one position. Determine whether $G$ is bipartite.</p>
",<graph-theory>
"<p>Consider the following graph orientation problem: we would like to orient the edges of a graph G in such a way that each vertex has at most k incoming edges. Prove that this is possible if and only if |E[W]| ≤ k|W| for each subset of vertices W, where E[W] is the set of edges with both endpoints in W.</p>
",<graph-theory>
"<p>I know I'm probably wrong, maybe someone can explain it to me. I'm doing practice problems in preparation for a test that is coming up.</p>

<blockquote>
  <ol start=""12"">
  <li>Let u and v be two vertices in a graph G. Show that if G has two simple  paths between vertices u and v, then G has a simple circuit.</li>
  </ol>
  
  <p>Sol: Assume that G has the following two simple paths: (u,..P1..,v)
  and (v,..P2..,u). Therefore, G has the circuit (u,..P1..,v,..P2..,u).
  If this circuit has no  repeated edges, then the proof is complete.</p>
  
  <p><strong>Otherwise, let (x,y) be the first  edge that occurs in P1 and is repeated in P2. In this case, the circuit can be  represented as
  follows: (u,..P1..,x,y,..P1..,v,..P2..,y,x,..P2..,u) This circuit can
  be reduced to become as follows: (u,..P1..,x,..P2..,u) where no edge
  in P1 is repeated in P2(because edge (x,y) was the first edge in  P1
  that is repeated in P2). Thus, the reduced circuit is simple.</strong></p>
</blockquote>

<p>I've marked the part I'm having trouble with in bold. I don't
understand how we can just strip out the y vertex and simplify the
graph. I suspect I'm not preparing the graph properly, but I've
uploaded an example image to show why the graph can't be simplified.
<img src=""http://i.stack.imgur.com/DKQ0p.png"" alt=""enter image description here""></p>
",<graph-theory>
"<p>A <em>module</em> in a graph $G$ is a subset $M$ of the vertices such that all the vertices in $M$ have the same neighbourhoods <em>outside</em> of $M$. That is, if $v_1, v_2 \in M$ and $x \not\in M$, then we have $v_1x\in E(G)$ iff $v_2x\in E(G)$. They generalise connected components. </p>

<p><a href=""http://en.wikipedia.org/wiki/Modular_decomposition"" rel=""nofollow"">It is known</a> that every graph who is both connected and whose complement is connected, can be expressed as a union of disjoint, maximal nontrivial modules. </p>

<p>My question is, does the modular decomposition of a graph have any relationship known to the related matrices of a graph, e.g. adjacency matrix, Laplacian etc? </p>
",<graph-theory>
"<blockquote>
  <p>Let $A$ be an alphabet, $K$ and $N$ be natural numbers and $X$ be a
  list of $N$ strings over $A$, each one consisting of $K$ letters. You
  have one operation ($@f$): convert a string from $X$ to
  another string from $X$. The cost of applying $@f$ over $(X_1, X_2)$ is
  the number of letters that one needs to change so that $X_1$ becomes
  $X_2$. (thank you @mvw for pointing out that this is called ""Hamming distance"")</p>
  
  <p>You want to know (in a polynomial time) the minimum cost of making all
  of the strings the same.</p>
</blockquote>

<p>My solution: Create a matrix (a graph) $N \times N$ showing the cost of every possible operation over $X$ and then find the minimum spanning tree (let's call it $@T$). Using $@T$ I can run a BFS like algorithm starting from the leafs down to the ""center"" and calculate the total cost of the operations.</p>

<p>My question: Is my solution correct (I can not prove it - it only feels correct) and if not - can you give me a correct one?</p>

<p>Thank you for your time</p>
",<graph-theory>
"<p>$G$ is factor-critical $\Leftrightarrow$ $c_o(G-U) \leq |U|$, $\forall U \subseteq V(G)$ except when $U = \emptyset$ (where $c_o$ is the number of odd components - basically, the RHS means that $U = \emptyset$ violates Tutte's condition)</p>

<p><em>My attempt:</em> ($\Rightarrow$) $G-v$ has a perfect matching $\forall v \in V(G)$. Consider $G' = G-v$ for some arbitrary $v$. Then $c_o(G'-W) = c_o(G - X - v) \leq |W| = |X+v|$, $\forall W \subseteq V(G')$. However, $|W| \geq 1$ since $W = X + v$ for some $v \in V(G)$, $X \subset V(G)$.</p>

<p>I feel like I'm just going in circles and not really getting anywhere, and I don't know how to begin proving the reverse direction.</p>
",<graph-theory>
"<p>Vizing's theorem states that a graph can be edge-colored in either $\Delta$ or $\Delta+1$ colors, where $\Delta$ is the maximum degree of the graph.</p>

<p>A graph with edge chromatic number equal to $\Delta$ is known as a class 1 graph.</p>

<p>A graph with edge chromatic number equal to $\Delta+1$ is known as a class 2 graph.</p>

<p>which one of them are bigger?(contains more graphs) and why?</p>
",<graph-theory>
"<p>Let $G=(V,E)$ be a bipartite graph, with partition $V=A \cup B$. Recall that an independent set $I$ of $G$ is a set of vertices sharing no edges. </p>

<p>The <em>independent domination number</em> $i(G)$ is deﬁned to be the minimum cardinality among all maximal independent sets of vertices of $G$. </p>

<p>Suppose $G$ is <em>balanced</em>, i.e. $|A|=|B|$. Is it true that there always exists a balanced independent set $I$ (i.e., $|I \cap A|=|I\cap B|$) of size $|I|=i(G)$?</p>

<p>If not, what additional hypotheses on $G$ would imply such statement?</p>
",<graph-theory>
"<p>Let $K_{a,b}$ be the complete bipartite graph. Show that
$K_{a,b}$ is a tree if and only if $a = 1$ or $b = 1$.</p>

<p>The way my professor showed us for a complete graph is as below. I just don't know how to start for a complete bipartite graph. </p>

<blockquote>
  <p>$K_a$ is a tree if and only if $a=2$ or $a=1$.</p>
</blockquote>

<p><em>Proof:</em> For all $u\in V(K_a)$, $\deg(u) = a-1$ implies that $$2|E(K_a)| = \sum \deg(u) = (a-1)|V(K_a)|=a(a-1),$$ so $|E(K_a)|=\frac{a(a-1)}{2}.$</p>

<p>Since $K_a$ is connected, it is a tree if and only if \begin{eqnarray*} 0 &amp;=&amp;|E(K_a)| -|V(K_a)| +1 \\
&amp;=&amp; a(a-1)/2 -a +1 \\ 
&amp;=&amp; \frac{1}{2}(a(a-1)-2a+2)\\
&amp;=&amp; 1/2[a(a-1)-2(a-2)] \\
&amp;=&amp; 1/2[(a-1)(a-2)] =0 \end{eqnarray*}</p>

<p>Thus $K_a$ is a tree if and only if $a-1=0$ or $a-2=0,$ i.e., $a=1$ or $a=2$.</p>

<blockquote>
  <p>$K_{a,b}$ is a tree if and only if $a=1$ or $b=1$.</p>
</blockquote>

<p><em>Proof:</em> For all $u,v \in V(K_{a,b})$, $\deg(u) = a$ and $\deg(v) = b $ implies that $$2|E(K_{a,b})| = \sum \deg(u) + \sum \deg(v)= ab+ab=2ab,$$ so $|E(K_{a,b})|=ab.$</p>

<p>Since $K_{a,b}$ is connected, it is a tree if and only if \begin{eqnarray*} 0 &amp;=&amp;|E(K_{a,b})| -|V(K_{a,b})| +1 \\
&amp;=&amp; ab-a-b +1 \\ 
&amp;=&amp; (a-1)(b-1)=0\end{eqnarray*}</p>

<p>Thus $K_{a,b}$ is a tree if and only if $a-1=0$ or $b-1=0,$ i.e., $a=1$ or $b=1$.</p>
",<graph-theory>
"<p>I was doing excercises about graphs theory and I came across a quite interesting excercise (which probably has something to do with Hamiltonian Cycle):
""Is it possible to step on every field of a 4x4 or 5x5 chessboard just once and return to the starting point using a knight?""
Does anyone have any idea how to tackle this problem? I am more interested in a outline of how to do it or just some hints.</p>
",<graph-theory>
"<p>Eulers Identity: n-m+r=2</p>

<p>a)If G is a planar graph which contains no cycles of length less than <em>g</em>, then give an improved version of Eulers identity by relating the number of regions to the number of edges.</p>

<p>b)use (a) to show Petersen graph is not planar</p>

<p>I'm thinking i need to relate n with g such that a new euler identity can be written with an inequality instead...not sure how they relate though.</p>
",<graph-theory>
"<p>In a group of 4 people, is it possible for each person to have exactly 3 friends? Why?</p>

<p>My solution</p>

<p>n Let G be a graph with 4 vertices, one vertex representing each person in the group.
Join two vertices u and v by an edge if and only if u and v are friends. Then the degree of
each vertex equals the number of friends that the corresponding person has. If each person
has exactly 3 friends, then each vertex has degree 3. Therefore, the total degree would be
3 · 4 = 12. This is an even number.</p>

<p>$n\equiv 0\pmod{2}$ and $n&gt;3$</p>

<p>So It is possible.</p>

<p>Is this correct?</p>
",<graph-theory>
"<p>I am trying to understand directed preorders, a.k.a. directed sets. Are they analogous to connected DAGs?</p>
",<graph-theory>
"<p>Let $G$ be a planar minimal 5-chromatic graph. That is, any of its proper subgraphs has chromatic number at most 4. I need to prove that its minimum degree is at least 5. I want to prove by contradiction, first by assuming that there is a vertex of degree 4. But I couldn't find any contradiction. Can anyone offer any ideas?</p>
",<graph-theory>
"<p>Let's say we have a graph, with a list of edges and vertexes $(E,V)$, all the vertexes are connected to at least one edge at one end. There are many ways a complete set of <a href=""http://www.mathreference.com/gph,basis.html"" rel=""nofollow"">cycle basis</a> can be found out from it.</p>

<p>Now the issue is, is it <strong>always</strong> possible to find a complete set of cycle basis that each edge is shared by at most $2$ cycles?</p>

<p><strong><em>Edit: There is a <a href=""http://math.stackexchange.com/questions/1340/in-a-graph-is-it-always-possible-to-construct-a-set-of-cycle-basis-with-each-an/1343#1343"">mathematical argument</a> proving why it is not possible. But admittedly such a highly abstract reasoning is a bit hard for me to grasp. I would appreciate if someone can provide a graphical example of such a graph.</em></strong> </p>
",<graph-theory>
"<p>Let $G$ be a graph of order $8$ with $V(G)=\{v_1, v_2,...,v_8\}$ such that deg $v_i=i$ for $1 \leq i \leq 7$. What is deg $v_8$.</p>

<p>Any help or hints would be greatly appreciated.</p>
",<graph-theory>
"<p>In any undirected tree $T$, what is the maximum distance from any vertex $v$ with $\text{deg}(v) \geq 3$ to the closest (in a shortest path sense) vertex $y$ with $\text{deg}(y) \leq 2$? That is, $y$ can be leaf.</p>

<p>It seems to me that this distance can be at most $\dfrac{\text{diam}(T)}{2}$, and furthermore that the maximum distance will be attained from a graph center. Is this true? There's probably simple argument for it somewhere.</p>
",<graph-theory>
"<p>So I'm working on proving (via contradiction) that the flow number $\phi(G)$ of a bridgeless graph $G$ is always defined. I'm using the flow polynomial, and I got to a point where I have $0=T(0,1-u)$.</p>

<p>So, my question:
If $T(x,y)=0$ where $T(x,y)$ is the Tutte polynomial, what does this mean about the graph? Does it mean it has no edges at all?</p>

<p>Thanks!</p>
",<graph-theory>
"<p>I recently found the following exercise:</p>

<blockquote>
  <p>Given a cubic, simple undirected graph $G$ without cut edges, then $G$ is matching covered. I.e. every edge is contained in a perfect matching.</p>
</blockquote>

<p>My idea was that, given an edge $e=(u,v)$, I could delete from $G$ the two edges with vertex $u$, say $(u,v_1)$ and $(u,v_2)$, in order to obtain a new graph with:</p>

<ul>
<li>$1$ vertex of degree $1$ ($u$ itself)</li>
<li>$2$ vertices of degree $2$ ($v_1$ and $v_2$)</li>
<li>any other vertex of degree $3$.</li>
</ul>

<p>Then, using the Tutte theorem, I am trying to prove the existence of a perfect matching in this new graph as this would be a perfect matching for $G$ containing $e$.</p>

<p>Does anyone have some sorts of advice about that? </p>
",<graph-theory>
"<p>I consider an edge-coloured graph with the colours red and blue. Gyarfas proofed in his paper <a href=""http://www.renyi.hu/~gyarfas/Cikkek/16_Gyarfas_VertexCoveringsByMonochromaticPathsAndCycles.pdf"" rel=""nofollow"">http://www.renyi.hu/~gyarfas/Cikkek/16_Gyarfas_VertexCoveringsByMonochromaticPathsAndCycles.pdf</a> the existence of two cycles covering the vertices and intersecting on at most one vertex. </p>

<p>He considered the longest path consisting of a red path followed by blue path (such a path $P$ is Hamiltonian). If a vertex $v$ is not covered it must be joined in blue to the origin of $P$ and in red to the end of $P$. Then you can cover the vertices of $P$ and $v$ using the edge from the starting point of $P$ to the end point (lets call them $a$ and $b$). Therefore there exists a monochromatic cycle $C$ and a monochromatic path $P$ with different colors partitioning the vertex set. </p>

<p>I am asking for help with the drawing. I think it looks like <img src=""http://i.stack.imgur.com/CpNlu.jpg"" alt=""enter image description here""> but where is the cycle $C$?</p>
",<graph-theory>
"<p>Is there a graph homomorphism between the 6-hypercube (<a href=""https://en.wikipedia.org/wiki/File:6-cube_graph.svg"" rel=""nofollow"">https://en.wikipedia.org/wiki/File:6-cube_graph.svg</a>) and the cube (<a href=""https://en.wikipedia.org/wiki/File:3-cube_graph.svg"" rel=""nofollow"">https://en.wikipedia.org/wiki/File:3-cube_graph.svg</a>)?</p>
",<graph-theory>
"<p>Here $G_{n,p}$ represents the Erdős-Rényi random graph model, where the graph has order $n$ and each edge is added independently with probability $p$. I am faced with proving the following claim:</p>

<blockquote>
  <p>Show that there is a constant $c&gt;0$ such that, for every $p$ we have: </p>
  
  <p>$\mathbb{P}(G_{n,p}$ is disconnected) $\leq c \mathbb{P}(G_{n,p}$ has an isolated vertex).   $\,\,\,(*)$</p>
</blockquote>

<p>From the appearance of the question I think it is meant to be interpreted as asking ''in the limit $n \to \infty$''. It is clear that $\mathbb{P}($a fixed vertex of $G_{n,p}$ is isolated$)=(1-p)^{n-1}$. It is easy to calculate the expected number of isolated vertices using this, but I'm not convinced that helps.</p>

<p>As a last thought, a followup to the question asks ""What value of $c$ would be acceptable""? It is therefore probably not the case that a valid choice of $c$ will actually be obtained in the proof, although it may be reasonably clear how to calculate one; perhaps that clarifies the nature of the solution a little. Many thanks for your help.</p>

<p><strong>Edit:</strong> Update - I have thought a little more about it, and I have the following theorem we can hopefully make use of (if anyone is willing to help me!): suppose $p = \frac{\log{n}+\gamma(n)}{n}$ and $\gamma(n)$ grows at most slowly (say $o(\log \log n)$); then </p>

<blockquote>
  <p>if $\gamma(n) \to +\infty$, $\mathbb{P}(G_{n,p}$ disconnected)$\to 0$, </p>
  
  <p>if $\gamma(n) \to -\infty$, $\mathbb{P}(G_{n,p}$ disconnected)$\to 1$, </p>
  
  <p>if $\gamma(n) \to k$, $\mathbb{P}(G_{n,p}$ disconnected)$\to 1-e^{-e^{-k}}$.</p>
</blockquote>

<p>Now in the first case, being connected implies no isolated vertex, so $(*)$ holds with any constant $c$ since both probabilities are 0. Likewise, in the second case, the graph is almost surely disconnected: while this doesn't immediately imply that an isolated vertex exists, we can hopefully say for $X:=\#$ of isolated vertices,</p>

<p>$\mathbb{E}(X)=n(1-p)^{n-1} = n(1-\frac{\log{n}+\gamma(n)}{n})^{n-1} \sim ne^{-(\log{n}+\gamma(n))} = e^{-\gamma(n)} \to \infty$.</p>

<p>I <strong>think</strong> this last step holds but it may depend on $\gamma$: in general I'm not sure for which functions $(1+\frac{f(n)}{n})^n \to e^{f(n)}$, I know this is true for the log term but maybe not if $\gamma$ grows very fast (though obviously it can't grow any faster than $1-\frac{\log{n}}{n}$ otherwise we would have $p&gt;1$).</p>

<p>We can also calculate the second moment and get $\mathbb{E}(X^2)-\mathbb{E}(X)^2 \sim e^{-\gamma(n)}$ and deduce that with high probability there is an isolated vertex. Thus again, both probabilities are equal and we can take (e.g.) $c=1$. </p>

<p>The <em>hard</em> case is where $\gamma(n) \to k$: in this case we can reapply the same method to get $\mathbb{E}(X) \sim e^{-k}$, a constant. We can calculate again $\mathbb{E}(X^2) -\mathbb{E}(X)^2 \sim e^{-k}$, and use Chebyshev's inequality to calculate $\mathbb{P}(X=0) \leq e^{-k}/e^{-2k} = e^k$. If $k&lt;0$, then this gives us an actual bound on the probability; otherwise we just get $\mathbb{P}(X=0) \leq 1$. </p>

<p>Supposing $k&lt;0$ then; we can rewrite as $s:=\mathbb{P}(X&gt;0)=\mathbb{P}$(isolated vertex)$\geq 1-e^k$, $d:=\mathbb{P}($disconnected) and using the fact $d =1- e^{-e^{-k}}$ and a little rearranging I think we get out the inequality $d \leq 1-\exp\left(\frac{1}{s-1}\right)$. We can then find a $c$ which works by applying the lower bound to $s$ in terms of $k$ then looking at the values of $c$ such that $cx \geq 1-\exp\left(\frac{1}{s-1}\right),\,x \in [1-e^k,1]$. However, this is only for fixed $k$! If we try and do this for <em>every</em> $k&lt;0$ (i.e. every probability of this type) simultaneously, then we find that $c$ must be arbitrarily large. What's worse, this method doesn't work at all for $k \geq 0$ where we don't have a lower bound for $s$: in this case $s$ can be arbitrarily small and we can't pick a $c$ big enough to always work. So close and yet so far. </p>

<p>I am aware this question's length has spiralled out of control, so apologies for that - I know there's a good chance Math.SE is not going to provide me with an answer to this one. Nevertheless, it says add your working and this is what I managed to do! A proof which works for all <em>slowly</em> decreasing $\gamma$ or $\gamma \to k \in (-\infty,-\epsilon],$ any $\epsilon &gt; 0$. </p>

<p>I have a strong suspicion this is not how I was meant to try and tackle the question, but tragically this is the best I could do so far. Thank you in advance to anyone who actually reads through all this!</p>
",<graph-theory>
"<p>This is an interesting question where we are trying to solve another recursion which has same tree structure as the given recursion  and also has term similarities</p>

<p><strong>Given Data in question</strong></p>

<ol>
<li>$F_n=F_{n-1}+F_{n-2}$, where $F_1=F_2=1$, we have $F_n= \frac{(1+\sqrt{5})^n-(1-\sqrt{5})^n}{2^n\sqrt{5}}$ and generating function $g(x)= \sum_{n=0}^{\infty}F_nx^n=\frac{x}{1-x-x^2}$</li>
<li>More details of Fibonacci recursion and properties can be found <a href=""http://mathworld.wolfram.com/FibonacciNumber.html"" rel=""nofollow"">here</a>! .</li>
</ol>

<p><strong>Question</strong></p>

<p>Can we find solution for a)$Q_n$(interms of n)  b) $ g(x)= \sum_{n=0}^{\infty}Q_nx^n $ for the given recursion below   <strong>$nQ_n=Q_{n-1}+Q_{n-2}\tag 3$</strong> 
 $Q_1=Q_2=1$,by using the above results, given the fact that both follows same recursion tree  (in structure) even though results are different? if so please answer
<img src=""http://i.stack.imgur.com/qoTYd.png"" alt=""enter image description here""></p>

<p>NB :: This is not a home work problem. Logic is simple,the varying n will make it tough. And no  prof will give it as home work. I am trying this for weeks/months.. It is not simple. Attempt on a similar problem by me  can be found   <a href=""http://math.stackexchange.com/questions/881732/recurrence-solution-of-simple-recurrence"">here</a> </p>

<p>NB :: <strong>I know a method of using ODE. But I am trying to solve it with out ODE so that I can extent this to higher dimension like matrices in similar structure questions. Please avoid ODE solution</strong>  </p>
",<graph-theory>
"<p>I am wondering if there is proper terminology for a ""long"" loop in a social network. The end-game is to remove loops of n-edge length. (I need to research how to do that, but first I need the right terminology). </p>

<p><a href=""http://i.stack.imgur.com/hKXa3.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hKXa3.jpg"" alt=""enter image description here""></a></p>

<p>The code to recreate this graph in R igraph is here:</p>

<pre><code>g &lt;- data.frame( kin1 =c(1,15,15,5,4,17,5,18,19,20,21,22,23,24,25,26,27,28,29,30,30), 

       kin2 =c(4,5,16,4,16,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,5), stringsAsFactors = F)


    g1  &lt;- graph.data.frame(g, directed=FALSE)

    plot(g1, layout=layout.fruchterman.reingold)
</code></pre>
",<graph-theory>
"<p>Let $G=(V,E)$ be a graph.</p>

<p>Let $M1, M2$ be two matchings of $G$. Consider the new graph $G' = (V, M1 ∪ M2)$ (i.e. on the same vertex set, whose edges consist of all the edges that appear in either $M1$ or $M2$). Show that $G'$ is bipartite.</p>

<p>Helpful definition: A connected component is a subgraph of a graph consisting of some vertex and every node and edge that is connected to that vertex.</p>
",<graph-theory>
"<p>Given is a weighted complete graph where every weigth is a positive ineger. Let n be the amount of vertices.</p>

<p>I have to prove that the number of edges of a minimum spanning tree of that graph is equal to n-1.</p>
",<graph-theory>
"<p>Let $G$ be a graph with $\chi(G)=k+2$ for $k\ge3$. Prove that $G$ contains a cycle of a length $l$ such that $l \equiv 2 \;(\bmod\; k)$. </p>

<p>Not quite sure how to approach this at all. I know that there is a subgraph $H$ with $\delta(H)\ge k+1$ (critical subgraph/degeneracy condition), and therefore $H$ has a cycle with length at least $k+2$, but I don't know how to get the $l \equiv 2 \;(\bmod\; k)$.</p>

<p>Thanks in advance.</p>
",<graph-theory>
"<p>I am working through a proof of the following Theorem:
Let $G$ be a connected, $k$-regular graph, $G\neq K_n$, then $G$ is strongly regular if and only if $|Spec(G)|=3$.</p>

<p>Now I am having trouble with the given proof of the ""$\Leftarrow$"" direction:</p>

<p>Let $k,\beta_1,\beta_2$ be the eigenvalues of $A$. Consider the matrix
    \begin{align*}
		M:=\frac{1}{(k-\beta_1)(k-\beta_2)}(A-\beta_1 I)(A-\beta_2 I),
	\end{align*}
    then $M$ has all of its eigenvalues equal to $0$ or $1$. This is, since if $b_i$ is an eigenvector of $A$ to eigenvalue $\beta_i$, $i=1,2$, with $b_i\perp\mathbb{1}$, then
    \begin{align*}
		M\cdot b_1 =0,\\
		M\cdot b_2 =0,\\
		M\cdot\mathbb{1}=\mathbb{1}.
	\end{align*}
    Therefore $Spec(M)=\{1^{(1)},0^{(n-1)}\}$. Since $G$ is connected and because of the spectrum of $J$ (the $n\times n$ matrix with all 1's entries) as computed as $Spec(J)=\{1^{(1)},0^{(n-1)}\}$, we get $M=\frac{1}{n}\cdot J$. That is
    \begin{align*}
		\frac{1}{n}\cdot J=\frac{1}{(k-\beta_1)(k-\beta_2)}(A-\beta_1 I)(A-\beta_1 I)(A-\beta_2 I).
	\end{align*}
    Factorising this out gives
\begin{align*}
   A^2=k\cdot I+\lambda\cdot A+\mu\cdot(J-I-A),
\end{align*}
which is a characterization of $G$ being strongly regular.</p>

<p>Now I doubt the implication to $M=\frac{1}{n}J$. For all we know, $M$ might be a diagonal matrix with a single $1$ and otherwise only $0$'s. Does this implication hold?</p>
",<graph-theory>
"<p>Let $f(G)$ be the smallest $m$, such that one can find $2m$ vertices in $G$ with the following property: pair up the vertices in any way, and find $m$ paths that join each pair. Then every set of path constructed will have a intersection. (Is there a name for this graph property?)</p>

<p>I have a feeling this property might be connected to the connectedness of a graph. but for large grid graphs, I can chose many vertices and still find disjoint paths connecting the pairs, when it is only 4 connected.</p>

<p>For a $n\times n$ grid graph $G$, what is $f(G)$?</p>

<p><strong>Edit:</strong> I have found the name of the property. A graph is $k$-linked if there exist $2k$ elements, such that there are $k$ disjoint paths that pairs two of them.</p>
",<graph-theory>
"<p>If you could explain the answer simply It'd help me out as I'm new to this subject.</p>

<p><strong>For which values of n is the complete graph Kn bipartite? For which values of n is Cn (a cycle of length n) bipartite?</strong></p>

<p>Is it right to assume that the values of n in Kn will have to be even since no odd cycles can exist in a bipartite?
Also, for Cn is it correct that the length of the cycle will also need to be even since you need to take an even number of steps to travel from V1 to V2 and V2 back to V1?</p>
",<graph-theory>
"<p><a href=""http://i.stack.imgur.com/LB76M.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LB76M.png"" alt=""enter image description here""></a></p>

<p><a href=""https://staff.fnwi.uva.nl/n.s.walton/Notes/Hall_Birkhoff.pdf"" rel=""nofollow"">https://staff.fnwi.uva.nl/n.s.walton/Notes/Hall_Birkhoff.pdf</a> </p>

<p>Could someone possible explain how the inequality arises in $(44)$</p>
",<graph-theory>
"<p>Let $u$ and $v$ be 2 vertices in a tournament $T$. Prove that if $u$ and $v$ do not lie on a common cycle then $od(u)≠od(v)$</p>

<p>I have no idea how to start this proof. Please help.</p>
",<graph-theory>
"<p>An infinite $r$-regular graph is a graph with $\infty$ vertices where each vertex touches precisely $r$ edges.</p>

<p>We say an $r$-regular graph can be embedded in the $R^2$ Euclidean plane if its set of edges and vertices can be represented as a set of points on the plane where each point is connected via an edge to precisely the $r$ closest points to it. For example, some $4$-regular graphs can be embedded in the plane by placing each vertex of the graph on a unique point $(m,n)$ on the plane where $m$ and $n$ are integers. Some $8$-regular graphs can be embedded using the same placement.</p>

<p>The question is: for what values of $r$ do there exist connected $r$-regular graphs that can be embedded in the plane? </p>

<p>The graphs need not be planar, but an answer dealing with the planar case is welcome.</p>
",<graph-theory>
"<p>A permutation matrix is a square matrix with exactly one $\textbf{1}$ in each row and column, and zeros in all other positions of the matrix. Let $M$ be an $n\times n$ $\{0,1\}$ matrix with exactly $m$ ones in each row and column. Prove that $M$ can be written as the sum of $m$ permutation matrices.</p>

<p>I saw my lecturer about this problem and the hint he gave me was to think about decompositions of bipartite graphs into perfect matchings. </p>

<p>For the life of me I don't really understand what he means by that nor do I even know how to get started on the question.</p>

<p>Any help would be greatly appreciated.</p>
",<graph-theory>
"<p>Give an example of 2 non isomorphic regular tournament of the same order</p>

<p>I tried so many tournaments of the same order but got no luck, if they are regular, meaning all vertices of them have the same in and out degree, so they have arc preserved, which end up to make them isomorphic. I'm not sure there are such tournaments exist.</p>
",<graph-theory>
"<p>In doing a problem from graph theory by west. In one question it asks you to find the maximum sized clique in the graph.  I think it's 5 (using the top or bottom vertex). However in the solution manual it mentions that because the graph contains two points of degree 3 this is not possible. Is that correct? Can anyone explain why if so? Here's  a link to the graph. <a href=""http://imgur.com/wGCcf89"" rel=""nofollow"">http://imgur.com/wGCcf89</a></p>
",<graph-theory>
"<p>Is there a good database of unsolved problems in graph theory?</p>
",<graph-theory>
"<p>I'm taking an introductory graph theory course and I am having trouble going about answering this question. I've been told to look at the graph compliment but I don't quite understand how that ties into this.</p>
",<graph-theory>
"<p>A graph $G$ has the property that every edge of $G$ joins an odd vertex with an even vertex. Show that $G$ is bipartite and has even size.</p>
",<graph-theory>
"<p>Let $G=(V,E)$ be a connected undirected graph such that $E$ is the union of $n$ forests $F_1\cup F_2 \cup \dots \cup F_n$. Each forest has $V$ as its nodes and containts $k$ disconnected components. Each component is simply an edge connecting 2 vertices. </p>

<p>is it possible to compute the chromatic polynomial of $G$ through $F_1,F_2,\dots,F_n$? since the chromatic polynomial for $F_i$ is easy to compute.  </p>
",<graph-theory>
"<p>I recently met with a professor to discuss this problem and she didn't have an answer for how to do the calculation. What I did learn is that the counting itself is considered NP-Hard and is in a class known as #P. I'm currently running an efficient 4-coloring on a map of the US (contiguous 48) and was hoping to be able to compare the brute force count with a known value.</p>

<p>Any help is appreciated.</p>
",<graph-theory>
"<p>Give an undirected simple graph $G$ with $n$ vertices and $m$ edges, its <strong>2-Lift</strong> is constructed as follows:</p>

<ul>
<li>Define $G_1$ to be the original graph $G$. Make a duplicate copy of $G$ and call it $G_2$.</li>
<li>Now each edge $(u,v)$ in $G$ corresponds to a pair of edges $(u_1,v_1)$ in $G_1$ and $(u_2,v_2)$ in $G_2$.</li>
<li>For each edge $(u,v)$ in $G$, there are two choices and randomly decides what to do:
<ol>
<li>Leave each pair of edges as they are: $(u_1, v_1)$ and $(u_2, v_2)$</li>
<li>Make them cross: $(u_1, v_2)$ and $(u_2,v_1)$</li>
</ol></li>
</ul>

<p>As $G$ has $m$ edges, there are $2^m$ possibilities of 2-lifts of $G$. Naturally, there are two extreme cases when the 2-lift is not connected:</p>

<ul>
<li>When all the pairs of edges stay as they are, then $G_1$ and $G_2$ are isolated.</li>
<li>When all the pairs of edges are switched, then we still have two isolated graphs.</li>
</ul>

<p>Other than these two trivial cases, all the 2-lifts are connected graphs.</p>

<p>The 2-lifts of a graph are recently used for <a href=""http://arxiv.org/abs/1304.4132"" rel=""nofollow"">the existential proof of bipartite Ramanujan graphs</a> by Marcus, Spielman, and Srivastava. You may take a look at the visual representation of 2-lifts at the following urls.</p>

<ul>
<li><a href=""http://www.tcs.tifr.res.in/~prahladh/mysore2013/slides/Srivastava-mp1.pdf"" rel=""nofollow"">http://www.tcs.tifr.res.in/~prahladh/mysore2013/slides/Srivastava-mp1.pdf</a></li>
<li><a href=""http://cs-www.cs.yale.edu/homes/marcus/talks/bipartite_ramanujan.pdf"" rel=""nofollow"">http://cs-www.cs.yale.edu/homes/marcus/talks/bipartite_ramanujan.pdf</a></li>
</ul>

<hr>

<p>Now assume that the original graph $G$ is <a href=""http://en.wikipedia.org/wiki/Complete_bipartite_graph"" rel=""nofollow"">a complete bipartite graph</a> $K_{n,n}$. Except the two trivial extreme cases, it seems that all the 2-lifts are connected and have <a href=""http://mathworld.wolfram.com/GraphDiameter.html"" rel=""nofollow"">diameter</a> 4. (Remember that $dist(u,v)$, the distance of two vertices $u$ and $v$ in a graph, is defined to be the number of edges in a shortest path between $u$ and $v$, and the diameter of a graph is $\max_{u,v}dist(u,v)$.)</p>

<p>Is my conjecture true? If it is, how can I prove it?</p>
",<graph-theory>
"<p>I know Dijkstra's algorithm to find the shortest way between 2 nodes, but is there a way to find the shortest path between 3 nodes among $n$ nodes? Here are the details:</p>

<p>I have $n$ nodes, some of which are connected directly and some of which are connected indirectly, and I need to find the shortest path between 3 of them.</p>

<p>For example, given $n = 6$ nodes labelled A through F, and the following graph:</p>

<pre><code>A--&gt;B--&gt;C
A--&gt;D--&gt;E
D--&gt;F
</code></pre>

<p>How can I find the shortest path between the three nodes (A,E,F)?</p>

<p>I am looking for a solution similar to Dijkstra's shortest path algorithm, but for 3 nodes instead of 2.
<br/>
Please Note : <br/>
1- The Starting Node is A  <br/>
2- The Sequential is not important just the path needs to cover all these Nodes   <br/>
3- Their is no return back to A   <br/>
Please find the diagram Image
<img src=""http://i.stack.imgur.com/M1wxF.png"" alt=""enter image description here"">
Regards &amp; Thanks<br />
Nahed</p>
",<graph-theory>
"<p>I am seeking a listing of the distinct <a href=""http://mathworld.wolfram.com/HamiltonianCycle.html"" rel=""nofollow"">Hamiltonian cycles</a> following the edges of the icosahedron and the dodecahedron.  By <em>distinct</em> I mean they are not congruent by some symmetry
of the icosahedron or dodecahedron (respectively). So they do not make the same sequence of angular turns. For example (as Gerhard corrected me in the comments), there is just one distinct Hamiltonian cycle on the cube.</p>

<p>Hamiltonian cycles of the Platonic solids are all over the web, but I am not finding a definitive list of the number and a description of each.  Thanks to anyone who can point me in the right direction!</p>
",<graph-theory>
"<p>Let us denote by $\def\Graph{{\sf Graph}}\Graph$ the category of directed graphs
$G$ with multiple edges: they are given by a set $G_v$ of vertices, a set $G_e$ of edges,
and two functions from $G_e$ to $G_v$ determining the target ($\tau$) and the source ($\sigma$)
of every edge. The morphisms are called graph homomorphisms: given graphs
$G$ and $G'$, a graph homomorphism is a pair of functions $h_v\colon G_v \to G'_v$ and
$h_e\colon G_e \to G'_e$ such that the source and the target of every edge are preserved.</p>

<p>NOW,An algebraic theory is a small category $T$ with finite products.
An algebra for the theory $T$ is a functor $A\colon T \to {\sf Set}$ preserving finite products.
We denote by $\def\Alg{\mathop{\rm Alg}}\Alg T$ the category of algebras of $T$. Morphisms, called homomorphisms,
are the natural transformations; that is, $\Alg T$ is a full subcategory
of the functor category ${\sf Set}^T$
.</p>

<p>Q.HOW algebraic theory of $\Graph$ ($T_\Graph$) arises as the free completion, of the category $C$ consisting of two parallel arrows from $e$ to $v$:
$\tau$, $\sigma$, under products--what graph determines the $A$-image of a word in $\{e,v\}^*$ and a pair $(a, \alpha)$;see below.</p>

<p>Here,free completion under products in a category $C$ can be described as the category of all words over $\mathop{\rm obj} C$ (the
set of objects of $C$); that is, objects have the form of $n$-tuples $c_0\ldots c_{n-1}$,
where each $c_i$ is an object of $C$ (and where $n$ is identified with the set
$\{0,\ldots, n − 1\}$), including the case $n = 0$ (empty word). Morphisms from
$c_0\ldots c_{n-1}$ to $c'_0\ldots c_{k-1}'$ are pairs ($a$, $\alpha$) consisting of a function $a\colon k \to n$
and a $k$-tuple of $C$-morphisms $\alpha = (\alpha_0, \ldots, \alpha_{k-1})$ with $\alpha_i\colon c_{a(i)} \to  c'_i$.</p>
",<graph-theory>
"<p>To prove that two graphs are isomorphic I was taught to first consider the bijection between the two graphs. I was never taught however the rules when coming up with the bijection. 
Is my only rule, when coming up with a bijection between two graphs, that the vertices that I match, that they must have the same degree? 
eg. if <code>a</code> in graph 1 has a degree of 3 then I can only match it up with other vertices in graph 2 that have degrees of 3?</p>

<p>I am really confused on how to do this. </p>
",<graph-theory>
"<p>I am trying to understand this problem and yes this is from my assignment and I should be doing it myself, but I have been staring at it for 2 hours and not getting anywhere, so decided to post it here.</p>

<p>Let G be a connected cubic simple graph that contains 2 edge-disjoint spanning trees show that |G| = 4.</p>
",<graph-theory>
"<p>Is there a planar point set such that no matter how you colour the points with two colours can you can always find a triangle with exactly one point inside so that all four points have the same colour?</p>

<p>I'm not really sure how to start working on this graph problem any advice? </p>
",<graph-theory>
"<p>I have a graph, not necessarily connected, that I know for a fact has vertices with degrees at most $3$. I need to find it's chromatic number in polynomial time.</p>

<p>Well then it's just a matter of checking whether the graph is edgeless (if so, then $\chi(G)=1$), or if it's 2-colorable (simple DFS), but I'm stuck at checking if it's $3$-colorable. Brooks theorem states that </p>

<blockquote>
  <p>For any connected undirected graph G with maximum degree Δ, the chromatic number of G is at most Δ unless G is a clique or an odd cycle, in which case the chromatic number is Δ + 1.</p>
</blockquote>

<p>So basically I just have to check if it's a clique or if it's a cycle of odd length (which can be done by DFS also), and If they are not then $\chi(G)=3$, otherwise is $4$, right?</p>

<p>What I'm specifically asking is this. Brooks theorem assumes that $G$ is connected. But what if it's not? Won't I basically get the same thing by treating all components of $G$ separately? Each of them is connected and for each $H \in G$ $Δ_h \leq Δ$, so by Brooks theorem I'm safe if all of the components are not odd cycles, right?</p>
",<graph-theory>
"<p>I'm creating a directed graph from an adjacency list. The $0$ present that there is no relation while the $1$ represent that there is.</p>

<p>So i have a quick question regarding this.</p>

<p>Lets assume that $AB = 1$ that is that it has a connection.
$BA = 0$ which means that is does not have a connection. This continues throughout the graph. So we assume that it is anti symmetric. However the graph has loops. So $BB = 1$  $CC= 1$ etc.</p>

<p>It still considered anti symmetric?</p>
",<graph-theory>
"<p>Fáry's theorem is a (fairly famous) statement which asserts that every finite simple planar graph can be drawn in a way such that every edge is represented by a straight line segment.</p>

<blockquote>
  <p>Does this theorem extend to countably infinite graphs?</p>
</blockquote>

<p>A standard proof of this theorem proceeds by induction on the number of vertices, and hence is unapplicable here. I suspect the answer is <strong>no</strong>, but I can't think of a counterexample.</p>

<p>Thanks in advance.</p>
",<graph-theory>
"<p>This question came from a question asked earlier today linked <a href=""http://math.stackexchange.com/questions/1403945/grouping-kids-in-groups-of-4"">here</a></p>

<p>The question implicitly asked how to make a schedule with his/her class of 24 students such that:
1)  Everyday will consist of the 24 students split into 6 groups of 4 students.
2)  Over the course of several days, every pair of students will have been in a group with one another.</p>

<p>This seems like it might be a Steiner system, but i'm not exactly sure. <a href=""https://en.wikipedia.org/wiki/Steiner_system"" rel=""nofollow"">Steiner system</a> $S(2,4,24)$.
I've found that a necessary condition for this system existing is the existence of $S(1,3,23)$.  Now here's where I'm not sure if I'm following the definition of Steiner system exactly.  This system, $S(1,3,23)$ would be a $K_3$ decomposition of $K_{23}$, would it not?  <strong>Perhaps this is not exactly a Steiner system, this is part of my question.  Help me to clear this up</strong></p>

<p>How many days would be required to schedule this?<br>
Well, we need to partition the edges of $K_{24}$ which has $\binom{24}{2}$ edges.  Each $K_4$ has 6 edges.  So on a single day we would have 36 edges while we need a total of 276 edges.  However, $\frac{276}{36}$ is not an integer, so this actually tells us that <strong>we can't do this if we require no edges to be repeated.</strong>  So, some pairs of students will be in the same group with each-other more than once.</p>

<p><strong>The Question</strong>:  What is the minimum number of days required to schedule this?  Will $\lceil\frac{276}{36}\rceil = 8$ days be sufficient?  What exactly am I dealing with here?  Some sort of design?  A Steiner system?  What?</p>
",<graph-theory>
"<p>Given a simple graph on $n$ vertices, how many graphs are atmost there that are isomorphic to the given graph? Is it $\Theta(n^2!)$ or $\Theta(n!)$ which is number of permutations of rows or columns?</p>
",<graph-theory>
"<p>For any 3-coloring of $K_{17}$ I have to show there exists either a red, blue or green triangle. To start, can I use proof by contradiction with color red, blue, green? So $(0,0,136)$ means all 136 edges are green. Clearly this has green triangle. If we color the ""outside"" of the graph all one color, say red, then we have 17 edges that are red and no edge of the interior can be colored red in order to avoid red triangles. Then I try to obtain a contradiction?</p>
",<graph-theory>
"<p>I've been a talk with a PhD student about some graph issue and told me about GRAIL graph and have drawn it for me as you see in the picture, however, I try to generalize so-called ""Grail graph"" to k-pair problem. But I am not able to find a definition, though.</p>

<p>In the picture is an example of 2-pair problem in the GRAIL graph.</p>

<p><a href=""http://postimg.org/image/4a3xvsik9/"" rel=""nofollow"">http://postimg.org/image/4a3xvsik9/</a></p>

<p>so my question is: have you seen this kind of graph? or have you heard about this graph, or do you know a name for this graph (may be I misunderstood with this kind of graph).</p>

<p>I have to point that I found a paper called ""GRAIL: Scalable Reachability Index for Large Graphs"". However, it doesn't give me an example of a graph, instead it just give me a ""a good scale for reachability graph between fixed two nodes""; </p>

<p>Thank you.</p>
",<graph-theory>
"<blockquote>
  <p>Show that any connected graph $G$ satisfies $\lvert E(G)\rvert \geq \lvert V(G)\rvert -1 $ by induction on the number of edges.</p>
</blockquote>

<p>My attempt: </p>

<ol>
<li>Base Case: For any connected graph $G$ let number of vertices $= 1$
so $0 \geq 1-1$ which is true.</li>
<li>Now i'm not really sure what to do here.</li>
</ol>
",<graph-theory>
"<p>Let $G$ be an undirected graph and define </p>

<p>$$P=\{x \in R^{V}: x(u)+x(v) \leq 1 \:\:\text{for all edges}\:\: e=uv,\:\: x \geq 0\}$$</p>

<p>Show that any vertex $v$ of $P$ is half integral.</p>
",<graph-theory>
"<p>In software engineering, there is a coverage metric for testing called <a href=""https://en.wikipedia.org/wiki/Modified_condition/decision_coverage"" rel=""nofollow"">modified condition/decision coverage</a>, or MC/DC for short. This metric is well-known in the avionics industry due to showing up in the <a href=""https://en.wikipedia.org/wiki/DO-178B"" rel=""nofollow"">DO-178B</a> and <a href=""https://en.wikipedia.org/wiki/DO-178C"" rel=""nofollow"">DO-178C</a> standards as part of the testing requirements for Level A airborne software.</p>

<p>One way of thinking about MC/DC is in terms of Boolean functions. For an $n$-input Boolean function $f : \{0, 1\}^n \rightarrow \{0, 1\}$, MC/DC coverage is achieved if we can show that each of the $n$ input variables of $f$ has an <em>independent effect</em> on the output of $f$. That is, we have to exhibit pairs of input strings $x$ and $y$ such that $f(x) \neq f(y)$ and $x$ and $y$ differ only at a single position $i$. MC/DC coverage is achieved if we can find such pairs of strings for each input position $0 \leq i &lt; n$.</p>

<p>Now, test rig time on avionics hardware is often expensive. Companies have a strong desire to minimize the number of test cases that have to be run on their devices to achieve certification.* Obviously, if it is possible to achieve MC/DC coverage at all on some $n$-input function, then it can be done with $2n$ inputs: simply list all $n$ pairs of inputs individually. However, it is often possible to reuse the same input string to show the effect of multiple input variables. For example, if $f$ is a 4-input AND gate, then we can show the effect of all 4 input variables by evaluating $f$ at just 5 input strings: $\{1111, 0111, 1011, 1101, 1110\}$. This works because the necessary 4 pairs are $[\{1111, 0111\}, \{1111, 1011\}, \{1111, 1101\}, \{1111, 1110\}]$.</p>

<p>This prompts the question in the title: <strong>Is it always possible to get MC/DC coverage on an $n$-input Boolean function with $n + 1$ test cases (assuming that it is possible to get MC/DC coverage at all)?</strong></p>

<p>To state this question more formally, I will use the following definitions:</p>

<ul>
<li>Call two $n$-bit strings <em>$i$-adjacent</em> if they have <a href=""https://en.wikipedia.org/wiki/Hamming_distance"" rel=""nofollow"">Hamming distance</a> $1$, differing only at position $i$ ($0 \leq i &lt; n$).</li>
<li>Call an $n$-input Boolean function $f$ <em>dependent on all of its inputs</em> if for all $0 \leq i &lt; n$ there exist $i$-adjacent strings $x$ and $y$ such that $f(x) \neq f(y)$.</li>
<li>Let $f$ be an $n$-input Boolean function, and let $T$ be a list of $n$ unordered pairs of $n$-bit strings. Then $T$ <em>gets MC/DC coverage</em> on $f$ if for all $0 \leq i &lt; n$, we have $T_i = \{v_1, v_2\}$, $v_1$ and $v_2$ are $i$-adjacent, and $f(v_1) \neq f(v_2)$.</li>
</ul>

<p>Then my question is as follows: <strong>For any $n$-input Boolean function $f$ that is dependent on all of its inputs, does there always exist a list $T$ that gets MC/DC coverage on $f$ such that $|\bigcup_{0 \leq i &lt; n} T_i| = n + 1$?</strong></p>

<p>I have tried for a while to construct a counterexample function, but in every case I have found that either I could contrive a set of $n + 1$ test cases that works, or else the function ends up not actually depending on one or more of its input variables. For an example of a simple function that has an $n + 1$ solution that is not immediately obvious, consider the function $f(a, b, c, d) = (a \vee b) \wedge (c \vee d)$. The 5-element set $\{1010, 0010, 0110, 1000, 1001\}$ gets MC/DC coverage on this function, because we have $[\{1010, 0010\}, \{0010, 0110\}, \{1010, 1000\}, \{1000, 1001\}]$.</p>

<p>I have also made some minor efforts to prove the statement using subtrees of <a href=""https://en.wikipedia.org/wiki/Hypercube_graph"" rel=""nofollow"">hypercube graphs</a>, but I feel like my combinatorics background is not strong enough (or I'm missing something obvious). It's easy enough to check all the cases by hand for $n \leq 2$, but for $n \geq 3$ I'm stuck.</p>

<p><sub>* Don't think about this too hard the next time you're on an airplane.</sub></p>

<hr>

<p><strong>UPDATE:</strong> I have run a computer search with an SMT solver and determined that there are no counterexamples with $n \leq 4$.</p>

<p><strong>UPDATE 2:</strong> Here is a restatement of the problem in terms of hypercube graphs. Let $G$ be the $n$-dimensional hypercube graph, and let $f$ be a vertex coloring of $G$ with two colors (not requiring that adjacent vertices have different colors). Let $G'$ be the subgraph of $G$ obtained by removing any edge whose vertices are both the same color, so that the vertex coloring is proper. Suppose $G'$ contains an edge along each dimension. Then, does there always exist a set of $n + 1$ vertices such that the induced subgraph of $G'$ on these vertices also contains an edge along each dimension?</p>

<p>Also, here is a proof that it is not possible to use <em>fewer</em> than $n + 1$ vertices for any $f$. Suppose that we have a subgraph $H$ of the $n$-dimensional hypercube graph with $n$ edges and $k \leq n$ vertices. Since there are not more vertices than edges, $H$ must contain a cycle. But if $H$ contains a cycle, then it has to contain two edges that are both along the same dimension. Since there are only $n$ edges and at least two of them are along the same dimension, $H$ can't have edges along all $n$ dimensions. Note that this is purely a pigeonhole principle argument; it does not in any way depend upon $f$.</p>
",<graph-theory>
"<p>I'm currently working on the exercises of Noga Alon's book ""The Probabilistic method"". One exercise said that we are given a graph $H$ with $p$ vertices and that there exists a graph $G$ with $n &gt; p$ vertices and $m$ edges containing no copy of $H$.</p>

<p>We have to prove that if we have $k &gt; \frac{n^2ln(n)}m$ and we can find a $k$-coloring of a complete graph $K_n$ with $k$ colors then there exists a graph with no monochromatic $H$. I know this should be proved with probabilistic method, I just don't know how. Could any one give me a hint?</p>
",<graph-theory>
"<p>We say that $G$ is $∆$-critical if $G$ is connected with $∆(G) = ∆$, $χ'(G) = ∆ + 1$, and $χ'(G − e) &lt; χ'(G)$ for any $e ∈ E(G)$. Prove that if $G$ is $∆$-critical, then $d(x) + d(y) ≥ ∆ + 2$ for any $xy ∈ E(G)$.</p>

<p>Any hints or proofs are greatly appreciated.</p>
",<graph-theory>
"<p>Let $G$ be an undirected simple graph and let $A$ be its adjacency matrix. It is easy to see that $A$ is neither positive semidefinite nor negative semidefinite. </p>

<p>I would like to know if there are interesting graphs $G$ for which smallest eigenvalue of $A$ is at least $-1$. Clearly, cliques and disjoint union of cliques have this property. Are there any other graphs with this property?</p>

<p>Slightly related: A quick google search reveals that graphs with smallest eigenvalue $-2$ have been studied. What is special about $-2$ (or not special about $-1$)?</p>
",<graph-theory>
"<p>Let $H$ be a simple graph that has no cycles of length more than $3$. Each vertex has degree of $n$. Is it possible to prove $H$ has at least $2n$ vertices?</p>
",<graph-theory>
"<blockquote>
  <p>Each pair of cities in a nation has exactly one direct one-way road between them. Show that there is a path which visits each city exactly once.</p>
</blockquote>

<p>Now, this problem seems ripe for induction, but I have hit a bit of a snag trying to solve it that way. If we assume that this is true for $n$ cities, then I think it would be possible to add a city which only goes to other cities. This city would then not be able to fit onto the path already established, correct? I feel like I'm close, but not quite able to understand how to find a solution yet.</p>
",<graph-theory>
"<p>I am trying to figure out how to formally describe a probabilistic directed graph. In plain English the properties of the graph are as follows :</p>

<p>A graph is comprised of a set of nodes each with 2 edges. These edges can connect at random with any other two nodes. While each node can set only two outgoing edges, it can recieve any number of incoming edges. Each node re-assigns its edges at an individual rate.</p>

<p>If I have described it right, those nodes which reassign edges more frequently will cluster in terms of network connectivity over time, since the chance of their finding one another is higher. </p>

<p>If anyone could suggest a way to formally state this, or an online reference to a similar problem, that would be very helpful.</p>

<p>Thanks in advance,</p>

<p>Neil</p>
",<graph-theory>
"<p>Let's consider a query set $Q$ and a larger superset $S$. Each element of $Q$ exists in $S$. The goal is to express $Q$ using the smallest number of (connected) ""components"" of $S$.</p>

<p>Here is a concrete example:
$Q=\{\textrm{I love France and wine}\}$
$S=\{(\textrm{I live here}), (\textrm{I love you and her}), (\textrm{France is beautiful}), (\textrm{cheese and wine})\}$</p>

<p>A solution for $Q$ might:
- ""I"" from ""I live here""
- ""love"" from ""I love you and her""
- ""France"" from ""France is beautiful""
- ""and"" from ""I love you and her""
- ""wine"" from ""cheese and wine""
This results in 5 ""components"", i.e. ""I"", ""love"", ""France"", ""and"", ""wine""</p>

<p>A better solution is:
- ""I love"" from ""I love you and her""
- ""France"" from ""France is beautiful""
- ""and wine"" from ""cheese and wine""
This results in 3 ""components"", i.e. ""I love"", ""France"", ""and wine""
which might be the optimal solution for this example. We want to minimize this number of ""components"".</p>

<p>Is there anyone who knows how such algorithm is called?
I searched in text parsing, text mining and so on but I did not find anything appropriate.</p>
",<graph-theory>
"<p>Let $W$ be the non-negative, symmetric adjacency/affinity matrix for some connected graph. If $W_{ij}$ is large, then vertex $i$ and vertex $j$ have a heavily weighted edge between them. If $W_{ij} = 0$, then no edge connects vertex $i$ to vertex $j$. </p>

<p>Now $L = \mathrm{diag}(W\mathbf{1})-W$ is the (unnormalized) graph Laplacian. Let $v$ be the Fiedler vector of $L$, that is, a unit eigenvector corresponding to the second smallest eigenvalue of $L$. As $W_{ij}$ increases, all else equal, $|v_i - v_j|$ tends to decrease---at least this is the idea behind spectral clustering. What is an upper bound on $|v_i - v_j|$, given quantities that don't require computing $v$, like $W_{ij}$ and $\|W\|$? </p>

<p>Any suggestions or thoughts would be greatly appreciated.</p>
",<graph-theory>
"<p>I am having difficulties with problem 3.27 of Algorithms by Dasgupta, Papadimitriou and Vaziran. It reads:</p>

<blockquote>
  <p>Two paths in a graph are called edge-disjoint if they have no edges in
  common. Show that in any undirected graph, it is possible to pair up
  the vertices of odd degree and find paths between each such pair so
  that all these paths are edge-disjoint.</p>
</blockquote>

<p>I am new to graphs and have no idea how to start.</p>

<p>Could anyone help?</p>

<p>Thank you, any help will be appreciated</p>
",<graph-theory>
"<p>I know that if $A$ is a 0-1 adjacency matrix then $[A^k]_{ij}$ is the number of walks of length $k$ from $i$ to $j$. Does this generalize nicely? The reason for this question is to interpret a result in <a href=""http://press.princeton.edu/titles/8767.html"" rel=""nofollow"">Matthew O. Jackson's Social and Economic Networks</a> where he considers a model in which each person $i$ assigns assign weights $g_{ij}$ to how much he cares about the action of person $j$ (actions are numbers in $[0,\infty))$. Each player chooses an action taking into account the actions of all the other people in the network with respect to the weights that they place on those actions. I'll leave the details, but he comes up with this</p>

<p><img src=""http://i.stack.imgur.com/NR0V6.png"" alt=""enter image description here""></p>

<p>I am trying to understand what (9.9) is saying. </p>
",<graph-theory>
"<p>Given a set of points in 3D space, and a set of links between them which form a connected graph - is there a general strategy for extracting all simple loops from such an object?</p>

<p>I refer to simple loops as being those cycles in the graph which have no other edges on, or vertices intersecting with, the surface enclosed by the loop.</p>
",<graph-theory>
"<p>In a collection {$S_1, S_2, ... S_n$} of $n\geq 2$ nonempty sets, no two sets have the same number of elements. Show that this collection has a system of distinct representatives</p>

<p>(a) by using Hall's theorem:
A collection {$S_1, S_2, ... S_n$} of $n$ nonempty finite sets has a system of distinct representatives if and only if, for each integer $k$ with $1\leq k\leq n$, the union of any $k$ of these sets contains at least $k$ elements.</p>

<p>(b) without using Hall's theorem.</p>

<p>It seems that I should begin by letting $|S_1|&lt;|S_2|&lt;\cdots&lt;|S_n|$ and showing that, for any union of these sets, I will always get at least $k$ elements. It seems clear to me that the result is true, but I don't know how to prove this exactly.</p>
",<graph-theory>
"<p>Let $G$ be a simple, connected graph with $n\ge k+1$ vertices and $m\ge (k-1)(n-k-1)+{k+1 \choose 2}$ edges.<br>
Show there is a subgraph of $G$ with minimum degree at least $k$.  </p>

<p>(Not necessarily looking for a full solution yet, just some hints on how to go about proving this.)</p>
",<graph-theory>
"<p>It is known that one of the eigenvalues in the $k$-regular graph is $k$.</p>

<p>I have to prove that for a connected graph with eigenvalue $\Delta$, in which $\Delta$ is the maximum degree in G, the graph is regular.</p>

<p>For a generic graph, all it's eigenvalues are less or equal than $\Delta$, mi question is thus the following, is there a graph which has eigenvalue $\Delta$ and it's not regular? If not, then the statement is easily to prove.</p>
",<graph-theory>
"<p>I've been given the following statement, I need to decide if it's true/false and if true, prove it:</p>

<blockquote>
  <p>Every simple graph with no loops and more than one vertex has at least 2 vertices of the same degree.</p>
</blockquote>

<p>Could you guys give me some general advise on how to start these kind of proofs and some hints?</p>

<p>I just started learning graph theory, so these kind of proofs seem pretty hard.</p>
",<graph-theory>
"<p>In trying to deduce the lower bound of the ramsey number R(4,4) I am following my book's hint and considering the graph with vertex set $\mathbb{Z}_{17}$ in which $\{i,j\}$ is colored red if and only if $i-j\equiv\pm2^i,i=0,1,2,3$; the set of non-zero quadratic (mod 17) and blue otherwise. This graph shows that $R(4,4)\ge 18$. That's all fine but how am I expected to convince myself of this without drawing a 17-vertex graph.</p>

<p>Is there some way to mathematically justify that such a graph will not contain a monochromatic $K_4$ without drawing this graph?</p>

<p>Thanks.</p>
",<graph-theory>
"<p>I have a hard time to find a way to construct a <code>k</code>-regular graph out of <code>n</code> vertices. There seems to be a lot of theoretical material on regular graphs on the internet but I can't seem to extract construction rules for regular graphs.</p>

<p>My preconditions are</p>

<pre><code>k&lt;n and (n%2 == 0 or k%2 == 0)
</code></pre>

<p>Is an adjacency matrix the way to go here? If so, how would I use it?</p>

<p>Is this even a mathematical problem?</p>
",<graph-theory>
"<p>Could anyone describe for me why the maximum number of edge in simple diagraph with no cycle is $\text{combination}(2,n)$?</p>

<p>My thought:</p>

<p>If you have $N$ nodes, there are $N - 1$ directed edges than can lead from it (going to every other node). Therefore, the maximum number of edges is $N \times (N - 1)$.</p>
",<graph-theory>
"<blockquote>
  <p>Assume that $10$ people are sitting around a table. Determine the number of ways to choose a committee, where the committee is made up of two people who are NOT sitting next to each other.</p>
</blockquote>

<p>Take ${10 \choose 2}$ and take away the pairs with partners, which is still the $10$ which gives me the same result. ${10 \choose 2}-10=35$.</p>

<hr>

<blockquote>
  <p>Assume that $10$ people are sitting around a table. Determine the number of ways to choose a committee, where the committee is made up of <strong>three</strong> people of which NONE are sitting next to each other.</p>
</blockquote>

<p>${10\choose 3}=130$ take away the pairs that are sitting beside each other: (still 10??)</p>

<p>$120$</p>
",<graph-theory>
"<p><strong>Question:</strong></p>

<blockquote>
  <p>Decide if the following expression is true or false. Prove or give a counterexample.</p>
  
  <p>If $G$ is a simple, no loops graph, with $n$ vertices and $e$ edges, whose vertices have degree $k$ or $k+1$ then G has $n_k=(k+1)n-2e+1$ vertices of degree $k$.</p>
</blockquote>

<p><strong>Attempt:</strong></p>

<p>I think this is false, I tried to construct a counterexample but I think I haven't gotten it right.</p>

<p>Consider the graph $G$ of $1$ vertex and no edges. Then the $deg(v)=0$ and $n_k=k+2$.</p>

<p>I then separated in two cases, $deg(v)=k$ and $deg(v)=k+1$.</p>

<p>If $k=0$ then $n_0=2$ which is false, as there's only one vertex of degree $0$.</p>

<p>If $k+1=0$ then $n_{-1}=1$ which doesn't make much sense as the degree must be nonnegative, so where did I go wrong?</p>
",<graph-theory>
"<p>I would like to learn more about Graph Theory.  Just a ""Ted Talk"" caliber understanding.   I've played with Bridges of K. and 4-color map.  I get that stuff like Facebook or Netflix movie reviews may use some form of graph theory to represent people as vertices connected by edges in a graph.  I want to revisit some basic examples like traveling salesman, etc.  Can anyone point me in the right direction?  Links!</p>
",<graph-theory>
"<p>Let $G$ be a simple 2-connected graph with at least 4 vertices.<br>
If $V(G)$ the set of vertices, let $U,W$ be subsets of $V$, with no common elements, with $|U|=|W|=2$.<br>
Show that there are 2 distinct (that pass through different vertices) paths that connect $U$ and $W$.</p>
",<graph-theory>
"<p>I am trying to prove that if a finite graph has no isolated or pendant vertices then it contains at least one simple circuit.</p>

<p>Let the graph with no isolated or pendant vertices be $(V,E)$. A path in the graph cannot exceed $|V|-1$ since a path of length $m$ passes through $m+1$ vertices. I do not know where to go from here.</p>
",<graph-theory>
"<p>I'm studying for a graph theory exam. And while looking at some exams of previous years <strong>which were handed out by the professor</strong>, I found the following question.</p>

<p><img src=""http://i.stack.imgur.com/y5kWh.png"" alt=""Consider the bipartite graph G below: (unweighted bipartite graph). (a) Construct a maximal matching of G of size exactly 3.""></p>

<p>As I understand it, a maximal matching is a matching with the highest possible weight, but this graph doesn't display weights.</p>

<p>The next part of the question, not shown in the image, asks for a maximum matching, given the initial matching from (a), which I know how to construct using augmenting paths.</p>
",<graph-theory>
"<p>I have a question:</p>

<p>I have a set of points that represent a graph (x0,x1..x9) Lets say 10 points. They are at a linear 45 degree angle up (Gradient 1). I am also told that (x0+x1+x2..x9 = 1).</p>

<p>How can I solve for each xi? Can I get an idea of how to start?</p>
",<graph-theory>
"<p>These days I am reading the research paper <em><a href=""http://www.sciencedirect.com/science/article/pii/S0021869308002901"" rel=""nofollow"">Graphs associated to co-maximal ideals
of commutative rings</a></em> by Hsin-Ju Wang.</p>

<p>In this paper, $ R $ denotes a commutative ring with the identity element. $ \Gamma(R) $ is a graph with vertices as elements of $ R $, where two distinct vertices $ a $ and $ b $ are adjacent if and only if $ Ra + Rb = R $. $ \Gamma_{2}(R)$ denotes the subgraph of $ \Gamma(R) $ which consists of non-unit elements. In addition, $ J(R) $ is the Jacobson radical of $ R $ .</p>

<p>I am trying to understand the proof of Theorem 3.5. Theorem 3.5. states</p>

<blockquote>
  <p>The following are equivalent for $ \Gamma_{2}(R)\smallsetminus J(R) $.</p>
  
  <p>(i). $ \Gamma_{2}(R)\smallsetminus J(R) $ is a forest.</p>
  
  <p>(ii). $ \Gamma_{2}(R)\smallsetminus J(R) $ is either totally disconnected or a star graph.</p>
  
  <p>(iii). $ R $ is either a local ring which is not a field or $ R $ is isomorphic to $ \mathbb{Z}_{2}\times F $, where $ F $ is a field.</p>
</blockquote>

<p><img src=""http://i.stack.imgur.com/coWEE.png"" alt=""enter image description here""></p>

<p>Unfortunately, I can't understand the cases $ (i)\Rightarrow (ii) $ and $ (iii)\Rightarrow (i) $.</p>

<p>Can anyone please explain me how to show if $ \Gamma_{2}(R)\smallsetminus J(R) $ is a forest then it is either totally disconnected or a star graph and if $ R $ is either a local ring which is not a field or $ R $ is isomorphic to $ \mathbb{Z}_{2}\times F $, where $ F $ is a field then $ \Gamma_{2}(R)\smallsetminus J(R) $ is a forest ?</p>

<p>Any hints/ideas are much appreciated.</p>

<p>Thanks in advance for any replies.</p>
",<graph-theory>
"<p>i think if the graph G has an odd cycle, it's not two-colorable, otherwise it can be two colorable. </p>

<p>i read in one notes that the following is True: we couldent two-colorable any graph G that has cycle.  </p>

<p>anyone could clarify me ?</p>
",<graph-theory>
"<p>I found a statement in a book that I don't understand, it suggests that the number of DAGs on N nodes can be bounded from below by $$\prod \limits^N_{n=1} 2^{n-1}  = 2^{N(N-1)/2}$$
My way of thinking is, if we have a DAG it should have maximum of N-1 edges. So $2^{N-1}$ should be the number of different subsets of N-1 edges (so it already includes all DAGs with N-2, N-3 edges). </p>

<p>However formula suggest that we still have to multiply this number by number of DAGs that has less edges than N-1, which I can't understand, since we already have all the N-2, N-3 subsets included in $2^{N-1}$. </p>
",<graph-theory>
"<p>Consider a planar graph, where each node is associated with a weight. I would like to partition the graph such that the sum of the node weights in each group satisfy a minimum requirement. However, I would also like as much 'resolution' as possible - that is, I want to maximize the number of groups (minimize the number of nodes per group). Internal edges should be rewarded, to avoid long 'daisy chains' of nodes.</p>

<p>Does anyone have any suggestions as to how I can compute an (approximately) optimal solution? My instinct is to approach this using Monte Carlo, but I'm not sure how I would implement it here.</p>

<p>Thanks in advance for any insights or comments you might have!</p>
",<graph-theory>
"<p>Let $G$ be a graph such that $\delta(G) \geq k$.</p>

<ol>
<li><p>Prove that $G$ has a path of length at least $k$.</p>

<p>Solution: We know that 
$\delta(G) = \min\lbrace \deg(v) \mid v \in V(G) \rbrace$ </p>

<p>If $\delta(G) = k$ then there exists some $v \in V(G)$ such that $\deg(v) = k$.  This means all other vertices $u \in V(G)$ have $\deg(u) \geq k$.</p>

<p>Now I know this must be part of the proof. How would I prove that there exists a path of at least $k$?</p></li>
<li><p>If $k \geq 2$, prove that $G$ has a cycle of length at least $k+1$.</p></li>
</ol>
",<graph-theory>
"<p>Prove that if $T$ is a tree on at least $k+1$ vertices and max degree at most $d$, then there exists an edge $e$ such that the removal of $e$ causes $T$ to split into two trees where at least one of them has between $k$ and $dk$ vertices.</p>

<p>Obviously I know that if $T$ has at most $dk+1$ vertices then removing an edge leading to a leaf gives what we need. But what if there are more vertices? Where does the max degree come into play?</p>

<p>Thanks.</p>
",<graph-theory>
"<blockquote>
  <p>Given a graph $G = (V,E)$, $r-\text{regular}$, with no triangles/squares (no cycles of length $3$ or $4$,) prove that $|V(G)| \geq r^2 +1 $</p>
</blockquote>

<p>I'll be more than happy to get a direction, thanks!</p>
",<graph-theory>
"<p>The concrete example is:</p>

<p>I am given <code>n</code> currencies and pairwise exchange rates, and I have to change say dollars for euros. And I don't have to change money directly, for example, I could change dollars to British pounds first and then to euros and make more money than I would have by direct exchange. So my first idea was to draw complete graph with exchange rates as weights of edges and use Dijkstra's algorithm with a small change, I multiply weight, not sum. Actually it seems to be logical: if I go through a path and each time make appropriate multiplications I get number of units (in currency that corresponds to the node I have come to) I would have after these exchanges. And on each iteration of Dijkstra's algorithm if I see that the way I exchange the money at this step is better than previous one for this node (if visited) then I change the value. So when I finish a tree I can easily find a shortest path between to nodes, i.e. optimal way to exchange money from one currency to all another. Is there something what contradicts this idea?</p>

<p>Thanks in advance,
Cheers</p>
",<graph-theory>
"<p>If there exists a set of <code>n</code> points in a 2D coordinate system and an <code>n</code>-dimensional vector <code>V</code> that describes the shortest path containing all the <code>n</code> points and a second set of <code>n+1</code> points is created containing all the <code>n</code> points from the first set and another, arbitrary, point, can vector <code>V</code> be used to reduce the number of steps required to find an <code>(n+1)</code>-dimensional vector <code>V'</code> describing the shortest path in the second set? More precisely, must the <code>V'</code> be evaluated independently of <code>V</code>?</p>
",<graph-theory>
"<p>What are some good books on Ramsey theory? I have Van Lints book on Combinatorics: is this enough preparation to start reading about Ramsey theory? I want a book that includes important results and has good proofs.</p>
",<graph-theory>
"<p>Are laplacians for directed graphs used in any algorithms ? For example laplacians for the undirected graphs are used in algorithms such as spectral clustering.</p>
",<graph-theory>
"<p><img src=""http://i.stack.imgur.com/MtWvb.png"" alt=""enter image description here""></p>

<p>I have tried to understand the question but I got really confused.</p>

<p>So starting from node 3, the distance to other nodes are</p>

<p>3 to 1 = 3</p>

<p>3 to 2 = 1</p>

<p>3 to 4 = 4</p>

<p>3 to 5 = 2</p>

<p>3 to 6 = 3</p>

<p>3 to 7 = 2</p>

<p>and the question asks what is the third node added to the set S.</p>

<p>What I think it should be is 2->5->7->1->6->4 therefore the answer should be (B.7)</p>

<p>but the actual answer is (A.5)</p>

<p>Where did I made a mistake with the question? </p>
",<graph-theory>
"<p>The solution to the number of spanning trees of the graph below is given by $3 \times 2 \times 3 = 18$. I'm not sure how to get this. Please assist. Thanks! </p>

<p><img src=""http://i.stack.imgur.com/3OyuJ.jpg"" alt=""enter image description here""></p>

<p>Notes:
Just in case anyone was wondering, I drew the graph using the tool on <a href=""http://illuminations.nctm.org/Activity.aspx?id=3550"" rel=""nofollow"">http://illuminations.nctm.org/Activity.aspx?id=3550</a> and got the image by using the snipping tool on Windows.</p>
",<graph-theory>
"<p>Given a graph, G = (V,E), and conditions on members of V (that they must be connected to some m vertices, and disconnected to some n vertices), how can I efficiently find candidates for removal, based on m and n?
I keep coming up with the fact that once I've removed some v in V, I must then start again and search through the set V again to find objects that meet this criteria (as removing some v from V is going to have an affect on other members of V). 
Am I missing something obvious - or should I be looking into achieving what I'm trying in some other way?</p>
",<graph-theory>
"<p>I need to learn about Razborov's ""flag algebras"" (see <a href=""http://bit.ly/1u1a1NB"" rel=""nofollow"">http://bit.ly/1u1a1NB</a>) to solve a problem about graphs. Flag algebras are a very general new algebraic tool for studying combinatorial structures. The theory of flag algebras uses some elementary model theory in its definition which is something I have not encountered before (in fact I have not taken any classes in formal logic beyond the first-year undergrad stuff). I've read through the first few sections of Marker's ""Model Theory: An Introduction"" (<a href=""http://amzn.to/1zGuyKq"" rel=""nofollow"">http://amzn.to/1zGuyKq</a>) and I have easily understood the definitions and general approach; in particular, I think I only really need to understand the general concepts of <em>languages</em> $\mathcal{L}$, $\mathcal{L}$-<em>structures</em>, embeddings of $\mathcal{L}$-structures and some general facts about $\mathcal{L}$-<em>theories</em> and <em>models</em>, all of which seems fine.</p>

<p>Reading the beginning of Razborov's paper [p4], though, I found some terms that I have not seen defined which I'd like to clear up. In particular, he writes</p>

<p><em>""Let $T$ be a universal first-order theory with equality in a language $\mathcal{L}$ containing only predicate symbols; we assume that $T$ has infinite models. Our assumptions imply that every set of elements of a model of $T$ induces a model of $T$, and that $T$ has at least one finite model of every given size.""</em></p>

<p>I have two essential questions:</p>

<ul>
<li>what does ""universal"" mean in this context?</li>
<li>why do the assumptions imply that any subset of a model of $T$ induces a model of $T$?</li>
</ul>
",<graph-theory>
"<p>Is the statement below correct?</p>

<p>A graph which doesn't have a complete graph of order $4$ or more can be colored with $3$ colors, so that no two adjacent vertices have same color.</p>

<p>I don't know it is correct or not; if it is not correct, please someone give me a counterexample to that.</p>
",<graph-theory>
"<p>I am interested in counting the number of hyperedges in the complete, $t$-uniform hypergraph on $n$ vertices which intersect a specified set of $k$ vertices.  This is trivial, the answer is:</p>

<p>$$\sum_{i=1}^t {k \choose i}{n-k \choose t-i}.$$</p>

<p>My questions is whether there is a nice simplification of this expression; I'd like to get rid of the sum if possible.  Anyone know?</p>

<p>Thanks a lot for the help!</p>
",<graph-theory>
"<p>The solution to the number of spanning trees of the graph below is given by $6$ and $4 \times 4 - 1$ for Graph A and B respectively. I'm not sure how to get this. Please assist. I did ask a similar question a while ago but I'm still not able to figure out for these 2 figures. Thanks! </p>

<p>Graph A :</p>

<p><img src=""http://i.stack.imgur.com/7AXZS.jpg"" alt=""Graph A""></p>

<p>Graph B :</p>

<p><img src=""http://i.stack.imgur.com/zRThk.jpg"" alt=""Graph B""></p>

<p>What I know:</p>

<p>1) Number of spanning trees of a cycle with n vertices is, $\tau(C_n) = n$
2) I know how to solve the above using the contraction deletion theorem but I'm interested in other methods, thanks!</p>
",<graph-theory>
"<p>If $S$ is a strongly connected component of a digraph, then $S$ is the union of cycles of it's vertices, I conjecture. Is this true? I can nowhere find such a statement.</p>
",<graph-theory>
"<p>Let $G$ be a graph of order $n$. If deg $u$ $+$ deg $v$ $+$ deg $w$ $\geq n-1$ for every three pairwise nonadjacent vertices $u,v$ and $w$ of $G$, must $G$ be connected?</p>

<p>I know that if $H$ is a graph of order $n$ and deg $u$ $+$ deg $v$ $\geq n-1$ for every two nonadjacent vertices $u$ and $v$ of $H$, then $H$ is connected. Does this imply even further that $G$ is connected? Any help or hints would be greatly appreciated.</p>
",<graph-theory>
"<p>The question is as follows: Prove that in a graph $G$ a set of edges $X$ which is not contained in any spanning tree is a cycle (or possibly an edge disjoint union of cycles).</p>

<p>My thoughts: Proceed by contradiction. </p>

<p>Any help will be appreciated.</p>
",<graph-theory>
"<p>I am trying to brush up my graph theory skills. I have not done any in over 4 years and i am rusty...If someone could help me out with this simple proof i would appreciate it.</p>

<p>Prove that for any graph $G$ of order at least 2, the degree sequence has at least one pair of repeated entries.</p>

<p>So the degree sequence if a list of the degrees of each vertex (usually written in descending order).</p>

<p>I know that the sum of the degrees of the vertices of a graph is equal to $2|E|$ and that the number of vertices of odd degree is even.</p>

<p>If someone could help me out and point me in the right direction I would appreciate it.</p>
",<graph-theory>
"<p>I have seen lots of answers proving this theorem via induction. However, I'd like to know how to prove that every digraph such as all vertices have in degree equal to its out degree is Eulerian, using the concept of maximum trail.</p>

<p>Does anyone have a clue?</p>
",<graph-theory>
"<p>G = (V, E) - hamiltonian graph. </p>

<p>$A$ - is a some(any) set of vertex in $G$. $N(A)$ - number of vertexes in $G$ connected with at least one vertex in $A$.</p>

<p>I'm trying to prove that $|A| \leq |N(A)|$.</p>

<p>Generally, it seems to be obvious. If we have a hamiltonian cycle in G than to visit all vertexes in A we need at least |A| vertexes(different) connected by edges with nodes in A.</p>

<p>However, teacher says to build rigorous proof. But I can't see more rigorous way to prove it . 
Could you help me with it?</p>
",<graph-theory>
"<p>Let $P_{20}$ be a path of length 20 like so: $x_0$-$x_1$-$~\cdots~$-$x_{20}$ and $G$ a cycle of order 3.</p>

<p>Allegedly there are $3 \cdot 2^{20}$ mappings $P_{20}\rightarrow G$, which I don't quite see.</p>

<hr>

<p>We defined a mapping $\Psi$ of graphs $(V,E)\rightarrow (V',E')$ as a pair of mappings $\Psi_1:V\rightarrow V'$ and $\Psi_2:E\rightarrow E'$, such that $\forall e\in E, e=[x_1,x_2],\; \Psi_2(e)=[\Psi_1(x_1),\Psi_1(x_2)]$ </p>

<hr>
",<graph-theory>
"<p>The question is:</p>

<blockquote>
  <p>Does there exist a simple connected undirected graph $G$ with $7$ vertices with minimal degree $3$ but does not contain any hamiltonian cycle?</p>
</blockquote>

<p>I've been trying to find an example for quite long time, but I still cannot think of one. The restriction ""minimal degree 3"" is giving me an headache, since I can always find a graph with no-hamiltonian cycle with ""almost minimal degree 3"", but whenever one edge is added so to satisfy the condition, it becomes hamiltonian... </p>

<p>So the question comes. Is there even a single graph with above properties? Maybe I am being a bit un imaginative, but I've found questions about finding non-hamiltonian graphs with certain properties quite hard so far. </p>

<p>It would be great if you could explain your strategy too with an example, since I seem to lack what is needed in this kind of exercise: I can't get the ""feel"" of it.</p>

<p>Thanks in advance,</p>
",<graph-theory>
"<p>Why do people use the letter ""K"", rather than ""C"", to represent a complete graph? Does it come from German ""komplett""?</p>
",<graph-theory>
"<p>It is well known that the Petersen Graph is not Hamiltonian. I can show it by  case distinction, which is not too long - but it is not very elegant either.</p>

<p>Is there a simple (short) argument that the Petersen Graph does not contain a Hamiltonian cycle? </p>
",<graph-theory>
"<p>Could I get some help for part b(i) of below please? Thanks. (Part (a) follows from Hall's Marriage Thm, and b(ii) follows quickly from b(i) I think).</p>

<p>Let $G$ be a bipartite graph with parts $X$ and $Y$ , and with maximum degree $r$.</p>

<p>(a) Let $X_0$ be the set of vertices $x$ in $X$ with degree $d(x) = r$. Show that there is a matching covering $X_0$ (that is, such that each vertex in $X_0$ is incident with an edge in the matching).</p>

<p>(b) Let $X_1 \subseteq X$ and $Y_1 \subseteq Y$ , and suppose that there is a matching $R$ covering $X_1$ and a matching $B$ covering $Y_1$.</p>

<ul>
<li>(i) Consider the subgraph containing just the edges in exactly one of $R$, $B$. Suppose that some component of this subgraph is a path with first edge in $R$ and last edge in $B$. Let $Z$ be the set of vertices in $X_1 \cup Y_1$ on the path. Show that either the edges in $R$ on the path cover $Z$, or the edges in $B$ on the path cover $Z$.</li>
<li>(ii) Show that there is a matching in $G$ covering $X_1 \cup Y_1$.</li>
</ul>
",<graph-theory>
"<p>Given $n$ distinct nodes $1,2...n$, I wish to find the number of connected graphs with these $n$ nodes. I have seen the previous question : <a href=""http://math.stackexchange.com/questions/154941/how-to-calculate-the-number-of-possible-connected-simple-graphs-with-n-labelle"">How to calculate the number of possible connected simple graphs with $n$ labelled vertices</a>. But in my case multi-edges are allowed. </p>

<p>By multi-edges I mean, if I have $n$ nodes a multi edge is a subset of vertices connecting all those vertices. For example in case of $3$ nodes $\{1,2,3\}$ is a multi-edge connecting all the three edges. For example following are the few ( out of $96$ different ways ) ways of connecting three nodes:</p>

<ol>
<li>Select edge $\{1,2,3\}$.</li>
<li>Select edges $\{1,2,3\}$ , $\{1,3\}$ , $\{2,3\}$ and $\{1,2\}$.</li>
<li>Select edges $\{1,2,3\}$ and $\{1,3\}$.</li>
<li>Select edges $\{1,3\}$ and $\{2,3\}$ and so on.</li>
</ol>

<p>I am just looking for hints not solutions. As this is part of question from Project Euler question: <a href=""https://projecteuler.net/problem=553"" rel=""nofollow"">Power set of power sets</a>.</p>

<p>A friend of mine gave the above method of multi-edges as a hint to me and told me I had to modify the recurrence in the question: <a href=""http://math.stackexchange.com/questions/154941/how-to-calculate-the-number-of-possible-connected-simple-graphs-with-n-labelle"">How to calculate the number of possible connected simple graphs with $n$ labelled vertices</a>. But I am unable to figure it out and just need a new direction for thinking.</p>
",<graph-theory>
"<p>I'm struggling with drawing a graph from following exercise:
Set of vertices in undirected graph Gn is built from words of length n from alphabet {a,b,c,d}. Two words are adjacent when they have Hamming distance 1. Whats degree of Gn. How many vertices and edges does Gn have?</p>

<p>Now I know that degree is 3n, number of vertices: 4^n and edges: (3/2) * 4^n * n.
I'm unsure how to correctly picture edges for example in G2.</p>
",<graph-theory>
"<p>I have a problem with the following assignment.</p>

<p>Describe all graphs which do not contain a path whose length 3.
Could you help me solve it?</p>
",<graph-theory>
"<p>Prove the theorem 4.19: A non-decreasing sequence $\pi:s_1,s_2,\ldots,s_n$ of nonnegative integers is a score sequence of a strong tournament if and only if </p>

<p>$$\sum_{i=1}^ks_i &gt; \binom k 2 $$</p>

<p>for $1≤k≤n-1$ </p>

<p>and </p>

<p>$$\sum_{i=1}^ns_i = \binom n 2 $$</p>

<p>Furthermore, every tournament whose score sequence satisfies these conditions is strong.</p>

<p>I know how to show that $\pi$ is the score sequence of $T$ if and only if it satisfy these 2 conditions, but I can't see how these 2 conditions make the tournament strong</p>
",<graph-theory>
"<p>Is the following statement about Euclidean MSTs true, and if so could someone help me with a proof? Between any two nodes, the EMST minimizes the maximum edge cost of any edge required to traverse from one node to the other.</p>

<p>Edited Statement: Show that in a Euclidean MST-- found by Kruskal's algorithm, given any path between two points, there cannot have been any other path between those two points that used an edge of a smaller cost than any edge used in the original path.</p>
",<graph-theory>
"<p>For a tournament $T$ of order $n$, let 
$Δ=max \{od v:v∈V(T)\}$
And 
$δ=min\{od v:v∈V(T)\}$
Prove that if $Δ-δ&lt; \frac n2$, then $T$ is strong</p>

<p>Here is my final attemp. Prove this by contrapositive.</p>

<p>Assume that $T$ is not strong, by the result of some previous exercise, $T$ is not regular. So there exists a vertex $v$ such that $id(v) &lt;od(v)$. Note that $T$ is a tournament which is an oriented completed graph so</p>

<p>$$deg(v)=id(v)+od(v)=n-1$$</p>

<p>Since $id(v)&lt;od(v)$, $ \delta \leq od(v)&lt; \frac {n-1}2$</p>

<p>Let $\pi: s_1,s_2,...,s_n$ be the score sequence of $T$, then </p>

<p>$$\Delta= s_n= \sum_{i=1}^n S_i -\sum_{n=1}^{n-1}S_i \geq \left( \begin{array}{ccc}
n  \\
2  \\
 \end{array} \right) -\left( \begin{array}{ccc}
n-1  \\
2  \\
 \end{array} \right)=n-1$$</p>

<p>So</p>

<p>$$\Delta -\delta &gt;(n-1) -\frac {n-1}2= \frac {n-1}2 \geq \frac n2$$</p>

<p>By contrapositive, if $Δ-δ&lt; \frac n2$, then $T$ is strong</p>

<p>Is my reasoning acceptable?</p>
",<graph-theory>
"<p>The complement of a disconnected graph is necessarily connected, but the converse is not true. For instance, $C_5$ is connected and isomorphic to its complement. The following picture shows a graph and its complement which are both connected (and non-isomorphic).</p>

<p><img src=""http://i.stack.imgur.com/Nbbhv.jpg"" alt=""Example graph""></p>

<p>Is there a name for connected graphs whose complements are also connected? In the same vein as coplanar, I tried searching for ""co-connected"" graphs but didn't find anything. Are there any useful necessary or sufficient conditions for a connected graph to have this property? </p>

<p>Thanks. </p>
",<graph-theory>
"<p>In $O(|V|+|E|)$, we can detect whether a Directed Graph has a cycle or not. ---> True</p>

<p>In depth-first seach on DAG, there is no Back Edge. ---> True</p>

<p>With known Number of Edges, in $O(|V|)$ and not $O(|V|+|E|)$, we can detect whether a simple undirected Graph has a cycle or not. ---> True</p>

<p>Am I right about the above three questions about Graph Algorithm? I think all of them is True?</p>
",<graph-theory>
"<p>I have been reading a number of ""network science"" papers where the authors perform transformations on networks that seem to preserve the topology of those networks.  By ""topology"", I mean a collection of heuristics commonly used to probe the network: modularity, clustering coefficient, and mean path length.  Is there a name for these kinds of transformations and/or some review somewhere of their properties?  I don't have a good mental model for said properties and it would be helpful to have a list of examples or a formal treatment of them.</p>
",<graph-theory>
"<p>I have a graph with two sets, vertices and edges, $G=(V,E)$. In my case every vertex and every edge have some attributes like $type$.</p>

<p>What is the mathematical correct way to get the value of a vertex' property? Maybe $property(v)$ where $property$ is the property name and $v$ the single vertex?</p>

<p>What is the correct way to get only the vertices (a set of vertices) that have a specific value? For example if I want a set of vertices that only have $1$ as the value of $type$? </p>
",<graph-theory>
"<p>Consider a tournament with $799$ contestants. Each contestant plays against all other contestants exactly one; there are no draws. Prove that there exist two disjoint groups $A,B$, of $7$ contestants each, such that everyone in $A$ beats everyone in $B$.</p>

<p>Suppose we choose $B$ by choosing any $7$ contestants randomly. If we can show that the expected number of contestants we can put into $A$ is at least $7$, we will be done. But is that true?</p>
",<graph-theory>
"<p>I will be doing a course in Information Theory soon and to get some early learning in I have been attempting a question with a joint probability mass function represented by the following table:</p>

<p><img src=""http://i.stack.imgur.com/KG6I6.jpg"" alt=""joint probability mass function represented by a table""></p>

<p>In the question I have been asked to find the value of D(X || Y).</p>

<p>I have been attempting this question for a few hours now and cannot seem to get the right answer and any sources I have found have not featured an example like this.</p>

<p>As I am not a person with a background in maths any help to understand this question would be greatly appreciated.</p>

<p>Thank you very much for your time.</p>
",<graph-theory>
"<blockquote>
  <p>Given a simple and connected graph $G = (V,E)$, and an edge $e \in E$. Prove:</p>
  
  <p>$e$ is a cut edge <strong><em>if and only if</em></strong>  $e$ is in every spanning tree of $G$.</p>
</blockquote>

<p>I have been thinking about this question for a long time and have no</p>
",<graph-theory>
"<p>This is an extension over this question:
<a href=""http://math.stackexchange.com/questions/126216/inter-causal-reasoning-how-to-solve-probability-with-two-conditions"">Inter-causal reasoning: How to solve probability with two conditions?</a></p>

<p>I'm a beginner in probability, and trying to deeply understand what is happening underneath.</p>

<p>To sum up what the question is about:</p>

<p>We've got a graph of (binary) events:</p>

<p>$$
A \rightarrow C \leftarrow B
$$</p>

<p>We're given probabilities of:
$$
P(A), P(\bar A), P(B), P(\bar B)
\\
P(C|A, B)\\
P(C|A, \bar B)\\
P(C|\bar A, B)\\
P(C|\bar A, \bar B)
$$
Where $P(A)$ is a probability of occurrence of event $A$ and $P(\bar A)$ is a probability of event $A$ not occurring.</p>

<hr>

<p>We have to find probability of: $P(B|C)$ and $P(B|C,A)$.</p>

<p>Before going further I'd like to say, that I'd like to find out a bit more things and, of course, be aware of theorems used.</p>

<p>I'll begin with really simple ones (numbering done to ease answering the questions):</p>

<ol>
<li><p>Does $P(B|C,A)$ means: <em>Probability that event $B$ will occur given event $A$ and event $C$ occurred</em> ?</p></li>
<li><p>Are the events $A$ and $B$ independent? I see a <em>V-structure</em> in here, so we've got no <em>active trail</em> in here, right? So they're independent.</p></li>
<li><p>We can write (<em>Bayes theorem</em>) that $P(B|C) = \frac{P(C|B)P(B)}{P(C)}$. </p>

<p>3.1. To get $P(C|B)$ can we do conditioning and reduction on $B$ ?</p>

<p>3.1.1. If yes, then does it equals to (why, what is the rule; my intuition says ""B""): 
$$
Option~A\\
P(C|B) = P(C|B,A)+P(C|B,\bar A)\\
\\
Option~B\\
P(C|B) = P(C|B,A)P(A)+P(C|B,\bar A)P(\bar A)
$$</p>

<p>3.1.2. Does the (in)dependence of $A$ and $B$ affects somehow the way we can count $P(C|B)$ ?</p>

<p>3.2. How can we count $P(C)$? Is it:
$$
P(C) = ( P(C|A,B)+ P(C|A, \bar B) ) * ( P(C| \bar A,B) + P(C|\bar A, \bar B) )
$$</p></li>
<li><p>Counting $P(B|C,A)$:</p>

<p>4.1. <strong>I have counted it</strong>, but I cannot recall how, and I don't have my notes in here. It only means, that I didn't understood it, as I cannot do it again ;) I thought I can use a <em>Bayes theorem</em> in here, but will this turn out to be $\frac{P(C,A|B)P(C,A)}{P(B)}$ ? It doesn't look well.. And, can I use somehow the fact of (in)dependence of $A$ and $B$ in here?</p>

<p>4.2. I know there's an answer in the connected question, but it's not about getting the answer. I want to understand how to figure out this answer.</p></li>
</ol>
",<graph-theory>
"<p>I am having a hard time understanding the answer to the following problem from Grimaldi:</p>

<p>""At Professor Alfred's science camp, 17 students have lunch together each day at a circular table. They are trying to get to know one another better, so they make an effort to sit next to two different colleagues each afternoon. For how many afternoons can they do this? How can they arrange themselves on these occasions?""</p>

<p>They have solved it in the context of Hamilton cycles, which I have not understood at all. I have a feeling it can be solved by some other method of counting also. 
Any help will be nice..</p>
",<graph-theory>
"<p>How can I prove mathematically that the spectral radius of a binary tree approaches e as the number of nodes tends to infinity? 
I mean it is true that the increase in nodes number is exponential but so far I have computed the spectral radius by finding the largest nontrivial eigenvalue of det [A(G)- lambda*I]=0. How can I prove that if N tends to infinity, the spectral radius is exp(1)??</p>
",<graph-theory>
"<p>I found a proof of the fact that if a graph G is bipartite(1), then it cannot have any odd cycles(2). I have a question about $(2) \Rightarrow (1)$. Why is it sufficient to assume that  G is connected?</p>
",<graph-theory>
"<p>Is there a graph with $n$ vertices and $\lfloor n^2/4\rfloor$ edges that isn't bipartite and contains no triangles ($K_3$)?</p>

<p>Rather, what I am asking is whether Mantel's Theorem implies that every graph on $n$ vertices with $\lfloor n^2/4\rfloor$ edges and contains no triangles must also be bipartite. </p>

<p>EDIT: I apologize. I forgot to add that the graph must also not contain any triangles. Sorry, I realize that it was a trivial question before.</p>
",<graph-theory>
"<p>I know of Zero One laws for Random graphs (such as those concerning <a href=""http://link.springer.com/article/10.1007%2FBF02579198"" rel=""nofollow"">monotonic</a> or <a href=""https://jeremykun.com/2015/02/09/zero-one-laws-for-random-graphs/"" rel=""nofollow"">first-order-logic properties</a>). I also know about <a href=""https://en.wikipedia.org/wiki/Kolmogorov%27s_zero%E2%80%93one_law"" rel=""nofollow"">Kolmogorov's zero one law</a> for tail sigma algebras. </p>

<p>Apart from the obvious statements of these laws (a property exists or it doesn't), is there any relation between them? Is Kolmogorov a generalization or extension of Random Graph Zero One Laws in any way? </p>
",<graph-theory>
"<p>Let $G$ be a <em>color critical</em> graph and $S$ any independent set of $G$ then $\chi(G−S)=\chi(G)−1.$</p>

<p>I tried to show that any independent set induce one color, but I cannot match it with the <em>color critical</em> hypothesis.</p>
",<graph-theory>
"<p>Dyer and Frieze in ""ON THE COMPLEXITY OF PARTITIONING INTO CONNECTED SUBGRAPHS"" showed that the problem of deciding whether a planar graph has a connected-$k$-partition is NP-complete.</p>

<p>On a graph with fewer than $k$ nodes, of course the partition is impossible. Cutting a graph with sufficient nodes into pieces strikes me as trivial. Is there a planar graph of $k$ or more nodes that can't be partitioned into $k$ subgraphs? When does Dyer and Frieze's problem result in a ""no""?</p>
",<graph-theory>
"<p>A cubic cheese consists of 27 smaller cubes of cheeses (3x3x3). A mouse will eat the first cheese cube and then eat an adjacent cheese cube (no diagonal eating allowed). Show that the mouse can't end up eating the middle cheese cube last.</p>

<p><strong>EDIT:</strong> </p>

<p>The mouse has to eat every cube and can't move into empty space.</p>

<p>Any ideas on how to show this?</p>
",<graph-theory>
"<p>Following graph is a composition of $K_{4,4}$ bipartite graphs with all the edges are of same length. How do I know whether it is minor-closed or not?</p>

<p><img src=""http://i.stack.imgur.com/fItF8.jpg"" alt=""enter image description here""></p>

<p>The definition in the Wikipedia is as follows.</p>

<blockquote>
  <p>A family F of graphs is said to be closed under the operation of
  taking minors if every minor of a graph in F also belongs to F.</p>
</blockquote>

<p>I think to answer to the question, we have to find the smallest complete graph $K_n$ which is a minor of the graph but not present. Can anyone think of such a $K_n$?</p>
",<graph-theory>
"<p>In an undirected graph, we have that the $a_{ij}$ entry in the adjacency matrix $A$ is equal to $1 \iff$ there is an edge between $i$ to $j$.</p>

<p>Now for undirected graphs we know that if there is an edge from $i \to j$ then this edge is also considered as an edge from $j \to i$.</p>

<p>Does that mean that $A$ will always be a symmetric matrix and that $A= A^T$ for all undirected graphs ?</p>

<p>I think the answer is true, But I just want to make sure If I am right or not</p>
",<graph-theory>
"<p>I Have the following question :</p>

<blockquote>
  <p>Give three examples of simple, connected graphs, all with 8 vertices
  with degrees 2, 2, 2, 2, 3, 3, 4 and 4, no pairs of which are
  isomorphic</p>
</blockquote>

<p>What is the best method to find such 3 graphs ? In general how can I proceed to find 2 or more non isomorphic Graphs that have the same numer of vertices with the same  degree?  </p>

<p>Thank you !</p>
",<graph-theory>
"<p>There's a research project i'm currently working on which requires me to analyze various aspects of ""worlds"" represented by transition probability matrices, where the nodes represent objects in the world that our ""learner"" travels through.
I'm looking for a way to measure the degree to which the structure of the connections in a given a graph provide the learner with any predictive power, for example, in a graph where all the vertices have the same value, the learner has absolutely no ability to predict the next node/nodes given the current one.</p>

<p>The graphs are all weighted and directed, with the sum of outgoing connections from each node normalized to one.</p>

<p>I'm having a bit of trouble coming up with a good way to measure this and would really appreciate some advice,
Thanks,
Ron</p>
",<graph-theory>
"<p>I need to construct a graph that has the following properties: the vertices of G are the edges of a complete graph $K_5$ on 5 vertices. The vertices of G are adjacent if and only if the corresponding edges of $K_5$ have an endpoint in common. I then need to determine the chromatic number of this graph. </p>

<p>Can someone help me with at least the construction of the graph?</p>
",<graph-theory>
"<p>You have a herd of cattle moving in different directions. The cows in the herd are more or less always moving, at different direction and in different velocities.When a cow bumps another cow it affects its direction and perhaps its speed so that they would not keep brushing up against each other. If we take a snapshot of this herd of cattle, we can try to predict the eventual direction of the herd by looking at the edges - for example, if all cattle at the bottom of the herd and at the sides are moving up in a forceful manner, we can imagine the cattle in the middle will change their direction upwards as well (because that's the path of least resistance).</p>

<p>We can also think of a different example. We can imagine a group of academic researchers such that each researcher has some degree of influence on others. Each researcher also has his own topic, which could range from biotechnology to environmental science to algebra. When we observe this system over time, we can imagine that researchers (assuming they're not particularly independent) will change their ""research direction"" over time to fit the direction of the science herd, and we can probably point out the researchers that will have the most ""pull"" in setting this direction.</p>

<p>My question is, given this sort of setting in the abstract and in a specific moment in time, how do we identify which actors will have the most influence on the eventual direction of the group (Human beings seem to be able to do this quite intuitively!)? Has this problem been studied? </p>
",<graph-theory>
"<p>I am reading a paper which is regarding to the graph theory.</p>

<p>It is applied theory to computer science.</p>

<p>My background was industrial and management engineering, and computer science and engineering right now.</p>

<p>I am freshman at a grad school. Maybe because of the reason, I don't fully understand and know about graph theory.</p>

<p>By the paper's author, <strong>the density of a graph seems like</strong></p>

<p><strong>(density) = (the number of edges) / (the number of nodes)</strong></p>

<p><strong>The authors followed E. Lawler (1976), <em>Combinatorial Optimization: Networks and Matroids</em>.</strong></p>

<p>And they recommeded to see chapter 4 of that book.</p>

<p>I can't find about the density at the book.</p>

<p>So I searched google, maybe people say the density of a graph is</p>

<p><strong>(density) = (the number of edges) / (the number of possible edges)</strong></p>

<p>Those two definitions are different.</p>

<p>I would like to make it sure.</p>

<p>Thank you for your help in advance.</p>
",<graph-theory>
"<p>One of the defining characteristics of a Markov Chain is that it is memoryless: the next state depends only on the current state, and not on the set of preceding states.</p>

<p>I'm looking for a mathematical structure that is, essentially, a Markov Chain with memory. All the same, except for that the next state depends on some set of preceding states. Is there such a thing?</p>
",<graph-theory>
"<p>I don't know if I am asking a silly question, but what is the exact definition of a graph homomorphism (throughout, the graphs may be assumed to be simple, i.e. undirected, no loops, no multiple edges, etc)? 
I have a slight confusion with the existing definition, which says that a function f : V(G) to V(H) is called a graph homomorphism, if whenever (i,j) is an edge of G, (f(i),f(j)) is an edge of H. The confusion is that, suppose that f(i) = f(j), and suppose that (i,j) is an edge of G. Then, is f still a homomorphism? Or does a graph homomorphism necessarily have to be an injective function (in set-theoretic sense)?
Please help me settle this issue, as it would be very embarassing to have his kind of a misconception at the beginning of my study of graph theory!    </p>
",<graph-theory>
"<p>In the towers of Hanoi game, how do we know that we have the optimal algorithm for solving it? I thought about this and it seemed like any deviation from the standard strategies would be putting you back a step, but I had no idea how to demonstrate this rigorously.</p>

<p>All I know is that the proof involves the Lucas correspondence between the Hanoi graph and the odd coefficients in Pascal's triangle. How is Pascal's triangle turned into a graph? I assume the coefficient are the vertices, but I don't see how you form the edges?</p>

<p>I was also wondering how to generalize the strategy to n discs and k rods because it seems like this correspondence argument wouldn't really work in the general case.</p>

<p>Basically, I am wondering how the odd coefficients of the Pascal triangle are turned into a graph and whether or not there is a similar method to find and prove optimal strategy when we increase the number of rods.</p>
",<graph-theory>
"<p>(I have posted the following in theoretical CS stackexchange, but realized that it's the wrong place, so I'm reposting it here)</p>

<p>I'm reading CLRS's (Cormen et.a al) <em>Introduction to Algorithm</em>, and arrived at the maximum flow section. It shows that Edmonds-Karp algorithm runs in $O(E^2V)$ time by showing that:</p>

<p>1) If we let $\delta_f(s, u)$ be the shortest distance from source $s$ to vertex $u$ in the residual graph after flow $f$ is picked, then CLRS shows that $\delta_f(s, u)$ does not decrease as $f$ gets augmented, for a fixed $u$.</p>

<p>2) Let $c_f(e)$ be the capacity of edge $e$, and $c_f(p)$ the capacity of a path $p$ (the minimum of all $c_f$ on the edges in the path $p$) in the residual graph after flow $f$. If we say that an edge $(u, v)$ in a residual network $G_f$ is <em>critical</em> on an augmenting path $p$ if $c_f(p) = c_f(u, v)$, then CLRS shows that each edge can't become critical more than $|V|/2$ times, and therefore, there can be at most $|E||V|$ augmenting flows, if we sum up this bound over all of the potential $2|E|$ edges in residual graphs.</p>

<p>In particular, CLRS's proof of the 2nd point goes as follows:</p>

<p>If $(u, v)$ becomes critical at $f$, then $\delta_f(s, v) = \delta_f(s, u) + 1$, because we have picked the shortest path from $s$ to $t$.</p>

<p>The next time it becomes critical, there must have been some flow $f'$ that ran through $(v, u)$. Then $\delta_{f'}(s, u) = \delta_{f'}(s, v) + 1$. This must mean $\delta_{f'}(s, u) \ge \delta_f(s, u) + 2$. </p>

<p>Hence every time $(u, v)$ becomes critical after the 1st, $\delta_f(s, u)$ must have increased by 2. As this minimal distance can't exceed $|V|$, each edge can become critical at most $|V|/2$ times. Therefore, there can at most be $O(EV)$ augmenting paths and hence the number of augmentations.</p>

<p>So my comment now comes in as a possible improvement over this proof. Why can't we say the following:</p>

<p>If we augment the edge $(u, v)$ $k$ times,then $\delta_f(s, u)$ increases at least by $2(k-1)$. Now instead of looking at edges, we look at vertices and their degrees. We just need to guarantee the edges going out from any particular vertex $v$ become critical infrequently enough so that $\delta(v)$ doesn't exceed $|V|$ (from now on for convenience I will shorten $\delta_f(s, v)$ to $\delta(v)$). I claim that for any vertex $v$, $\sum_{(v,u)\in E \text{ or } (u, v)\in E} \text{number of times $(v, u)$ becomes critical}$ is at most $indegree(v)+outdegree(v)+|V|/2$. (we are looking at ""reverse"" edges here too because they may appear in the residual graph)</p>

<p>Indeed, the first time any edge becomes critical is ""free"": it doesn't bump up $\delta(v)$. But for every time after that $\delta(v)$ goes up by 2. So we can get every edge going out from $v$ to become critical once (this is the $indegree(v)+outdegree(v)$ term) and then all other times are bounded by $|V|/2$ for the same reason CLRS writes. </p>

<p>Now if we sum up this bound for all vertices, we get
$$\sum_v indegree(v)+outdegree(v)+|V|/2 = 2|E| + |V|^2/2 = O(V^2)$$</p>

<p>This bound is better than the $O(EV)$ bound given by CLRS. Combined with the $O(E+V)=O(E)$ bound for BFS, we find Edmonds-Karp is actually $O(EV^2)$ instead of $O(E^2V)$.</p>

<p>I'm suspicious that I'm missing something, since every where I looked, the bound given for Edmonds-Karp is $O(E^2V)$. So please nitpick away and see where I reasoned incorrectly.</p>
",<graph-theory>
"<p>Let $T$ be a tree with exactly two vertices of degree $7$ and exactly $20$ vertices of degree $100$. What is the minimum possible number of vertices in a tree $T$ that satisfies those restrictions?</p>
",<graph-theory>
"<p>I am doing a few tests with the Average Path Length formula. I am testing with a directed acyclic graph. I am generating all the shortest path using a breath first walk.</p>

<p>Everything works fine. However, I am trying to relate what I did with the Average Path Length formula I am seeing everywhere: <a href=""http://en.wikipedia.org/wiki/Average_path_length"" rel=""nofollow"">wikipedia</a>, <a href=""http://mathinsight.org/network_mean_path_length_definition"" rel=""nofollow"">math insight</a>, etc.</p>

<p>All the definitions says:
$$
\bar P = \frac{1}{n \cdot (n \mathbin{-} 1)} \cdot \sum_{i \neq j}^{} d(v_{i},v_{j})
$$</p>

<p>All of them says that $n$ is the number of nodes in $G$. </p>

<p>The problem is that I am about 20 000 nodes in my graph and about 600 000 shortest paths. If I do:</p>

<p>$$
\bar P = \frac{1}{S} \cdot \sum_{i \neq j}^{} d(v_{i},v_{j})
$$</p>

<p>where $S$ is the number of shortest paths (600 000) then I get an average of 6.05 which is what I am expecting after looking at the distribution of my shortest path on an histogram.</p>

<p>However, if I do what I think that the formula is telling me, then I endup with doing:</p>

<p>$$
\bar P = \frac{1}{400000000} \cdot \sum_{i \neq j}^{} d(v_{i},v_{j})
$$</p>

<p>which is clearly not the average of my shortest paths.</p>

<p>So my question is: what is my issue with understanding this formula? I am clearly missing something in my thinking, but I don't know what.</p>
",<graph-theory>
"<p>A Paley graph is strongly regular with parameters $(p,\frac{p-1}{2},\frac{p-5}{4},\frac{p-1}{4})$. I need to prove that, and obtain the parameters too. Proving it is regular valency $\frac{p-1}{2}$ is easy but for the rest I am struggling. Also, proofs I found are very vague and only prove the third parameter and then use the equation $k(k-a-1)=b(v-k-1)$ to find the other. However, I dont think we can use that as we have not yet proved it is strongly regular, so we dont know if the equation holds.. is that right? Can someone please provide a proof of the statement? </p>

<p><a href=""http://en.wikipedia.org/wiki/Paley_graph"" rel=""nofollow"">http://en.wikipedia.org/wiki/Paley_graph</a></p>
",<graph-theory>
"<p>Given a definition below (source: On Hedetniemi's Conjecture and the color template scheme by C. Tardiff and X Zhu): <br/>
The exponential graph $G^H$ has all the functions from vertex-set of $H$ to that of $G$ as vertices and two of these functions $f,g$ are joined by an edge if $[f(u),g(v) ∈ E(G)]$ for all $[u,v] ∈ E(H)$.<br/>
Can anyone give me an example of exponential graph based on that definition?
Thank you.</p>
",<graph-theory>
"<p>I'm am trying to define a data structure to represent road networks.  The immediately obvious structure is that of a graph - a set of nodes and edges that connect pairs of nodes.  The nodes would represent things like intersections, and the edges would represent lanes.  </p>

<p>However, the basic concept of a graph is insufficient to describe what I want.  I also want to be able to describe how certain lanes are ""reachable"" or ""adjacent"" to other lanes.  (Imagine parallel lanes with no median between them.  You can just switch lanes.)  This seems to imply a need to have some sort of meta-edge that connects pairs of edges.  Is there any mathematical structure that sounds like this?  Is this reducible to the standard concept of a graph? Thanks.</p>
",<graph-theory>
"<p>This is an extension of the problem at this thread: <a href=""http://math.stackexchange.com/q/239120/74947"">Graphs, line graph and complement of graph</a>.</p>

<p>Just to reiterate definitions:</p>

<blockquote>
  <p>The <em>line graph</em> $L(G)$ of a graph $G$ is defined in the following way: the vertices of $L(G)$ are the edges of $G$, $V(L(G))=E(G)$, and two vertices in $L(G)$ are adjacent if and only if the corresponding edges in $G$ share a vertex.</p>
</blockquote>

<p><strong>Question</strong>: </p>

<p>Suppose $G$ has $n$ vertices, labeled $v_1,v_2,\ldots,v_n$ and the degree of vertex $v_i$ is $r_i$. Let $m$ denote the size of $G$, so $r_1+r_2+\ldots+r_n=2m$. Find formulas for the order and size of $L(G)$ in terms of $n,m,$ and $r_i$.</p>

<p><strong>So far</strong>:</p>

<ol>
<li>Clearly, the order is $m$.</li>
<li>Suppose that the edge $e=(v_i,v_j)$ exists in $G$. Then, the $\deg(e)=r_i+r_j - 2$ in $L(G)$, because at the $v_i$ end, $e$ shares $v_i$ with $r_i - 1$ other edges, and at the $v_j$ end, $e$ shares $v_j$ with $r_j - 1$ other edges.</li>
<li>I tried to use the above to sum over all the edges in $G$, but this depends on knowing which edges exist in the graph $G$, which we are not given. We only know the <em>degree sequence</em>, not the edges.</li>
<li>Another approach I tried was to find this expression for a regular graph with degree $k$, and this can be found using the above: $$m(2k - 2) = 2|E(L(G))| \implies |E(L(G))| = m(k - 1).$$ I then tried this for a complete graph (so $k = m - 1$), and then removing edges one at a time until it becomes $G$, but this also requires knowledge of the edges.</li>
</ol>

<p>Any help would be greatly appreciated. One piece of information that I seem to be not using is the variable $n$, the order of $G$, but I am not sure where it would be appropriate to use.</p>
",<graph-theory>
"<p>I am working on the statement "" A graph $G$ is a $nKn$ (collection of $n$ number of complete graph each of order $n$) graph if and only if $\bar{G}$ is $α(G) = α(\bar{G}) = n$ and $(p-n)$-regular , where $p$ is the order of $G$ and $α(G)$ is the independence number of $G$ with $n\geqslant 1$.""</p>

<p>The converse part seem to be a confusion. Kindly help me with it.</p>
",<graph-theory>
"<p>I was given this problem to solve by a professor who promised an A if anyone could solve it. I'm nearly certain it is impossible, because at some point you have too many vertices and inevitably box yourself in, but I would like to know how to prove it.</p>

<p>How can you prove that it is impossible to connect every <code>*</code> with every <code>0</code> without overlapping lines?</p>

<pre><code>*    *    *   

0    0    0
</code></pre>

<p>EDIT: Professor said you get an A if you <strong>solve it</strong>, not prove it's impossible. I'm not even in the class, it was my friend's question, I am just curious.</p>
",<graph-theory>
"<p>Let $G=(V,E)$ be a planar graph. Suppose a planar representation of $G$ has been chosen and that $$v-e+f=2,$$ where $v,e$ and $f$ are the number of vertices, edges and faces respectively. See <a href=""http://en.wikipedia.org/wiki/Planar_graph#Euler.27s_formula"" rel=""nofollow"">Wikipedia</a>.</p>

<p>Does this imply that $G$ must be connected?</p>
",<graph-theory>
"<p>So I have a 10 cube insanity puzzle, I constructed the graph (I'll post pc later) based on the colors the problem is that there is just way to many lines and nodes to see the solution. Is there any way to find those paths in the graph without trying every permutation. Guess I was unclear. The puzzle is that each cube has 3 different colors, I need to line them up so that each side will have no repeating colors, so with 10 cubes we have 10 unique colors, now when constructing a graph based on problem I need to find a path that will take me through each node but wont repeat same number path (paths are numbered based on cube numbers). so question is how can I find the path from the graph I attached.
<img src=""http://i.stack.imgur.com/2chgj.jpg"" alt=""enter image description here""></p>
",<graph-theory>
"<p>Prove that a bipartite graph has a unique bipartition if and only if it is connected. </p>

<p>I am completely stuck on this question. I assume it is a proof that involves a mixing of definitions but I am not sure. </p>
",<graph-theory>
"<p>How would you connect each black box once to each colored box without any lines overlapping, this is racking my brain so please help.</p>

<p>Note that you can move the boxes where ever you want.</p>

<p>Maybe there's some math trickery involved as I have spent hours trying to crack it but cannot seem to without having no option but to overlap (which I can't) or start again.</p>

<p><a href=""http://i.stack.imgur.com/ACEFt.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ACEFt.png"" alt=""boxes_to_connect""></a></p>

<p>I am unsure if this is the correct forum to post in.</p>
",<graph-theory>
"<p><a href=""http://i.stack.imgur.com/upWTS.png"" rel=""nofollow"">Graph</a></p>

<p>I need to show this graph is not planar. I've attempted to find $K_5$ and $K_{3,3}$ as a subgraphs but haven't been successful yet. It's possible but unlikely this graph <em>is</em> planar but I haven't been able to draw it as a plane graph.</p>
",<graph-theory>
"<p>suppose you have the $K_{2n}$ covered by a multigraph consisting of $2n-1$ bicliques, each consisting of a partition of the vertex set into two sets of equal size.</p>

<p>Here is a picture of $K_{6}$ with 5 bicliques. </p>

<p><img src=""http://i.stack.imgur.com/01hOM.jpg"" alt=""enter image description here""></p>

<p>My question is: under which circumstances is it possible to pick a perfect matching of each biclique, such that every edge of $K_{2n}$ is covered exactly once?</p>

<p>I know so far, that obviously every edge should be covered at least once and every vertex has $n$ edges of one color. Moreover a biclique can only occur at most $n-1$ ($n$ odd) or $n$ ($n$ even) times.</p>

<p>Is there any theory regarding this question? </p>
",<graph-theory>
"<p>Prove that if $G$ is a graph in which every edge is a part of at most one odd length cycle, then the graph is 3 colorable.</p>

<p>I want to show that if a graph is 4-critical there are two odd cycles which share an edge, which would prove the question. I'm not sure how to get that. I thought to take a maximum path, and then an end point has at least 3 neighbours in the path, which ensures 3 cycles, but I don't see a reason they can't all be even.</p>
",<graph-theory>
"<p>I know that a hamilton cycle exists for a bipartite graph $K_{m,n}$ if and only if $m=n$</p>

<p>But my question is why is it not possible to have a bipartite graph if $m=n=1$</p>

<p>I mean we would go from $x$ to $y$ and back to $x$</p>

<p>Why this is not allowed, We don't have a restriction on the edges, I mean we are allowed to use an edge more than once in a Hamilton cycles unlike Euler cycles.</p>

<p>So I am just surprised why is it not possible, does someone have an explanation for that ?</p>

<p>Because according to the definition of a Hamilton cycle, It's basically a cycle that begins and ends on the same vertex and we don't have restrictions as I said on the usage of the edges more than once.</p>
",<graph-theory>
"<p>Is there a name for the subset of graph theory dealing with vertices and edges of distinct classes? For example, I could have a graph in which each vertex must be either blue, yellow or red and each vertex must be either dashed or solid. </p>

<p>I'm working on a problem with these characteristics but I have no background in graph theory. This makes it hard to find the right terminology to search to find solutions.</p>
",<graph-theory>
"<p>an agent is works between n producer and m consumers. i'th producer, generate $s_i$ candy and j'th consumer, consumes $b_j$ candy, in this year. for each candy that sales, agent get 1 dollar payoff. for some problems, one strict rule was defined that a producer should be sales candy to any producer that the distance between them is not greater than 100 KM (kilometers). if we have list of all pairs of producer-consumer that the distance  between them is lower than 100 KM, which of the following algorithm is goof for finding maximum payoffs? (suppose $s_i$ and $b_j$ may be becomes very large).</p>

<pre><code>1) Maximal Matching

2) Dynamic Programming

3) Maximum Flow 

4) 1 , 3
</code></pre>

<p>this is a last question on 2013-Final Exam on DS course.  any hint or idea? </p>
",<graph-theory>
"<p>I saw this interesting problem, I did my best so solve it but I failed.</p>

<p>In the graph $G = (V,E)$ , if  $|V| = 10$ and number of ordered pairs $(x,y)$ of vertices equals to <strong>72</strong> such that $d(x,y) \ge 2$, then how many edges G has? the answer is one of the following choices.</p>

<ol>
<li><p>9</p></li>
<li><p>18</p></li>
<li><p>28</p></li>
<li><p>36</p></li>
</ol>
",<graph-theory>
"<p>Let G be a forest with two components and at least four vertices. Is it true that G has at least four leaves?</p>

<hr>

<p><img src=""http://i.stack.imgur.com/79ch3.png"" alt=""enter image description here""></p>

<p>the graph is which is I mentioned you</p>
",<graph-theory>
"<p>Given a CNF SAT formula, we can turn it into a Hamiltonian graph, which is Hamiltonian iff the formula is satisfiable. Now, we can transform the CNF formula into a DNF one. My question is, can the graph be transformed into a ""DNF"" graph, from which the property of satisfiability or Hamiltonian-ness can be observed obviously?</p>
",<graph-theory>
"<p>Let $V = v_1, \dots, v_n$ be the locations the items can spawn at, and let $U = u_1, \dots, u_k$ be the current positions of the items. We will assume a new items spawns instantly every time we collect an item, so there are always $k$ items (Assume uniform distribution for spawn location). Let $w$ be the current position of the character. Then $S = (V, U, w)$ is the state of the game.</p>

<p>Assume the character can move at a constant speed, so the time metric we care about is proportional to the distance between two points.</p>

<p>Essentially I want a function $\text{bestItem}(S)$ that tells me which item in $U$ to go to first to maximize the number of items I collect per hour.</p>

<p>This seems similar to shortest path on a clique with weighted edges, but it doesn't stop after one step. Or a traveling salesman problem where the traveling salesman doesn't know all the locations he needs to visit from the start.</p>

<p>Is there a name for this problem? I am curious if it is already solved, and how good the greedy method of just always taking the nearest item is compared to the optimal method. Or the method of computing the shortest path through the points of $U$, and updating it after some number of spawns.</p>

<p>Has anyone worked with this type of problem and point me at some reading material? My interest is inspired by video games which have mechanics like this.</p>

<p>Thanks.</p>
",<graph-theory>
"<p>I am reading on my own the notes of this lecture series from 2012: <a href=""http://www.cs.yale.edu/homes/spielman/561/2012/lect04-12.pdf"">http://www.cs.yale.edu/homes/spielman/561/2012/lect04-12.pdf</a>. In section 4.7.2 (page 8) it's mentioned that we can prove a lower bound of $O(1/n)$ for the 2nd eigenvalue of the binary tree. I found a proof of this fact here: <a href=""https://www.cs.cmu.edu/~glmiller/Publications/Papers/GuMi94-tr.pdf"">https://www.cs.cmu.edu/~glmiller/Publications/Papers/GuMi94-tr.pdf</a> (lemma 3.8), but I could not prove it using the methods suggested in the lecture. Could someone help me understand this?</p>
",<graph-theory>
"<p>Consider a random walk on an undirected graph which, at each step, moves to a uniformly random neighbor. Define $T(u,v)$ to be the expected time until such a walk, starting from $u$, arrives at $v$, and let $T = \max_{u,v} T(u,v)$. Define $G(u)$ to be the expected time until such a walk, starting from $u$, visits every vertex and let $G = \max_u G(u)$. Is it true that  $$G \leq cT$$ for some constant $c$ which does not depend on the graph and any of its parameters (e.g., number of nodes)?</p>
",<graph-theory>
"<p>This is a followup to my question <a href=""http://math.stackexchange.com/questions/143245/cover-times-and-hitting-times-of-random-walks"">Cover times and hitting times of random walks</a>.</p>

<p>Consider a random walk on an undirected graph with $n$ vertices which, at each step, moves to a uniformly random neighbor. Define $T(u,v)$ to be the expected time until such a walk, starting from $u$, arrives at $v$, and let $T = \max_{u,v} T(u,v)$. Define $G(u)$ to be the expected time until such a walk, starting from $u$, visits every vertex and let $G = \max_u G(u)$. Is it true that  $$G \leq cT \log^k n$$ for some constants $c,k$ which do not depend on the graph or on $n$? </p>
",<graph-theory>
"<p>This concerns a problem from Extremal Combinatorics by Jukna that I cannot solve myself. </p>

<p>First some preliminaries. A biclique covering of a graph is a covering of a graph with complete bipartite graphs (so that every edge of the initial graph belongs to at least one of the bipartite graphs that cover the initial graph). For each such covering of a graph, let the weight of a covering be the number of vertices of each of the subgraphs (the bipartite graphs) all added together. (i.e. if the graphs $H_1, H_2, \dots, H_m$ are the bipartite graphs that cover $G,$ then the weight of this covering will be $\displaystyle\sum_{i=1}^m |V(H_i)|,$ where $|V(H_i)|$ denotes the number of vertices in $H_i.$) The covering with the minimum such weight is $\text{bc}(G)$ for the graph $G.$ </p>

<p>If $\mu_G$ is the minimum over all the $(a+b)/ab$ for pairs of integers $a,b \ge 1$ such that $G$ contains a copy of the complete bipartite graph: $a \times b$ (or $K_{a,b}$). Prove that $\text{bc}(G) \ge \mu_G \cdot |E|.$ (Where $|E|$ denotes the number of edges of $G.$) </p>

<p>Thank you very much for the help. </p>
",<graph-theory>
"<p>A 3 regular, plane, connected graph have all surfaces either $4$ or $6$ edges (including the outer surface). How many surfaces has $4$ edges?</p>

<p>Let $x$ be the number of surfaces that have $4$ edges</p>

<p>Let $y$ be the number of surfaces that have $6$ edges</p>

<p>Euler's formula gives:</p>

<p>$$v - e+ (x+y) = 2$$</p>

<p>I am stuck here. Ideally I would like another equation that has $x$ and $y$ in it so I thought of the one that says that the sum of the grade of every vertex is twice the edges. </p>

<p>$$3v = 2e$$</p>

<p>But it don't see how that would help. </p>

<p>Any ideas?</p>
",<graph-theory>
"<p>Given an undirected graph G(V,E). A and B are elements of V. Identify a subgraph of G containing A &amp; B with 2 edge disjoint spanning trees (or prove one doesn't exist).</p>

<p>I have found several sources stating to convert the problem to a directed graph and use max-flow min-cut.</p>

<p><a href=""http://stackoverflow.com/questions/3271763/how-to-find-two-disjoint-spanning-trees-of-an-undirected-graph"">http://stackoverflow.com/questions/3271763/how-to-find-two-disjoint-spanning-trees-of-an-undirected-graph</a></p>

<p>I'm not seeing how this proves edge disjoint spanning trees (only edge disjoint paths). I have seen it cited multiple times, so I am guessing I am just missing something.</p>
",<graph-theory>
"<p>I came across this question and I tried to give myself a direction but couldn't:</p>

<blockquote>
  <p>Given $T1 = (V, E_1)$ and $T2 = (V, E_2)$ two trees with same group of vertices. Prove that there is a vertex $v$ such that $deg(v)_{T1} + deg(v)_{T2} \leq 3$.</p>
</blockquote>

<p>Now I thought about that in a tree there are at least two leafs, we take one and we show that its' degree in $T2$ is less or equal 2, but that is obviously not a condition and not always correct.</p>

<p>I would be happy for a direction!</p>
",<graph-theory>
"<p>Show that in a graph $G$ where every two different edges are connected (i.e there is an edge incident with both) we have that $\delta&lt;\omega+{\omega \choose 2}$ where $\delta$ is the min degree and $\omega$ is the clique number.</p>

<p>It looks like it should some kind of combinatoric argument where we get a larger clique by contradiction.</p>

<p>Note that the original question was to show that in a graph $G$ where every two different edges are connected (i.e there is an edge incident with both) we have that $\chi \le \omega+{\omega \choose 2}$, and what I did was assume by negation that this wasn't true and found a subgraph with min degree at least that, looking to find a contradiction. It looks to me like it should be a valid direction with some kind of combinatoric argument as mentioned, however I just can't find one.</p>

<p>Thanks</p>
",<graph-theory>
"<p>I tried to find a book or paper to understanding discrete association scheme but I could not get any book for that. What is the good references for that?</p>
",<graph-theory>
"<blockquote>
  <p>let $G=(V,E)$ be a connected graph, with $|E| = 19$ and $deg(v) \geq
 4$ for all vertices $v$ in $V$</p>
  
  <p>What is the largest possible value of $|V|$ ?</p>
</blockquote>

<p>Well I know that the sum of the degree is equal to two times the number of edges and so $\sum deg(v) = 2 \times 19 = 38$ </p>

<p>Now I think if we want to find the largest possible value of $|V|$ then we try to minimize the degree of every vertex but we have that $deg(v) \geq 4$ and so $\frac{38}{4} = 9.5$ but we can't have fractions in our answer, and if we take $9 \times 4 = 36$  that can't happen, then we can have $8$ vertices each with degree $4$ and the last one should be degree $38 -32 = 6$</p>

<p>So My final answer is $9$ vertices, $8$ of them with degree $4$ and one of them has degree $6$ does that make sense ?</p>
",<graph-theory>
"<blockquote>
  <p>If the edges of $K_6$ are coloured blue or red, prove that there is a
  red triangle or a blue triangle that is a sub-graph.</p>
</blockquote>

<p>Well I am having a hard time proving this, I try to prove it by contradiction, Meaning that Assume that there is no red or blue triangle that is a sub-graph and I argue that You can't avoid but to have a red or a blue triangle.</p>

<p>But I can't even start the argument.</p>

<p>That was my attempt, </p>

<p>We consider all cycles of length $3$ in $K_6$ which are the triangles,</p>

<p>Now We start coloring one of the edge in this triangle with one red and the other edge with blue, Now the third vertex can be either red or blue, but without loss of generality we choose blue. So now we have a triangle with two blue edges and one red edge.</p>

<p>But then what can I say ?</p>
",<graph-theory>
"<p>I'm having trouble knowing whether this graph is planar or nonplanar - a seven vertex graph with each vertex of degree 4.</p>

<p><a href=""http://imgur.com/09u1UlH"" rel=""nofollow""><img src=""http://i.imgur.com/09u1UlH.png"" title=""source: imgur.com"" /></a></p>

<p>So it's planar if e ≤ 3v - 6, and there are 14 edges and 7 vertices, so 14 ≤ 15. So it is planar then? I can't seem to find any way this would be planar, and if it is nonplanar, I can't find a sensible k3,3 configuration.</p>
",<graph-theory>
"<p>Let $(G,E)$ be a finite undirected graph, and $d$ be the usual shortest path distance on $G$. The graph is not necessary connected, so $d(v',v'') = \infty$ if there are no paths from $v$ to $v'$. For $A\subseteq G$ we put
$$
d(v,A) = \min\limits_{u\in A}d(v,u).
$$
My question is the following: let us define for $A\subseteq G$
$$
m(A) = \max\limits_{v\in A}d(v,G\setminus A).
$$
Is there a name for $m(A)$, maybe you can give a reference on this?</p>
",<graph-theory>
"<p>Hint: Suppose that the chromatic number is at least 6 and consider graphs induced by 3 colour classes.</p>

<p>Any hints would be appreciated. I do not understand what a graph induced by 3 colour classes are. They have not been defined in my lectures. </p>

<p>Thanks</p>
",<graph-theory>
"<p>I am having trouble with the following Question:
Prove that a graph is 2-connected(biconnected) iff for every triple distinct vertices u,v,w there is a path connecting u and v and goes through w.
Now , proving $\leftarrow$ is really easy, but I am having trouble proving the $\rightarrow$ part was harder for me and I will be happy if someone could ge me a clue :)
p.s. : our lecturer defined the connectivity of a graph by the number of vertices that need to be removed in order for the graph to become disconnected.  </p>
",<graph-theory>
"<blockquote>
  <p>Suppose that four judges $J_1$, $J_2$, $J_3$, and $J_4$ each rank eight objects: $O_1,O_2,\ldots,O_8$ independently. Their rankings are</p>
  
  <p>$$\begin{array}{cc}
J_1: &amp; O_1\ O_2\ O_3\ O_4\ O_5\ O_6\ O_7\ O_8 \\
J_2: &amp; O_2\ O_4\ O_6\ O_8\ O_1\ O_3\ O_5\ O_7 \\
J_3: &amp; O_3\ O_5\ O_4\ O_8\ O_7\ O_6\ O_2\ O_1 \\
J_4: &amp; O_6\ O_7\ O_1\ O_2\ O_3\ O_4\ O_5\ O_8
\end{array}$$</p>
  
  <p>Construct a digraph which reflects these rankings. Use component analysis to interpret these rankings. </p>
</blockquote>

<p>This is question 7.1 (pages 120-121) from the text <a href=""http://dx.doi.org/10.1007/978-1-4612-0933-1"" rel=""nofollow"">Graph Theory Applications</a> by L. R. Foulds.</p>

<p>I tried to answer this question but I am confused about how to construct the graph. Because what I am thinking is that this digraph should be considered as a tournament and use tournament analysis to interpret rankings. But when I am drawing edges based on the judges rankings there is more than one edges between two vertices like $O_1 \rightarrow O_2$ based on $J_1$ and $O_2 \rightarrow O_1$ based on $J_3$. These cannot be present in a tournament. So I am confused how to approach this question.</p>

<p>Please help me to solve this question</p>
",<graph-theory>
"<p>Say you have a sparse matrix in CSC or CSR format (or whatever format is suitable for this to work) and all you know are it's dimensions: $n$, $m$ and $nz$, and the data in the structure. You are told nothing about the structure or layout.</p>

<p>A description of the CSR format: <a href=""http://netlib.org/utk/papers/templates/node91.html#SECTION00931100000000000000"" rel=""nofollow"">http://netlib.org/utk/papers/templates/node91.html#SECTION00931100000000000000</a></p>

<p>How would you determine (from the $row\_ptr$ and $col\_ind$ arrays) if a sparse matrix is structurally symmetric?
Is there a simple and efficient method?</p>

<p>An (obvious) property of a structurally symmetric matrix is that every row $x$ in the matrix has the same number of elements as each corresponding column $x$. </p>

<p>Is that one of the defining properties of a sparse symmetric matrix?</p>

<p>If we had a square sparse matrix $A$ where every row had the same number of elements as its corresponding column, i.e.:</p>

<ul>
<li>$numel(A(i,:)) = numel(A(:,j))$ where $i = j $ $\ \forall i,j \in n$</li>
</ul>

<p>Would it be then true that $A$ is structurally symmetric?</p>

<p>Experimentally this does seem to be the case. Unfortunately my Linear Algebra and Graph Theory is not up to any sort of proof.</p>

<p><strong>EDIT:</strong>
Ok. The above does not hold true for a permutation matrix where every row/column just has a single entry.
However, can anyone suggest a matrix where $nz&gt;n$ or there is a at least one row/column with 2 or more elements, which is not symmetric?</p>

<p>Cheers,</p>

<p>-- El Bee</p>
",<graph-theory>
"<p>I am having some difficulty following the concept of shrinking a set of node as given in the paper <a href=""http://www.zib.de/Publications/Reports/ZR-06-22.pdf"" rel=""nofollow"">On the Bottleneck Shortest Path Problem</a></p>

<p>It says:</p>

<blockquote>
  <p>Shrinking a set of nodes can be done
  in linear time. More precisely, given
  a set S ⊆ V of nodes of an
  (undirected) graph G = (V,E), one can
  construct in linear time another graph
  with nodes (V \S)$\cup${vnew} (where vnew
  represents the shrunken set S), where
  v,w ∈ V \ S are adjacent if and only
  if v and w are adjacent in G (in this
  case,the edge keeps its weight), and
  vnew and w ∈ V \ S are adjacent if and
  only if there is some v ∈ S such that
  v and w are adjacent in G (in which
  case the edge receives the biggest
  weight of any edge connecting S and w
  in G).</p>
</blockquote>

<p>I am particularly confused about the last sentence of how vnew and w are adjacent.  Actually I am pretty much confused about it all and any clarity would be appreciated.</p>
",<graph-theory>
"<p>Ramsey number $R(3,6)=18$. How to construct a graph of $17$ nodes which does not contain neither a clique of order $3$ or an independent set of order $6$. could you show me the tactics or the adjacency matrix?</p>
",<graph-theory>
"<p>I'm stuck on this problem, posting my progress so far below. I've looked at similar questions <a href=""http://math.stackexchange.com/questions/237134/prove-by-induction-that-every-connected-undirected-graph-with-n-vertices-has-at"">here</a> and <a href=""http://math.stackexchange.com/questions/457042/prove-that-a-connected-graph-with-n-vertices-has-at-least-n-1-edges"">here</a>, but neither seem to directly prove the predicate by induction, with a base case followed by the inductive step. </p>

<p><strong>Solution(?):</strong></p>

<p>Let P(n) be the predicate (All n, n >= 1, any tree with n vertices has (n-1) edges).</p>

<p><strong>Base case (n=1):</strong></p>

<p>P(1): (n=1, any tree with 1 vertice has 0 edges). A tree <em>G</em> is a connected graph with no cycles of length at least three. An edge <em>E</em> is a two-element subset of the set of all vertices <em>V</em> of <em>G</em>. In the base case, <em>V</em> contains one vertice. Thus there are zero two-element subsets of <em>V</em>, or zero edges, in <em>G</em>.</p>

<p><strong>Inductive Step:</strong></p>

<p><s>For every n>0, P(n) => P(n+1).</p>

<p>For P(n+1), where n>0 in <em>G</em>, n+1 vertices will form exactly n edges. All vertices are of degree 1, which means there is exactly one less edge than vertice (one edge per vertice, excluding the root vertice). Thus, any tree with n vertices will have exactly n-1 edges.</p>

<hr>

<p>Assume P(n) is true. Then, we want to prove that the graph <em>G</em>, for P(n+1), has n edges. </p>

<p><em>G</em> contains no cycles of length at least three. Thus, <em>G</em> must contain only one region, meaning that the edges in <em>G</em> do not form any regions. This means that <em>G</em> must contain at least one leaf (vertex with degree of 1). </p>

<p>Obtain a graph <em>G'</em> by removing one leaf from <em>G</em>. Because a leaf is a vertex of 1, by the definition of vertex degree, the leaf is connected to exactly one edge. Thus, removing the leaf will remove one vertex and one edge from <em>G</em>, to create <em>G'</em>. This means that for all n, n>=1, any tree with (n+1) vertices will have ((n+1)-1). or n, edges. </p>

<p>We have proven that the case base P(1) is true, and assuming that P(n) is true, we see that the statement remains true for n => n+1. Thus, the predicate (All n, n >= 1, any tree with n vertices has (n-1) edges) is true by induction.
</s></p>

<hr>

<p>Assume P(n) is true. Then, we want to prove that the graph <em>G</em>, for P(n+1), has n edges.</p>

<p><em>G</em> contains no cycles of length at least three. Thus, <em>G</em> must contain only one region, meaning that the edges in <em>G</em> do not form any regions. This means that <em>G</em> must contain at least one leaf (vertex with degree of 1).  </p>

<p>Remove a leaf <em>l</em> from <em>G</em> to obtain a tree <em>G′</em> with n vertices. Then, <em>G'</em> has n−1 edges, by the inductive hypothesis. The addition of <em>l</em> to <em>G′</em> produces a graph with n+1 vertices and n edges, since <em>l</em> has degree 1.</p>

<p>P(1) is true, and for every n>0, P(n) => P(n+1). Thus, the predicate must be true.</p>

<hr>

<p>I'm not exactly sure how to proceed for the inductive step., or if I doing it correctly. Any hints, or confirmation that I'm going in the right direction would be greatly appreciated. </p>
",<graph-theory>
"<p>I have a strong feeling that complete graph minors ($K^i$ minors) are like a sort of cycles in a simplicial complex. Is there a homology theory (for graphs) counting complete graph minors?</p>
",<graph-theory>
"<p>Assuming a single source, single sink digraph with |V| vertices, including source s and sink t. How many “cuts” does a flow network have? </p>
",<graph-theory>
"<blockquote>
  <p>Let $G=((2,3,4,5,6,7),E)$ be a graph such that {$x$,$y$} $\in E$ if and only if the product of $x$ and $y$ is even, decide if G is an Eulerian graph.</p>
</blockquote>

<p><strong>My attempt</strong></p>

<p>I tried to plot the graph, this is the result:</p>

<p><img src=""http://i.stack.imgur.com/jeo9w.jpg"" alt=""enter image description here""></p>

<p>So, if my deductions are true, this is <strong>not</strong> an Eulerian graph because it's connected but all the vertices doesn't have an even degree. For example $deg(2)=5$. Moreover, there is no trace of Eulerian trails.</p>

<p>I cannot figure out if this assumptions are presumably correct.</p>
",<graph-theory>
"<p>What are some interesting partial orders on the set of all finite graphs (identified up to isomorphism), apart from the usual (induced) subgraph relation and the (topological) minor relation, and why are they interesting?</p>
",<graph-theory>
"<p>From ""Lecture Notes in Computer Science"" by Christoph M. Hoffmann ,</p>

<blockquote>
  <p>Assume that both $X$ and $X'$ have $n$ vertices. We plan to code the
  graph labels as suitable subgraphs which we attach to the vertices of
  $X$ and of $X'$. In time polynomial in the length of the input we can rename 
  the labels and may assume, therefore, that $L = \{1 ..... k\}$ is
  the set of labels assigned by $\lambda$ and $\mu$. <strong>Note that $k\leq2n$</strong> .We describe
  how to construct $Z$ from $(X,\lambda)$. The construction of
  $Z'$ is done in the same way. Let $X = (V,E), V = \{v_1..... v_n\}$.
  Intuitively, we obtain $Z$ from $X$ by attaching to each vertex $v_i$ a
  complete graph with $n + r$ vertices, where $\lambda (v_i) = r$. <strong>Note that $r\leq2n$.</strong>
  The vertices of the attached complete graph will be, for $v_i$, $\{v_{(i,1)},..... v_{(i,n+r)} \}$
   The subgraph is atached to $v_i$ by an edge $(v_i, v_{i,1})$.</p>
  
  <p>It is easy to see that $(X,\lambda)$ is
  isomorphic to $(X,\mu)$ iff $Z$ is isomorphic to $Z'$. <strong>Since there are at most</strong>
  <strong>$2n$ distinct labels</strong>, the graphs $Z$ and $Z'$ have no more than
  $2n^2+n$ vortices each and can thus be constructed in
  polynomial time.</p>
</blockquote>

<p>the definition of graph label is given as-</p>

<blockquote>
  <p>Let $X = (V,E)$ be a graph, $\lambda$ is  a mapping from $V$ onto a set $L = \{ l_1,...l_k\}$.
   Then the pair  $(X,\lambda)$ is a labelled graph.</p>
</blockquote>

<p>Question 1: Why $k\leq 2n$ which implies $r\leq2n$?   It seems that it should be $k\leq n$ since <a href=""https://en.wikipedia.org/wiki/Graph_labeling"" rel=""nofollow"">the passage of wikipedia here</a> tells</p>

<blockquote>
  <p>When used without qualification, the term labeled graph generally
  refers to a vertex-labeled graph with all labels distinct. Such a
  graph may equivalently be labeled by the consecutive integers $\{1, …,
&gt; |E |\}$, where $|E |$ is the number of vertices in the graph.</p>
</blockquote>

<p><strong>Question 2:</strong>  What is the explanation of -</p>

<blockquote>
  <p>Since there are at most
  $2n$ distinct labels, the graphs $Z$ and $Z'$ have no more than
  $2n^2+n$ vortices each and can thus be constructed in</p>
</blockquote>

<p>Thanks in advance.</p>
",<graph-theory>
"<p>I've been reading up on <a href=""http://en.wikipedia.org/wiki/Cycle_graph_%28algebra%29"" rel=""nofollow"">cycle graphs</a>, and the different and unique structures that groups produce really interest me. My question is, what is the requirements for a graph to be the cycle graph of a group?</p>

<p>For example, the graph on the left is not the cycle graph of a group, and the graph on the right is the cycle graph of $S_3$:</p>

<pre><code>    o       o  o  o
    |        \ | /
    e          e
   / \        / \ 
  o - o      o - o
</code></pre>

<p>By hand I know how I could tediously prove that the graph on the right isn't a cycle graph by showing that is doesn't have closure. The proof would go somehting along the lines of:</p>

<p>From the graph, we know that the group has four elements. We'll call them $e$ (identity), $b$ (element at the top), $a$ and $a^2$ (elements in the cycle at the bottom).</p>

<p>We now prove that $ab$ cannot be in the group by trying all the possibilities and come to contradictions in each case:
$$ab=e \implies b=a^2$$
$$ab=a \implies b=e$$
$$ab=a^2 \implies b=a$$
$$ab=b \implies a=e$$</p>

<p>and since each element is unique, none of these statements can be true.</p>

<p>This method works for this graph, however I run into trouble with more complicated graphs, and I am just wondering if there is an easier way of doing this or if there is a certain criteria for a graph to be a cycle graph of a group.</p>
",<graph-theory>
"<p>I am struggling with this problem for hours but it seems to be easy. Here is the problem:</p>

<p>Proof that every vertex $v$ in 2-connected graph $G$ has neighbour $u$ such that $G - v - u$ is connected.</p>

<p>Any help is highly appreciated! I think that I might be missing a trivial solution here =(</p>

<p>EDIT:
So far, I tried to create an incremental process that will yield an appropriate neighbour for selected vertex $v$. So, one can remove $v$ and any $v$'s neighbour $u$ from $G$. The resulting graph can be connected or not. If it is connected, then we have found our neighbour and can stop the process. Otherwise, Graph $G$ is spliced into several components. It is clear that in each component should be at least one neighbour of $v$ as $G$ is 2-connected. I think that on this step I should choose one of these neighbours and remove it instead of initial $u$. But I cannot find the proof that it will not lead to another cut of the graph.</p>
",<graph-theory>
"<p>In Paul Erdős and Rényi's 1959 paper <em><a href=""http://www.renyi.hu/~p_erdos/1959-11.pdf"" rel=""nofollow"">On Random Graphs I</a></em>, they describe the number of edges in a random graph by the function</p>

<blockquote>
  <p>(1) Nc = [1/2 * nlogn + cn]</p>
</blockquote>

<p>where <em>n</em> is the number of nodes in the graph, <em>c</em> is ""an arbitrary fixed real number"" and [x] denotes the integer part of x. They go on to use a number of graphs of the form G(n, Nc), where G(n, Nc) denotes a random graph with <em>n</em> nodes and <em>Nc</em> edges.</p>

<p>Nc appears to be an arbitrary (?) function to return a number of edges based on some <em>c</em> and <em>n</em>, but I don't understand how <em>c</em> is selected or why the function is relevant. When the authors discuss graphs of the type G(n, Nc), is there any special property besides the graph just having some arbitrary number of edges? I.e, could I replace Nc with some function for a random number of edges between 0 and the maximum possible edges for the graph with the same results?</p>
",<graph-theory>
"<p>Let $\textbf{Grph}$ be the category whose objects are graphs $G = (V,E)$ such that $V$ is a set and $E \subseteq \mathcal{P}_2(V) := \{\{a,b\} \subseteq V: a\neq b\}$. We sometimes write $E(G)$ for $E$. The morphisms are maps $f:G\to H$ such that whenever $\{v,w\}\in E(G)$ then $\{f(v),f(w)\}\in E(H)$.</p>

<p>How can regular epimorphisms in $\textbf{Grph}$ be characterized?</p>
",<graph-theory>
"<p>I'm trying to figure out a way to detect highly popular (very high in-degree) nodes in a <strong>directed*</strong> graph (its an unstructured p2p network), without using the global knowledge of the topology. Each node has its own neighbor list and a regular node has access to its neighbors' neighbor lists as well. A node should be able to check its neighbor list and figure out which one of its neighbors has the highest in-degree.</p>

<ul>
<li>if node a has node b in its neighbor list:  a ---> b</li>
</ul>

<p><strong>Edit: Makes more sense to ask how to find the local sink of a directed graph, using the above mentioned info</strong></p>
",<graph-theory>
"<blockquote>
  <p>If $k$ is an integer such that $\operatorname{rad}G\le k\le\operatorname{diam}G$, then show that there is a vertex $w$ such that $e(w)=k$.</p>
</blockquote>

<p>For a connected graph $G$ , we define the distance $d(u,v)$ between two vertices $u$ and $v$ as the minimum of the lengths of the $u-v$ paths of G.</p>

<p>The eccentricity $ e(v)$ of a vertex $v$ of a connected graph $G$ is the number $\max_{u \in V}(d(u,v))$. that is ,$e(v)$ is the distance between $v$ and a vertex furthest from $v$.</p>

<p>The <strong>radius</strong> $\operatorname{rad}G$ of $G$ is the minimum eccentricity among the vertices of $G$ ,</p>

<p>while the <strong>diameter</strong> $\operatorname{diam} G$ is greatest distance between any two vertices of $G$.</p>

<p>In other words:<br>
$\operatorname{diam} G=\max_{u \in V}(e(v))$
and</p>

<p>$\operatorname{rad}G=\min_{ v \in V}(e(v))$ </p>

<p>and
$e(v)=\max_{u \in V}(d(u,v))$.</p>

<p>Reference: <a href=""http://gen.lib.rus.ec/book/index.php?md5=CCE6AA121C9935E19EC651802ADF7B09"" rel=""nofollow"">Original Book, <em>Graphs and Digraphs</em> By Chartrand, page 24</a></p>
",<graph-theory>
"<p>Among all $5-regular$ graphs, let $s$ be the smallest chromatic number, and $t$ be the largest chromatic number . For each integer $k$ such that $s \leq k \leq t$, show that there exist a $5-regular$ graph $G_k$ such that $\chi(G_k)=k$</p>

<p>For this problem, the book say $s=2$ and $t=6$. It's easy to see that </p>

<p>$$\chi(G_k) \leq 1+ \Delta(G)$$$</p>

<p>Since this is a $5-regular$, $\Delta(G)=5$, so $\chi(G_k)=k\leq 6$. However, I can't see why $s=2$.</p>

<p>I know that $\chi(G) \geq \omega(G)$ for every graph $G$, but there nothing guarantee that the largest complete subgraph of $G$ has order $2$</p>
",<graph-theory>
"<p>I was studying graph theory in Bollobas book's and I found a result given without proof that I wasn't able to understand. Here is my problem:
""Le G be an eulerian graph, then its planar dual is a bipartite graph"". Can anyone explain me why it is true.
Thanks in advance for the help.</p>
",<graph-theory>
"<p>I am interested in exhaustively rendering quartic (4-regular) simple graphs of increasing order and bi-colored black and white such that the closed neighborhood of each vertex contains an even number of black vertices. Here, the closed neighborhood of a vertex is defined as the vertex itself plus its adjacent vertices. </p>

<p>A non-trivial (not 'all white') example is provided by the quartic graph of order five ($K_5$, the complete graph with 5 vertices) with two or four vertices colored black and the remaining ones colored white. </p>

<p>How to generate non-trivial graphs of higher order? Can this be done exhaustively?</p>
",<graph-theory>
"<p>I really don't understand this definition from <a href=""http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.69.3223"" rel=""nofollow"">this</a> paper which is: </p>

<p>$A-bridge$: if $A \subseteq V(G)$, then an $A-bridge$ of $G$ is either an edge joining two vertices of $A$ or an edge-maximal sub-graph $H$ of $G$ that does not contain an edge between two vertices of $H$ and such that there is a path between any two vertices of $H$ with all its inner vertices distinct from the vertices of A.</p>

<p>Another thing to add is,that all the graphs in this paper are series parallel graphs.</p>

<p>It would be great to clarify this definition with some pictures or reference to some good sources!</p>
",<graph-theory>
"<p>Let $G$ be a <a href=""http://en.wikipedia.org/wiki/Directed_graph"" rel=""nofollow"">directed graph</a> and $A$ the corresponding <a href=""http://en.wikipedia.org/wiki/Adjacency_matrix"" rel=""nofollow"">adjacency matrix</a>. I'll denote with $\rho$ the <a href=""http://en.wikipedia.org/wiki/Adjacency_matrix"" rel=""nofollow"">spectral radius</a>, and with $I$ the <a href=""http://en.wikipedia.org/wiki/Identity_matrix"" rel=""nofollow"">identity matrix</a>.</p>

<p>What can we say about $G$ when the spectral radius of $A$ is less then $1$? Is there eny graph property what show us that fact? More general which is equivalent to it?</p>

<p>I know that this is spectral graph theory topic, but I'm not an expert in this field. I know that classic result, that $[A^k]_{ij}$ shows the $k$ length walks from $i$ to $j$, and when $\rho(A) &lt; 1$, then exist $(I-A)^{-1}$, and the <a href=""http://en.wikipedia.org/wiki/Neumann_series"" rel=""nofollow"">Neumann-series</a> of $A$ converges to it. Furthermore because $A$ is nonnegative $\rho(A)=\lambda_{\max}$ without abolute value, and $(I-A)^{-1}$ is also nonnegative. That's all I know. Is there any result, that say some graph property which is equivalent to $\rho(A)&lt;1\,$?</p>
",<graph-theory>
"<p>In the plane, Euler's Polyhedral formula tells us that $V - E + F = \chi$, where for graph embeddings we have that $\chi = 1$. Alternatively, we can think of a graph embedding as a simplicial $1$-complex embedded in the plane.</p>

<p>My question is about if there exists a straightforward generalization of the value of the Euler characteristic to higher dimensions. In particular, for a graph embedded in 3-space, or equivalently a simplicial 2-complex or simplicial 3-complex embedded in $\mathbb{E}^3$, is there a simple expression that defines the Euler characteristic as a constant value, as there is for the plane? (namely $\chi =1$).</p>

<p>Thank you!</p>

<p>EDIT: I am aware that there is a general formula for $\chi$ if you know the number of faces of a given dimension, I am looking to determine those faces by knowing what the Euler characteristic is from external information.</p>
",<graph-theory>
"<p>I have a dense, adjacency matrix (square, symmetric) representing a graph. I want to threshold that graph so that it only contains the largest weights (cells in the matrix), but is still fully connected. Do you know of any fast algorithms?</p>
",<graph-theory>
"<p>I have difficulties understanding the proof given below showing that there exist 2 distinct vertices $u,v$ in $G$  such that $d(u) = d(v)$ where $G$ is a non-trivial graph.</p>

<p>Proof:
It's clear that $0 \leq d(x) \leq n-1$ for each $x \in G$. If the above statement is false, then there exist 2 vertices $y$ and $z$ in $G$ such that $d(y) = 0$ and $d(z)=n-1$, which however is impossible.  </p>

<p>I'm not sure how they deduce from the falsity of $0 \leq d(x) \leq n-1$ that there exist 2 vertices $y$ and $z$ in $G$ such that $d(y) = 0$ and $d(z)=n-1$.</p>
",<graph-theory>
"<p>Consider a simple lazy random walk on an $n$-vertex undirected, connected graph: this is the Markov chain which transitions from $i$ to $j$ with probability $p_{ij}=1/(2d(i))$ where $d(i)$ is the degree of node $i$. Note that $p_{ii}=1/2$ for all $i$.  Define $C(i)$ be the expected time until a walk starting from node $i$ visits every vertex and let $C = \max_i C(i)$. Let $I(k,l)$ be the expected time until two random walks, starting at vertices $k$ and $l$, intersect (i.e., until they visit the same vertex at the same time). Let $I = \max_{k,l} I(k,l)$.</p>

<p>My question is: how big can $I/C$ get as a function of the number of vertices $n$? Is it true that $I/C$ is upper bounded by a constant which is independent of $n$ or of the graph? If not, is it true that $$ \frac{I}{C} \leq k \log^l n$$ for some constants $k,l$ independent of $n$ and of the graph? </p>
",<graph-theory>
"<p>What formula would find the number of vertices within a 'normal' hexagonal graph, based on its radius (number of hexagons from center to edge)?</p>

<p>I've figured with pseudo code:</p>

<p><code>for (int i = 0; i &lt; r; i++)
{
   vertices += ((r + i) * 2) + 1;
}
vertices = vertices * 2;</code></p>

<p>Given the graph below, with a radius of 2, the above results in:</p>

<p>([((2 + 0) *2) +1] + [((2 + 1) *2) +1]) *2 = 24</p>

<p><img src=""http://i.stack.imgur.com/O5KMM.png"" alt=""enter image description here""></p>

<p>So... Is there a formula that can do this for a radius of n?</p>

<p>Or, given V - F + E = 2, and knowing F = (3* r^2) - (3* r) + 1; a formula to derive the number of edges would work just as well.</p>

<p>Thanks in advance!!</p>
",<graph-theory>
"<p>I am currently studying Graph Theory and want to know the difference in between Path , Cycle and Circuit. </p>

<p>I know the difference between Path and the cycle but What is the Circuit actually mean. </p>
",<graph-theory>
"<p>The <a href=""https://en.wikipedia.org/wiki/Hemi-dodecahedron"" rel=""nofollow"">Hemi-dodecahedron</a> can be nicely represented in five dimensions.  Here are the six faces, where all edges have length 2.  </p>

<p>{{{0,0,0,1,1},{1,1,0,0,0},{0,0,1,1,0},{1,0,0,0,1},{0,1,1,0,0}},<br>
{{1,1,0,0,0},{0,0,1,1,0},{0,1,0,0,1},{1,0,0,1,0},{0,0,1,0,1}},<br>
{{1,0,1,0,0},{0,1,0,1,0},{1,0,0,0,1},{0,0,1,1,0},{0,1,0,0,1}},<br>
{{0,1,1,0,0},{1,0,0,1,0},{0,0,1,0,1},{0,1,0,1,0},{1,0,0,0,1}},<br>
{{0,0,0,1,1},{0,1,1,0,0},{1,0,0,1,0},{0,1,0,0,1},{1,0,1,0,0}},<br>
{{0,0,0,1,1},{1,1,0,0,0},{0,0,1,0,1},{0,1,0,1,0},{1,0,1,0,0}}}    </p>

<p>Is there a nice set of coordinates in 4D or 5D (or higher) for the <a href=""https://en.wikipedia.org/wiki/57-cell"" rel=""nofollow"">57-cell</a>?  Could the above hemi-dodecahedron be used as one of the cells?</p>

<p>EDIT:  In 4D, these coordinates can be used for the six faces of the hemi-dodecahedron / Petersen graph. All edges have length 1, and the angle between edges is always 3/4. But I'm still unclear on how to glue these cells together.</p>

<p>$$(  
((\sqrt{5},1,1,1),(0,0,-2,0),(\sqrt{5},-1,1,-1),(0,0,0,2),(0,0,0,-2)),  
((\sqrt{5},1,1,1),(0,-2,0,0),(\sqrt{5},1,-1,-1),(0,0,2,0),(0,0,-2,0)),    
((\sqrt{5},1,-1,-1),(0,0,2,0),(\sqrt{5},-1,-1,1),(0,0,0,-2),(0,0,0,2)),    
((\sqrt{5},-1,1,-1),(0,0,0,2),(\sqrt{5},1,-1,-1),(0,-2,0,0),(0,2,0,0)),    
((\sqrt{5},-1,-1,1),(0,0,0,-2),(\sqrt{5},1,1,1),(0,-2,0,0),(0,2,0,0)),    
((\sqrt{5},-1,-1,1),(0,2,0,0),(\sqrt{5},-1,1,-1),(0,0,-2,0),(0,0,2,0)))/4$$</p>
",<graph-theory>
"<p>I received a review of one of my papers in which the reviewer made an objection:</p>

<p>""...from the equations e(G) = v(G) you derive that there is a unique simple cycle of G. This is false for non-simple graphs (with loops).""</p>

<p>Here e(G) and v(G) are the number of edges and vertices of the graph, respectively. G is assumed to be connected.</p>

<p>I think the definition of a simple cycle is a path that begins and ends at the same vertex and does not repeat any vertices or edges (and uniqueness is up to cyclic permutation, i.e., the starting point doesn't matter).</p>

<p>I can't think of any counterexample even when I allow multiple edges between vertices, or loops (an edge from a vertex to itself). In fact, I feel like this should be easy to prove. Am I missing something?</p>
",<graph-theory>
"<p>I never worked in this field before, I just thought about this set of rules and never saw something similar before. I apologise if I don't use the right mathematical vocabulary for my question.</p>

<p>Imagine a graph, in which bulbs are linked. The light can whether be <strong>on</strong> or <strong>off</strong>.
Bounds between bulbs are one-way. But more than one bound between two bulbs are possible.</p>

<p><img src=""http://i.stack.imgur.com/mHmwF.png"" alt=""Quick example of connected bulbs lighten or not""></p>

<p>Then there is this rule : Each time the clock ticks, the bulbs who receive light from at least two alight bulbs become lit too. The others are turned off.</p>

<p>For example, the previous graph will, the next time the clock ticks, become like this :
<img src=""http://i.stack.imgur.com/NFINq.png"" alt=""Next tick""></p>

<p>And then it will become : <img src=""http://i.stack.imgur.com/a26J0.png"" alt=""enter image description here""></p>

<p>I made a quick search, but I'm not used to the mathematical vocabulary of this field but I'm pretty sure it exists. Then I studied this... ""set of rules"" a little bit.</p>

<p>Before the question, here are some interesting and maybe useful circuits.</p>

<p>This narcissistic bulb will never get off, because it's connected to itself by two bounds.
<img src=""http://i.stack.imgur.com/WHfom.png"" alt=""Narcissistic turned on bulb (wow)""></p>

<p>This is an AND gate because, the bulb C will be lit (after 1 cycle) if and if only the bulb A AND the bulb B are lit.</p>

<p><img src=""http://i.stack.imgur.com/9MXZ8.png"" alt=""AND gate""></p>

<p>This is an OR gate because, the bulb C will be lit (after 1 cycle) if and if only the bulb A OR the bulb B is lit.</p>

<p><img src=""http://i.stack.imgur.com/A16wE.png"" alt=""OR gate""></p>

<p>I wanted to determine if I could build a computer with this set of rules. This is why I tried to determine the classical logic Boolean gates. But the NOT gate is essential to Boolean arithmetic, and I can't think of a way to create it or to prove it's impossible.</p>

<p>A NOT gate is supposed to be a circuit where if a bulb A is lit, then after a predetermined clock ticks, the bulb B will be off, and if A is off, B will be lit. A kind of inverter.</p>

<p>My questions are : </p>

<ul>
<li>How is this field named in mathematics ?</li>
<li>Is it possible to create a NOT gate ?</li>
</ul>
",<graph-theory>
"<p>Given a set of vertices $\{x_\alpha\}$ whose cardinality exceeds $\aleph_1$, (assume the axiom of choice) connect each vertex with its successor by an edge, forming a linear graph. Choose two vertices $x_i$ and $x_j$ such that $card\{x|x_i \lt x \lt x_j\} \gt \aleph_1$. On one hand, it is impossible to construct a path corresponding to the edge path from $x_i$ to $x_j$, since there does not exist a continuous, surjective map $\mathcal f: \mathbf I \to \mathbf S$, from the unit interval to the subgraph $\mathbf S$ between $x_i$ and $x_j$. On the other hand, the graph is connected and locally path-connected, and thereby path-connected, so that there must exist a path from $x_i$ to $x_j$, a contradiction.</p>
",<graph-theory>
"<p>A cycle on n vertices has n maximum cliques (i.e. K_2). [Except n=3] </p>

<p>Can you do better than this? </p>
",<graph-theory>
"<p>Given a graph $G$ on $n$ vertices and $m$ edges a standard existence proof for a cut of size $\geq \frac{m}{2}$ is to randomly assign vertices to a cut $S\subseteq G(V)$ and then in expectation half of the edges cross the cut from $S$ to $\bar{S}.$ Again using a similar (or perhaps even the same) randomized method, how does one increase the existence bound to $\frac{mn}{(2n-1)}$? I've attempted using the same vertex assignment scheme but have not been able to improve upon $\frac{m}{2}.$</p>
",<graph-theory>
"<p>Let $G$ be a $k$-connected graph. An $x,U$-fan is a set $U\subseteq V(G)$ of size $|U|\ge k$ together with a vertex $x\in V(G)\backslash U$ and a set of disjoint $x,U$-paths whose only common vertex is $x$. The number of disjoint $x,U$-paths is the size of the $x,U$-fan.</p>

<p>The problem is to show that in a $k$-connected graph there is always an $x,U$-fan of size $k$.</p>

<p>I was thinking induction over $k$, but the inductive step is rather messy.</p>
",<graph-theory>
"<p>What is the meaning of the term ""edge percolation""? </p>

<p>Context is graph theory, specifically, random graphs. In general, what does ""percolation"" mean in the context of random graphs?</p>

<p>Thanks.</p>
",<graph-theory>
"<p>Let $T_{1},T_{2}$ be two trees lying inside some common larger graph, say $G$. Under the hypothesis that $T_{1}\cap T_{2}$ is connected, is the union $T_{1}\cup T_{2}$ also a tree? This seems 'right' to me, but I've been unable to provide a proof of this intuitively 'right' assertion. Please help!</p>
",<graph-theory>
"<p>![image]<a href=""https://www.dropbox.com/s/o5ybtns0qd7t4b4/s.png?dl=0"" rel=""nofollow"">https://www.dropbox.com/s/o5ybtns0qd7t4b4/s.png?dl=0</a></p>

<p>Give an example of a Eulerian Path of the graph that starts at A</p>

<p>Isn't the graph Eulerian if it has 2 odd number of degrees?
when i counted the degrees they were all 4 so how do calculate the eulerian path?</p>
",<graph-theory>
"<p>What freely available graph theory resources are there on the web? In particular, I am interested in books and lecture notes containing topics such as trees, connectivity, planar graphs, the probabilistic method ect., though no resources is expected to be comprehensive. </p>

<p>Note that I have searched MathOnline, which yielded the book Graph Theory by Reinhard Diestel. This resources seems to be good, but I prefer to have multiple resources when studying a topic.</p>

<p>Note: I am aware of the many questions on this site regarding book suggestions for particular topics. While this question is related, I am only interested in freely available resources, as I am a broke college student. </p>
",<graph-theory>
"<p>I'm reading the book Probabilistic Graphical Models (Koller and Friedamn). I'm not quite sure about this example:</p>

<p>Given the next graph:</p>

<p><a href=""http://i.stack.imgur.com/DIzDz.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/DIzDz.jpg"" alt=""enter image description here""></a></p>

<p>The updwardly closed subgraph K+[C] is:</p>

<p><a href=""http://i.stack.imgur.com/ADoTz.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ADoTz.png"" alt=""enter image description here""></a></p>

<p>I don't get it. I understand why nodes A and D are present but why B and E? Thanks.</p>

<p><strong>EDIT</strong>: Definition of upwardly closed subgraph</p>

<p><a href=""http://i.stack.imgur.com/clA6E.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/clA6E.png"" alt=""enter image description here""></a></p>
",<graph-theory>
"<p>Can someone please point me in the direction of any theory on graphs where the edge weights are not scalars but represent some relation between the nodes that is a simple function of a single variable (simple, say piecewise linear). </p>

<p>In particular, I'm interested in various basic graph properties and also thinking of the graph as representing a network. So, for example, if the graph represented a communication network over time then the edge weights would be a function represented connectivity as a function of time, how do you find a valid path between nodes? I'm looking for help both on specific algorithms but also general theory if it exists?</p>

<p>I'm aware of time-extended networks where you explicitly expand out the dependence on the variable but, from what I've read, this incomplete and of limited applicability. </p>
",<graph-theory>
"<p>In my book, it's written that the following are equivalent. I don't understand why it is so. </p>

<ul>
<li>G is bipartite;</li>
<li>G is 2-colourable;</li>
<li>G does not contain any odd length cycles.</li>
</ul>
",<graph-theory>
"<p>Given $n$, what is the smallest number $N=N(n)$ with the property that there exists a tree on $N$ (unlabelled) vertices that contains a copy of every tree on $n$ vertices?</p>

<p>That such $N$ must exist is easy to see in a number of ways.</p>
",<graph-theory>
"<p>Is there any intuition behind Cayley's formula $n^{n-2}$ for the number of spanning trees of a graph?</p>
",<graph-theory>
"<p>Let $G$ be a graph with $n$ vertices, whose average degree is $k$. What is the probability that between any two vertices, there exists a path of length at most $l$? 
NOTE: For the above problem the random graphs follow a $G(n, p)$ model, i.e., in a random graph, each edge occurs with a probability $p=k/n$.</p>
",<graph-theory>
"<p>To show that 2-colorable belongs to $\mathsf P$, I have a straightforward mental description in mind that I don't think will be considered as a formal proof. Hence I am interested to know how this must be said as an answer to this question. Here's what I think: Say we have 2 colors and n vertices. 2-coloring will be applied such that no two adjacent vertices have the same color. If a vertex is in color A, the other is B and so on. Is my reasoning correct?</p>

<p><strong>Appendix:</strong> A graph G is said to be k-colourable if and only if a k-colouring of G exists.</p>

<p>A k-colouring is an assignment of k colours to the vertices of a graph G such that no edge joins two vertices of the same colour.</p>
",<graph-theory>
"<p>Is there any undergraduate textbook on graph theory using linear algebra? A request is a beginning with graph matrices that explain most concepts in graph theory? </p>

<p><em>P.s. This thread has more specific requests than this thread <a href=""http://math.stackexchange.com/questions/27480/what-are-good-books-to-learn-graph-theory"">What are good books to learn graph theory?</a>.</em></p>
",<graph-theory>
"<p>I am trying to work with graphs in MAGMA, but to my surprise I noticed that the vertices of the graph are no longer considered elements of the set I started with.
Here is an example:</p>

<p>X:={1.. 13};
g:=Graph&lt; X | { {u,v} : u,v in X |(u-v)^6 mod 13 eq 1}>;
Random(Vertices(g)) in X;</p>

<p>The output is:</p>

<p>Runtime error in 'in': No valid universe containing all elements</p>

<p>This is a bit unfortunate for my purposes, because I want to construct cliques and cocliques and then compare this to other structural relations in the original set.
How can I solve this?</p>

<p>Many thanks,
Kind regards</p>
",<graph-theory>
"<p>Let $G_n$ denote the $2^n$ vertices graph in which every vertex is labeled with a string of $n$ bits. A pair of vertices are adjacent if and only if their bit strings differ in exactly 3 digits.</p>

<blockquote>
  <p>i)Show $G_4\simeq Q_4$ where $Q_n$ denotes the <a href=""http://mathworld.wolfram.com/HypercubeGraph.html"" rel=""nofollow"">hypercube graph</a>.</p>
  
  <p>ii) Decide if true or false. Prove or give a counterexample: $G_n$ is bipartite for all $n\geq 4$.</p>
</blockquote>

<p><strong>Attempt:</strong></p>

<p>For $i)$ what I did was to draw both graphs and tried to find an isomorphism, although I obviously couldn't get it right (pretty hard with 16 vertices), is there a simpler way?</p>

<p>For $ii)$ I drew $G_4$ and saw it was bipartite, so then I tried to use induction on the number of vertices, but I couldn't get it right either.</p>
",<graph-theory>
"<p>Suppose that we have a decision tree of height $r + 1$ that describes how to increment an $n$-bit integer in the range $[0, 2^n -1]$. That is, the internal nodes are labelled with a bit position that you read, then depending on its value, you either go left or right and proceed to read the next bit indicated. The leaf nodes are labelled with a set of bits that are flipped to perform the increment operation. Note that the sequence of integers may not be the traditional Standard Binary Code, but could be any cyclic sequence of the $2^n$ integers. A Gray Code is another example of a sequence that may be produced (which corresponds to a Hamiltonian cycle in the hypercube graph).</p>

<p>Now, consider the root to ""right before"" leaf paths as sets of size $r$ from a universe of size $n$. I am trying to argue the following claim: Suppose that the root node of our decision tree is labelled by index $i$. If we take the paths on the left side of the tree (when $i$ is read as  $0$) such that bit $i$ is not flipped, together with the paths on the right side of the tree (when $i$ is read as $1$) such that bit $i$ is flipped, then I am interested in bounding the size of the common intersection of those sets. In particular, we know that every set will contain index $i$ since it sits at our root node, but I would like to show that these sets must also have another bit in common.</p>

<p>I know that I do not want to use the sunflower lemma, as these sets may not have the same pairwise intersection, but I believe that I may be interested in something similar that characterizes the global intersection of all of these sets). Perhaps it may help to view the problem as a hypergraph, though I am not sure of results that may be of use. The other thing to note is that these are not arbitrary sets, as they are derived from a decision tree for increment integers, so there is a lot of structure involved.</p>

<p>Any help would be greatly appreciated, whether it's some guidance on how to prove this, or perhaps references to potentially useful theorems.</p>

<p>Edit: If it helps, we can restrict our attention to when $r = n - 1$, though I am ultimately interested in the general case as well.</p>
",<graph-theory>
"<p>In this particular question I'm asked to find all the isomorphism classes of simple graphs, without loops whose degree sequence is: $3,3,2,2,2$, and to prove the ones I found are all the ones that exist.</p>

<p>I don't know how to do this, and I want to learn how to solve these kind of questions. Also, if I'm given the amount of vertices and edges, is there a closed form for the amount of isomorphism classes?</p>
",<graph-theory>
"<p><strong>Question:</strong></p>

<blockquote>
  <p>Prove the statement:</p>
  
  <p>If $G$ is a  connected graph with no cycles, then it has at least two vertices with degree 1.</p>
</blockquote>

<p>This seems pretty obvious, as if the graph has no cycles then it must have 2 ""endpoints"" which must have degree $1$, but obviously enough this is nothing quite like a proof.</p>

<p>E: The question didn't actually said the graph had no loops or that it was simple, I accidentally added that in.</p>
",<graph-theory>
"<ol>
<li><p>Let $G=(V,E)$ be a graph with $|V|=n$. Prove that $G$ is connected if $d(v)\geq \frac{n-1}{2}$ for all $v\in V$.</p></li>
<li><p>Let $G=(V,E)$ be a graph with $|V|=n$ and $|E|=m$. Prove that $\min\limits_{u\in V} d(u) \leq 2\dfrac{m}{n}\leq \max\limits_{v\in V}d(v)$.</p></li>
</ol>

<hr>

<p>My attempts:</p>

<ol>
<li><p>$|E|\leq n(n-1)$ since every node can have at most $n-1$ edges, and there are $n$        nodes in total. Hence making use of the formula $\sum\limits_{v\in V}d(v)=2|E|$, we obtain $\sum\limits_{v\in V}d(v)\leq 2n(n-1)$. I don't know how to proceed from here.</p></li>
<li><p>All I can come up with is that $\max\limits_{v\in V}d(v)=n-1$. I don't think this helps me though.</p></li>
</ol>

<hr>

<p>Could anyone please provide any additional hints to these problems? Thank you in advance</p>
",<graph-theory>
"<p>According to wikipedia, the Traveling Salesman Problem (TSP) is:</p>

<blockquote>
  <p>Given a list of cities and the distances between each pair of cities,
  what is the shortest possible route that visits each city exactly once
  and returns to the origin city?</p>
</blockquote>

<p>Okay, that's a cool problem, but the part about ""visiting each city exactly once"" makes little sense to me. If I were a traveling salesman, I would just want to minimize the length (time, cost, whatever) of my route, and if visiting the same city $17$ times achieves this (say, because that city has an especially ""central"" position in the graph) then so be it. There seems to be little sense in restricting attention to Hamiltonian cycles (i.e. cycles in which each vertex occurs precisely once); in particular, I would imagine that this restriction simultaneously makes the problem harder (computationally) and also less applicable (e.g. to problems ""from the real world."")</p>

<p>Wikipedia goes on to say that:</p>

<blockquote>
  <p>The problem was first formulated in 1930 and is one of the most
  intensively studied problems in optimization.</p>
</blockquote>

<p>In light of my previous comments, I find this surprising.</p>

<blockquote>
  <p><strong>Question.</strong> Why has the TSP been so intensively studied, while the variant (which I find more natural) has apparently received much less
  attention?</p>
  
  <p>In simple terms: why visit each city only once?</p>
</blockquote>

<p>Let me just add that according to wikipedia, the general problem does not include the assumption that the triangle inequality holds; that special case is called the <a href=""http://en.wikipedia.org/wiki/Travelling_salesman_problem#Metric_TSP"" rel=""nofollow"">metric TSP</a>. In this case, the restriction to Hamiltonian cycles is of course innocuous. </p>
",<graph-theory>
"<p>I am having quite a hard-time with this question, been thinking about it for a few hours and have not got a clue on how to even start proving this, because it is trivial but proving it has been hard for me.</p>

<blockquote>
  <p>Given two forests $F_1 = (V,A)$ and $F_2 = (V,B)$ with same vertices group $V$. It is also given that $|B| &gt; |A|$. Prove: there exists an edge $e \in B \backslash A$ where $F_1 \cup \{e\}$ is still a forest.</p>
</blockquote>

<p>Any help will be appreciated!</p>
",<graph-theory>
"<p>In real world networks, we have no further information about the structure of the networks. For example, in the Facebook network, we assume each one has some known particular probabilities to influence his friends. But how can we know the probability that I influence some guy who is randomly selected in the network?</p>

<p>If the network is like a chain, or like a complete graph and all the probabilities are equal, it seems easy. However, in a real world network, it is usually not the case.</p>

<p>I mean, how can classical random graph theory be applied to real world networks? Maybe I have fallen into a trap. I am not sure if such question is legal to ask here. But I am very interested about how to deal with the gap between theoretical things with real world things.</p>
",<graph-theory>
"<p>This might come across as a slightly petty question. Apologies for this, I am only asking as I have an exam on Graph Theory soon and want to make sure I do things correctly.</p>

<p>The definition of a Hamilton path is a path which includes every vertex of the graph exactly once.</p>

<p>The definition of a circuit is a path whose first and last vertices are the same.</p>

<p>A Hamilton circuit is a Hamilton path which is a circuit.</p>

<p>Question - isn't this a contradiction (it is supposed to include every vertex exactly once but then include the initial vertex twice)? What exception on these rules is brought in so it isn't contradictory?</p>

<p>Furthermore, if I wanted to quote a Hamilton circuit in the undirected and unweighted cyclic graph G, with vertices a,b,c where edges are { (a,b) , (b,c) , (c,a) } - would I quote the Hamilton circuit as abca or abc ?</p>

<p>Thanks for any help on this.</p>
",<graph-theory>
"<p>How to partition $n$ weighted elements into $m$ disjoint subsets such that the sum of weight of all elements in a subset is less than equals to the capacity of $j$th subset ($c_j$) . It is given that $m&lt;n$</p>

<p>Example:
$x_1,x_2,x_3$ are weights of 3 elements (here $n=3$). Divide these 3 elements into 2 subsets (here $m=2$) such that the $sum(X_1)&lt;= c_1$ and $sum(X_2)&lt;= c_2$. Here $sum(X_1)$ and $sum(X_2)$ are the summation of weights of all elements in subset 1 and 2 respectively.</p>

<p>The answer of the given problem is ($x_1,x_3$),($x_2$) if $x_1+x_3&lt;=c_1$ and $x_2&lt;=c_2$             </p>
",<graph-theory>
"<p>Find an example of a regular triangle-free $4$-chromatic graph</p>

<p>I know that for every $k \geq 3$ there exists a triangle-free $k$-chromatic graph.</p>

<p>So if I can find a triangle-free graph $H$ such that $\chi(H)=3$, then I can use the Mycielski construction to obtain a triangle-free graph $G$ such that $\chi(G)=4$. However, the regular part keep getting me stuck. I try some odd cycle, I also tried the Petersen graph but still can't get a regular triangle-free $4$-chromatic graph.</p>

<p>I wonder if anyone can give me a hint, please.</p>
",<graph-theory>
"<p>Let $k$ be the maximum length of a path in a connected graph $G$. If $P, Q$ are paths of
length $k$ in $G$, prove that $P$ and $Q$ have a common vertex. </p>

<p>My solution:</p>

<p>Suppose that $P$ and $Q$ are vertex disjoint. Let $P = (u_1, u_2,...,u_k)$ and $Q = (v_1, v_2,...,v_k)$. Let $S = (u_i = w_1, w_2,...,w_l = v_j)$ be the shortest path form a vertex in $P$ to a vertex in $Q$. This path exists because $G$ is connected. $w_2, w_3,...,w_{l-1}$ are not vertices in $P$ and $Q$ since $S$ is the shortest path. Assume that $i, j &gt;= k/2$. The path $u_1, u_2,..., u_i, w_2,..., w_{l-1}, u_j, u_{j-1},..., u_1$ has length at least $i + j + 1 &gt;= k + 1$. And therefore a contradiction. </p>

<p>Is this correct?</p>
",<graph-theory>
"<p>I'm currently trying to solve this problem:</p>

<p>""Show that the number of isomorphism classes of tree on n vertices is at least $\frac{n^{n-2}}{n!}$.""</p>

<p>I'm pretty stumped to be honest.  I know of Cayley's formula; there are $n^{n-2}$ trees on n labelled vertices, so I'm guessing that this may come in handy.  </p>

<p>One idea I had was to find the most number of isomorphism classes.  This, I believe, should be n-2, for $n \geq 2$, and 1, for $n=0, 1$.  But then I don't particularly think this would be of any use or would lead to a very forced way to prove it.</p>

<p>Any hints / ideas? 
Thanks!</p>
",<graph-theory>
"<p>I'm having a bit of a problem with producing a venn diagram of this relationship. I have three circles:  $U$, $V$, $W$. The identity I have to create is:</p>

<p>$$( U \setminus V ) \setminus W = U \setminus ( V \cup W)$$</p>

<p>I thought the shaded region would be the parts that are only in $U$, but that really seems like a wrong answer. Can someone help me out? Cheers</p>
",<graph-theory>
"<p>A function $f$ is defined on the set $\{0,1,2,3,…,n-1\}$ to itself. This is a function such that if you take any $k$ from the set $\{0,1,2,3,…,n-1\}$ then $f^m (k)=0$ for some natural number $m$. </p>

<blockquote>
  <p>Question is how many such $f$ exist? </p>
</blockquote>

<p>My strong conviction about the answer is $n^{n-1}$. </p>

<p>If it is, how can we prove this. I need the proof.</p>
",<graph-theory>
"<p>Determine if there exists a graph whose degree sequence is the one specified. Draw a graph, or explain why no graph exists. The sequence is 5,4,3,2,1,1</p>
",<graph-theory>
"<p>I realize this is a quite a general request. I'm just looking for examples of path searching algorithms for directed graphs which are capable of utilizing simple modifications (adding vertices, adding edges, deleting edges; all from a set of specific allowed modifications). I've been looking everywhere online for information on dynamic algorithms for graphs, but so far almost everything I've found is almost exclusively concerning run time of such algorithms. Run time is irrelevant to me as there are very few modifications that are allowed to be made in the problem I'm working on. Does anyone know of any algorithms like this? </p>
",<graph-theory>
"<p>I'm reading Bondy and Murthy's <em>Graph Theory</em>, and I'm doing the proposed exercise in the title. I've tried to do the following:</p>

<blockquote>
  <p>$m$: Edges</p>
  
  <p>$n$: Vertices</p>
</blockquote>

<p>A simple graph with $n$ vertices has a maximum of $m=(n-1)+(n-2)+\dots+(n-n)$ and hence </p>

<p>$$m=(n-1)n-\frac{(n-1)(n)}{2}=\frac{n^2-n}{2}$$</p>

<p>Which is $(n-1)$ times the number of $n$'s and the sum of the first $(n-1)$ natural numbers. Knowing that the maximum number of edges in a simple graph is ${n \choose 2}$, we can write:</p>

<p>$$\begin{eqnarray*}
  {m}&amp;\leq&amp;{{n \choose 2}}\\
  {}&amp;&amp;{}\\
  {\frac{n^2-n}{2}}&amp;\leq&amp;{\frac{n!}{2(n-2)}}\\
  {}&amp;&amp;{}\\
  {n^2-n}&amp;\leq&amp;{\frac{n!}{(n-2)!}}\\
  {}&amp;&amp;{}\\
  {n^2-n}&amp;\leq&amp;{n \cdot (n-1)}\\
  {}&amp;&amp;{}\\
  {n^2-n}&amp;\leq&amp;{n^2-n}\\
  {}&amp;&amp;{}\\
  {n^2-n}&amp;\leq&amp;{n^2-n}
\end{eqnarray*}$$</p>
",<graph-theory>
"<p>Here's the proof: <a href=""http://www.cs.cornell.edu/courses/CS6820/2014fa/matchingNotes.pdf"" rel=""nofollow"">http://www.cs.cornell.edu/courses/CS6820/2014fa/matchingNotes.pdf</a></p>

<p>What I don't understand is why ""For an edge f there can be f(e) for up to at most two edges e, conflicting with edge f at the two different ends.""</p>
",<graph-theory>
"<p>This question seems similar to <a href=""http://math.stackexchange.com/questions/354551/is-a-loop-actually-a-circuit"">Is a loop actually a circuit?</a> but it is different. The linked question refers to a undirectional graph so the vertex of the self-loop ends up with degree 2. Also the linked question talks about circuits which might not be same as a cycle.</p>
",<graph-theory>
"<p>I saw in my graph theory notes this statement ""Up to isomorphism, there is one and only one $K_4$"". What does the phrase ""up to isomorphism"" mean? </p>
",<graph-theory>
"<p>[HOMEWORK]</p>

<p>I asked my professor and he said that a counter example would be two nodes, by which the pathw ould go from one node and back. this would be a closed path but does not contain a cycle. But I am confused after looking at this again. Why is this not a cycle? Need there be another node?</p>
",<graph-theory>
"<p>It is stated that path exists between two vertices. </p>

<p>My idea is that if there is a direct path from source to vertex then the length is 1 which is less than n. But we need the minimum path to be as maximum as possible to disprove the statement.
So we include as many vertices as possible in the path. The maximum can be n. If all n are included in the path, then a simple path would have n-1 length. We can't go any longer than that. So, it is proved. 
Can you help me present this formally? And also please correct any mistakes I may have in my idea.</p>

<p>n is the number of vertices.</p>
",<graph-theory>
"<p>I have noticed a recurring theme in Graph Theory / Theoretical Computer Science (abbreviated GT and TCS throughout this post) in that notions typically belonging to differential calculus / geometry / topology are ""discretized"" (by which I mean ""reformulated for a discrete-mathematical setting"") to be used for the service of GT/TCS. Examples include:</p>

<ul>
<li>Discrete Morse Theory, which has been useful in the analysis of Boolean functions,</li>
<li>The Laplacian matrix, which carries a lot of worthwhile algebro-graphtheoretical properties,</li>
<li>The Cheeger constant for graphs, which is useful in the theory of expander graphs, as well as in the theory of distributed computing.</li>
</ul>

<p>Of course these are based on, respectively, the usual Morse Theory, the Laplace operator, and the Cheeger constant from Riemannian geometry.</p>

<p>My (soft) question is simple: Are there any other cases in GT, TCS or discrete math in general where this type of discretization has occurred? And why are those notions useful discretized? </p>

<p>For example, the Cheeger constant measures the ""bottle-neckedness"" of a graph, which when interpreted as describing a network used for distributed computing gives us limits on how fast we can compute things in a distributed network. The Laplacian is used heavily in algebraic GT / combinatorics and carries analogous structure to the ordinary Laplace operator, and Discrete Morse theory proves powerful theorems based on inequalities and critical points…(ok, I haven't delved too deep into any form of Morse theory yet…).</p>

<p>(Sure, this is a list-based question, but I figure there can't be that many examples to list?) </p>

<p><strong>Additional Examples, suggested by comments:</strong></p>

<ul>
<li>Discrete Fourier Transform / Analysis has many applications in TCS and combinatorics too, and are obviously based on ordinary Fourier analysis.</li>
</ul>
",<graph-theory>
"<p>Let $G$ be a <strong>connected</strong> graph. Prove that two longest paths in $G$ have atleast one node in common. Note that two longest paths do not neccessarily have the same length.</p>

<p>I began by defining two paths namely $P_1 = &lt;v_1,...,v_k&gt;$ and $P_2 =&lt;u_1,...,u_m&gt;$ where $m\neq k$ because of the note given. Now I state the definition of a conneted graph. </p>

<blockquote>
  <p>A graph G is connected if for each pair of vertices u; v, there is a u; v-walk
  in G.</p>
</blockquote>

<p>now my first intuition is to consider the fact that if no node was shared than connecting the two paths would give a (slightly ?) longer path which would make them not the largest paths. Given the fact that they are the two largest (say $n$ and $n-1$). They should be connected via atleast one node.</p>

<p>HOWEVER it seems odd for me to now state that this is a proof.</p>
",<graph-theory>
"<p>I am going through <a href=""http://www.sciencedirect.com/science/article/pii/0012365X81902533"" rel=""nofollow"">Degree sequences of random graphs</a> by Béla Bollobás.</p>

<p>On page $3$ the author introduces the quantity $S(n: K, L)$ without any explanation. </p>

<p><a href=""http://i.stack.imgur.com/P0HAj.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/P0HAj.png"" alt=""enter image description here""></a></p>

<p>Could anyone please help me in understanding the significance of the quantity?</p>
",<graph-theory>
"<p>From wikipedia's page <a href=""http://en.wikipedia.org/wiki/Cage_graph"" rel=""nofollow"">Cage graph</a></p>

<blockquote>
  <p>Formally, an $(r,g)$-graph is defined to be a graph in which each
  vertex has exactly $r$ neighbors, and in which the shortest cycle has
  length exactly $g$. <strong>It is known that an $(r,g)$-graph exists for any
  combination of $r \geq 2$ and $g \geq 3$</strong>.</p>
</blockquote>

<p>Can someone provide a proof of this claim?</p>

<p>Thanks.</p>
",<graph-theory>
"<p>Suppose we have a graph on $n$ nodes. We would like to assign to each node either a $+1$ or a $-1$. Call this a configuration $\sigma \in \{+1,-1\}^n$. The number of $+1$s that we have to assign is exactly $s$ (hence the number of $-1$s is $n-s$.) Given a configuration $\sigma$, we look at each node $i$ and sum the values assigned to its neighbors, call this $\xi_i(\sigma)$. We then count the number of nodes for which $\xi_i(\sigma)$ is nonnegative:</p>

<p>$$ N(\sigma) := \sum_{i=1}^n 1\{ \xi_i(\sigma) \ge 0\}. $$</p>

<p>The question is: what is the configuration $\sigma$ that maximizes $N(\sigma)$? Can we give a bound on $(\max N)/n$ in terms of $s/n$. If it helps, the graph can be assumed to be Erdos-Renyi.  </p>
",<graph-theory>
"<p><strong>Why edg-chromatic number of Cartesian product of two graph equal maximum degree of Cartesian product G and H?</strong>
for example G and k2</p>
",<graph-theory>
"<p>Let $G=(V,E)$ be a graph with $|V|=n$ and $|E|=m$ prove that
$$
\min_{u\in V} \{d(u)\}\leq 2\frac{m}{n}\leq \max_{v\in V} \{ d(v)\} 
$$
now my first intuition is to assume that $\min\limits_{u\in V} \{d(u)\} =0$ holds because it is not a connected graph. And my second assumption is to use $\max\limits_{v\in V} \{ d(v)\} =n -1$ (connected to everything, except it self) , and from another exercise we got</p>

<blockquote>
  <p>$$
d(v)\geq \frac{n-1}{2}
$$</p>
</blockquote>

<p>which could be interpreted as
$$
n-1\leq 2d(v)
$$
which would help in getting the right side, the only thing baffling me at the moment is the $\frac{m}{n}$ part, any intuition on this?</p>
",<graph-theory>
"<p>I'll appreciate any help in the following question:</p>

<blockquote>
  <p>Let $G=(V,E)$ be a connected undirected finite graph.
  Let $\def\MaxD{\mathrm{MaxD}}\MaxD(v)$ ($v$ is a vertex) be the <em>maximal distance</em> of $v$ from any vertex in $G$.</p>
  
  <p>Prove for every two vertices $v_1$,$v_2 \in V$:
  $\MaxD(v_1)/\MaxD(v_2) \le 2$.</p>
</blockquote>
",<graph-theory>
"<p>Evan Chen's recount of the Taiwanese IMO team's journey recorded a game the team members played at their free time, which runs as the following:</p>

<p>There are $n$ team members (in the actual case $n=6$ but here we simply take $n\geq2$)Every team member points at another team member (whom must be different from him/herself) and thus we obtain a (directed) graph with $n$ vertices and $n$ edges. It has one more edge than a tree and therefore must contain a cycle. Any member who is a vertex of any cycle in this graph loses the game. (So it is possible that everyone loses but it's impossible that no one loses.)</p>

<p>Assume everyone chooses the person s/he points at randomly, what is the probability of a player losing the game? </p>

<p>Reference: <a href=""http://www.mit.edu/~evanchen/handouts/IMO-2014/IMO-2014.pdf"" rel=""nofollow"">Evan Chen's recount</a></p>
",<graph-theory>
"<p>I'm trying to show that all graphs with 5 vertices, each of degree 2, are isomorphic to each other.  Is there a more clever way than simply listing them all out?</p>
",<graph-theory>
"<p>Given is a graph defines a flow network. 
I need to formal proof the following :</p>

<blockquote>
  <p>If we multiply all edge capacities by a positive number $X$, the minimum cut remains unchanged</p>
</blockquote>
",<graph-theory>
"<p>I believe this is a standard graph theory problem, but I am not sure. I am having a lot of trouble with it though. Give it a go</p>

<p>You have n jelly beans. You want to ship them all to a friend. For 1 ≤ i ≤ m, you can buy any number of boxes, where each box can hold Bi jelly beans. The smallest box fits one bean (b1 = 1). Every box you use must be fully packed. Each box costs a dollar. The goal is to ship your beans with the smallest cost. What is the time and space complexity of finding the optimal solution?</p>

<p>Thanks a lot!</p>
",<graph-theory>
"<p>I am considering a route planning problem, which I try to model with a graph.
I understand that 
1. to find a shortest path in a graph, we need to know the weights on the edges. 
2. as some places are more desirable to visit than others, we can also have some kind of 'weight' on the nodes. In the case that we want to find a path that maximize the sum of the weights of the nodes passed through, we can convert the original graph to a new one and solve the problem as a shortest path problem in that graph.</p>

<p>I want a way to incorporate both the distance b/w bus stops and the desirability of the bus stops in my consideration of choosing the path. </p>

<ol>
<li>Is there way a sensible way to still model this situation by using a graph? </li>
<li>How can we define the vertices and the edges of this new graph?</li>
<li>What would be the weight function on the edges?</li>
</ol>

<p>edit: Not sure why this gets downvoted. Is the question meaningless or a well-known exercise?</p>
",<graph-theory>
"<p>A minimally imperfect graph is a graph that is not perfect but all of it's proper induced subgraphs are. I can prove that, when G is a minimally imperfect graph with n vertices: </p>

<ol>
<li><p>alpha(G)omega(G)+1=n (so I have proved the Perfect Graph Theorem) </p></li>
<li><p>G has two sequences S_0,...S_n of maximum stable sets and C_0,...C_n of maximum cliques such that S_i intersects C_j unless i=j. So they come in nonintersecting pair. </p></li>
</ol>

<p><strong><em>Apparently</em></strong>, these are all the maximum stable sets, and maximum cliques. So there are exactly n of each. But how do I prove this? </p>
",<graph-theory>
"<p>I would like to learn Graph Theory from the beginning. It seems to me that one does not need to be familiar with many abstract type subjects to be able to understand the more basic concepts of graphs.</p>

<ol>
<li><p>Which subjects should one know prior to learn Graph Theory at the introductory level?</p></li>
<li><p>And which book or lecture notes would you advise to learn it? </p></li>
</ol>
",<graph-theory>
"<p>Given an $n$ by $n$ grid of which some of the squares are black and some are white. I'm allowed to mark some of these squares and the question is to prove whether a given grid with given black squares can meet these conditions:</p>

<p>1) Each column has only one marked square.</p>

<p>2) Each row has only one marked square.</p>

<p>3) Only white squares can be marked.</p>

<p>This is similar to how sudoku can only have the same number on only one column and one row. In fact it's an easier problem. However...</p>

<p>I am struggling to figure out an algorithm that reduces this problem to a max flow network problem.</p>

<p>I'm thinking something along the lines of making each square in the grid a  node in a graph. Then connect your source point to all the white nodes.</p>

<p>I also believe that in the end the way you prove whether such conditions can be met is by whether or not the max flow through this graph is exactly $n$. Because a solution to the above problem requires that there are exactly $n$ marked squares. Any less means that there was a row or column that doesn't have any white square or that there is no way of adding points that end up in distinct rows/columns.</p>
",<graph-theory>
"<blockquote>
  <p>Must the number of people at the party who do not know an odd number of people be even? Describe a graph model and then answers the question. </p>
</blockquote>

<p>I'm confused because I do not understand the question. For example if we take people to be a vertex, then if there is an edge between them it means that people know each other, 
and the formula is $$\sum d(v) = 2*e$$ $\Rightarrow$ the sum of degrees of all vertices should be even so it means that there should be an even number of vertices. But why in the question is ""people who do not know an odd number of other people be even""..?</p>
",<graph-theory>
"<p>I'm traying to prove (or disprove) the following statement:</p>

<p>Any connected $r$-regular graph of <a href=""http://en.wikipedia.org/wiki/Girth_%28graph_theory%29"" rel=""nofollow"">girth</a> $g$ such that every edge is shared by the same number of minimum length cycles (that is, cycles of length $g$), is <a href=""http://en.wikipedia.org/wiki/Vertex-transitive_graph"" rel=""nofollow"">vertex-transitive</a> and <a href=""http://en.wikipedia.org/wiki/Edge-transitive_graph"" rel=""nofollow"">edge-transitive</a>.</p>

<p>This is not a textbook exercise. Any ideas appreciated.</p>

<p>Thanks.</p>
",<graph-theory>
"<p>First of all - what's a directed graph? Is this just some normal graph but with no loops or edges? I don't really understand definitions found in the web...</p>

<p>What is a path? I mean, as far as I know, it is a directed chain, where the chain is a ""path between two vertices, connected anyhow"". Ok, I understand what the chain is, but what is the DIRECTED chain?</p>
",<graph-theory>
"<p>Let $A$ be a $k$ by $n$ matrix where the rows are permutations of $\{1,2,3,..,n\}$ and in each column all elements are different (i.e columns are subsets of permutations of $\{1,2,3,..,n\}$). 
Show that there exists an $n$ by $n$ matrix $B$ such that each row and column is a permutation of $\{1,2,3,..,n\}$ and its first $k$ rows are the rows of $A$.</p>

<p>I can see that this should be an application of Hall's theorem but I'm having trouble choosing the graph.</p>

<p>Thanks.</p>
",<graph-theory>
"<p>Let's say you have a graph such that every vertex has exactly 3 edges. You try to number every edge of the graph with either a 0, 1, or 2 so that every vertex has exactly one of each type of edge. Is this always possible to do? </p>
",<graph-theory>
"<p>Consider a simple graph $G$ with $n$ vertices. For any two vertices, either they are connected by an edge, or there is a third vertex which is connected to both of them by an edge. (It is possible that both conditions hold.) Show that there exists a set $W$ of no more than $\sqrt{n\log n}+1$ vertices such that every vertex in $G$ is connected to some vertex in $W$.</p>

<p>If some vertex $u$ has degree $\leq \sqrt{n\log n}$, then the set of neighbors of $U$ can be seen to satisfy the condition. What if every vertex $u$ has degree $&gt;\sqrt{n\log n}$?</p>
",<graph-theory>
"<p>For a given order $n$, the number of graphs that are determined uniquely by their  <a href=""http://en.wikipedia.org/wiki/Chromatic_polynomial"" rel=""nofollow"">chromatic polynomial</a> is <a href=""http://oeis.org/A137568"" rel=""nofollow"">A137568</a>. This sequence starting with n=1 is:</p>

<pre><code>1, 2, 4, 7, 16, 41, 139, 704, 7270, 183606
</code></pre>

<p>I am looking for something slightly different, the sequence formed by the number of <strong>distinct chromatic polynomials</strong> for <strong>connected</strong> graphs.</p>
",<graph-theory>
"<p>Given following graph 
<img src=""http://i.stack.imgur.com/sGmX3.png"" alt=""enter image description here""></p>

<p>a) Find the upper bound for $\chi(G)$ using theorem 8.20</p>

<p>b)What is $\chi(G)$?</p>

<blockquote>
  <p><strong>Theorem 8.20</strong>: For every graph $G$, $\chi(G) \leq 1+\max\{\delta(H)\}$, where maximum is taken over all subgraph $H$ of $G$</p>
</blockquote>

<p>I know that $\chi(G)=5$ so part b) is done. For part a), I'm not quite sure about $\max\{\delta(H)\}$ part, so I need to find the minimum degree in all subgraph $H$ and pick the biggest one? Is $\max\{\delta(H)\}=5$? Do I need to prove it?</p>
",<graph-theory>
"<p>How many different triangles are there in $K_5$?</p>

<p><a href=""http://i.stack.imgur.com/sOgqv.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/sOgqv.png"" alt=""enter image description here""></a></p>

<p>The Answer is 35.(The Moscow Mathematics Puzzle) </p>

<p>Then I asked what about $K_6$, $K_7$ and so on ...?</p>

<p>With my intuition I arrived at this conjecture   </p>

<blockquote>
  <p>Conjecture: The number of different triangles in a complete graph of order $n$ ($K_n$) is </p>
  
  <p>\begin{cases}
\frac{n^2(n-2)^2(n-1)^2)}{6^2},&amp;\mbox{ where } \binom{n}{3} \mbox{ is odd.}\\
\frac{(n^2-n)(n^2-4)(n^2-5n+12)}{12^2} ,&amp;\mbox{where }\binom{n}{3} \mbox{is even.}
\end{cases}
  What do you say about this?</p>
</blockquote>
",<graph-theory>
"<p>How to build a 4-regular, vertex-transitive, 'least diameter' graph with $v$ vertices?</p>

<p>This implies to know what is the minimum diameter of a 4-regular vertex-transitive graph with $v$ vertices.</p>

<hr>

<h1>What I found</h1>

<p>If $v &lt;= 4 + 1$, the diameter is $1$, and in the particular case $v = 4 + 1$, the only matching graph is $K_5$.</p>

<p>In the case $v = 6$, the graph shown below matches, with a diameter of $2$. The diameter can't be less than 2, since it would then be 1, meaning every vertex is adjacent to the five others, what contradicts with the 4-regularity.</p>

<p><a href=""http://i.stack.imgur.com/Ao9FL.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/Ao9FL.png"" alt=""4-regular graph with 6 vertices""></a></p>

<p>The cases $v=6,7,8$ and $9$ are diameter 2 as well and can be treated with the same pattern: putting a star in a polygon. Example of star used :</p>

<ul>
<li>6: Two triangles (A double edge between opposed vertices works too)</li>
<li>7: Star formed by jumping over one vertex. (Over two vertices works too)</li>
<li>8: Two squares (The star formed by jumping over two vertices works too)</li>
<li>9: Three triangles (Seems to be the only possibility)</li>
</ul>

<p>For the case $v=10$, the least diameter is 2 too. A matching graph can be obtained  from Petersen graph, by adding parallel edges to the ones linking the pentagon to the central star.</p>
",<graph-theory>
"<p>In a previous <a href=""http://math.stackexchange.com/questions/1888216/how-can-i-prove-that-left-sum-i-0r-1i-binomai-binomn-ar-i-ri"">question</a> I asked about the maximum module reached by the quantity  $f_{n,r} (a) = \sum_{i=0}^r (-1)^i \binom{a}{i} \binom{n-a}{r-i}$. Now I ask when this maximum value can be reached. </p>

<p>This is a conjecture:</p>

<blockquote>
  <p>How can I prove that the equality</p>
  
  <p>\begin{equation}
\left|\sum_{i=0}^r (-1)^i \binom{a}{i} \binom{n-a}{r-i}\right| = \binom{n}{r}
\end{equation}</p>
  
  <p>where $0\leq a \leq n$, $0\leq r \leq n$ and $n,r,a \in \mathbb{N}$, $\mathbf{r \neq n, r\neq0}$, holds only for $a=0$ and for $a=n$?</p>
</blockquote>

<p>What I'm trying to prove is that for $r$ even I cannot have, for any $0\leq a \leq n$ with $r \neq n, r\neq0$:</p>

<p>\begin{equation}
\sum_{i=0}^r (-1)^i \binom{a}{i} \binom{n-a}{r-i} = - \binom{n}{r}
\end{equation}</p>
",<graph-theory>
"<p>In spectral graph partition theory, the eigenvector corresponding to the second smallest eigenvalue of the laplacian matrix of a graph, in general, is used to partition the graph. </p>

<p>What is the underlying philosophy behind this? Any reference to any related proofs on this?</p>
",<graph-theory>
"<p>Let $G$ be a $5$-regular triangulation of the plane. By Euler's formula, $G$ must have $12$ vertices. My question is, how can we conclude that $G$ is isomorphic to the icosahedral graph?</p>
",<graph-theory>
"<p>For $p$ and $q$ distinct primes.
Two conditions are given:</p>

<p>vertices of graph $G$ are integers in the set $\{0, 1, 2, \ldots, pq-2, pq-1\}$; </p>

<p>there is an edge of graph $G$ between $a$ and $b$ if $ab$ either $p∣a−b$ or $q∣a−b$. In this case, how many edges of $G$ are there?</p>
",<graph-theory>
"<p>Are graphs with eigenvalue $1$ of multiplicity more than $1$, important one? Please guide me to any book or article discussing such graphs.</p>
",<graph-theory>
"<p>Given any undirected connected graph. If we redefine the weight of a spanning tree to the maximum weight of an edge (if the largest weight is 10 the weight of the tree is 10) are there any cases where there minimum spanning tree will be different based on this assumption? By this I mean are there any cases where we can use a more costly route in order to have a lower maximum edge weight?</p>
",<graph-theory>
"<p>An automorphism is a mapping of a graphs nodes onto it's own nodes. Whereas an isomorphism is the mapping of a graphs nodes onto another graphs nodes. </p>

<p>Doesn't this mean the are fundamentally the same thing, it's just a matter of what the nodes are labelled (for example in one graph the nodes may numbered sequentially, but in the other graph they are labelled alphabetically)?</p>

<p>So will the number of automorphisms be the same as the number of isomorphisms?</p>
",<graph-theory>
"<p>Is there (sufficient and) nessary condition for which a graph $G$ has a homomorphism but no surjective homomorphism to $H$? Where the surjective means both vertex and edge are surjective.</p>

<p>Or say if $H$ is a fixed graph like $K_n$, is there sufficient and nessary condition?</p>
",<graph-theory>
"<p>Let $G$ be a <a href=""http://en.wikipedia.org/wiki/Directed_graph"" rel=""nofollow"">directed graph</a> which contains $N$ vertices, and which satisfies the following condition: each vertex of $G$ has at least one incoming edge.</p>

<p>(For clarity, a vertex $V_1$ has an ""incoming edge"" if there is an edge $E$ which connects $V_1$ to some other vertex on the graph, $V_2$, for which $E$ is directed from $V_2$ to $V_1$)</p>

<p>Then I believe it follows that $G$ contains a closed cycle.  The reason I am posting this is because the result seems straightforward/obvious, and yet it would apparently solve an <a href=""http://mathoverflow.net/questions/16857/existence-of-a-zero-sum-subset"">open problem on MO regarding zero sum subsets</a>, so I am concerned that I may have a mistake.  (To be frank, I am not comfortable posting on MO)</p>

<p>The basic idea is to show that starting with a set of $N$ vertices, and no edges.  That it is impossible to construct a directed graph with no closed cycles by adding one (directed) edge at a time.</p>

<p>Let a set of $N$ vertices be given, and let us attempt to construct a directed graph with no closed cycle by adding one directed edge at a time.  Choose some arbitrary vertex $V_1 \in G$, then by the defining condition of $G$, $V_1$ must have an incoming edge, say from $V_2$.  It's clear that $V_2$ has an incoming edge, if this edge originates from $V_1$, then the graph would have a closed cycle, so we choose some other vertex which connects to $V_2$: and in this way we are forced to create a chain of $N-1$ vertices, $V_1$, has an edge coming from $V_2$, which has an edge coming from $V_3, \ldots, V_{N-2}$, which has an edge coming from $V_{N-1}$.  It's clear that if any of the vertices in this chain has an incoming edge originating from another vertex on the chain, that $G$ will contain a closed cycle.  So once we construct a chain of $N-1$ distinct vertices, there is one vertex left, and since this vertex must have an incoming edge originating from another vertex which is already on the chain, $G$ must contain a closed cycle.</p>

<p>Now let me explain why I think this fairly trivial result solves the <a href=""http://mathoverflow.net/questions/16857/existence-of-a-zero-sum-subset"">open zero sum subset problem</a> on MO.</p>

<p>This question asks: given a finite set of real numbers $S$ with the property that every number in the set can be written as the sum of two numbers in the set (not necessarily different), if there exists a subset $S$ which sums to zero.</p>

<p>Well, clearly a set $S$ which satisfies the condition of the problem stated above corresponds to a directed graph for which each vertex has at least one incoming edge (each real number in the set $S$ is associated with a vertex, and if given three numbers $a,b,c \in S$, with $a = b + c$, then the vertex associated with $a$ will have incoming edges from the vertices associated with $b$ and $c$).   Therefore the graph corresponding to an arbitrary S which satisfies the condition contains a closed cycle.  By adding the numbers in the closed cycle, we obtain a sum of the form:</p>

<p>$a + b + c + \ldots = a$, hence it follows that $b + c + \ldots = 0$, i.&thinsp;e. a set $S$ which satisfies the condition must contain a zero sum subset.  Have I made a mistake somewhere, or does this solve the problem? </p>
",<graph-theory>
"<p>How can we describe a polynomial-time algorithm to find an edge $e$ in a graph $G$ such that $G/e$ is 3-connected. Given the fact that $G$ is 3-connected and $\lvert V(G) \rvert \geq 5$.</p>
",<graph-theory>
"<p>Why is there only the trivial automorphism on the Frucht graph?</p>

<p>We have a rooted tree in the Frucht graph which allows to totally order the vertices. But how does this imply that there is only the identity on the Frucht graph?</p>
",<graph-theory>
"<p>i just started going through biconnected components can someone explain me this</p>

<p>Show that if G is a connected undirected graph, then no edge of G can be in two different
biconnected components</p>
",<graph-theory>
"<p>I am looking at a graph theory problem that describes the partite sets of a bipartite as two copies of the $(m+1)$-dimensional vector space over the finite field $\mathbb{F}_{p^n}$ ($p$ is prime and $n\geq 1$ is an integer), $P$ and $L$. They call the elements of $P$ ""points"" and the elements of $L$ ""lines"". </p>

<p>They say a point $(p)=(p_1, \ldots, p_{m+1})\in P$ is adjacent to a line $[l]=[l_1, \ldots, l_{m+1}]$ if and only if</p>

<p>$$ l_{i+1}+p_{i+1}=l_ip_1  $$</p>

<p>for every $i\in \left\{1,2,\ldots, m \right\}$.</p>

<p>I would like to know what the sum on the left hand side and the product on the right side mean geometrically in terms of lines and points. Perhaps I should ignore the geometric meaning? I just want to know how I should be viewing the above definition.</p>

<p>Thank you!</p>
",<graph-theory>
"<p>Let $G$ be a connected graph and $v$ be a vertex in $G$. Suppose a DFS traversal from $u$ is performed resulting in a tree $T$, and a BFS from $u$ also results in the same tree $T$. I would like to show that $T = G$.</p>

<p>My main problem for this is that I am not sure how to initially approach this proof, and given my current approach is within reason I am stuck as for my next step. </p>

<p>My current thought is to go by contradiction and suppose that $T$ is both the DFS and BFS tree of $G$ with $u$ being the root, but $T \neq G$. As $T \subset G$, there must exist an edge $e \in G$ such that $e \notin T$ (since $G$ is connected it cannot be the case that there is a vertex in $G$ that is not in $T$). From here I believe my contradiction will come from the DFS and BFS traversals not being equivalent, but this is where I am stuck in this approach. </p>

<p>Could anyone hint at my next step or give me an idea for another approach to the proof?</p>
",<graph-theory>
"<p>According to the Law of mathematics, the product of slopes of $2$ perpendicular lines has to be $ -1 $.</p>

<p>Then, how do you prove that the following lines are perpendicular.</p>

<p>$x=4$ , $y=6 $</p>

<p><img src=""http://i.stack.imgur.com/7fUDz.gif"" alt=""Perpendicular  lines""></p>

<p>My Calculation :</p>

<p>Slope of line $1$ ($x=4$) = Infinity</p>

<p>Slope of line $2$ ($y=6$) = $0$</p>

<p>Product of both slopes != $0$</p>
",<graph-theory>
"<p>I have seen some relation about the simple graph $G$ which is not directed. </p>

<p>Suppose $G$ has $n=|V|\ge1$ vertices, $m$ edges, $k$ connected components, $p$ odd cycles, $q$ even cycles. Do the following hold?</p>

<ol>
<li><p>$p+q\ge 1$ then $m\ge n$ </p></li>
<li><p>$p+q=1, k=1,$ then $ m=n$</p></li>
<li><p>$k\ge n-m$</p></li>
<li><p>If $p = 0$, then $G$ is bipartite.</p></li>
<li><p>If $q = 0$, then there are $2k$ proper colorings of $G$ using $2$ colors.</p></li>
</ol>

<p>Attempts: For question i, I believe it is False as we may have many isolated vertices.
for question 2 i think it is true. $p+q=1$ mean there exist cycle and k=1 mean that it is a connected graph. I believe it is true but then i have no idea how to show m=n.</p>

<p>for Q4, i think that it is true as it may a graph with no cycle is bipartile and also a graph with even with even cycle only seems to be a bipartitle too.</p>

<p>For question 5 i think it is false as for the case that the graph has no cycle, q=0 but we cna actually colour with 2 colours.</p>

<p>Is my interpretation or guess true? and is there any hints about the proof of some details?</p>
",<graph-theory>
"<p>Show that if $G$ is a connected graph that is not regular, then $G$ contains adjacent vertices $u$ and $v$ such that deg $u$ does not equal deg $v$.</p>
",<graph-theory>
"<p>$K_4$ is an example of a graph that requires 4 colours to be coloured but it contains triangle cycles and a square cycle too.</p>

<p>I've tried drawing ever more complicated graphs made up of pentagons, hexagons, etc. but I've been able to colour all of them with 3 colours. </p>

<p>If such a graph exists without any triangle or square cycles does anyone have a hint as to how I could discover it? </p>
",<graph-theory>
"<p>I am a student from Iraq studying Graph to get in to a college in Georgia. I have trouble understanding this question.</p>

<p>Show that the two definitions below are logically equivalent.
Definition 1. A graph G = (V, E) is disconnected if there exist non-empty subgraphs H1 = (V1,E1) and H2 = (V2,E2) such that V1 and V2 partition V and E1 and E2 partition E. A graph is connected if it is not disconnected.
Definition 2. A graph G is connected if for any two vertices v, w there is a walk between v and w.</p>

<p>Can someone kindly explain this situation???</p>
",<graph-theory>
"<p>This <a href=""http://www.dis.uniroma1.it/~leon/tcs/lecture2.pdf"" rel=""nofollow"">tutorial</a> (page 22) on Hopcroft-Karp algorithm for maximum bipartite matching states the following:</p>

<blockquote>
  <p>Let $M^*$ be a maximum matching, and let $M$ be any matching in $G = (V, E)$. ...</p>
  
  <p>Let us consider the graph $G'=(V, M \oplus M^*)$. It contains at most $|M^*|-|M|$ augmenting paths with respect to $M$.</p>
</blockquote>

<p>Here $M\oplus M^*$ is the symmetric difference between $M$ and $M^*$. But how does the last line (<em>It contains at most $|M^*|-|M|$ augmenting paths with respect to $M$</em>) follow?</p>
",<graph-theory>
"<p>I'm trying to solve the following exercise from the book A Textbook of Graph Theory by R. Balakrishnan and K. Ranganathan</p>

<blockquote>
  <p>Show that for a simple bipartite graph, $m\leq \frac{n^2}{4}$</p>
</blockquote>

<p>$m$ is the size of the graph.</p>

<p>$n$ number of vertices.</p>

<p>I was considering the following cases:</p>

<p>Let $X$ and $Y$ be a partition of the set of vertices of $V(G)$ and I tried to verify some cases accordingly to the cardinality of the each set. If $|V(G)|=n$ and $|X|=|Y|=\frac{n}{2}$ it is easy to see, but I got stuck trying to show the case when the cardinalities are $|X|&lt;|Y|$</p>

<p>Any help will be greatly appreciated!</p>

<p>Thanks!</p>
",<graph-theory>
"<p>In a <a href=""http://demonstrations.wolfram.com/GracefulGraphs/"" rel=""nofollow"">graceful graph</a>, the vertices have number values that range from 0 to $n$ and $n$ edges with all values from 1 to $n$ that are differences between the vertex values. Here's a graceful but boring path graph with an Eulerian tour.</p>

<p><a href=""http://i.stack.imgur.com/MmgzQ.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/MmgzQ.png"" alt=""enter image description here""></a></p>

<p>A more interesting graph is the <a href=""http://mathworld.wolfram.com/PetersenGraph.html"" rel=""nofollow"">Petersen graph</a>, which can be labeled gracefully.</p>

<p><a href=""http://i.stack.imgur.com/FoLor.gif"" rel=""nofollow""><img src=""http://i.stack.imgur.com/FoLor.gif"" alt=""Graceful Petersen graph""></a></p>

<p>In this graph, 1-2-3 is a path for vertices 2-3-5-8. Three edges out of 15 means this is a 20% path.  An <a href=""https://en.wikipedia.org/wiki/Eulerian_path"" rel=""nofollow"">Eulerian path</a> would be 100%.  Is there a graceful Eulerian labelling for anything other than the path graph? For a partial <a href=""http://mathworld.wolfram.com/QueenGraph.html"" rel=""nofollow"">queen graph</a> on a 4x4 grid, here is a semi-graceful labeling with length 36. </p>

<p><a href=""http://i.stack.imgur.com/epQWc.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/epQWc.jpg"" alt=""increment maze""></a></p>

<p>Here is a similar semi-graceful graph. If vertices had values ranging from 0 and 29, then it would be a graceful graph. Let's call this an Eulerian semi-graceful graph.</p>

<p><a href=""http://i.stack.imgur.com/vbt4J.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/vbt4J.png"" alt=""Eulerian semi-graphful graph""></a></p>

<p>For semi-graceful Eulerian graphs, are any of them interesting graphs? The vertex sequence 9, 10, 8, 5, 9, 14, 8, 1, 9, 0, 10, 21, 9, 22, 36, 21, 5, 22, 40, 21, 1, 22, 0, 23, 47, 22, 48, 21, 49, 78, 48, 79, 47, 14, 48, 83, 47, 10, 48, 9, 49, 90, 48, 5, 49, 94, 48, 1, 49, 0 can be used to make a 20 vertex graph with path length 49, but it isn't all that interesting. Is it possible to find a <a href=""http://mathworld.wolfram.com/PolyhedralGraph.html"" rel=""nofollow"">polyhedral graph</a> with an Eulerian semi-graceful labeling?</p>

<p>For $n$ vertices, what is the longest possible path?</p>

<p>EDIT: Leen Droogendijk points out an Eulerian graceful graph. Are there others?</p>

<p><a href=""http://i.stack.imgur.com/5kuXp.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/5kuXp.png"" alt=""Leen Droogendijk graph""></a></p>

<p>EDIT 2:  Turns out there are many Eulerian graceful graphs. So far, all of them seem to have at least one vertex with valence 1 or 2.</p>

<p><a href=""http://i.stack.imgur.com/tbKV1.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/tbKV1.png"" alt=""Large Eulerian graceful graph""></a></p>
",<graph-theory>
"<p>So far I've come across a bunch of different terms for matrices used in graph theory - adjacency matrix, connectivity/connection matrix, vertex matrix, etc.</p>

<p>Are there any differences between these matrices or are all of these terms just referring to the same thing?</p>
",<graph-theory>
"<p>Was reading an introductory graph theory book, and it says that nets of solids can be represented using adjacency graphs, and new nets can be discovered by searching for all the spanning trees of the graph.</p>

<p>It also mentions that for some solids known as non-manifold, the spanning trees might not result in feasible nets. It does not elaborate.</p>

<p>I have tried searching this on the internet but it seems quite obscure and all I got were discussions about 3D graphic renderers</p>
",<graph-theory>
"<p>Source is in <a href=""http://mathworld.wolfram.com/HamiltonianGraph.html"" rel=""nofollow"">Mathworld</a>, but it has no proof. Also, I am not completely sure what ""unbalanced"" means here. Non-equal number of even and uneven degrees?</p>

<p>What I am interested in, are there some universal properties of (connected) graphs that prevent hamiltonicity? Something to do with number of vertices versus edges, or average degrees or such? Something that is easy to demonstrate?</p>

<p>I will (hopefully) be using them as counterexamples of hamiltonian graphs in my thesis.</p>
",<graph-theory>
"<p>Sorry for the english, I tried to make it the most clear possible.</p>

<p>Be $G$ a connected graph not directed, I have to find an algorithm that given $n$ the quantity of vertex and $0&lt;k&lt;n$ disjoint components gives a graph such the weight of it's maximum edge is the minimum possible. Then, proof that this algorithm is correct.</p>

<p>So the algorithm I came up with is:</p>

<p>First whit some known algorithm, generate the minimum spanning tree. (for example whith the prim <a href=""http://en.wikipedia.org/wiki/Prim%27s_algorithm"" rel=""nofollow"">algorithm</a>)</p>

<p>For 1 to k get the maximum edge of this Minimum spanning tree and delete it.</p>

<p>done.</p>

<p>Proof that this work: </p>

<p>By induction:</p>

<p>Base Step($k=1$) :</p>

<p>Let $G$ be our graph of $n$ vertex and connected, getting a minimum spanning tree of $G$, we know we have a subgraph such all the vertex are connected there are no circuits and the sum of the weights of the edges is minimum.</p>

<p>If the sum of the weights of the edges is minimum, then this means the maximum weight of an edge is minimum. Then this step holds.</p>

<p>Inductive step(if $i-1 \rightarrow i$):</p>

<p>Let $G'$ be the minimum spanning tree. By Inductive hypothesis I know i can get the firsts $i-1$ edges and I get the disjoint graph the maximum weight of an edge is minimum for $i-1$ connected components. Then deleting the maximum weight edge, I get the disjoint graph the maximum weight of an edge is minimum for $i$ connected components.</p>

<p>I'm not very convinced this proof is correct, the inductive step feels very rare, and I think I used it wrong. </p>

<p>What do you think?</p>
",<graph-theory>
"<p>I have a lemma that say:</p>

<p>Let $G$ be a planar graph whose exterior face is bounded by a cycle $u_1,...,u_k$. Then there is a vertex $u_i$ ($i\neq 1,k$) not adjacent to any $u_j$ other than $u_{i-1}$ and $u_{i+1}$.</p>

<p>But for example $K_4$ is planar and this condition doesn't look satisfy. So I don't really understand this lemma.</p>

<p><img src=""http://i.stack.imgur.com/lcfrA.png"" alt=""enter image description here""></p>
",<graph-theory>
"<p>Suppose we know that a simple graph (no multiedges or loops) with finitely many vertices is connected, regular (every vertex has the same degree), and bipartite. Must the graph be a hypercube or an even cycle?</p>
",<graph-theory>
"<p>Where:</p>

<p>SCC - strongly connected components<br>
DFS - depth first search</p>

<p>This is the graph I was trying to ""solve"" this way is:</p>

<p><img src=""http://i.stack.imgur.com/oO5nX.jpg"" alt=""enter image description here""></p>

<p>So I'm starting from the A vertex and go straight to G, setting proccessing times all the way - A = &lt;1, 0> and G = &lt;7, 0>.</p>

<p>Then I go back, G = &lt;7, 8>, then to H = &lt;6, 9>, I = &lt;5, 10>, E = &lt;4, 11>...is this correct? And what's next? I can go to C = &lt;3, 0>, or to F which was not visited yet...I guess I should do:</p>

<p>F = &lt;12, 0>, D = &lt;13, 0>, D = &lt;13, 14>, F = &lt;12, 15>, E = &lt;4, 16>, C = &lt;3, 17>, B = &lt;2, 18>, A = &lt;1, 19></p>

<p>Is this correct?</p>

<p>Then I reverse the edges (arrows)...How to get SCC now?</p>
",<graph-theory>
"<p>I know the <a href=""http://en.wikipedia.org/wiki/Four_color_theorem"" rel=""nofollow"">four colour theorem</a> was solved by a computer checking a large number of cases. What I don't understand is why there are only a large but finite number of cases. It seems like there should be infinitely many planar graphs... What is the intuition behind cutting this down to a finite number of configurations?</p>
",<graph-theory>
"<p>A polyomino is a connected subset of $\mathbb{Z}^2$ - a set of squares joined along their edges such that the resulting form is connected (or, more shortly, a generalized form of Tetris cube). A polyomino can be easily thought of as graph with vertices being elements of $\mathbb{Z}^2$ and edges between two vertices of hamming distance 1 (i.e. with one coordinate equal and the other different by 1).</p>

<p>The Hamiltonian path/cycle problem is the problem of determining for a given graph whether it contains a path/cycle that visits every vertex exactly once. It is known to be NP-complete for general graphs and even for planar graphs. My question is whether it's still NP-complete when restricting the graphs to be polyominoes (and if it is NP-complete, how it is shown).</p>
",<graph-theory>
"<p>In a graph $G$, a <em>diametral path</em> is a path of length $\text{diam}(G)$ joining two vertices that are at a distance $\text{diam}(G)$ from each other. Given a tree $T$, consider the set of all diametral paths of $T$. For example, if $T$ is say the star graph $S_4$, it is not true that the set of all diametral paths would share an edge. But when do they?</p>

<p>How can we characterize trees where every diametral path has an edge in common? Looking at some examples, it seems always true for at least <a href=""http://mathworld.wolfram.com/BicenteredTree.html"" rel=""nofollow"">bicentered trees</a>. The intuition would be that every such path passes through the center, and thus have the edge between the two centers in common.</p>
",<graph-theory>
"<p>I know that many types graphs have unique names, ie. a directed graph where each node has exactly one outbound edge is known as a <code>functional</code> graph. Do the graphs with exactly 2 inbound/outbound edges have a name?</p>

<p><strong>Bonus</strong>: If the 2 inbound/outbound graph is named, is the nth inbound/outbound graph named?</p>
",<graph-theory>
"<p>Each <strong>Problem</strong> has one or many <strong>Solution(s)</strong>.</p>

<p>Each <strong>Solution</strong> has its <strong>Weight</strong>.</p>

<p>Each <strong>Solution</strong> leads to zero or many <strong>Consequent Problems</strong>, that must be solved if this solution is used.</p>

<p>There are <strong>no cycles possible</strong>.</p>

<p>Given the root problem, goal is to find <strong>optimal combination of {Problem:Solution} pairs</strong> that gives minimum (or maximum) Total Weight (sum of weights of all used solutions), so all consequent problems of all used solutions are resolved.</p>

<p>Is there any algorithm with the runtime less or equal to $O((P+S)^2)$ that calculates the goal? other words - without recursively enumerating all possible combinations (via cartesian product).</p>
",<graph-theory>
"<p>Is it possible to quantify the number of dimensions in combinatorial spaces.  The space I am particularly interested is all partitions of a set, bounded by the Bell number, where objects in this space are particular partitions.</p>
",<graph-theory>
"<p>The graphs that I consider are:</p>

<ol>
<li>labelled (so, I do not want to count them up to isomorphism).</li>
<li>simple (no loops and at most one edge between two nodes).</li>
<li>connected.</li>
</ol>

<p>In </p>

<p><a href=""http://math.stackexchange.com/questions/154941/how-to-calculate-the-number-of-possible-connected-simple-graphs-with-n-labelle"">How to calculate the number of possible connected simple graphs with $n$ labelled vertices</a></p>

<p>there is an answer to the count of such graphs given $n$ vertices.  Would there be a formula of the number of such graphs if the number of vertices $n$ and the number of edges $m$ are given?  My situation is that I need to estimate the number of such graphs when $n$ is large (say 3000), but $m$ is small.  </p>
",<graph-theory>
"<p>Say we have a undirected, connected graph that represents a network of streets.  How would you prove that there always exists a tour of the network, where one ``drives'' in both directions on every street exactly once?</p>
",<graph-theory>
"<p>Put another way ... Colour the edges of the complete graph with 3 colours, so that three subgraphs are each a copy of the Petersen Graph.
I heard somewhere that it can be done (Maybe I should not go on MathOverFlow !) but I have spent all weekend trying and have convinced myself it is impossible. Thanks in advance for your comments.</p>
",<graph-theory>
"<p>On our practice exam, our teacher gave us this problem and this solution:</p>

<blockquote>
  <p>What is the fewest number of vertices required to construct a complete
  graph with at least $500$ edges? (Show your work but do not attempt to
  simplify your answer too much!)</p>
</blockquote>

<p><strong>Answer:</strong> We need to select $n$ such that $\dbinom{n}{2} \geq 500$.</p>

<p>I do not understand how she got to this answer. I tried to start with the definition of a complete graph, but where to go from there, I had no idea. </p>
",<graph-theory>
"<p>On our practice exam, our teacher gave us this problem and this solution:</p>

<p>Let $G$ be a forest with $k \geq 1$ components. What type of (sub)graph is each component? Suppose each component has $n_i$ vertices with $n_i \geq 1$. How many edges does $G$ have? (Show your work!)</p>

<p>ANSWER: Each component is a tree. Suppose there are $c$ components. Then $|E(G)| =\sum_{i=1}^c  n_i − 1 = |V(G)| − c$.</p>

<p>I am confused as to how she got that answer. I do not understand how she knows to use that summation for the number of edges.</p>
",<graph-theory>
"<p>This is a vague question asking about the existence of a mathematical object, instead of properties of a well defined one. I am sorry if this is not the correct forum.</p>

<p>I know if you have a random graph $g \in G(n, p)$ on $n$ vertices ($n$ large) where each edge is included with probability $p$, as you increase $p$ from zero there is a very acute point where the probability that $g$ is connected becomes almost certain. That it goes from the state of ""being disconnected"" to ""being connected""</p>

<p>I am wondering if there is discrete structure $s \in S(n, p, q)$ on an underlying set of size $n$, parameterized with two probabilities $p$ and $q$, over which you can describe three mutually exclusive states. In such a structure can there exist a point $(p_t, q_t) \in [0,1]\times[0,1]$ that in a reasonably small neighborhood (with an $\epsilon$ which shrinks as $n$ grows) each of the states become almost certain. A mathematical equivalent to a triple point.</p>
",<graph-theory>
"<p>I need to use the Euler's formula. I know there are $62$ faces...first, how do I find the number of vertices it has. From there, I can get the amount of edges, which will then in turn get me the number of faces. The question states as follows: A rhombicosadodecahedron is a polyhedron whose every vertex is incident to one triangular, one pentagonal, and two (opposite) quadrilateral faces. Find the number of faces. </p>
",<graph-theory>
"<p>Say I have some collection of ordered sets $C = \{S_i\}$.  Is there an efficient way to determine the minimal ordered set $S^*$ such that each of the original $S_i \subseteq S^*$, with order preserved?</p>

<p>Simple example: $C = \{\{a, b, c\}, \{b, d, f\}, \{b, c, d\}\}$, then $S^* = \{a, b, c, d, f\}$</p>

<p>More complex example: $C = \{\{a, b, c\}, \{b, c, d\}, \{x, a, d\}, \{b, b, a, c\}, \{y, b, c, b\}\}$, then one solution is $S^* = \{y, x, a, b, c, b, a, c, d \}$.</p>

<p>Another example: $C = \{\{a, b, c\}, \{b, a, c\}, \{c, a, b\}, \{b, c, a\}, \{c, b, c\}, \{b, b, c\}\}$, $S^* = \{b, c, a, b, c\}$</p>

<p>The naïve solution is to just concatenate the $S_i$. When $\cap S_i = \emptyset$, then of course this happens to also be the minimal solution.</p>

<p>Perhaps finding some traversal of a digraph representing all elements and their ordering?</p>
",<graph-theory>
"<p>Let $G=(V,E)$ be a graph, $M$ a matching in $G$. I have read that $M$ is maximal iff there is no augmenting path of $M$ in $G$.</p>

<p>Now consider the graph $G=(\{a_1,b_1,a_2,b_2\},\{a_1b_1,a_2b_2\})$, which is just 4 vertices where two of them are pairwise connected. Now $M=\{a_1b_1\}$ is a matching in $G$, but there is no alternating path to augment it to $M'=\{a_1b_1,a_2b_2\}$, because $G$ does not support any edges for that.</p>

<p>Where is my mistake?</p>

<p>In general, I am trying to proof that given any graph $G$ and some matching $M$ in $G$. Then there always exists an alternating path $P=p_1p_2...p_k, p_i\in V(G)$ w.r.t. to $M$, i.e. $p_1 \not\in V(M)$ and the edges are alternating being in and not in $M$. In the case that $M$ is not maximal, one of these alternating paths is augmenting.</p>
",<graph-theory>
"<blockquote>
  <p>Describe a graph model for solving the following problem: Can the permutations of $\{1,2,\ldots,n\}$ be arranged in a sequence so that the adjacent permutations $$p:p_1,\ldots,p_n \text{ and }  q:q_1,\ldots,q_n$$ satisfy $p_i\neq q_i$ for all $i$?</p>
</blockquote>

<p>I have problem understanding what the exercise asks. What does ""adjacent permutation"" mean? Also, in a follow-up question, it says that the statement is true for $n\geq 5$. </p>
",<graph-theory>
"<p>I'm doing a research project in graph theory and need to program some stuff to help me study it. I used to have access to Mathematica but now I don't. So, when I'm programming things I'm entering the graph data in by hand which is really time consuming. So I was wondering if there was anywhere on the internet that contained a file I could download a file or files containing the descriptions of a variety cubic graphs to help automate this process. </p>

<p>Does anyone know of such a website? </p>
",<graph-theory>
"<p>Well to start off, $5n=2e$ since there will be $n$ number of vertices of degree 5 and with the handshaking lemma, there will be $2e$.  The problem is finding the number of vertices and faces. What do I do?</p>
",<graph-theory>
"<p>In group theory we have Cayley graphs. Are there analogous or anyway useful visual representations of magma structures?</p>

<p>I am unsure about how to construct a graph representing, for instance, a free magma with two generators.</p>
",<graph-theory>
"<p>Let $G,H$ be finite graphs with $|V(H)| &gt; |V(G)|$. </p>

<p>Let $\mathcal{f}:G\rightarrow H$ be an injective homomorphism - that is, $\mathcal{f}$ is:</p>

<p>$1.$ Injective - $\mathcal{f}(g_1)\neq\mathcal{f}(g_2)$, $\forall g_i\in V(H)$.</p>

<p>$2.$ Homomorphic - $g_1g_2\in E(G)\implies \mathcal{f}(g_1)\mathcal{f}(g_2)\in E(H)$.</p>

<p>Let $\odot$ imply one of the graph products:</p>

<p>$1.$ Tensor</p>

<p>$2.$ Cartesian</p>

<p>$3.$ Strong</p>

<p>$f \odot f:G \odot G \rightarrow H\odot H$ is injective.</p>

<p>Is $f \odot f:G \odot G \rightarrow H\odot H$ a homomorphism?</p>

<p>Is there a reference for this fact?</p>
",<graph-theory>
"<p>Let $A$ be the adjacency matrix of some directed graph with $m$ vertices labeled as $v_1, v_2, \ldots, v_m$. So here $A_{ij} = 1$ if there is an edge from $v_i$ to $v_j$, and $A_{ij} = 0$ otherwise. By induction it is not too hard to show that $(A^n)_{ij}$ is the number of $n$-step walks from $v_i$ to $v_j$.</p>

<p>Why is this the case? Is there some geometric explanation, or some explanation using linear algebra? The definition of matrix multiplication comes from composition of linear maps, and it seems surprising to me to see this connection between linear maps and graphs.</p>
",<graph-theory>
"<p>I would like to know how to define the chain complex of a given graph, 0-cell are the vertices of the graph and 1-cell are the edges. How about 2-cell? are they cycles, or triangles?</p>

<p>does anyway who can give me an example how to compute the homology group of $K_4$ and $K_5$? </p>
",<graph-theory>
"<p>Is it true that any undirected graph is a union of maximal cliques? 
Since an edge is a size 2 clique?</p>
",<graph-theory>
"<p>Here's a question I came upon while fiddling around with yarn on spindles.</p>

<p>I joined three spindles so that they were orthogonal.. then, beginning at the base of a particular spindle (A), wound it around another spindle, creating an arc connecting the two. After making some interesting figures I started to wonder about the combinatorial question of how many possible patterns could be created in this manner.</p>

<p><img src=""http://i.stack.imgur.com/aO7Vl.jpg"" alt=""Basic graphic example of question""></p>

<p>Assuming that each half spindle can hold some number ( arbitrary, but the same for all spindles) of winds (visits, essentially), and not excluding redundant patterns due to symmetry (once the basic question is answered, I will consider excluding these degenerate patterns).</p>

<p>A spindle <strong>can</strong> have arcs going back to itself (loops).</p>

<p>I've tried figuring it out using markov graphs, basic linear algebra and combinatoric techniques, but am very curious as to how other people would go about it.</p>

<p>-There will be times when the yarn 'paints itself into a corner', e.g. when 4/6 spindles have filled their holding capacity, and the thread must travel within the remaining nodes, ultimately cornering itself to eventually loop back its own node until completion (possibly even excluding some nodes entirely, leaving them untouched and unreachable). How could this be figured in to the problem?</p>

<p>I'm very excited to see how more mathematically mature problem solvers attack this :) </p>
",<graph-theory>
"<p>In my textbook, they provide a graph $H$ and then list examples of the cliques and the independent sets in $H$. $$\begin{align*}V(H)&amp;=\{1,2,3,4,5,6,7,8,9\}\\E(H)&amp;=\{12,23,39,98,87,74,41,26,25,56,36,69,68,57\}\end{align*}$$</p>

<p>They list the set $\{4\}$ to be both a clique and an independent set.
I am having trouble understanding why $\{4\}$ is both a clique and an independent set. </p>

<p>I know that a subset $S$ is a clique provided that any two distinct vertices are adjacent. So, since $\{4\}$ has only vertex, is it vacuously a clique?</p>

<p>I know that a set $S$ is independent provided $G[S]$ is an edgeless graph. I can see clearly how $\{4\}$ is an edgeless graph since it only has one vertex. </p>

<p>So, can a subset be both a clique and an independent set? Is $\{4\}$ both a clique and an independent set?</p>
",<graph-theory>
"<p>I would like to ask you if there is a way for checking if we can decompose a specific graph into $N$ planar sub-graphs that can be drawn on $N$ planes without an edge crossing any of the planes.</p>
",<graph-theory>
"<p>I am confused with this question. My teacher asked us at class but I cannot solve it. Can you help me?</p>

<p>""Let $T$ be a tree with exactly two vertices of degree $7$ and exactly $20$ vertices of degree $100.$ What is the minimum possible number of vertices in a tree $T$ that satisfies those restrictions?""</p>
",<graph-theory>
"<p>I understand vertex transitivity of a graph $G$ as the property that the local environment, i.e., all incident edges and their vertices, of any 2 vertices looks the same. </p>

<p>What if we extend this notion of similarity to k-hop neighborhood of vertices, including the environment up to some distance k? Is there a generalization of this property such that the k-hop environments of any 2 vertices look the same?</p>
",<graph-theory>
"<p>As most of you know, the problem of finding a minimal vertex cover for an arbitrary graph is an NP-hard problem. I was wondering, if there existed a non-constructive way of calculating the size of a minimal vertex cover. That is, if you find a minimal vertex cover, this tells you the minimum size of a vertex cover. However, what if you know that the size of a minimal vertex cover is m, does this help you find such a covering?</p>

<p>My back of the envelope calculation tells me that, on a graph with n vertices, if you know that the size of a minimal vertex cover is m, you just need to check all subsets of V (the set of verticies) of size m, that is ${n \choose m} $. As a function of n, this has growth $O(n^n)$ i.e. not polynomial. So naively you don't get a polynomial reduction, and so the problem doesn't appear to be NP-hard.</p>

<p>So is finding the minimum size of a vertex cover an easier problem than finding a minimal vertex cover?</p>
",<graph-theory>
"<p>I have a graph $G=(V,E)$ where to each vertex $v$ I have associated a value, $\hat{v}$ (ie I have a ""network"" in the terminology here <a href=""http://snap.stanford.edu/snap/index.html"" rel=""nofollow"">http://snap.stanford.edu/snap/index.html</a> ).</p>

<p>Let $\phi : \hat{V} \rightarrow \hat{V}$ be the function which takes the value associated to each node and replaces it with median of the values of the adjacent nodes.</p>

<p>Empirically, iterating $\phi$ converges. Why?</p>
",<graph-theory>
"<p>I'm not sure what topic in math this is (probably Graph Theory), but maybe you can help me. </p>

<h1><strong>The Challenge</strong></h1>

<p>My task is like this: I have to create an automatic program that will assign each student in a class (of variable size, but my own class now has 11 students) a reviewer for his assignment, according to a set of rules:</p>

<ul>
<li>The students sit in a row</li>
<li>One cannot review himself</li>
<li>Two students cannot review each other (reciprocally)</li>
<li>One cannot review his neighbor (from both sides)</li>
<li>One cannot review the student he reviewed the last time</li>
</ul>

<p>And the real trouble maker: </p>

<ul>
<li>One cannot review the student he reviewed two times before</li>
</ul>

<p>The programming implementation of this problem is not important – though so far the logic and algorithms that I've tried to implement have all failed (i.e. reaching a dead-end, or in the case of my programs, an infinite loop).</p>

<h1><strong>My previous approaches</strong></h1>

<p>At first I tried to simply throw a random number between 1 and the class size (11), check if it follows the rules (updating the availability of each student as we go, of course), and if so assigning that student as a reviewer. I found out very quickly that it often gets stuck on the last student, because the random assignments done prior leave the last student with no available reviewers according to the rules. </p>

<p>So this reminded me of the problem of coloring the map of the US states with 4 colors, and I tried another approach where before each assignment the program checks if that assignment will leave another student without any available reviewers – and only if it doesn't it assigns the reviewer, otherwise it looks for another random reviewer. </p>

<p>And this worked for 2 to 3 times, until it ALSO GOT STUCK (!!) And the reason is that it came into a situation where, say, after assigning 8 students, student 9 had two possibilities, and choosing 1 of them would leave student 10 with no available students while choosing the other would leave student 11 without one…</p>

<p>I didn't want to go up two levels and solve this problem, because it seemed I'm just pushing the problem one level higher each time. So I went for another approach:</p>

<blockquote>
  <p>Before each assignment check for the students with the least number of possible reviewers, and assign to him/her first. (Though I didn't implement it, you could probably still make it random after adding the rule that if there is more than one student who has the lowest number, choose one of them randomly.)</p>
</blockquote>

<p>This worked for about 10 times, until… you guessed it, it also got stuck. And for the last student there was no reviewer left which abided by the restrictions.</p>

<h1><strong>What I'm Looking For</strong></h1>

<ol>
<li>Well, an algorithm which will solve my problem; </li>
</ol>

<p>or</p>

<ol start=""2"">
<li>A proof that shows it cannot be done, i.e. that you cannot guarantee you would not get stuck at some point (at least not with 11 students and checking 2 times back);</li>
</ol>

<p>or at the very least</p>

<ol start=""3"">
<li>Some help of finding relevant material that can advance me in my quest for the holy grail. </li>
</ol>

<p><strong>NOTE:</strong> When checking only 1 time back, I didn't encounter any problem, though I'm not sure if this means that you can't encounter any dead-ends, or that it's just rarer to encounter them. </p>

<h2><strong>Big Note</strong></h2>

<p>I think there's an easy way, where you just assign each student the first available option, and rotate everyone exactly the same. </p>

<p>i.e.:</p>

<pre><code>    1 2 3 4 5 6 7 8 9 . ..
1   m 0 l s 1 1 1 1 1 1 1
2   0 m 0 l s 1 1 1 1 1 1
3   1 0 m 0 l s 1 1 1 1 1
4   1 1 0 m 0 l s 1 1 1 1
5   1 1 1 0 m 0 l s 1 1 1
6   1 1 1 1 0 m 0 l s 1 1
7   1 1 1 1 1 0 m 0 l s 1
8   1 1 1 1 1 1 0 m 0 l s
9   s 1 1 1 1 1 1 0 m 0 l
10  l s 1 1 1 1 1 1 0 m 0
11  1 l s 1 1 1 1 1 1 0 m
</code></pre>

<p>(m - self, l - last review , s - second last, 1 - available option, 0 - not available)</p>

<p>so, for the first student you choose number 5, for the 2nd you choose 6, etc. And the next time you rotate everyone one position.</p>

<p>It guess this will work, but I'm looking for a solution that doesn't require such a tidy arrangement of the students. Also – this is not random at all – it will be a series of (in this case) 8 other reviewers that each student will traverse, and it will keep repeating itself. I would prefer a more random assignment.</p>

<p>Though if you can show this is the only/best way, that would be good enough for me. </p>

<p>Thanks,
David</p>
",<graph-theory>
"<blockquote>
  <p>find equation of a line perpendicular to the given line through the given point
  the line is y=1/2x-2 
  the point is (3,6) 
  I don't know what to do ,or try , please help me , its probably very easy to other people</p>
</blockquote>
",<graph-theory>
"<blockquote>
  <p>your classmate juan missed the lesson on graphing linear  equations using slope-intercept form. you attend the class and what to help juan understand the material he missed. assume juan knows how to graph y=x</p>
  
  <p>part A= write a explanation for juan that describes the y-intercept of a line and what happens to the graph of y=x as you change the y- intercept. be specific and consider several cases </p>
  
  <p>part b= write an explanation for juan that describes the slope m of a line  and what happens to the graph of y=x as you change the slope. be specific and consider several cases</p>
  
  <p>well here it it seems hard right... well it does to me , I really don't know what to do</p>
</blockquote>
",<graph-theory>
"<p>You can move from 1 to 2, 2 to 3, and so on, step by step, till 100.
Also, between the points give below, you can move directly within every pair:</p>

<p>(10 and 60)</p>

<p>(50 and 100)</p>

<p>(70 and 100)</p>

<p>(80 and 100)</p>

<p>How many different paths exist?
I tried it, and could find only 7. But a c-program i created says 8 paths. so, what do you think?</p>

<p>Here are the 7 paths in could find:</p>

<p>1-2-3....99-100;</p>

<p>1-2-3....10-60-61-62....100;</p>

<p>1-2-3....50-100;</p>

<p>1-2-3....10-60-61-62....70-100;</p>

<p>1-2-3....10-60-61-62....80-100;</p>

<p>1-2-3....70-100;</p>

<p>1-2-3....80-100;</p>
",<graph-theory>
"<p>There are four groups of people who want to be carried to a destination. If we have three vehicle to carry the following number of people:</p>

<blockquote>
  <p>Veh 1, five; </p>
  
  <p>Veh 2, four; </p>
  
  <p>Veh 3, four. </p>
</blockquote>

<p>Each group has four people, and no vehicle can carry more than two people from any of the given group. How do we use network flow to find the maximum number of people who can be carried in the three vehicles. </p>

<p>I have built a digraph where A, B, C and D represent the edges for the groups and V1, V2 and V3 represent the three vehicles. Here's the solution I came up with. Can someone look at it and see if I am doing it right?</p>

<p><img src=""http://i.stack.imgur.com/1pbT5.png"" alt=""enter image description here""></p>
",<graph-theory>
"<p>Given a DAG $G=(V,E)$, Let $\dot{G}$ be $G$ after flipping the direction of a single edge $e\in E$. Are there sufficient (and\or necessary) conditions under which $\dot{G}$ is guaranteed to be DAG ? Are there known classes of DAGs that maintain this property? </p>
",<graph-theory>
"<h3>Background:</h3>

<p>I was studying Theorem 2.3.3 from Introduction to Graph Theory by W. B. West. The main idea of his proof is as follows:
<br>
<b>T</b>, resulting tree.
<br>
<b>T<sup> *</sup></b>, spanning tree of minimum weight.
<br>
Let, T $\neq$ T<sup> *</sup>, then there are edges in T<sup> *</sup> that are not in T.
We first consider only 1 edge. Let's name it e. Hence, T<sup> *</sup> has all the edges that are in T, except e. Now, if we add e to T<sup> *</sup>, we should get a cycle, say 'C'. This implies, C has an edge that is not in T. Let's name it e'. Now we get spanning tree T<sup> *</sup>+ e - e'.
Now, let us think what happened when Kruskal ran and produced T. It examined all edges in G. Including e and e'. But, it included only e. So, w(e)$\leq$w(e').</p>

<blockquote>
  <p>Thus T<sup> *</sup>+ e - e' is a spanning tree with weight at most T<sup> *</sup> that agrees with T for a <em>longer initial list</em> of edges than T<sup> *</sup> does.</p>
</blockquote>

<ul>
<li>What does it mean actually?</li>
<li>Anybody knows any other proof?</li>
</ul>
",<graph-theory>
"<p>What is the degree of Johnson graph $J(n,k)$ where $n&gt;k$?
<a href=""http://en.wikipedia.org/wiki/Johnson_graph"" rel=""nofollow"">http://en.wikipedia.org/wiki/Johnson_graph</a></p>

<p>What are some good examples of subgraphs of Johnson graphs? The Johnson graph I am interested in is $J(n,2)$.</p>
",<graph-theory>
"<blockquote>
  <p>Is the graph connected? Justify.</p>
</blockquote>

<p>Because there is a path connecting all pairs of vertices, this graph is therefore connected?</p>

<p>Is that right?</p>

<p><img src=""http://i.stack.imgur.com/tMaUp.png"" alt=""graph""></p>
",<graph-theory>
"<p>I have the following adjacency matrix:</p>

<pre><code>   a  b  c  d 
a [0, 0, 1, 1]
b [0, 0, 1, 0]
c [1, 1, 0, 1]
d [1, 1, 1, 0]
</code></pre>

<p>How do I draw the graph, given its adjacency matrix above (I've added <code>a,b,c,d</code> to label vertices).</p>

<p>I don't understand how the vertex $d$ (e.g., the row $d$) is adjacent to the vertex $b$, but the vertex $b$ (the row $b$) is not adjacent to the vertex $d$ (the column $d$).</p>

<p>Is this possible?</p>

<p>Thanks!</p>

<p>EDIT:  Maybe it's directed?  If so, would that explain why d --> b, but b =/ d?</p>
",<graph-theory>
"<p>I found a nice introduction on how to <a href=""https://crazyproject.wordpress.com/2011/03/04/use-grbner-bases-to-construct-the-colorings-of-a-finite-graph/"" rel=""nofollow"">Use Gröbner bases to construct the colorings of a finite graph</a>. </p>

<p>Now my graphs $G=(V,E)$ are the line graphs planar of cubic graphs, so they are $4$-regular. The corresponding edge-adjacency matrices can be constructed, as shown <a href=""http://math.stackexchange.com/q/1629748/19341"">here</a> (in a crude way, I admit...). 
Planarity assures the existence of $3$-colorings of the edges! </p>

<p>Now let there be a field $F = \mathbb{Z}/3\mathbb{Z}$ and let $S = F = \{0,1,2\}$ be our set of colors.</p>

<p>Let's define two types of polynomials on $F$: </p>

<ul>
<li>$f(z) = z(z-1)(z-2) = z^3-z$ </li>
<li>$g(y,w) = y^2+yw+w^2-1$. </li>
</ul>

<p>Let $I$ be the ideal $I = (x_t^3-x_t \ |\ t \in [1,n]) + (x_r^2+x_rx_s+x_s^2-1 \ |\ \{r,s\} \in E)$. </p>

<blockquote>
  <p>Moreover, every solution to this system yields a coloring and can be calculated by using the reduced Gröbner bases for the ideal $I \subseteq F[x_1, \ldots, x_m]$</p>
</blockquote>

<p>Is it possible to calculate the number of solutions of the system of equations using Gröbner bases and if so how to do that?</p>
",<graph-theory>
"<p>If $G$ is 2 - self centered graph. then how to prove that $G$ has at least $2n - 5$ edges?
where $n\geq 5$.</p>

<p>I started by assuming if number of edges $\mid E\mid\leq 2n-6$
then there exist a vertex say $u$ such that $deg = 2$ otherwise if no such vertex exists then</p>

<p>$\mid E\mid\geq \frac{3n}{2}&gt;2n-5$ (I am stucked here. How to prove this. Sincerely thanks for giving me time.)</p>
",<graph-theory>
"<p>Let $m\leq n-1$. Is there a closed expression counting the subgraphs of minimum degree $\geq m$ (resp. maximum degree $\geq m$) on $n$ labelled vertices?   </p>
",<graph-theory>
"<p>Let's say I'm given several degree sequences like</p>

<p>{4,3,3,2,2}</p>

<p>{3,3,3,3}</p>

<p>{5,3,3,2,2,1}</p>

<p>I can find the number of edges using the handshaking lemma</p>

<p><img src=""http://upload.wikimedia.org/math/7/a/b/7abc7ac49467cd9bcc2260f59a190d00.png"" alt=""graph""></p>

<p>But how do I construct a graph just given these degree sequences? There are multiple types of graphs that satisfy the degree sequences, but beyond guess &amp; checking, is there a logic/pattern to follow when making graphs? Right now I'm at the guess &amp; check stage.</p>
",<graph-theory>
"<p>Given two Prufer sequences $P_1$ and $P_2$ that correspond to trees $T_1$ and $T_2$ of order $n$  what is the most efficient method to determine whether $T_1$ is isomorphic to $T_2$ as an <em>unlabelled</em> tree? </p>

<p>Having read about the difficulty of detecting isomorphic graphs via graph invariants (<a href=""http://mathoverflow.net/questions/11631/complete-graph-invariants"">http://mathoverflow.net/questions/11631/complete-graph-invariants</a>) I suspect this is a hard problem.   </p>
",<graph-theory>
"<p>The efficient algorithm needs to be done and proved for the best solution for the given problem:</p>

<p><strong>User inputs:</strong> (#) Size of the NxN Grid. (N); <strong>(#)</strong> No. of Paths:  Z; <strong>(#)</strong> Source and Destination Coordinates of each of the individual Z paths.</p>

<p><strong>Given:</strong> (#) Each path comprises of the cells which are ADJACENT to each other and NOT diagonal.
<strong>(#)</strong> Time taken to cross each cell is EQUAL and is an unit time.
<strong>(#)</strong> Travelling starts from the respective source coordinates of each path in the grid SIMULTANEOUSLY.
<strong>(#)</strong> Paths may intersect, BUT after say 'x'th unit time no two paths must intersect in one particular cell at a time. If such case arises, then one of the paths need to bypass into a new path taking alternate adjacent cell just before the clashing cell.</p>

<p><strong>Output:</strong>
Z no. of paths which are SHORTEST possible without clash.</p>

<p>[Say there are two paths (Z=2), say X to Y and W to Z. While travelling, both X and W start simultaneously from their respective source coordinates, going to their respective destinations Y and Z such that they should take the SHORTEST possible path, AND after 'x'th unit time, they must not CLASH (when both the paths have traveled exactly (x-1) no. of cells and clashing into the 'x'th cell) into one single cell. If that happens, one of the path needs to be bypassed towards its destination keeping in mind without clash, shortest possible path. And this has to be implemented for Z no. of lines starting simultaneously in the grid.]</p>

<p>P.S.- Yes, I should have mentioned it clearly. I mean the total of all the path lengths must be as less as possible. And, since I'm a new user, I wasn't allowed to use any image. Please check the image link as follows so that you can get an idea about the clash thing as how it is supposed to happen: mediafire.com/view/?6y7t8vbtex0s299</p>
",<graph-theory>
"<p>The <a href=""https://en.wikipedia.org/wiki/Friendship_paradox"" rel=""nofollow"">friendship paradox</a> is sometimes summarised as stating that:</p>

<ol>
<li><strong>Most people have fewer friends than most of their friends.</strong></li>
</ol>

<p>However, the mathematical explanation provided usually shows something different: namely, that</p>

<ol start=""2"">
<li><strong>The average number of friends that a typical person has is less than the average number that a typical friend has.</strong></li>
</ol>

<p>In other words, the average degree of a randomly selected vertex in an undirected graph is less than the average degree of a randomly selected endpoint of a randomly selected edge.</p>

<p>Furthermore, the following variation of the paradox is <em>not</em> always true:</p>

<ol start=""3"">
<li><strong>Most people have fewer friends than their friends have on average.</strong></li>
</ol>

<p>This can be seen by removing one edge from the complete graph $K_5$: 3 out of 5 people then have more friends (4) than their friends do on average (3.5). Note however, that statement 1 is still true for this graph (assuming less-than-or-equals definitions of 'most' and 'fewer') as those 3 don't actually have more friends than half of their friends.</p>

<p><strong>Is statement 1 always true? If not, under what conditions does it hold?</strong></p>

<p><strong><em>Clarification</strong>: statement 1 asserts that for an arbitrary finite undirected graph, fewer than half of the vertices are popular, where a vertex is popular iff it has a greater degree than over half of its neighbours.</em></p>
",<graph-theory>
"<p>If $G$ be a Tree with degree $(5,r,s,1,1,1,1,1) $. (I wrote degree in non-increasing order). why all of this condition is True sometimes (I means on some condition)? I try to find an example that includes all following condition. any friends could help me?</p>

<p>1) $G$ has a vertex of degree 2.</p>

<p>2) $G$ has a vertex of degree 3.</p>

<p>3) there is two vertices in $G$ that distance between them is 3.</p>

<p>4) there is two vertices in $G$ that distance between them is 4.</p>

<p>Thanks.</p>
",<graph-theory>
"<p>I am trying to understand the proof to a random graph problem (the threshold for connectivity of $G \sim G(n,p)$ being $\frac{logn}{n}$). I am struggling to see exactly why the following holds:</p>

<p>$\text{When } p^{*} \ll p,\\
n(1-p) ^{n-1} \leq ne^{-p(n-1)} = o(ne^{-\log n}) = o(1).\\$</p>

<p>I am aware that the following inequalities may be used in the derivation but my further deductions (after the $\implies$ symbol) show that the LHS of ($\text{*}$) and RHS of ($\text{**}$) are both greater or equal to the same quantity. However, the line of the proof provided above states something different, to which I am unable to arrive.</p>

<p>$\text{(*) }(1-p)^{n-1} \geq 1 - (n - 1)p, \forall 0 \leq p \leq 1, \text{and}\\
\text{(**) }e^{p} \geq p + 1, \forall p \implies e^{-p(n-1)} \geq 1-(n-1)p.$</p>

<p>Another amateurish question follows here - is the following the case in the last part of the aforementioned line from the proof:
$ne^{-\log n} = nn^{-1} \text{?}$</p>
",<graph-theory>
"<p>I have been doing practise problems in designing algorithms and came across the following in a past test from an American university (see attached): </p>

<p>A directed graph is <em><strong>unilaterally connected</strong></em> if, for each pair of vertices, there exists a path from one of the vertices to the other. </p>

<p>Design an algorithm to decide whether or not a given graph is unilaterally connected. </p>

<p>Can I please have some help with this? I so far have:</p>

<p>-Run a strongly connected components algorithm on the graph (this then produces a directed acyclic graph where the vertices are the strongly connected components)</p>

<p>-since we have a DAG, we can produce a topological ordering of the DAG</p>

<p>-then we need to somehow use this ordering to decide whether the graph is unilaterally connected or not. </p>

<p>I would appreciate any help/feedback and also any pseudocode to help with visualising exactly how this would work. And obviously suggestions of better algorithms would help too! Thank you. </p>

<p><a href=""http://cs.smith.edu/~streinu/Teaching/Courses/252/Fall00/AllEx/THfinF00.html"" rel=""nofollow"">http://cs.smith.edu/~streinu/Teaching/Courses/252/Fall00/AllEx/THfinF00.html</a> (past test)</p>
",<graph-theory>
"<p>$\chi(G)$ ( vertex-chromatic number of a graph like $G$) is the minimum number of colors which is enough to color every vertex of $G$ such that no two adjacent vertices have the same color.  </p>

<p>A graph like G is called k-vertex-critical if $\chi(G)=k$ and for each $v \in V(G)\space\space\space\space\chi(G-v) \lt \chi(G)$  .  </p>

<p>$\chi'(G)$ ( edge-chromatic number of a graph like $G$) is the minimum number of colors which is enough to color every edge of $G$ such that no two edges of the same color share a common vertex.  </p>

<p>A graph like G is called k-edge-critical if $\chi'(G)=k$ and for each $e \in E(G)\space\space\space\space\chi'(G-e) \lt \chi'(G)$  .  </p>

<p>Now the question :<br>
Can you find a vertex-critical graph which is not edge-critical?  </p>

<p>Note:  <strong>I don't want</strong> a graph with critical vertices and without critical edges. This question is different. Here, we deal with every edge and every vertex not just one.</p>

<p>Thanks in advance.</p>
",<graph-theory>
"<p>I am self studying graph theory and was wondering: is there a simple way to count/compute the number of subgraphs G that are isomorphic to another graph (say, G')?</p>

<p>For instance, if G = the complete graph ""K_10"" with 10 vertices, and G' = Hypercube H_3 with 3 vertices</p>

<p>Or, if G = Hypercube ""H_4"" with 4 vertices, and G' = Complete biparite graph ""K_1, 3"".</p>
",<graph-theory>
"<p>Let $G=(V,E)$ be a countably infinite directed acyclic graph and $L$ be a finite set of vertex labels. The number $\left|V\right|$ of vertices is countable infinity and some vertices may have an infinite number of ingoing edges. I am interested in the existence of labelings $f: V \rightarrow L$ where the label $f(v)$ is determined by the labels of the predecessors of $v$.</p>

<p>More formally, let there be a function $g_v:L^{\mathrm{indeg}(v)} \rightarrow L$ for each vertex $v \in V$. Does there exist a labeling $f : V \rightarrow L$ such that for every vertex $v \in V$ with predecessors $p_0, p_1,\ldots$ the following holds?
$$f(v) = g_v(f(p_0), f(p_1), \ldots)$$</p>

<p>Intuitively, it appears very clear to me that the acyclicity of $G$ guarantees the existence of such labelings $f$. But how do I prove that?</p>

<p>For a trivial example, consider the infinite path $G = (\mathbb{Z}, \{(v,v+1):v\in\mathbb{Z}\})$, $L = \{0, 1\}$ and $g_v:L\rightarrow L:x\mapsto 1 - x$. In this case there are exactly two labelings, one that assigns 0 and 1 to every even and odd vertex, respectively, and the other with 0 and 1 switched.</p>

<p>If we drop the requirement of acyclicity, a counterexample would be an odd directed cycle graph and $L$ and $g$ as in the example above. Here no labeling exists.</p>

<p>In the case of finite DAGs, I would prove the existence inductively over an topological ordering of $G$, starting by an arbitrary label-assignment for vertices without ingoing edges and then iteratively constructing the labeling for vertices whose predecessors have already been labeled. However, I fail to generalize this technique to infinite graphs in the absence of an induction basis.</p>
",<graph-theory>
"<p>I have to answer the question : ""Is it true that in all finite graphs (connected graphs) wchich $\delta \ge 2$ ($\delta$ is the smallest degree of vertices in a graph) exists the cycle of length equal to $\delta+1$ or longer"". I believe this statement is true. I drew some graphs and it seems to be true. I do not know how to prove this in mathematical way. I will glad for help and advice.</p>
",<graph-theory>
"<p>Let $f:G \to G$ be a bijective function on a finite simple connected graph which preserves adjacency (if $x$ is adjacent to $x'$ then $f(x)$ is adjacent to $f(x')$). Is $f$ automatically a graph automorphism?</p>
",<graph-theory>
"<p>I was reading up on probabilistic graphical theory and it's not very clear how the flow of probabilistic influence differs between directed and undirected graph. My initial assumption was that flow is one way i.e. from parent to child only in case of directed graphs but that does not seem to be the case. </p>

<p>I will highly appreciate if anyone can elaborate upon this? </p>
",<graph-theory>
"<p>How to prove that the number of unlabeled binary trees is the same as the number of Binary Search Tree possible. I know the number of binary search tree is equal to nth catalan number but how would I prove that this is equal to the number of unlabeled binary tree.</p>
",<graph-theory>
"<p>Can someone explain to me how these graphs are homeomorphic?</p>

<p><img src=""http://i.imgur.com/BocvrbP.png"" alt=""1"">
<img src=""http://i.imgur.com/Yif4Yqw.png"" alt=""2""></p>
",<graph-theory>
"<ol>
<li><p>Can every disconnected graph be decomposed into 2 disjoint subgraphs ? If yes then edge-disjoint or vertex-disjoint ? and Why ?
If not then what are the exceptions ?</p></li>
<li><p>Given n vertices is it always possible to draw edges such that the graph has a hamiltonian circuit ?</p></li>
<li><p>Can we say that every connected graph is either an Euler graph or an unicursal graph ?</p></li>
</ol>

<p>PS:I am fairly new to Graph Theory, and I am learning on my own. Please don't mind if the question is not technically correct. I just want to get the concepts clear.</p>
",<graph-theory>
"<p>I am currently working on determining the maximum number of times the minimum spherical distance can occur among $n$ points in $\mathbb{S}^2$, and I have the following question.</p>

<blockquote>
  <p>In $\mathbb{E}^2$, I cite from Pach and Agarwal's Combinatorial Geometry: ""The internal angle of a simple closed polygon $C$ which bounds a graph $G$ at a vertex of degree $d$ is at least $(d-1)\frac{\pi}{3}$."" How does this statement generalize to $\mathbb{S}^2$?</p>
</blockquote>

<p>In particular, I am not sure how the statement is arrived at in $\mathbb{E}^2$, and for this reason I am unable to generalize it to $\mathbb{S}^2$.</p>

<p>EDIT: To make the question clear, I am asking for a proof or explanation of the statement quoted in $\mathbb{E}^2$, and maybe an idea for how to generalize it to $\mathbb{S}^2$. An explanation of the statement in $\mathbb{E}^2$ is sufficient for an accepted answer though. When I try constructing a graph the $(d-1)\frac{\pi}{3}$ makes sense and gives a lower bound for the angle at a vertex of degree $d$ but I don't know where it is coming from.</p>
",<graph-theory>
"<p>Suppose I have an linear transformation described by a square matrix $A$.
Then, $A$ takes a vector $x\in\mathbb{R}^n$ to a vector $y\in\mathbb{R}^n$ related to $x$ by the equation $y=Ax$.</p>

<p>Grant Sanderson defined the determinant of $A$ on his youtube series on linear algebra in the following way:</p>

<p>Two vectors $x$ and $y$ determine a unique paralelogram. On the same way $Ax$ and $Ay$ determine another paralelogram. The ratio between its areas is defined to be the determinant of $A$. That is $|\det(A)|=\text{Area}(Ax,Ay)/\text{Area}(x,y)$. (The sign of the determinant is given by the orientation of the vectors. If $A$ preserves orientation, its determinant is positive.)</p>

<p>Can't we define a determinant to non-square matrices by the same formula?</p>

<p>Link to Grant's video: <a href=""https://www.youtube.com/watch?v=v8VSDg_WQlA"" rel=""nofollow"">https://www.youtube.com/watch?v=v8VSDg_WQlA</a></p>

<p>EDIT: (Example)
Take 
$$A=\begin{bmatrix}
1 &amp; 2 \\
3 &amp; 4
\end{bmatrix}$$
and
$$x=\begin{bmatrix} 1 \\ 0\end{bmatrix}, \quad y=\begin{bmatrix} 0 \\ 1\end{bmatrix}.$$
Using the notation defined above, we clearly have $\text{Area}(x,y)=1$.
We have that $$Ax=\begin{bmatrix} 1 \\ 3\end{bmatrix}, \quad Ay=\begin{bmatrix} 2 \\ 4\end{bmatrix}$$ too. So, $\text{Area}(Ax,Ay)=2$. Then the ratio between the areas is $2$, which coincides with the modulus of the determinant of $A$. (Of course $A$ changes the orientation of the vectors, so it's determinant is negative.)</p>

<p>EDIT 2: Here's an explicit definition of the determinant.
$$|\det(A)| =
  \begin{cases}
    \text{Area}(Ax,Ay)/\text{Area}(x,y)       &amp; \quad \text{if } \text{kernel}(A) \text{ is zero}\\
    0  &amp; \quad \text{if } \text{kernel}(A) \text{ is non-zero}\\
  \end{cases},$$
for any linearly independent vectors $x$ and $y$.</p>

<p>If $A$ preserves orientation, then $\det(A)&gt;0$. $\det(A)\leq 0$ otherwise.</p>
",<linear-algebra>
"<p>Let $T$ be a linear operator on a vector space $V$ such that its matrix representation with respect to the basis $(v_1, v_2, v_3, v_4, v_5, v_6)$ is</p>

<p>\begin{bmatrix}
    1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 \\
\end{bmatrix}</p>

<p>Find a basis for $V$ such that the matrix associated to $T$ is</p>

<p>\begin{bmatrix}
    2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 2 &amp; 0 &amp; 0 &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 1 \\
    0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
\end{bmatrix}</p>

<p>From the first matrix, I can tell the following information:
$$T(v_1)=v_1,$$
$$T(v_2)=v_1+v_2,$$
$$T(v_3)=v_3,$$
$$T(v_4)=2v_4+v_5,$$
$$T(v_5)=2v_5+v_6,$$
$$T(v_6)=2v_6.$$</p>

<p>Does that information help me attain any information on the basis associated to the second matrix? These matrices look like they are in Jordan form (almost), but I don't know what to do with that information. Please help me! Thank you.</p>
",<linear-algebra>
"<p>Let $\left\{ q_1, q_2, q_3, q_4 \right\}$ be an orthonormal basis of an inner product space $H$, and let</p>

<p>$$H_1=\text{span} [q_1, q_2], \quad H_2=\text{span} [q_3, q_4]. $$</p>

<p>Let $T$ be an operator on $H$ that satisfies</p>

<p>$$ Tw= \alpha w \quad \forall w\in H_1, \quad \quad Tw'=I\alpha w' \quad\forall w'\in H_2, $$</p>

<p>where $\alpha \neq 0$ is a complex number and $I$ is the imaginary unit. Show that $T^4=\alpha^4 I$. Use this to give a closed form for $T^{-1}$.</p>

<p>Here is what I have done:</p>

<p>Since $\left\{ q_1, q_2, q_3, q_4 \right\}$ is an orthonormal basis of $H$, then $H=H_1 \oplus H_2$ $\,$ ($H$ is the direct sum of $H_1$ and $H_2$), since every vector in $H_1$ is orthogonal to every vector in $H_2$, and vice versa. </p>

<p>Let $v\in H$. Then we can uniquely express $v=w+w'$, where $w \in H_1$ and $w'\in H_2$. Then
$$Tv=Tw+Tw'=\alpha(w+iw'),$$
$$T^2v= \alpha T(w+iw')=\alpha^2(w-w'),$$
$$T^3v=\alpha^2T(w-w')=\alpha^3(w-iw'),$$
$$T^4v=\alpha^3T(w-iw')=\alpha^4(w+w')=\alpha^4v=\alpha^4Iv.$$</p>

<p>Hence, $T^4=\alpha^4 I$.</p>

<p>How can I use this to find a closed form for $T^{-1}$? Since $T^4=\alpha^4 I$ I know that $T^4$ is invertible, but I am not sure where to go from there. I appreciate any help.</p>
",<linear-algebra>
"<p>Define a series of matrices$$H_N=
\begin{bmatrix}
1&amp;1/N&amp;1/N&amp;\cdots&amp;1/N\\
1/N&amp;2&amp;1/N&amp;\cdots&amp;1/N\\
1/N&amp;1/N&amp;3&amp;\cdots&amp;1/N\\
\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\
1/N&amp;1/N&amp;1/N&amp;\cdots&amp;N
\end{bmatrix}$$
My question is, when $N\to+\infty$, would the eigenvalues of $H$ be different from $\{1,\ldots,N\}$ ?
The answer is not obvious, as,  for the matrix series
$$G_N=\begin{bmatrix}
1&amp;1/N&amp;1/N&amp;\cdots&amp;1/N\\
1/N&amp;1&amp;1/N&amp;\cdots&amp;1/N\\
1/N&amp;1/N&amp;1&amp;\cdots&amp;1/N\\
\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\
1/N&amp;1/N&amp;1/N&amp;\cdots&amp;1
\end{bmatrix}$$
You can verify that $G_N$ has an eigenvalue of $2$.</p>
",<linear-algebra>
"<p>Number of 2x2 matrix over $z_3$ with determinant 1 
I know number of elements in $z_3$ are {0,1,2} now possibly determinant can be 1 in this case </p>

<p>$$
\begin
{bmatrix}
1&amp;2\\
0&amp;1\\
\end{bmatrix}
$$
and there can be many more but how to find the exact number of such matrices? ""the answer is 24""</p>
",<linear-algebra>
"<p>If there are two subspaces of Euclidean space of dimension $n$, denoted as $R_1$ and $R_2$. Suppose that $M$ is $n$ by $n$ matrix, and suppose that $R_1$, $R_2$ and their orthogonal complement are invariant with left multiplication of $M$.</p>

<p>The problem is to prove that the orthogonal complement of intersection of $R_1$ and $R_2$ are invariant with $M$ as well.</p>
",<linear-algebra>
"<p>For each of the following linear operators:</p>

<ul>
<li>Choose a basis in each of L, M</li>
<li>Find the corresponding matrix of given linear operator D : L → M</li>
</ul>

<p>a) L = {f(x); f(x) is a polynomial, degree of f is ≤ 3}, </p>

<p>M = {f(x); f(x) is a polynomial, degree of f is ≤3 and f(1)=f′(1)=0},</p>

<p>D(f)=(x−1)2f′′</p>

<p>This is what work is done thus far, any suggestions after this?</p>

<p>f: ax<sup>3</sup> + bx<sup>2</sup> + cx + d</p>

<p>f ': 3ax<sup>2</sup> 2bx + c</p>

<p>f '': 6ax + 2b</p>

<p>f<sub>new</sub>: 6ab(x-1)<sup>2</sup> + 2b = 6a(x<sup>2</sup>-2x+1)+2b = 6ax<sup>2</sup>-12ax+6a+2b</p>
",<linear-algebra>
"<blockquote>
  <p>Consider a linear map $L:\mathbb{R}^3 \to \mathbb{R}^3$ with
  $L\left( {\bf x} \right) = \left[ {\begin{array}{*{20}{c}}
1&amp;0&amp;1\\
1&amp;1&amp;2\\
2&amp;1&amp;3
\end{array}} \right]{\bf x}$. Show that this $L$ is not one-to-one.</p>
</blockquote>

<p>I know this is not one-to-one because the kernel of $L$ is not $\{{\bf 0}\}$. However, if I want to stick with the one-to-one definition, I got some trouble there. Here is my thinking: Assume
$L({\bf x}) = L({\bf y})$, then I must show ${\bf x} = {\bf y}$. So pick 
${\bf x} = [x_1, x_2, x_3]^T$ and ${\bf y} = [y_1, y_2, y_3]^T$ both lives in $\mathbb{R}^3$.
Then from the hypothesis of $L({\bf x}) = L({\bf y})$, I got
$$\left[ \begin{array}{l}
{x_1} + {x_3}\\
{x_1} + {x_2} + 2{x_3}\\
2{x_1} + {x_2} + 3{x_3}
\end{array} \right] = \left[ \begin{array}{l}
{y_1} + {y_3}\\
{y_1} + {y_2} + 2{y_3}\\
2{y_1} + {y_2} + 3{y_3}
\end{array} \right]$$
I think this should gives me $x_i = y_i$ for $i=1,2,3.$ which means ${\bf x} = {\bf y}$. and I got $L$ one-to-one but this contradicts to the kernel statement...</p>

<p>Can anyone help to pointing out which part I went wrong. Thank you.</p>
",<linear-algebra>
"<p>In the last 2 lectures of linear algebra we have talked about linear mappings and other stuff, but I missed actually the last one and I am quite in bad situation.</p>

<p>What matrix transforms $\left(\begin{matrix} 1 \\ 0\end{matrix}\right)$ into $\left(\begin{matrix} 2 \\ 6\end{matrix}\right)$ and tranforms $\left(\begin{matrix} 0 \\ 1\end{matrix}\right)$ into $\left(\begin{matrix} 4 \\ 8\end{matrix}\right)$?</p>

<p>I think I understood what I need to find: a matrix that multiplies our initial matrix formed by our initial vectors $$\left(\begin{matrix} 1 &amp; 0 \\ 0 &amp; 1\end{matrix}\right)$$ </p>

<p>and the resulting matrix is:
$$\left(\begin{matrix} 2 &amp; 6 \\ 4 &amp; 8\end{matrix}\right)$$</p>

<p>Am I right?</p>

<p>Is there a way to automate this process? </p>
",<linear-algebra>
"<p>Like the title says, ""If an $n\times n$ matrix $A$ is diagonalizable and has only one eigenvalue $\lambda$ with multiplicity $n$, then $A = \lambda I$. True or False?"" </p>

<p>My gut is telling me that this is true, but I'm having a little difficulty proving it formally. There's probably a very obvious proof, but thus far it has alluded me. </p>

<p>In any case, this is what I've done:</p>

<p>Since $A$ is diagonalizable, there exists a factorization such that $A = S^{-1}\Lambda S$. Since we know the eigenvalues are $\lambda$ with multiplicity $n$, $\Lambda = \text{diag}(\lambda, \lambda, ...)$. </p>

<p>If I'm given $A = \lambda I$, it's easy to show $\lambda I = S^{-1}\Lambda S$ where $S = I$ and $\Lambda = \text{diag}(\lambda, \lambda, ...)$. (Having normalized the eigenvectors). The trouble I'm having is showing $A = \lambda I$ under the given conditions. </p>

<p>I can simply let $S = \lambda I$, and thus $A = \lambda I$, but something about this approach feels off. (This is just my line of thought right now).</p>

<p>Any input would be appreciated.</p>
",<linear-algebra>
"<p>I have just started auditing Linear Algebra, and the first thing we learned was how to solve a system of linear equations by reducing it to row echelon form (or reduced row echelon form). I know how to solve a system of linear equations using determinants, and so the using row echelon form seems to be very inefficient and an easy way to make mistakes.</p>

<p>The lecturer seemed to indicate that this will be useful in the future for something else, but a friend who has a PhD in mathematics said it is totally useless.</p>

<p>Is this true? Is this technique totally useless or is there some ""higher mathematics"" field/ technique where row echelon form will be useful?</p>
",<linear-algebra>
"<p>I am working a bit on a collection of Linear Algebra examples, 
as well as some examples on induction. This is what is taught freshman year at our university.</p>

<p>I intend to release this to the public, either by selling printed copies or releasing it online. </p>

<p>Since I do not have experience using such material myself, there are some questions I would like some opinions on:</p>

<ul>
<li>How much theory should I include? Is references to course litterature enough?</li>
<li>Is there a format preference? Small text, so that the collection is more enviromental-friendly, or with big marginals for notes?</li>
<li>Best way to deal with misprints?</li>
<li>Should induction and Linear algeba be separate pieces? </li>
</ul>

<p>Please share your experience if you have done something similar.</p>

<p>EDIT:
An answer I seek is something along the lines of:
""I am a ""something"" stident, and I prefer ""something"", and would like to see more of ""something"".</p>
",<linear-algebra>
"<p>I have a 2x2 matrix A with rows (1 0) and (1 1).  How can I find their matrix exponential, e^A ?</p>

<p>I understand I need to plug it into the Taylor series but I'm lost at how to solve the series here.</p>
",<linear-algebra>
"<p>A tutorial sheet has the following problem.</p>

<blockquote>
  <p>Find a unit normal vector and a basis for the tangent space of the
  following smooth manifold $M \subseteq \mathbb{R}^2$ at a point $(a,b) \in M$. $$M=\{(x,y) \in \mathbb{R}^2 : x^2+y^2=1\}$$</p>
</blockquote>

<p>The idea is to do it without finding an explicit parametrization for $M$ (although of course, that is quite easy.)</p>

<p>The solution sheet says this:</p>

<blockquote>
  <p>Have $f(x,y) = x^2+y^2 = 1$ so a normal is $\nabla f = (2x,2y) = (2a,2b)$ at $(x,y) = (a,b)$.</p>
  
  <p>Tangent space = $\mathrm{ker}(Df) = (\nabla f)^\perp = \{v \in \mathbb{R}^2 : v \cdot (2a,2b) = 0\}$ has basis $\{(-b,a)\}$.</p>
</blockquote>

<p>The stuff about normals makes sense, but I don't get the line about tangent spaces. In particular:</p>

<ol>
<li>What is the difference between $Df$ and $\nabla f$? Aren't they the same?</li>
<li>How do we get from $\mathrm{ker}(Df)$ to $(\nabla f)^\perp$? What is the general principle here?</li>
</ol>
",<linear-algebra>
"<p>Hi I am trying to work on a proof question that I took note of because the answer did not come to mind immediately. I also found it was a good question to illustrate something I do not understand very well.</p>

<p>The question asks to prove, or disprove that if the system $AX=0$ has infinite solutions, then so to does the system $AX=B$ for any choice of $B$.</p>

<p>First off, I get confused by ""For any choice of $B$"". I thought $B$ represented the column of constants? How could you choose any $B$? Is that not implying that $B$ can be $(a,b,c,,,n)$ for any $a,b,c,,n$, in the real numbers?</p>

<p>Anyways, in regards to the proof. I know that if $AX=0$ has infinite solutions, then there must be atleast one parameter, thus $(n-r)\geq 1$ where $n$ is the number of variables and $r$ the rank of the matrix. I am not sure where to go from here. </p>

<p>Hopefully you guys can help me with this and get a better understanding!</p>

<p>Thank you,</p>
",<linear-algebra>
"<p>Denote: $[A]$ as the span of $A$. <br></p>

<p>Theorem: Every linearly independent subset of a vector space is a subset of a basis of a space. <br></p>

<p><em>Proof:</em> Let $A$ be a linearly independent subset of a vector space $V$ and let $\,B\,$ be a basis of $\,V$. <br></p>

<p>Case 1. $B\subseteq [A]\,$. Then $\,[B]\subseteq [A]\,$. But $\,[B]=V\,$ , hence, $\,A=B\,$. <br></p>

<p>Case 2. $B\not\subseteq [A]$. Then there exists $\alpha \in B$ such that $\alpha \not\in [A]$.
Thus $A\cup \{\alpha\}$ is linearly independent. Repeat the argument until an enlarged set is produced that spans $V$.</p>

<blockquote>
  <blockquote>
    <p>My question is: In case 2, how is it that $A$ is contained in a basis $B$? I just can't get the idea in the <em>repeating the argument</em> part. Thanks for your help.</p>
  </blockquote>
</blockquote>
",<linear-algebra>
"<p>In my class we're learning vector spaces, and in the text book there's an example with no solution and it goes like this:</p>

<blockquote>
  <p>If the domain of functions $f$ and $g$ is $[-1,1]$ and if they are defined $f(x) = \arcsin\left(\displaystyle\frac{2x}{1+x^2}\right)$ and $g(x) = \arctan(x)$, then $(f,g)$ is linearly independent?</p>
</blockquote>

<p>I don't know how to prove this, if I can make a linear combination of one of them using the other it's dependent, but how should I go about doing that?</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>Let $P_3(R)$ be the vectorspace of all real polynomials $\le 3$, such that the polynomial $p(x)=a_0+a_1x+a_2x^2+a_3x^3$ and let T be the linear operator on $P_3(R)$ that we get by defining $T$ as $T(p(x))=(x^3+x)p''(x)-2x^2p'(x)$ for all polynomials $p(x)\in P_3(R)$. Now decide a basis for the nullspace and the columnvector-space(Is this the same as $Im\,T$ in  english?) for $T$ or show that is only contains the zero-polynomial. Furthermore, find all eigenvalues for $T$, and examine if $T$ is diagonalizable(correct translation?)?</p>

<p>$p'(x)=a_1+2a_2x+3a_3x^2$, $p''(x)=2a_2+6a_3x$ </p>

<p>$Attempt:$
$T(a_0+a_1x+a_2x^2+a_3x^3)=(x^3+x)(2a_2+6a_3x)-2x^2(a_1+2a_2x+3a_3x^2) = x(2a_2)+x^2(-2a_1-4a_2+6a_3)+x^3(2a_2)=a_0(0)+a_1(-2x^2)+a_2(x-4x^2+2x^3)+a_3(6x^2)$</p>

<p>For the nullspace, let us find the solutions to $T(a_0+a_1x+a_2x^2+a_3x^3)=0$
Which means that $2a_2=-2a_1-4a_2+6a_3=2a_2=0 \to a_1=a_2=a_3=0$ If this is true then $a_0=0$. And the nullspace only contains the zero-polynomial. Im confused here, is this correct?   </p>

<p><em>Edit</em>: After correcting the computation mistake - I tried to convert to matrix-format, and got that the nullspace is spanned by $[1,3x+x^3]$. If this is correct, I should be able to continue.</p>

<p>For the $Im(a)$(?) we have $a_0(0)+a_1(-2x^2)+a_2(x-4x^2+2x^3)+a_3(6x^2)$ so we get the basis $[-2x^2, x-4x^2+2x^3,6x^2]$. I dont really understand this, and it´s probably wrong.</p>

<p>To find the eigenvalues we need to get this in matrix-form, right? How do we translate it into a matrix? If I could do that it would probably be easier for me to uderstand the basis aswell...</p>
",<linear-algebra>
"<p>Let $K/k$ be an extension of fields and let $v_1,\ldots,v_r,u_1,\ldots,u_r\in k^n$.  If the span of the $v$'s over $K$ equals the span of the $u$'s over $K$, must the two spans also be equal over $k$?</p>

<p>$$_K\langle v_1,\ldots,v_r\rangle=_K\langle u_1,\ldots,u_r\rangle\qquad\overset{?}\Longrightarrow\qquad_k\langle v_1,\ldots,v_r\rangle=_k\langle u_1,\ldots,u_r\rangle$$</p>

<p>Here is another way to look at this question: Given a subspace $U\subset K^n$ with some basis in $k^n$, is the procedure of restricting scalars to $k$ well-defined, or does it depend on the choice of basis of $U$?</p>
",<linear-algebra>
"<p>The information I have is for a matrix transformation from R^3 to R^3 (denoted by L()), L(a_1) = 3(a_1) and L(2(a_1))= (5,-3,6). Find L(3a_1-22a_1), L(-4a_1), L(0), L(4a_1).</p>

<p>What I tried to do was first solve for a_1. I factored out the two of L(2a_1) and then replaced L(a_1) with 3a_1. Resulting in the equation 2*3a_1 = (5,-3,6) so a_1 = (5/6,-1/2,1). </p>

<p>Now I tried to reduce L(3a_1 - 22a_1) to -19L(a_1) but that was not the correct answer, tried the same process with L(-4a_1)  = -2L(2a_1) which was also not the right result. </p>

<p>Not sure about L(0), I am assuming the zero represents the zero vector for R^3, but I would think that transforms to the zero vector but apparently not. </p>

<p>I'm sure I am misunderstanding the theory somewhere but I am not sure where. One thought I had was that I was using the properties of linear transformations but the problem only specified that it was a ""matrix transformation"". Any help is greatly appreciated. </p>
",<linear-algebra>
"<p>What is the derivative of Hadamard product of two matrices with respect to one of them?
I.e. what is $D(AB)$ with respect to $A$?</p>
",<linear-algebra>
"<p>Let $\ell^\infty$ be the bounded sequence space over the complex numbers and let $c_0$ the subspace of all sequences converging to $0$. </p>

<p>I am attempting to show that $\ell^\infty/c_0$ has infinite dimension. </p>

<p>I have looked at several different ways to show this but I think perhaps the easiest is to show that there exists an infinite linearly independent set in the quotient. It is easy to find an infinite countable set in $\ell^\infty$ (just take the sequences of the form $(0,\dots,0,1,0,\dots)$, but of course this set is identified in the quotient). I also was thinking of doing a proof by contradiction, and show that no matter what finite linearly independent set in $\ell^\infty/c_0$ you give, we can construct a sequence not in the span. But the precise way to construct such a sequence is not evident to me at this point (some sort of diagonal argument, perhaps).  </p>

<p>Will a cardinality argument be necessary?</p>
",<linear-algebra>
"<p>For the dynamic system $\dot x = Ax + Bu$ </p>

<p>There's a saying that this system is controllable when $Ker(B) \in Ker(A)$, which means that $u$ have the control in every dimension of $x$.</p>

<p>I have no problem with this theorem when you have single input $u$, which means $u$ is just a number and $B$ is with $n\times1$ dimension.</p>

<p>But I cannot completely understand when $u$ is a vector. Look at this example:</p>

<p>$A = \begin{bmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\ 
                     0 &amp;0&amp;0&amp;0\\0 &amp;0&amp;0&amp;0\\0 &amp;0&amp;0&amp;0\\ \end{bmatrix}$ , 
$B = \begin{bmatrix} 0&amp;0\\0&amp;0\\0&amp;0\\0&amp;0\\\end{bmatrix}$</p>

<p>This is obvious uncontrollable. However, look at the null space:</p>

<p>$Ker(A)=span\{\begin{bmatrix} 0\\1\\0\\0\\ \end{bmatrix},
            \begin{bmatrix} 0\\0\\1\\0\\ \end{bmatrix},
            \begin{bmatrix} 0\\0\\0\\1\\ \end{bmatrix} 
            \}$</p>

<p>$Ker(B)=span\{\begin{bmatrix} 1\\0 \end{bmatrix},
              \begin{bmatrix} 0\\1 \end{bmatrix}
            \}
$</p>

<p>Thus we have $Ker(B) \notin Ker(A)$, but this system is uncontrollable.</p>

<p>Can anyone help me with this contradictory?</p>

<p>Thank you very much and I appreciate all of your replies!</p>
",<linear-algebra>
"<p>Let $A \in \mathbb{C}^{n \times n}$ be an upper triangular matrix that satisfies $A^{*}A=AA^{*}$. Prove that $A$ must be diagonal.</p>

<p>My attempt is to partition $A$ as follows:</p>

<p>$$
A = \left[\begin{array}{cc} a_{11} &amp; \alpha\\
0 &amp; \hat{A}\end{array}\right]
$$</p>

<p>where $\alpha = (a_{12}, a_{13}, \cdots , a_{1n})$ and $\hat{A}$ is $A$ with the first row and column removed. Using this partitioning, we have:</p>

<p>$$
A^{*}A = \left[\begin{array}{cc} a_{11}^{2} &amp; a_{11}\alpha\\
a_{11}\alpha^{*} &amp; \alpha^{*}\alpha + \hat{A}^{*}\hat{A}\end{array}\right]
$$
$$
AA^{*} = \left[\begin{array}{cc} a_{11}^{2} + \alpha\alpha^{*} &amp; \alpha\hat{A}^{*}\\
\hat{A}\alpha^{*} &amp; \hat{A}\hat{A}^{*}\end{array}\right]
$$</p>

<p>Examining entry (1,1) of each of these matrix products, we see that $\alpha\alpha^{*} = 0$. From this, I would like to conclude that $\alpha = 0$ and thus, the first row of $A$ has non-zero entry only at (1,1). Then repeat this process continuously on $\hat{A}$.</p>

<p>However, I can see a flaw in my argument. If the entries of A were real, then this argument seems like it would work. But since the entries can be complex, this means that $\alpha\alpha^{*} = 0$ even with $\alpha \ne 0$. For example, $\alpha = (1, i, 1, i)$ gives $\alpha\alpha^{*} = 0$.</p>

<p>Any ideas how to proceed here? I know that with this partitioning, I must have $\alpha = 0$ since the problem statement is true (i.e. $A$ is diagonal).</p>
",<linear-algebra>
"<blockquote>
  <p>Let $\mathbb{R}^3$ be a vector space with canonical product, and $\alpha \in \mathbb{R}$. For each $\alpha$ let $F_{\alpha}=\{(x,y,z) \in \mathbb{R}^3: x=\alpha y \wedge \alpha y=\alpha z\}$ be a subspace.</p>
  
  <p>Find a basis for $F_{\alpha}$, based on $\alpha$.</p>
</blockquote>

<p>My work was:</p>

<p>$$
\begin{cases}
x=\alpha y \\
\alpha y=\alpha z
\end{cases} \Leftrightarrow\begin{cases}
x=\alpha y \\
\alpha y-\alpha z=0
\end{cases} \Leftrightarrow\begin{cases}
x=0 \\
\alpha=0 
\end{cases} \vee \begin{cases}x= \alpha z \\y=z \end{cases} $$</p>

<p>If $\alpha=0$ then $F_{0}=\{(0,y,z): y,z \in \mathbb{R}\}$. So $F_{0}=\langle (0,1,0),(0,0,1)\rangle$.</p>

<p>If $\alpha \neq0$ then $F_{\alpha}=\{(\alpha z,z,z): \alpha, z \in \mathbb{R}\}$. So $F_{\alpha}=\langle (\alpha,1,1)\rangle$.</p>

<p>But now which will be the basis of $F_{\alpha}$ based on $\alpha$ ? Thanks.</p>
",<linear-algebra>
"<p>I have determined a tangent plane to be
$$z = a(-b+y) + x(b-1)$$
$$ab = x(b-1) + ay - z$$
At the point (a,b)</p>

<p>I want to determine the normal to this plane as a function of a and b. I am not entirely sure as to what this means ""As a function of a and b"".</p>

<p>So I assume I will find the normal using the dot product of plane to normal = 0.
$$(b-1, a,-1) * (\zeta,\beta,\gamma) = 0$$
$$\zeta(b-1) + \beta*a - \gamma = 0$$
$$-\frac{\zeta(b-1) + \gamma,}\beta = a$$
$$\frac{a\beta - \gamma,}\zeta + 1 = b$$</p>

<p>I am not sure if this is what is wanted, or if there is a better way to solve this. If anyone can shed some light, that would be appreciated. This is from a past exam 2012S2 UQ, for course 'Multivariate Calculus and Ordinary Differential Equations'.</p>
",<linear-algebra>
"<p>Is it true that If the 2-norm of a symmetric real matrix is small, then the trace of the matrix is also small? I played around with some matrices in MATLAB and discovered this phenomenon. Does there exist any theorem that relates the 2-norm of a symmetric real matrix to the magnitude of its trace? </p>

<p>Thanks.</p>
",<linear-algebra>
"<p>Consider 
$$ S = \left\{(x,y) \in \mathbb{R}^2; -N-\frac{1}2 \le x \le N + \frac{1}2, |\alpha x-y| \le \frac{1}N \right\}$$</p>

<p>where $N \in \mathbb{N}, \alpha \in \mathbb{R}$.</p>

<p>I'm having a hard time beliveing that $S$ is convex and Area $S &gt; 4$. Can someone please elaborate why? </p>

<p>Thanks.</p>
",<linear-algebra>
"<p>So, I used Gaussian elimination on this matrix</p>

<p>$$\left( \begin{array}{c} -1 &amp; 3 &amp; 5 &amp; 13 \\ 3 &amp; -2 &amp; 2 &amp; 16 \end{array}\right)$$</p>

<p>to turn it to this:</p>

<p>$$\left( \begin{array}{c} -1 &amp; 3 &amp; 5 &amp; 13 \\ 0 &amp; 7 &amp; 17 &amp; 55 \end{array} \right)$$</p>

<p>I don't think this could be eliminated any further. </p>

<p>Which gives me these two equations:</p>

<p>$$-1a + 3b + 5c = 13$$</p>

<p>$$          7b + 17c = 55$$</p>

<p>Is there another method after this to simplify finding what the variables are, or is the only way to guess and check?</p>

<p>Thank you.</p>
",<linear-algebra>
"<p>If I have a set $S$ of vectors, say,
$$S=\{[4,6,2],[8,3,8],[2,4,5] \}.$$</p>

<p>How can I get all linear combinations of these vectors such that the linear combination will equal $[180,180,90]$ ?</p>

<p>edit:</p>

<p>I should be more specific, I am concerned with a set $S$ of arbitrary size. if the size of $S$ is greater than $3$, I will have free variables in my solution, right? Doesn't that mean that there are infinitely many solutions to the equality</p>

<p>suppose I am interested in all of those such that all constant multiples of the vectors in $S$ are greater than $0$. How would I go about finding the existence, and the exact values of such constants.</p>
",<linear-algebra>
"<p>Assume $A,B \in M_{n\times m}(\Bbb{R})$,and $A^TA=B^TB$,show that there exists an orthogonal matrix $P$, such that $A=PB$. </p>
",<linear-algebra>
"<p>Let $A$ be a linear operator which acts on the vector space $V=\langle x_1,x_2, \ldots,x_n\rangle$  by permutation of the basis vectors. Suppose we know its eigenvalues ( some roots of unity  ): $\lambda_1, \lambda_2, \ldots, \lambda_n.$</p>

<p>Now consider the vector space $V^{(2)} \subset {\rm Sym}^2 V$ generated by elements $x_i x_j, i&lt;j,$ $\dim V^{(2)}=\binom{n}{2}.$ Let us expand the operator $A$ on $V^{(2)}$    by linearity and by $A(x_i x_j)=A(x_i)A(x_j)$. Denote the extension by $A^{(2)}$. It is clear that  $A^{(2)}$ permutes the basis vectors of   $V^{(2)}$ so $A^{(2)}$   is an endomorphism of $V^{(2)}$.</p>

<p><strong>Question.</strong> What is the trace  of the $A^{(2)}?$ </p>

<p>By  method of trial and error I have found a formula for the trace
$$
{\rm Tr}(A^{(2)})=\sum_{i=1}^n\lambda_i^2+\sum_{i&lt;j}\lambda_i \lambda_j-\sum_{i=1}^n \lambda_i
$$
but I can't prove it. Any ideas?</p>
",<linear-algebra>
"<p>We need to determine values of k for which these have zero, one and infinite solutions.
$$x-2y=1,x-y+kz=-2,ky+4z=6$$
Now what I did:
$$A=\begin{pmatrix}1&amp;-2&amp;0\\1&amp;-1&amp;k\\0&amp;k&amp;4\end{pmatrix}$$ and $|A|=4-k^2$. When it has zero solution or when it is inconsistent we have $k^2=4\implies k=\pm 2$</p>

<p>When it is consistent or has one or infinite solutions then $|A|\ne0\implies k\ne\pm2$. How to differentiate between one and infinite solutions case of k.</p>

<p>I have also made rref:
$$M\sim\left[\begin{array}{ccc|c}1&amp;-2&amp;0&amp;1\\1&amp;-1&amp;k&amp;-2\\0&amp;0&amp;4-k^2&amp;6+k\end{array}\right]\sim\left[\begin{array}{ccc|c}1&amp;-2&amp;0&amp;1\\0&amp;1&amp;k&amp;-3\\0&amp;0&amp;4-k^2&amp;6+k\end{array}\right]\sim\left[\begin{array}{ccc|c}1&amp;-2&amp;0&amp;1\\0&amp;1&amp;k&amp;-3\\0&amp;0&amp;1&amp;\frac{6+k}{4-k^2}\end{array}\right]$$</p>
",<linear-algebra>
"<p>Given a $n \times n$ matrix $A = \begin{bmatrix}
a_1 &amp; a_2 &amp; \dots &amp; a_n
\end{bmatrix}$, where each $a_i$ are columns of $A$ for $i = 1 \dots n$.</p>

<p>Column mean is calculated by $\bar{a} =(a_1 + a_2 + \dots + a_n)/n$.</p>

<p>Then, define:</p>

<p>$B = \begin{bmatrix}
a_1 - \bar{a} &amp; a_2 - \bar{a}&amp; \dots &amp; a_n - \bar{a}
\end{bmatrix}$.</p>

<p>May I know why the rank$(B)$ is $n-1$?</p>

<p>I did try many times with computer program to prove but I want a proof of this.</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>I have the following 2 equations:</p>

<p>${6x + 9y = 3}$</p>

<p>${6x -3y = -2}$</p>

<p>The textbook asks to use the substitution method so I would appreciate if we stuck to this method, I could use the addition method but the book asks for this method.</p>

<p>So my steps of doing this are to isolate x in the first equation:</p>

<p>${6x + 9y = 3}$</p>

<p>I then divide both sides by 3 to give</p>

<p>${2x + 3y = 1}$</p>

<p>I then get:</p>

<p>${x = {-3y + 1\over 2}}$</p>

<p>I'm not entirely sure you to take this further using the substitute method.</p>
",<linear-algebra>
"<p>What is the generalization of the Pauli matrices and Dirac matrices in higher dimensions? I am actually looking for $\sqrt{\mathbb{I}}$ but I can't use the principal root which is just $\mathbb{I}$. For $n=3$, I found that the Gell-Mann matrices don't work. I'm thinking that these actually are the $SU(n)$ generators. Any ideas?</p>
",<linear-algebra>
"<p>I'm wondering about this problem on bilinear forms :</p>

<p>We have $\phi : \mathbb{M_{n}(R)}*\mathbb{M_{n}(R)} \rightarrow \mathbb{R}$ $$(A,B) \rightarrow trace(AB)$$</p>

<p>I've proved $\phi$ is a bilinear form and symetric, but how could we do to prove it is a non generate form ? </p>

<p>Besides if we have $\mathbb{S_{n}(R)}$ the subspace of $\mathbb{M_{n}(R)}$ formed by symmetric matrix, prove the restriction of $\phi$ to $\mathbb{S_{n}(R)}*\mathbb{S_{n}(R)}$ is also a non degenerate form, and then determine the orthogonal of $\mathbb{S_{n}(R)}$ for $\phi$</p>

<p>Thanks</p>
",<linear-algebra>
"<p>First of all, i don't know if the correct word is normalise or not, but I'll try to explain my issue.</p>

<p>I have a relationship between an object <code>A</code> and an object <code>B</code> equals 0.5 (max is 1)</p>

<p>you can consider this relationship as a vector that its length is 0.5</p>

<p>I have another vector between <code>A</code> and <code>C</code> and its value is 0.3</p>

<p>I have another vector between <code>A</code> and <code>D</code> and its value is 0.2</p>

<p>so these are the values that I have:</p>

<pre><code>A -&gt; B = 0.5
A -&gt; C = 0.3
A -&gt; D = 0.2
</code></pre>

<p>Now I want to <strong>give more weight to the relationships</strong>. In other words, I really care about <code>A -&gt; B</code> a lot, but I don't care so much about <code>A -&gt; C</code></p>

<p>so I want to give weight to those numbers, the weights are as this:</p>

<pre><code>the value of A -&gt; B should be multiply by 4
the value of A -&gt; C should be multiply by 1 (so no change)
the value of A -&gt; D should be multiply by 2
</code></pre>

<p>my problem is that if i multiply <code>A -&gt; B</code> by 4, then the result is 2, which is bigger than 1. all the values must be between 0 and 1.</p>

<p>my question is how (and what is the abstract equation) to normalise all these values in a correct way?</p>

<p>Regards</p>

<h3>update 1</h3>

<p>In the example I gave to you, it was co incidence that the sum of the values is 1, that's not necessary at all</p>
",<linear-algebra>
"<blockquote>
  <p>Let $N\in \text{Mat}(10 \times 10,\mathbb{C})$ be nilpotent. Furthermore let $\text{dim} \ker N =3 $, $\text{dim} \ker N^2=6$ and $\text{dim} \ker N^3=7$. What is the Jordan Normal Form?</p>
</blockquote>

<p>The only thing I know is that there have to be three blocks, since $\text{dim} \ker N = 3$.</p>

<p>Thank you very much in advance for your help.</p>
",<linear-algebra>
"<p>Let $A\in M_n$ and $\operatorname{rank}A=k$. Is the following true?</p>

<blockquote>
  <p>There are $A_i\in M_n$ ($i=1,...,k$), such that $\operatorname{rank}A_i=1$ and $A=A_1+....+A_k$.</p>
</blockquote>
",<linear-algebra>
"<p>There is an array which contains points as shown below;</p>

<pre><code>[ -0.0249795, -0.00442094, -0.00397789, -0.00390947, -0.00384182, -0.0037756, -0.00371057, 0.00180882, 0.00251853, 0.00239539, 0.00244367, 0.00249255, 0.00254166, 0.00259185, 0.0116467, 0.0155782, 0.016471 ]
</code></pre>

<p>First of all, honestly, i don't know whether there is a measurement of nonlinearity or not. If there is, i would like to know what that's name is.</p>

<p>So how can i calculate the linearity or nonlinearity of this points distribution. I mean, after you draw a line from these points, how much will the line be linear and non-linear?</p>

<p>e.g. some line points, <code>p1= [1,-0.0249795], p2= [2, -0.00442094] ...</code></p>
",<linear-algebra>
"<p>Given a basis $U$, what conditions are needed for an orthogonal basis for it?</p>

<p>For example, in the following vector space $U$, if $U =sp\{(1,1,1),(1,3,7)\}$ then what conditions are needed for an orthogonal basis for it? </p>

<p>Is it enough to have a basis of dimension $2$ that's orthogonal? or are there more conditions? </p>

<p>EDIT: If for example I find an orthogonal span of dimension 2, say $V=sp\{(1,1,1), v_2\} $ such that $v_2$ is orthogonal to $(1,1,1)$, is any vector that's orthogonal to $(1,1,1)$ fine for it to be an orthogonal span for $U$?</p>

<p>PS: I know there's GS algorithm, but I'm asking if other bases that we get in other ways are also fine.</p>
",<linear-algebra>
"<p>$Tr(XY) = 1$ and $Tr(Y) = 1$ implies that $Tr(X) = 1$.</p>

<p>I tried to prove by contradiction and switch the dummy variable of $X$ and $Y$. But I don't think my approach is right and if there is any much easier proof.</p>
",<linear-algebra>
"<p>Question 3: (6 pts) A company makes 3 kinds of snacks, using almonds and raisins. The Fruity snack
contains 100g of almonds and 300g of raisins. The Nutty snack contains 300g of almonds and 100g of
raisins. The Variety snack contains 200g of almonds and 200g of raisins. There are currently 900g of
almonds and 700g of raisins available, and we want to determine how many of each kind of snacks can be
made so that all the ingredients are completely used.
a) Define variables and set up a linear system in order to solve the problem.
b) Find all the realistic solutions to the problem.</p>

<p>I answered A)</p>

<pre><code>100x + 300y + 200z = 900

300x + 100y + 200z = 700
</code></pre>

<p>How do I do B)?</p>
",<linear-algebra>
"<p>Let $T :R2[X]→R2[X]$be linear and such that$T(1)=1+X$,$T(X)=X+X^2$ and
$T (X^2) = 1 − X^2$. Is $T$ an isomorphism?</p>

<p>ok so Ive noticed that $T(1)-T(X)= 1- X^2$ which is $T(X^2)$, but that disproves that its linear, which is already given, also how is this related to it being an isomorphism, am I missing something here or?</p>
",<linear-algebra>
"<p>Given two $n \times n$ symmetric matrices $A$ and $B$, is there a generic way to construct a larger block matrix $M$ such that $\det(M) = \det(A) - \det(B)$?</p>

<p>A simple block expression is desired, in the sense that the block components of $M$ are constant matrices or obtained by solving matrix equations involving $A$ and $B$.  Constructions of $M$ involving short algebraic expressions for its components in terms of the components of $A$ and $B$ would also be interesting, but not something that expands to an exponential number of terms in the components of $A$ and $B$ like just sticking $\det(A)$ in as a term of $M$.</p>

<p>If this is not possible in the general case, what restrictions can be placed on $A$ and $B$ to make this possible?</p>

<p>The only case I know of is when the difference of $B$ and $A$ can be written as a product of a column vector $C$ and its transpose: $B-A = CC^T$. This allows us to construct a matrix $M$ such that:</p>

<p>$$ M = \begin{bmatrix} A &amp; C \\ C^T &amp; 0 \end{bmatrix} $$
$$ \det(M) = \det(A) - \det(A + CC^T) = \det(A) - \det(B) $$</p>

<p>I'm curious if there is some way to construct an appropriate block matrix to make this possible for arbitrary symmetric matrices $A$ and $B$.</p>

<p>The first comment to this question<br>
<a href=""http://math.stackexchange.com/questions/1834227/find-a-matrix-with-determinant-equals-to-deta-detd-detb-detc"">Find a matrix with determinant equals to $\det{(A)}\det{(D)}-\det{(B)}\det{(C)}$</a><br>
suggests the answer is trivial by choosing
$$M = \begin{bmatrix} A &amp; B \\ I &amp; I \end{bmatrix}$$
but that doesn't appear to work when I tried some numerical examples.</p>
",<linear-algebra>
"<p>I need help in formulating an optimization problem. I have a system of equations as follows:<br>
    $c_1x_1+c_2x_2+c_3x_3=1$<br>
    $b_1x_1+b_2x_2+b_3x_3=1$<br>
    $a_1x_1+a_2x_2+a_3x_3=1$<br>
In my case the system is not necessarily in three variables (the number of variables can increase).</p>

<p>The chosen $x_1$, $x_2$, $x_3$ should also satisfy the following system of equations:<br>
    $c_1'x_1+c_2'x_2+c_3'x_3&lt;1$<br>
    $b_1'x_1+b_2'x_2+b_3'x_3&lt;1$<br>
    $a_1'x_1+a_2'x_2+a_3'x_3&lt;1$<br>
where<br>
$0&lt;c_1'&lt;c_1$, $0&lt;c_2'&lt;c_2$, $0&lt;c_3'&lt;c_3$<br>
$0&lt;b_1'&lt;b_1$, $0&lt;b_2'&lt;b_2$, $0&lt;b_3'&lt;b_3$<br>
$0&lt;a_1'&lt;a_1$, $0&lt;a_2'&lt;a_2$, $0&lt;a_3'&lt;a_3$  </p>

<p>I have no idea where should I look for a solution. Any pointers in the right direction would also be appreciated.</p>

<p><strong>UPDATE</strong>: I understand that the constraints $0&lt;c_1'&lt;c_1$, $0&lt;c_2'&lt;c_2$, $0&lt;c_3'&lt;c_3$ would form a rectangular area and the value of $x_1$, $x_2$, $x_3$ should be chosen so that over this rectangular area the inequalities should be met. But I don't know how to formulate the constraints into an equation.</p>
",<linear-algebra>
"<p>The non-homogenous system is as follows:
$$3x+2y+5z=10\\
3x-2y=7\\
6x+4y-10z=k$$</p>

<p>I have determined that:
$$z=1-\frac{k}{20}\\
y=\frac{k}{16}-\frac{1}{2}\\
x=2+\frac{k}{24}$$</p>

<p>What are the values of $k$ to form a matrix which is inconsistent and then consistent?</p>
",<linear-algebra>
"<p>T:V→V ; prove that if T*T = T ⇒ ImT ∩ NullT = 0v and V = ImT ⊕ NullT.</p>

<p>let v∈V.</p>

<p>T(T(v)) = T(v) ⇒ T(v) = v</p>

<p>⇒ T is the identity transformation.</p>

<p>⇒ DimNullT = 0 ⇒ ImT ∩ NullT = 0v ⇒ V = ImT ⊕ NullT</p>

<p>Is the answer correct?</p>

<p>Also does an example exist where the ImT ∩ NullT ≠ 0v? Isn't the intersection empty by definition?</p>
",<linear-algebra>
"<p>Let $x=(a,b)$, where $a,b$ are in $N$</p>

<p>Now we have the transformations:
$$T_1(x) = (ka, b+1)$$ 
$$T_2(x) = (b,a)$$ where $k$ is in $N$.
Where the order of choosing a transformation is not fixed. 
(E.g. you can first apply 3 times $$T_1(x)$$, then $$T_2(x)$$</p>

<p>Will we for all $(a,b)$ produce $a=b$ by only being allowed to use these two transformations? If so, is this true for every $k$? I'm specifically interested in the case $k=2$ though.</p>
",<linear-algebra>
"<p>I know that the following statement shouldn't be true :</p>

<blockquote>
  <p>Let $A$ be a real $n\times n$ matrix which satisfies $f_A(x)=(x+3)^2(x-1)^2$ and $m_A(x) \ne f_A(x)$ then $A$ is diagonalizable over $\mathbb R$. </p>
</blockquote>

<p>Notice that $f_A(x)$ is the characteristic polynomial of $A$ and $m_A(x)$ is the minimal polynomial of $A$.</p>

<p>I'm trying to look for a counter-example but I can't find any. I tried to pick some diagonal matrices but I found out it cannot be an appropriate counter-example because a diagonal matrix is always diagonalizable. Besides that, all I tried is some other guessing where the diagonal contains $-3,-3,1,1$ with different combinations of $1$'s on the upper diagonal and the lower diagonal.</p>

<p>Is there any way to construct such a matrix?</p>

<p>$\underline {\mbox {Note:}}$</p>

<p>I know that the opposite statement is correct (that is, if we knew that $A$ is diagonalizable over $\mathbb R$ then $m_A(x) \ne f_A(x)$ because $m_A(x)$ must be simple in that case and we know $f_A(x)$ is not simple).</p>
",<linear-algebra>
"<p>A study buddy and I were going through this question and while we made some progress, we were ultimately unsure if what we were doing was correct or not.</p>

<p>The question is:</p>

<p>Let <em>A</em> be a diagonalizable matrix and let <em>X</em> be the diagonalizing matrix. Show that the column vectors of <em>X</em> correspond to the nonzero eigenvalues of <em>R</em>(<em>A</em>).</p>

<p>What we did was this:</p>

<p>Say that <em>A</em> is <em>n</em> x <em>n</em>. Therefore <em>X</em> must be <em>n</em> x <em>n</em> and a matrix <em>D</em> must be <em>n</em> x <em>n</em> since if A is diagonalizable and X is the diagonalizing matrix:</p>

<p><em>A</em> = <em>X</em> <em>D</em> <em>X</em> $^{-1}$</p>

<p>Since <em>X</em> is invertible, its eigenvectors must be linearly independent. Therefore, because <em>X</em> is <em>n</em> x <em>n</em>, there are <em>n</em> linearly independent eigenvectors. These eigenvectors span $\mathbb{R}$$^n$.</p>

<p>Because of this, <em>R</em>(<em>A</em>) $\subset$ $\mathbb{R}$$^n$, we've shown that the the column vectors of <em>X</em> correspond to the nonzero eigenvalues of <em>R</em>(<em>A</em>)... except we haven't. My buddy and I got stuck here and we weren't sure what to do since we haven't accounted for the eigenvalues that are equal to 0.</p>

<p>Thanks for any help in advance. </p>
",<linear-algebra>
"<p>Suppose that $V$ is an n-dimensional vector space over a field $F$ and $\{\vec{v_1}, ...,\vec{v_m}\}$ is a linearly independent set in $V$. How do I show that $m \leq n$ and $\exists \{\vec{v_{m+1}},...\vec{v_n}\} $ such that $\{\vec{v_1}, ...,\vec{v_m},\vec{v_{m+1}},...\vec{v_n}\}$ is a basis for $V$? </p>

<p>The book I'm using gives a hint involving using the basis for $V$ and creating the set of vectors composed of $\{\vec{v_1}, ...,\vec{v_m}\}$ and the basis of $V$, but i don't really understand what it is asking with that.</p>
",<linear-algebra>
"<p>My intuition says no. But what if W is a zero vector?</p>
",<linear-algebra>
"<p>Suppose $A$ is a $4×4$ matrix over $C$ s.t. $Rank(A)=2$ and $A^3=A^2\neq0$. If $A$ is not diagonalizable then how to prove that:</p>

<p>There exists a vector $v$ s.t. $Av\neq 0$ and $A^2v=0$.</p>

<p>I know it is to be proved that $Imsp(A)$ is contained in $Nullsp(A)$, but really got no clue how to approach.</p>

<p><strong>My work:</strong>
$x^2(x-1)$ is  the annihilating polynomial for $A$, but I am stuck in finding the characteristic polynomial. The only two possibilities are $x^3(x-1)$ and $x^2(x-1)^2$, but how to reject the later one?</p>

<p>Thanks for any hint.</p>
",<linear-algebra>
"<p>I know that the component of <code>x</code> along <code>u</code> is $\frac{u.x}{|u|}.$ ($x$,$u$ are both vectors) but my teacher said that this is equal to $u^T x u$ (||u|| = 1). I understand the first formula, But I cant understand the second one well.  </p>
",<linear-algebra>
"<p>Both matrix multiplication and quaternion multiplication are non-commutative; hence the use of terms like ""premultiplication"" and ""postmultiplication"". After encountering the concept of ""quaternion matrices"", I am a bit puzzled as to how one may multiply two of these things, since there are at least four ways to do this.</p>

<p>Some searching has netted <a href=""http://en.cnki.com.cn/Article_en/CJFDTOTAL-LXXB198402006.htm"">this paper</a>, but not having any access to it, I have no way towards enlightenment except to ask this question here.</p>

<p>If there are indeed these four ways to multiply quaternion matrices, how does one figure out which one to use in a situation, and what shorthand might be used to talk about a particular version of a multiplication?</p>
",<linear-algebra>
"<p>I need to find eigenvalues/eigenvectors of different kinds of $n \times n$ matrices. For example, how would I determine these for the matrices listed below? What is the typical process? Should I always go by the route of finding eigenvalues by finding roots of characteristic polynomial and then getting eigenvectors by solving $(\mathbf{A} - \lambda \mathbf{I})\mathbf{x} = 0$?<br><br></p>

<p>$\begin{bmatrix}
2&amp;0&amp;0\\ 1&amp;2&amp;0\\ 
0&amp; 1 &amp; 2
\end{bmatrix}
$
 <br><br>
$\begin{bmatrix}
4 &amp;1  &amp;1  &amp;1 \\ 
 1&amp;4  &amp;1  &amp;1 \\ 
 1&amp;1  &amp;4  &amp;1 \\ 
 1&amp;  1&amp;  1&amp; 4
\end{bmatrix}$ <br><br>
These are just examples. Typically I want to find eigenvectors of $n \times n$ matrices. If you can show me the process of finding solution of one of these matrices, that would be helpful.</p>
",<linear-algebra>
"<p>How would you find eigenvalues/eigenvectors of a $n\times n$ matrix where each diagonal entry is scalar $d$ and all other entries are $1$ ? I am looking for a decomposition but cannot find anything for this. <br>For example:</p>

<p>$\begin{pmatrix}2&amp;1&amp;1&amp;1\\1&amp;2&amp;1&amp;1\\1&amp;1&amp;2&amp;1\\1&amp;1&amp;1&amp;2\end{pmatrix}$</p>
",<linear-algebra>
"<p>Consider $n$ real, symmetric, and positive semi-definite matrices as: $A_1,A_2,\cdots,A_n$. These matrices are convertible to each other under appropriate permutation ($A_i(p_i,p_i)=A_j$). Moreover, we have $A_i=Q_iV_iQ^{-1}_i$ because of symmetry. Can we say that $\rho(A_1+A_2+\cdots+A_n )= \rho(V_1+V_2+\cdots+V_n)$ where $\rho(X)$ represents spectral radius of $X$? </p>
",<linear-algebra>
"<p>I am reading <a href=""http://biomet.oxfordjournals.org/content/61/2/383.full.pdf"" rel=""nofollow"">an old paper dated back in 70'</a>, where I encounter this
$$\mid\text{det}(A,G)\mid=(\text{det}\{(A,G)'(A,G)\})^{\frac{1}{2}}.$$</p>

<p>We compute the determinant of a single matrix, don't we? What doest it mean by $\mid\text{det}(A,G)\mid$?</p>
",<linear-algebra>
"<p>The question is the following: Given a matrix $A$ with rank $k$, we are looking for a matrix $B$ of rank $j$, where $j&lt;k$ such that $\|A-B\|_2$ is minimal.</p>

<p>My idea was to choose, if $A=P \operatorname{diag}(\sigma_1,\ldots,\sigma_k,0,\ldots) Q^H$ then $B=P \operatorname{diag}(\sigma_1,\ldots,\sigma_j,0,\ldots) Q^H$.</p>

<p>Is this approach correct? if so, then i would try to proove that this is actually the best approximation.</p>
",<linear-algebra>
"<p>Would someone please explain the proof strategy at <a href=""http://math.stackexchange.com/q/462982/53259"">Need verification - Prove a Hermitian matrix $(\textbf{A}^\ast = \textbf{A})$ has only real eigenvalues</a>? I brook the algebra so I'm not asking about formal arguments or proofs. For example, $1.$ How would you determine/divine/previse to take the Hermitian conjugate and to right-multiply by $\color{orangered}{\vec{v}}$?</p>

<p>$2.$ Since we are given that $A$ is Hermitian and has eigenvalues, why not start the proof with $A^*\mathbf{v} = \lambda^*\mathbf{v}$? Here, $\mathbf{v}$ is an eigenvector and so by definition $\neq \mathbf{0}$.</p>

<p>Then $\begin{align} LHS = Av &amp; = \\ \lambda v &amp; = \end{align}$</p>

<p>$\iff \lambda \mathbf{v} = \lambda^*\mathbf{v} \iff \mathbf{0} = (\lambda^* - \lambda )\mathbf{v} \iff \mathbf{v}  \neq \mathbf{0}, so \, (\lambda^* - \lambda )=0. $</p>
",<linear-algebra>
"<p>Suppose I am going to locate an emergency point on a field having 100 houses.</p>

<p>If we see this as a scatter graph with 100 points on it, I need to find a point (or let's call it an optimal point) which is the optimal choice for the emergency point. In other words, the SUM of travelling from the emergency point to all the houses (i.e. Emgncy point to Hous.No1 + Emgncy point to Hous.No2 + ... Emgncy point to Hous.No100) is the lowest possible value.</p>
",<linear-algebra>
"<p>When does the circulant matrix have only integral roots? <br>
For example: adjacency matrix for $K_n$ has all the roots integral which is circulant, but in case of Cycle on $n&gt;3$ it is circulant but it may not have an integral roots. </p>
",<linear-algebra>
"<p>If A and B are n dimensional vector spaces </p>

<p>1) Is A+B a vector space?</p>

<p>2) Is A and B a vector space?</p>
",<linear-algebra>
"<p>Question is as follows:</p>

<p>Suppose $A^2 = I$ (the identity matrix) and F = Q, R or C. Eigenvalues of A are then $\lambda=1$ or $\lambda=-1$. Show that $\ker(L (I+A))=E(-1)(A)$ and that $im(L (I+A))=E(1)(A)$.</p>

<p>$E(-1)(A)$ means eigenspace of eigenvalue $-1$.</p>

<p>I have managed to do the part with kernel, but I'm struggling with image and eigenspace.</p>

<p>I know how to prove that image is included in eigenspace $(E)$, but I don't know how to show the other way round.</p>

<p>Definition of $E$ I'm using: $E(1)(A) = \{x : Ax = x\}$.</p>

<p>Definition of image: $im(In + A) = \{y : y=(I + A)x\}$.</p>

<p>This is how I proved image is in eigenspace:
need to be shown: $Ay = y$</p>

<p>LHS: $Ay=A(I+A)x=A(Ix + Ax)=Ax + AAx= Ax + Ix=(A + I)x = y =$ RHS</p>
",<linear-algebra>
"<p><img src=""http://i.stack.imgur.com/SJ6MW.jpg"" alt=""enter image description here""></p>

<p>This is a very basic definition of orientable and very basic example 20.5 however ı could not understand definition in an good way so ı want you to explain my green writing please :) and my example please help me ı want to learn orientation on manifold if ı could not understand in a good way this ı will not understand in a good way rest of the subject please help me </p>
",<linear-algebra>
"<p>The eigenspace corresponding with the eigenvalue <strong>zero</strong> is the same as the null space of the original matrix. All vectors in the null space are linearly independent so the eigenvectors of <strong>zero</strong> are also independent.</p>

<p>Is this conclusion right?</p>
",<linear-algebra>
"<p>Hi could you help me with following</p>

<p>$$
\begin{pmatrix}
1 &amp; a &amp; a \\
a &amp; 1 &amp; a \\
a &amp; a &amp; 1
\end{pmatrix}
$$</p>

<p>is a $3 \times 3$ matrix.</p>

<p>Find the largest interval for a such that this matrix is positive definite.</p>

<p>How to do this??</p>

<p>Thanks a lot!</p>
",<linear-algebra>
"<p>Let $d_1$, $d_2$, ..., $d_n$ be positive integers. Let $B$ be the $n \times n$ matrix
$$\begin{pmatrix}
d_1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 \\
1 &amp; d_2 &amp; 1 &amp; \cdots &amp; 1 \\
1 &amp; 1 &amp; d_3 &amp; \cdots &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 1 &amp; 1 &amp; \cdots &amp; d_n \end{pmatrix}.$$
When does $B$ have a square root in $\mathrm{Mat}_n(\mathbb{Z})$?</p>

<p>Motivation: The <a href=""http://en.wikipedia.org/wiki/Friendship_graph"" rel=""nofollow"">Friendship Theorem</a> states that the only graph in which every pair of vertices is joined by a path of length $2$ is the ""Friendship Graph"", which you can see at the linked article. If $A$ is the adjacency matrix of such a graph, with degree sequence $(d_1, d_2, \ldots, d_n)$, then $A^2=B$. So this contributes the solution $(d_1, d_2, \ldots, d_n) = (2,2,2,\ldots,2,2m)$, with $n=2m+1$.</p>

<p>I was preparing notes on the friendship theorem and got distracted by trying to figure out when this matrix has an integer square root at all. It seemed like it might make a nice challenge for here.</p>
",<linear-algebra>
"<p>I am currently going through ""Log-gases and random matrices"" by PJ Forrester. I'm coming from a totally different academic background, and I cannot understand a point of his notation. More precisely, he defines an <em>antinunitary</em> time reversal <strong>operator</strong> $T=\mathbf{Z}_{2N}K$ where $\mathbf{Z}_{2N}$ is the tensor product of the $N\times N$ identity matrix and of the $2\times2$ matrix $\begin{bmatrix}0 &amp;-1\\ 1 &amp; 0 \end{bmatrix}$, and where $K$ is the complex conjugate operator. So if I understand correctly, $T$ acts on a matrix $\mathbf{A}$ like so
\begin{equation}
T\mathbf{A} =\mathbf{Z}_{2N}\bar{\mathbf{A}},
\end{equation}
right?
Where I stop following is how one should deal with $K$ in ""mixed operations"", i.e. since it is not presented as a martix, how does one invert $K$, apply it ""from the right"", etc. </p>

<p>The property i'm trying to understand is the following: commutation relations between Hermitian matrices $\mathbf{A}$ and $T$ lead to
\begin{equation}
\mathbf{A} = T \mathbf{A} T^{-1} = \mathbf{Z}_{2N}K\mathbf{A} K^{-1} \mathbf{Z}_{2N}^{-1}   = \mathbf{Z}_{2N}K\mathbf{A} K \mathbf{Z}_{2N}^{-1}  = \mathbf{Z}_{2N}\bar{\mathbf{A}}\mathbf{Z}_{2N}^{-1}
\end{equation}
From what I gather, $T$ is more or less treated as a matrix (at least with regard to inversion operation), and $K$ is it's own inverse (that, i understard). The puzzling point for me is the last equality. Does The second $K$ disappears because it is applied (with no effect) to the real matrix $\mathbf{Z}_{2N}^{-1}$? I cannot seem to find anything on the subject easily... I just need a confirmation of my understanding, and possibly further reading material! </p>
",<linear-algebra>
"<p>I have a formula I use to determine how opaque some validation text should be based upon the length of a user's input compared to the maximum lenth allowed.  I want to modify it so that the ""ramping up"" of the opacity percentage only starts when they are at 80% of max, and then scales up proportionally from there.</p>

<p>Here is my current function:</p>

<p><strong>OpacityPercentage = CharactersEntered / MaxCharacters</strong></p>

<p>Therefore, if I have MaxCharacters of 50, then the opacity is as follows:</p>

<ul>
<li>30 chars = 60%</li>
<li>40 chars = 80%</li>
<li>41 chars = 82%</li>
<li>45 chars = 90%</li>
<li>50 chars = 100%</li>
</ul>

<p>What I want is for the opacity to be 0% until I get to 80% of max, then scale up from there.  So I would want the table to look as follows:</p>

<ul>
<li>30 chars = 0%</li>
<li>40 chars = 0%</li>
<li>41 chars = 10%</li>
<li>45 chars = 50%</li>
<li>50 chars = 100%</li>
</ul>

<p>I thought this would be simple, but I can't seem to figure out what I need to change in my existing formula.  Any advise is appreciated!</p>
",<linear-algebra>
"<p><em>Problem</em></p>

<p>Determine whether the indicated subset is a subspace of the given euclidean space:</p>

<p>$ \{[x,y,z]\ |\ x,y,z \in \mathbb{R} $ and $z=3x+2\}$ in $\mathbb{R}^{3}$</p>

<p><em>Solution</em></p>

<p>By definition, in order for a subset to be a subspace 3 conditions must be occur:</p>

<ol>
<li><strike>To pass by the origin</strike> To contain the origin.</li>
<li>To be closed under addition. </li>
<li>To be closed under scalar multiplication.</li>
</ol>

<p>So I try to solve the exercise by this way:</p>

<p>$1.$ The origin $(0,0,0) \in \mathbb{W} $</p>

<p>$2.$ Let $\vec u$ and $\vec v \in \mathbb{W} $. We have</p>

<p>$$ 
\begin{cases}
3u_1 + 2 - u_3 = 0 \\
3v_1 + 2 - v_3 = 0 \\
\end{cases}
$$
The sum is $ 6(u_1 + v_1) + 4 - (u_3+v_3) = 0 $ (which $\in \mathbb{W} $)</p>

<p>$3.$ Let $\vec u$ $ \in \mathbb{W} $ and  $\ r$ $ \in \mathbb{R} $. We have</p>

<p>$r(3u_1) + r(2) - r(u_3) = 0 \\$</p>

<p>Which, also, $ \in \mathbb{W} $</p>

<p>So, why is the book's answer: It <strong>isn't</strong> a subspace? </p>
",<linear-algebra>
"<p>I'm (reasonably) familiar with <a href=""http://en.wikipedia.org/wiki/Cholesky_decomposition"" rel=""nofollow"">factoring</a> a positive definite matrix $\mathbf{P} = \mathbf{L} \mathbf{L}^T =  \mathbf{R}^T \mathbf{R}$, and is supported by  <a href=""http://www.mathworks.com.au/help/techdoc/ref/chol.html"" rel=""nofollow"">MATLAB</a> and <a href=""http://eigen.tuxfamily.org/dox-devel/TopicLinearAlgebraDecompositions.html"" rel=""nofollow"">Eigen</a>.</p>

<p>However, I have also seen a factorization of the (same)  $\mathbf{P} = \mathbf{U} \mathbf{U}^T =  \mathbf{L&#39;}^T \mathbf{L&#39;}$</p>

<p>The following illustrates:</p>

<pre><code>&gt;&gt; A = rand(3, 4)

A =

    0.2785    0.9649    0.9572    0.1419
    0.5469    0.1576    0.4854    0.4218
    0.9575    0.9706    0.8003    0.9157

&gt;&gt; P = A * A.'

P =

    1.9449    0.8288    2.0991
    0.8288    0.7374    1.4513
    2.0991    1.4513    3.3379

&gt;&gt; R = chol(P)

R =

    1.3946    0.5943    1.5052
         0    0.6198    0.8982
         0         0    0.5153

% This function computes such that U * U.' = A * A.'
% Part of: http://www.iau.dtu.dk/research/control/kalmtool2.html 
&gt;&gt; U = triag(A)

U =

   -0.7475    0.2571   -1.1489
         0   -0.3262   -0.7944
         0         0   -1.8270

&gt;&gt; P2 = R.' * R

P2 =

    1.9449    0.8288    2.0991
    0.8288    0.7374    1.4513
    2.0991    1.4513    3.3379

&gt;&gt; P3 = U * U.'

P3 =

    1.9449    0.8288    2.0991
    0.8288    0.7374    1.4513
    2.0991    1.4513    3.3379
</code></pre>

<p>I haven't seen this particular factorization  $\mathbf{P} = \mathbf{U} \mathbf{U}^T$ before. I have a couple of questions:</p>

<ul>
<li>Is it still, by definition, Cholesky factoriation? If not, what is it called? </li>
<li>Is the simple means to compute this particular variant (e.g. a MATLAB command)</li>
<li>Is there a specific relationship between $\mathbf{U}$ and $\mathbf{R}$?</li>
</ul>
",<linear-algebra>
"<p>Choose a possible $a$ such that the linear equations have a root</p>

<p>$$\begin{matrix} x+2y+3z=a \\
                 4x+5y+6z=a^2 \\
                 7x+8y+9z=a^3 \end{matrix}$$</p>

<p>Do I begin by finding the possible values of $a$ such that the system is consistent?</p>
",<linear-algebra>
"<p>I am stuck trying to solve the following problem:</p>

<p>In diagonalizing a symmetric matrix $S$, we find that two of the eigenvalues ($\lambda_1$ and $\lambda_2$) are equal but the third ($\lambda_3$) is different. Show that <em>any</em> vector which is normal to $\hat{n}_3$ (which is the eigenvector corresponding to $\lambda_3$) is then an eigenvector of $S$ with eigenvalue equal to $\lambda_1$</p>

<p>Can anyone offer hints on, or an outline of, the solution?</p>

<p>Thank you. </p>
",<linear-algebra>
"<p>Vectors $\vec{b}$ and $\vec{c}$ are given. ∠(b,c)=2pi/3. Find vector $\vec{a}$, coplanar with $\vec{b}$ and $\vec{c}$, length $|\vec{a}|=4$ and ∠(a,b)=pi/6</p>

<p>I know it's something with triple product. Not sure where that gets me.</p>

<p>EDIT: $|\vec{b}|$=$|\vec{c}|$=1</p>
",<linear-algebra>
"<p>Given a set of linear equations $AX=B$, say $A$ is an ill posed matrix (has a few singular values equal or very close to zero), which numerical algorithm (conjugate gradient, least squares or steepest decent etc ) should be used to obtain the best solution? More specifically, is there a concrete comparison between these methods?</p>
",<linear-algebra>
"<p>Are these sets of equations linear? What is the number of variables and equations in each system? Please correct me if my answer is wrong:</p>

<p><strong>a)</strong> $Ax = b, x \in R^n$ - <strong>yes</strong>, classic system of linear equations, $var = n, eq = m$ where $A \in R^{m \times n}$</p>

<p><strong>b)</strong> $x^TAx = 1, x \in R^n$ - <strong>no</strong>, its a quadratic form, $var = n, eq = 1$</p>

<p><strong>c)</strong> $a^TXb = 0, X \in R^{m \times n}$ - <strong>yes</strong>, $var = m*n, eq = 1$</p>

<p><strong>d)</strong> $AX + XA^T = C,X \in R^{m \times n}$ - <strong>yes</strong>, not sure</p>

<p>Thanks for any help..</p>

<p>EDIT: are the first 3 solutions correct now?</p>
",<linear-algebra>
"<p>I have a few proofs I need some help with.</p>

<p><strong>a)</strong> Prove that $AB-BA = I$ does not have any solutions for any $A,B$. All matrices are regular.</p>

<p>I based my proof on matrix traces. $tr(AB) = tr(BA)$. Since $tr(X+Y) = tr(X) + tr(Y)$, it holds that $tr(AB - BA) = tr(AB) - tr(BA) = 0$ and $tr(I) = m$ so the diagonal numbers cant be ""önes"". Is this proof correct or do I have to use some other method?</p>

<p><strong>b)</strong> Prove that $(AB)^{-1} = B^{-1}A^{-1}$. I believe that I can prove this by simply writing all the matrices products down.. is there some simplier and more ""elegant"" way how to prove this?</p>

<p><strong>c)</strong> Prove that $A + A^T$ is symetric for a square $A$. Not sure abotu this one...</p>

<p>Thanks for any help in advance!</p>

<p>EDIT: $A$ in c) is square, not rectangular!</p>
",<linear-algebra>
"<p>In the vector space of $f:\mathbb R \to \mathbb R$, how do I prove that functions $\sin(x)$ and $\cos(x)$ are linearly independent. By def., two elements of a vector space are linearly independent if $0 = a\cos(x) + b\sin(x)$ implies that $a=b=0$, but how can I formalize that? Giving $x$ different values? Thanks in advance.</p>
",<linear-algebra>
"<p>I have a $t \times l$-polynomial matrix $A$ over $\mathbb{F}_q[x]$. The entries of $A$ are of degree $\le m$. I want to reduce $A$ to upper-triangular form by Gaussian elimination in case of using the Euclidean division instead of the exact one. What the worst-case computational (time) complexity it will take?</p>
",<linear-algebra>
"<p>I have several subspaces where I have to determine their dimension and whether they are affine or linear? These are my answers- are they correct? Thanks for help!</p>

<p><strong>a)</strong> $X = \{ x \in R^n | a^Tx = 0 \}, a \in R^n$ is given</p>

<ul>
<li>linear since any linear combination $\alpha x + \beta y, x, y \in X$ is in X. I believe its affine, too, since actual coefficients $\alpha, \beta$ doesnt really matter in this case because $\alpha(a_1x_1 + \dots + a_nx_n) + \beta(a_1y_1 + a_ny_n) = 0$ every time...</li>
<li>dimension? Im guessing its max $n-1$ but Im not sure</li>
</ul>

<p><strong>b)</strong> $X = \{ x \in R^n | a^Tx = c \}, a \in R^n, c \in R$ are given</p>

<ul>
<li>affine because in order for the linear combination to be in $X$, $\alpha_i$ must sum to 1</li>
<li>dimension again max $n-1$?</li>
</ul>

<p><strong>c)</strong> $X = \{ x \in R^n | x^Tx = 1 \}$</p>

<ul>
<li>affine (same justification as in b)) </li>
</ul>

<p><strong>d)</strong> $X = \{ x \in R^n | a^Tx = I \}, a \in R^n$ is given</p>

<ul>
<li>not sure at all
Are my assumptions correct</li>
</ul>
",<linear-algebra>
"<p>Let $A$ and $B$ be isomorphic unitary rings. Suppose that both of them admit a structure of (maybe finite dimensional) vector space over some field $k$. I would like to know if then $A$ and $B$ are isomorphic as vector spaces over $k$ (if they are forced to have the same dimension). Notice that in general I am not requiring $A$ and $B$ to be $k$-algebras, i.e. I am not requiring any kind of compatibility between the multiplicative structure and the product with scalars from the field. My guess is that in this generality the answer is no, but I can't provide nor find any example.</p>

<p>Here I gather some things I can prove:</p>

<p>1) if the field is $k=\mathbb{Q}$ and $A$ and $B$ are $k$-algebras, then the answer is yes.</p>

<p>2) if the field is $k=\mathbb{R}$, $A$ and $B$ are $k$-algebras and they are fields, then the answer is yes again.</p>

<p>3) if $A$ and $B$ are finite $k$-algebras (and so $k$ is finite too) the answer is yes again.</p>

<p>Unfortunately these rule out most of the examples from a first course in ring theory, so I suspect the answer would be more exotic than this, but I can't find anything. Maybe the answer is yes even in the general setting (or maybe just for $k$-algebras), and in this case I'd like to see a proof.</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>For example, I want to this to happen:
$$\begin{bmatrix}1&amp; 2&amp; 3\end{bmatrix}\times\begin{bmatrix}2&amp; 3&amp; 4\end{bmatrix} = \begin{bmatrix}2&amp; 6&amp; 12\end{bmatrix}$$
It's not exactly matrix multiplication, but I hope you can see what I'm getting at. Is there some notation in linear algebra that allows this function to be valid?</p>
",<linear-algebra>
"<p>I want a rotation matrix $R$ that transforms +x axis to +y, +y to +z, and +z to +x. One way of doing it is by a rotation about +x by 90 deg anti-clockwise, followed by a rotation about +y by 90 deg anti-clockwise. The matrices respectively are:</p>

<p>$$R_1 = \begin{bmatrix}
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; -1 \\
0 &amp; 1 &amp; 0 \\
\end{bmatrix}$$</p>

<p>$$R_2 = \begin{bmatrix}
0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
-1 &amp; 0 &amp; 0 \\
\end{bmatrix}$$</p>

<p>Since R1 first operates first followed by R2, the resultant, I think, is R=(R2)(R1). But when I cross-check by applying R to an arbitrary point, say (1,2,3), I don't get the right transform of (3,1,2). Instead, I get it right when I do R=(R1)(R2)  which doesn't make sense to me. Please help</p>
",<linear-algebra>
"<blockquote>
  <p>Let $c_0=\{ (x_n) : x_n\in \Bbb{R}, x_n \to 0\}$ and $M=\{(x_n)\in c_0 : x_1+x_2+\cdots + x_{10}=0\}$. Then, what is dim($c_0/M$) ?</p>
</blockquote>
",<linear-algebra>
"<p>In the textbook that I'm using the standard inner product is defined as </p>

<p>$$\langle x,y\rangle = \sum_{i=1}^{n}a_{i}\overline{b_{i}}$$</p>

<p>where $x=(a_{1}, a_{2}, {...}, a_{n})$ and $y=(b_{1}, b_{2}, {...}, b_{n})$ and the bar denotes complex conjugation. </p>

<p>In one the practice problems I was doing they used the fact that </p>

<p>$\langle x,y\rangle = y^{*}x$</p>

<p>where $y^{*}$ denotes the conjugate transpose of y. I'm having trouble understanding why the definition for the standard inner product is the same as $y^{*}x$. </p>

<p>I tried doing an example where $x = (i,i)$ and $y = (-i,-i)$ </p>

<p>For $$\langle x,y\rangle = i*i+i*i=-2$$  </p>

<p>On the other hand </p>

<p>$y^{*}x=\begin{pmatrix} i\\i\\ \end{pmatrix} \begin{pmatrix} i &amp; i\\ \end{pmatrix} = \begin{pmatrix} -1 &amp; -1\\ -1 &amp; -1\\ \end{pmatrix} $</p>

<p>I was wondering why I am getting $\langle x,y\rangle \ne y^{*}x$</p>
",<linear-algebra>
"<p>If the characteristic polynomial $f_A(x)$ has multiples of the same product, for example $f_A(x)= (x+2)^2(x-1)$ so  $(x+2)$ has a multiple of $2$, then is there a condition on $A$ such that we know for sure that $m_A(x)= (x+2)(x-1)$ or $(x-1)$ or $(x+2)$ ? i.e, no multiples. </p>
",<linear-algebra>
"<p>Is there a simple way to show that the least square solution of an overdetermined linear system is equal to the right singular vector of the coefficient matrix corresponding to the smallest singular value?</p>
",<linear-algebra>
"<p>I was reading up on the Fibonacci Sequence when I've noticed some were able to calculate specific numbers. So far I've only figured out creating an array and counting to the value, which is incredibly simple, but I reckon I can't find any formula for calculating a Fibonacci number based on it's position.</p>

<p>Is there a way to do this? If so, how are we able to apply these formulas to arrays?</p>
",<linear-algebra>
"<p>By matrix-defined, I mean</p>

<p>$$\left&lt;a,b,c\right&gt;\times\left&lt;d,e,f\right&gt; = \left| 

\begin{array}{ccc}
i &amp; j &amp; k\\
a &amp; b &amp; c\\
d &amp; e &amp; f
\end{array}

\right|$$</p>

<p>...instead of the definition of the product of the magnitudes multiplied by the sign of their angle, in the direction orthogonal)</p>

<p>If I try cross producting two vectors with no $k$ component, I get one with only $k$, which is expected. But why?</p>

<p>As has been pointed out, I am asking why the algebraic definition lines up with the geometric definition.</p>
",<linear-algebra>
"<p>As I've understood it, what I've learned is that the dot product is just one of many possible <strong>inner product spaces</strong>. Can someone explain this concept? When is it useful to define it as something other than the <strong>dot product</strong>?</p>
",<linear-algebra>
"<p>Given two points around an origin $(0,0)$ in $2D$ space, how would you calculate an angle from $p_1$ to $p_2$?</p>

<p>How would this change in $3D$ space?</p>
",<linear-algebra>
"<p>I've recently started reading about Quaternions, and I keep reading that for example they're used in computer graphics and mechanics calculations to calculate movement and rotation, but without real explanations of the benefits of using them.</p>

<p>I'm wondering what exactly can be done with Quaternions that can't be done as easily (or easier) using more tradition approaches, such as with Vectors?</p>
",<linear-algebra>
"<p>What purposes do the Dot and Cross products serve? </p>

<p>Do you have any clear examples of when you would use them?</p>
",<linear-algebra>
"<p>I wonder how can I find the largest delta in which if initial condition is inside circle (How do I specify radius of that circle) bounded by delta then the solution is always within the dashed circle in the figure?
How do you approach this solely based on geometry of phase portrait since no equation is given?</p>

<p><img src=""http://i.stack.imgur.com/dovTu.jpg"" alt=""enter image description here""></p>
",<linear-algebra>
"<p>A simple question. Why is the sum of the singular values of a matrix called its <a href=""http://en.wikipedia.org/wiki/Matrix_norm#Schatten_norms"" rel=""nofollow"">nuclear norm</a>? What is the origin of, and motivation for, this term?</p>

<p>Apparently the term <em>nucleus</em> is sometimes used to refer to the kernel of a linear transformation, but that doesn't seem to have anything to do with singular values.</p>

<p>To save you the effort, neither <em>nucleus</em> nor <em>nuclear</em> have entries in <a href=""http://jeff560.tripod.com/n.html"" rel=""nofollow""><em>Earliest Known Uses of Some of the Words of Mathematics</em></a>.</p>
",<linear-algebra>
"<ol>
<li><p>In the process of proving some theorem, I have assumed that if $A B$
is invertible, then the matrices $A$ and $B$ are invertible as
well. However I'm not sure about it. I know that if $A$ and $B$ are
invertible then $AB$ is also invertible. But does it hold in the
opposite direction?</p></li>
<li><p>Is $AB=I$ enough to deduce that $A$ is invertible? Or must I prove that $BA=I$ also holds? </p></li>
</ol>

<p>(I'm dealing only with square matrices)</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>I have a set of vectors:</p>

<p>$$V=\lbrace x\in\Bbb{R}^5:x_1+x_2+x_3+x_4+x_5=0\rbrace$$</p>

<p>I need to find basis for this set of vectors. What is the ""algorithm"" for solving such problems? Where should I start?</p>

<p>EDIT:</p>

<p>Is this a valid basis:
$\lbrace[1,0,0,0,-1]^T,[0,1,0,0,-1]^T,[0,0,1,0,-1]^T,[0,0,0,1,-1]^T\rbrace$?</p>
",<linear-algebra>
"<p>Can someone explain how Cramer's rule works. I understand the mechanics of it, and it's fairly straightforward to show algebraically that it's equivalent to GJ and substitution, but what's happening under the hood? I'm guessing it has to do with properties of determinants but...</p>
",<linear-algebra>
"<p>My linear algebra book (<a href=""http://www.cin.ufpe.br/~jrsl/Books/Linear%20Algebra%20Done%20Right%20-%20Sheldon%20Axler.pdf"" rel=""nofollow"">Linear Algebra Done Right</a> by Sheldon Axler) has the following problem as exercise 1.6:</p>

<blockquote>
  <p>Give an example of a nonempty subset $U$ of $\mathbb{R}^2$ such that $U$ is closed under addition and under taking additive inverses (meaning $-u \in U$ whenever $u \in U$), but $U$ is not a subspace of $\mathbb{R}^2$.</p>
</blockquote>

<p>It seems to me that such a set cannot exist, since the only subspace condition it's not mandated to fulfill is containing $0$, and for any $u \in U$, I can negate it to get $-u$ and then get $u + (-u) = 0$.</p>

<p>What is going on?</p>
",<linear-algebra>
"<p>I have problem with this:
$$\| BC\| \leq \| B\|\| C\|$$
where $\|\cdot\|$ means spectral norm defined as $$\|A\|=\text{max}\lbrace\|Ax\|:x\in\mathbb{C^n, \|x\|=1}\rbrace$$ 
where the norms on the are according to standard scalar product. </p>

<p>For my homework I had to solve $\|A+B\|\leq\|A\|+\|B\|$ which I managed to do by myself but this second inequality seems to be (at least for me) much harder. Any ideas? </p>
",<linear-algebra>
"<p>I need some help with this problem please: </p>

<p>Let $V$ be a vector space finitely generated over $\mathbb Q$ and
let $α, β ∈ \operatorname{ End}(V )$ satisfy $3α^3 + 7α^2 − 2αβ + 4α − σ_1 = σ_0$. Show that $αβ = βα$.</p>

<p>Thanks.</p>

<p>This is an exercise from the book: The Linear Algebra a Beginning Graduate Student Should Know by Golan.</p>
",<linear-algebra>
"<p>I need some help with this problem please:</p>

<p>Let $V$ be as vector space over a field $F$ and let $α, β, γ ∈
\operatorname{End}(V )$ satisfy $αβ = σ_1 = αγ$. Show that $βγ \neq γβ$.</p>

<p>$σ_1$ is the identity function and $\operatorname{End}(V )$ the endomorphism of $V$.</p>

<p>I think this problem is wrong for if we take $β=γ$ that is not true, but if someone have an idea it will be appreciated.
Thanks.</p>

<p>This is an exercise from the book: The Linear Algebra a Beginning Graduate Student Should Know by Golan.</p>
",<linear-algebra>
"<p>Apologies for the poor title.</p>

<p>What I'm wondering is:
Say that we have a non-surjective operator $A:X\rightarrow X$ where $X$ is a Banach space, and the operator is defined in terms of the basis vectors $ê_i$:
$$
A(x)=\sum\limits_{i,k=1}^\infty\alpha_{i,k}\cdot x_{k}\cdotê_i=\sum\limits_{k=1}^\infty{y_k\cdotê_k}
$$
edit: where there are countably many $i$ for which $\alpha_{i,k}$ is non-zero for some $k$.</p>

<p>A <strike>restriction</strike> projection of the operator is:
$$
A'(x)=\sum\limits_{i\in I,k=1}^\infty\alpha_{i,k}\cdot x_{k}\cdotê_i=\sum\limits_{k\in I}{y_k\cdotê_k}
$$
Where $I$ is a finite index set.</p>

<p>Given $y=\sum\limits_{k=1}^\infty{y_k\cdotê_k}$ that is not in the image of $A$, is it always possible to take such a projection in a way so that the finite set of linear equations you get don't have a solution?</p>

<p>If one of the basis vector isn't in the image it's easy to realise that you can, for example the right shift operator:$$(x_1,x_2,x_3,x_4,...) \rightarrow (0,x_1,x_2,x_3,x_4,...)$$</p>

<p>My next thought was that there has to be a basis vector not in the image, because otherwise the image would span $X$ but I don't think that's enough.</p>

<p>This isn't a homework problem, it's just something that's been distracting me.</p>
",<linear-algebra>
"<p>Let's assume that $V$ and $W$ are vector spaces over a field $\mathbb{K}$, $\lambda\in\mathbb{K}$, $\lambda\neq0$.</p>

<p>$S: V\rightarrow W$ and $T: W\rightarrow V$ are linear maps. Prove, that</p>

<p>$\lambda$ is an eigenvalue of $TS\iff\lambda$ is an eigenvalue of $ST$</p>

<p>What can be stated about the eigenvalues of the maps $TS$ and $ST$?
Would it also be correct if $\lambda=0$?</p>

<p>That's how far I've come:
I have to prove, that </p>

<ol>
<li>$\lambda$ is an eigenvalue of $TS\Rightarrow\lambda$ is an eigenvalue of $ST$ </li>
<li>$\lambda$ is an eigenvalue of $ST\Rightarrow\lambda$ is an eigenvalue of $TS$</li>
</ol>

<p>Assuming $V=\mathbb{K}^n$ and $W=\mathbb{K}^m$, such that $S:\mathbb{K}^n\rightarrow\mathbb{K}^m$ and $T:\mathbb{K}^m\rightarrow\mathbb{K}^n$. Hence, $TS: \mathbb{K}^n\rightarrow\mathbb{K}^n$ and $TS: \mathbb{K}^m\rightarrow\mathbb{K}^m$. $TS$ and $ST$ are both endomorphisms. Since the eigenvalue is not zero, the matrices must be invertible and the determinant of both matrices is not zero. Let's say $A$ is the transformation matrix of $TS$ and $B$ the transformation matrix of $ST$</p>

<p>That's where I'm stuck right now. How do I go on from here?</p>

<p>Do I have to prove, that $det(A-2*I_3)=0=det(B-2*I_2)$?</p>
",<linear-algebra>
"<p>Is it true that if $A$ is unitary diagonalizable (i.e. $A$ is normal) and $B$ is similar to $A$ then $B$ is also unitary diagonalizable (i.e. $B$ is normal) ?</p>

<p>Here are my thoughts:</p>

<p>If $A$ is unitary diagonalizable matrix then there exist unitary matrix $P$ and diagonal matrix $D$ such that $P^*AP=D$.  $(*)$</p>

<p>We also know from the Spectral theorem that $A$ is normal ($\because$ $A$ is unitary diagonalizable $ \iff A$ is normal).</p>

<p>From the fact that $B$ is similar to $A$ we can derive that there exists invertible matrix $Q$ such that $Q^{-1}BQ=A$.</p>

<p>Finally, by replacing $A$ in $(*)$ with $Q^{-1}BQ$ we get:</p>

<p>$$ P^*Q^{-1}BQP=D \ \ (**)$$</p>

<p>The only thing I can derive from this equation $(**)$ is that $B$ is diagonalizable, because :</p>

<p>$$ P^*Q^{-1}BQP=D \implies P^{-1}Q^{-1}BQP=D \implies (QP)^{-1}B(QP)=D \implies C^{-1}BC=D$$ </p>

<p>Am I missing something? Is it possible that I can also derive that $B$ is <strong>unitary</strong> diagonalizable ?</p>
",<linear-algebra>
"<p>I'm a little bit confused by this sentence:</p>

<p>""The matrix A is the sum of two matrices that can be diagonalised independently,
and hence A is trivially reducible.""</p>

<p>How can I show that A is reducible? </p>
",<linear-algebra>
"<p>I'm able to prove $44$, but how would one deduce $43$ from it without further industry, forthwith?<br>
 $43$ seems like a reduced, 2D version of $44$? I'm not enquiring about individual proofs.</p>

<blockquote>
  <p>$44.$ Let $P$ be a point not on the plane that passes through the
  points $Q, R, S$.<br>
  Show that the distance $h$ from $P$ to the plane $ = \dfrac {\left| \left( \mathbb{a} \times \mathbb{b} \right) \cdot \mathbb{p}\right| } {\left| \mathbb{a} \times \mathbb{b}\right| }$. 
  <img src=""http://i.stack.imgur.com/IB6o9.png"" alt=""enter image description here""></p>
  
  <p>$43 = 39$ (5th edition). Let $P$ be a point not on the line $L$ that passes through the
  points $Q,R$.<br>
  Show that the distance $d$ from the point P to the line $L = \dfrac {\left| \mathbb{a} \times \mathbb{b} \right| } {\left| \mathbb{a}\right| }$. 
  <img src=""http://i.stack.imgur.com/gMcvB.png"" alt=""enter image description here""></p>
</blockquote>
",<linear-algebra>
"<p>$37.$ Find an equation of the plane that passes through the point $(1, -2, 1)$<br>
and contains
the line of intersection of the planes $x + y - z = 2$ and $2x - y + 3z = 1$. 
<img src=""http://i.stack.imgur.com/EdLNj.png"" alt=""enter image description here"">
$\bbox[3px,border:2px solid grey]{\text{ Official solution : }}$My modified <a href=""http://www.mathematics-online.org/kurse/kurs8/seite41.html"" rel=""nofollow"">picture</a> illustrates that the red line of intersection belongs to both planes. A normal vector of each plane $\perp$ to this red line of intersection. Thus, the direction vector of the red line = $\mathbb{n_1} \times \mathbb{n_2} = (1, 1, -1) \times (2, -1, 3) = \color{#C154C1}{(2, -5, -3)}$</p>

<p>For want of a normal vector to the requested plane, require another direction vector on this requested plane. Thus need the vector containing any point on the red line of intersection to the given point $(-1, 2, 1)$ in the plane. WLOG, set $\color{#FF4F00}{x = 0}$:
$\begin{cases} x + y - z = 2 \\ 2x - y + 3z = 1 \end{cases} \implies \begin{cases} y - z = 2 \\ -y + 3z = 1 \end{cases}$ $\implies (\color{#FF4F00}{x}, y, z) = (\color{#FF4F00}{0}, 7/2, 3/2).$<br>
Thus another vector parallel to the plane is $(-1, 2, 1) - (\color{#FF4F00}{0}, 7/2, 3/2) = \color{#C154C1}{(-1, -3/2, -1/2)}$. </p>

<p>In toto, a normal vector to the plane $= \color{#C154C1}{(2, -5, -3) \times (-1, -3/2, -1/2)} = 2(-1, 2, -4).$  $... \blacksquare$</p>

<p>$\bbox[3px,border:2px solid grey]{\text{ My solution : }}$
♦ Any vector on the line of intersection of the two planes produces one vector $\parallel$ to the requested plane.</p>

<p>♠ The other vector $\parallel$  the requested plane is any vector connecting any point on this line of intersection with the given point $(-1, 2, 1)$ in the plane.</p>

<p>♦ Subtract the two equations of the plane to produce their line of intersection: $3x + 2z = 3$. For a direction vector of this line, subtract any two vectors on it. WLOG, put $z = 0 \implies \color{brown}{(1, 0, 0)}$ and put $z = 3 \implies \color{brown}{(-1, 0, 3)}$. Thus direction vector $= (-1, 0, 3) - (1, 0, 0) =  (-2, 0, 3)$. </p>

<p>♠ Observe the given point $(-1, 2, 1)$ isn't on this line (but is given to be on the plane). Thus either $\color{brown}{(1, 0, 0)} - (-1, 2, 1) = (2, -2, -1)$ or $\color{brown}{(-1, 0, 3)} - (-1, 2, 1)$ are on the plane. </p>

<p>In toto, a normal vector to the plane = $(-2, 0, 3) \times (2, -2, -1) = 2(3, 2, 2) \neq k(-1, 2, -4)... \blacksquare$</p>

<p>Since the solution's and my normal vectors differ already, what's wrong with my solution? </p>
",<linear-algebra>
"<p>So far I have:</p>

<p>$\boldsymbol{f^{-1}} \circ \boldsymbol{f}(\boldsymbol{a}) = \boldsymbol{a}
\implies [\boldsymbol{D}(\boldsymbol{f^{-1}}(\boldsymbol{a}) \circ \boldsymbol{f}(\boldsymbol{a}))] = I_n
\implies [\boldsymbol{D}\boldsymbol{f^{-1}}(\boldsymbol{f}(\boldsymbol{a}))][\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})] = I_n $ by the chain rule.</p>

<p>Interpreting $[\boldsymbol{D}\boldsymbol{f^{-1}}(\boldsymbol{f}(\boldsymbol{a}))]$ as the matrix composed of row-reduction operations, $[\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})]$ row-reduces to $I_n$. Now $[\boldsymbol{D}\boldsymbol{f}(\boldsymbol{a})]$ is a $m \times n$ matrix, therefore we have $m \le n$: Is this convincing? How do I make it rigorous?</p>

<p>This looks like a similar question to <a href=""http://math.stackexchange.com/questions/257599/the-existence-of-an-inverse-to-a-differentiable-function"">Existence of an inverse to differentiable function</a></p>
",<linear-algebra>
"<blockquote>
  <p>Describe all $m$ by $n$ matrices $A$ and $B$ such that $ref(A) + ref(B) = ref(A + B)$.<br>
  Is it true that $ref(A) = A$ and $ref(B) = B$? Does $ref(A - B) = rref(A - B)$?<br>
  Here, ref = Row Echelon Form, rref = Reduced Row Echelon Form.</p>
  
  <p><strong>Terse Answer:</strong> I think $ref(A) = A$ and $ref(B) = B$ are true.<br>
  But $REF(A) - REF(B)$ may have $-1$ in some pivots.</p>
</blockquote>

<p>Would someone please explain and uncloak how to start, still less solve, this question?</p>
",<linear-algebra>
"<p>Let $V$ be finite dimensional real vector space and let $f$ and $g$ be non zero linear functionals on $V$.  Assume that $\ker(f) \subset \ker(g).$ Which of the following are true??</p>

<p>a. $\ker(f)=\ker(g)$</p>

<p>b. $f=\lambda g$ for some real number $\lambda \ne 0$.</p>

<p>c. The linear map $A\colon V\to \mathbb{R}^2$ defined by $Ax=(f(x),g(x))$ for all $x \in V$, is onto.</p>

<p>Since $\ker(f)\subset \ker(g)$ we get $\ker(f)=\ker(g)$ and hence (a) and (b) are true. Now the linear map will look like $Ax=(\lambda g(x),g(x))$ . I guess (c) will be false. Not sure though.</p>
",<linear-algebra>
"<p>Let $T_1$ and $T_2$ be diagonalisable operators on a real vector space $V$. Does it follow that $T_1+T_2$ is a diagonalisable operator?</p>

<p>My intution says no. But I can't find any counterexample.</p>
",<linear-algebra>
"<p>A cashier has a total of 30 bills, made up of ones, fives, and twenties. The number of twenties is 9 more than the number of ones. The total value of the money is $351. How many of each denomination of bills are there? (Hint: Let x = the number of ones, y = the number of fives, and z = the number of twenties)</p>

<p>I'm having a hard time figuring out the three equations for this problem.</p>
",<linear-algebra>
"<p>For $S-N$ Decomposition of a linear operator $T$ i.e $T=S+N$(Unique Expression) where $S$ is a semi-simple opearator and $N$ is a nilpotent operator and $SN=NS$ then prove that semi-simple part and nilpotent part of $T$ commutes with $A$ if $T$ commutes with $A$, where $A$ is a linear operator.</p>
",<linear-algebra>
"<p>Do you know is there any way to compute SVD or Cholesky decomposition of a matrix such as $$\begin{pmatrix}1&amp;-\infty \\ 0 &amp; 2 \end{pmatrix}$$ or not?</p>
",<linear-algebra>
"<p>I think that I should to rewrite the matrix in a appropriate form, but I can't find it. For a $2\times2$ matrix I get the characteristics polynomial $x^2-1$ and the eigenvalues $-1,1$. For $3\times3$ matrix I get $-x^3+3x+2$ and the eigenvalues are $2,-1,-1$.</p>
",<linear-algebra>
"<p>Let $A,B \in M(n,F)$ be such that $AB=BA$. Then show that $A$ and $B$ has same characteristic polynomial.</p>

<p>How can I proceed to this? If I take $|B||\lambda I-A|=|\lambda B- BA|= |\lambda B- AB|$ But this will not help me..</p>
",<linear-algebra>
"<p>Is matrix $Svv^\top S$ positive semidefinite, given $S$ is symmetric positive semidefinite? Where $v\in\mathbb{R}^n$,  $S\in \mathbb{R}^{n\times n}$</p>
",<linear-algebra>
"<p>Let $T(θ) : \Bbb R^2 → \Bbb R^2$ be the transformation that rotates each vector counterclockwise by angle $θ$.</p>

<p>(a) Write the standard matrix for $T(θ)$.</p>

<p>(b) Explain in words or pictures why $T(θ_1+θ_2) = T(θ_1) ◦ T(θ_2)$.</p>

<p>(c) Derive the angle sum formula for cosine and sine by finding the standard matrix for $T(θ_1+θ_2) = T(θ_1) ◦ T(θ_2)$; that is, prove that $\cos(θ_1+θ_2) = \cos(θ_1)\cos(θ_2)−\sin(θ_1) \sin(θ_2)$ and $\sin(θ_1+θ_2) = \sin(θ_1)\cos(θ_2) + \sin(θ_2)\cos(θ_1)$.</p>

<p>For part a - I'm not sure how to find the standard matrix when I don't know the initial values of the matrix before the vectors rotate.</p>

<p>Part b - I know $T(θ_1) ◦ T(θ_2)$ is the Hadamard product of $T(θ_1)$ and $T(θ_2)$, but I was never taught any properties to prove part b.</p>

<p>Part c - this problem completely confuses me.</p>

<p>Any help would be appreciated.</p>
",<linear-algebra>
"<p>Lyapunov's equation says: given any $Q &gt; 0$ ($Q$ positive definite) there is $P &gt; 0$ such that $A^T P + P A + Q = 0$ if and only if for $\frac{dx(t)}{dt}=A x(t)$ it is the case that the real part of each eigenvalue of $A$ is negative. Then, the ellipsoid $x^T P x \leq 1$ is an invariant of $\frac{dx(t)}{dt}=A x(t)$.</p>

<p>The geometric interpretation of $P$ is such that the eigenvectors of $P$ form the principal axes of the ellipsoid and each eigenvalue is related to the length of the ellipsoid along the axis represented by the corresponding eigenvector.</p>

<p>What is the geometric interpretation of $Q$ resp. how does the choice of $Q$ affect $P$?</p>
",<linear-algebra>
"<p>Let A =</p>

<p>\begin{bmatrix}1/4&amp;\sqrt3/4\\\sqrt3/4&amp;3/4\end{bmatrix}</p>

<p>(a) Show that for each x ∈ R, Ax is the projection of x onto the line passing through the origin making an angle of 60 degrees with the positive x-axis.</p>

<p>(b) Show that null(A) is the line passing through the origin making an angle of 30 degrees with the negative x-axis.</p>

<p>(c) Show that null(A) = row(I − A).</p>

<p>For part a - I created an Ax augmented matrix, but after reducing the matrix I'm not sure how to determine that the lines formed make an angle of 60 degrees.</p>

<p>Part b - How would I find the null space of the matrix when the reduced form of A doesn't have any free variables?</p>

<p>Part c - I can't really prove part c since I'm not sure how to find the null space or row space of I-A.</p>
",<linear-algebra>
"<p>Let $F$ be a field and let $n$ be a positive integer. Let $A,B ∈ M_{n×n}(F)$ be matrices
satisfying $A^2 + B^2 = I$ and $AB + BA = O$. Show that $tr(A) = tr(B) = 0$.</p>

<p>I know that $tr(A^2)+tr(B^2)=n$ and that $(A+B)^2=I$. I'm having a hard time showing this. I've been playing around for a bit with no success. Any solutions/hints are greatly appreciated.</p>
",<linear-algebra>
"<p>Let $\mathbf C$ be a positive-definite $k\times k$ matrix. For all vectors $\mathbf u\in \mathbb R^k$ of length $\|\mathbf u\|=1$, consider vectors $\mathbf {uu}^\top\mathbf{Cu}$; they form a surface in $\mathbb R^k$. What is this surface? In particular, what is it in case of $k=2$?</p>

<p>Here is an example for $\mathbf C = \left(\begin{array}{cc}4&amp;2\\2&amp;2\end{array}\right)$:</p>

<p><a href=""http://i.stack.imgur.com/fztne.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/fztne.png"" alt=""Weird shape""></a></p>

<p>I do realize that the ""main axes"" (the longest and the shortest cuts) of this curve are given by the eigenvectors of $\mathbf C$ scaled by the respective eigenvalues. But the whole shape looks weird.</p>
",<linear-algebra>
"<p>the time averaged total energy, $\bar E$,
has the following $\varepsilon$ expansion in $D$ dimension:
\begin{equation}
\bar{E}=\varepsilon^{2-D}\frac{E_0}{2\lambda}+ \varepsilon^{4-D}E_1
\end{equation}
 from the above equation how can we write 
\begin{equation}
\omega_{\rm m}^2=
1-\frac{1}{2\lambda}\frac{(D-2)E_0}{(4-D)E_1}\,.
\end{equation}
where $$\omega_{\rm m}^2=1-\varepsilon_{\rm m}^2,$$  and $\lambda$ is a constant factor.
m denotes that angular frequency $\omega $ is minimum.</p>

<blockquote>
  <p>Can someone explain me how equation (2) can be written from equation
  (1)
  If you feel problem or need more information then please comment. 
  thanks in advance.
  for more info see <a href=""http://arxiv.org/abs/0802.3525"" rel=""nofollow"">here in equation (39 and 40)</a></p>
</blockquote>
",<linear-algebra>
"<p>Let's say I have the following equalities</p>

<p>$a_1x_1 + a_2x_2 + a_3x_3 + a_4x_4 = b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 = c_1x_1 + c_2x_2 + c_3x_3 + c_4x_4$</p>

<p>Where the $a$'s, $b$'s, and $c$'s are known, non-negative integers.</p>

<p>Is there an efficient way to check if a solution exists (the $x$'s) such that they are non-negative real numbers (except for the trivial case of all $x$'s being 0)? I don't need to actually calculate them, just need some way to see if a solution even exists.</p>
",<linear-algebra>
"<p>I'm trying to figure out what the dimension of the set of self-adjoint operators on V would be, or in more concrete terms:</p>

<p>Let $dim V =n$. Let $S(V)$ denote the set of self-adjoint linear operators on V. What is its dimension?</p>

<p>The only thing I know that somewhat resembles this might be that $dim L(V) = (dim V)^2$ but I'm not sure how I might be able to apply that to this problem.</p>

<p>Any tips or assistance would be greatly appreciated. Thanks!</p>

<p>EDIT: Also, V is a finite dimensional inner product (or Hermitian) space.</p>
",<linear-algebra>
"<p>Find the absolute extreme values taken by $f(x,y) = x^2 + 4y^2 + x - 2y$ on the closed region enclosed by the ellipse $1\over4$$x^2 + y^2 = 1$. </p>

<p>I know this might be a basic question but could someone please explain how to solve this problem?</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>What does it say about the eigenvectors of a matrix $A$ if the row-reduced form of the characteristic polynomal in coefficient matrix form has a row of 0's?</p>

<p>I know that it indicates something about the eigenvectors of $A$ but I can't remember what exactly... </p>
",<linear-algebra>
"<p>I just wanted to ask whether my proof is correct:</p>

<p>Suppose instead that $\mathbb{R}$ had a countable $\mathbb{Q}$-basis, say $v_1,v_2,v_3,\ldots$ (possibly finite).</p>

<p>Since $\mathbb{Q}$ is countable, $\,\text{span}(v_1,\ldots,v_k)$ is countable for each $k$ (possibly finitely many).</p>

<p>We have $\mathbb{R}=\bigcup_{k}\text{span}(v_1,\ldots,v_k)$ which is a countable union of countable sets.</p>

<p>It follows that $\mathbb{R}$ is countable. Contradiction.</p>

<p>I would be very grateful for any feedback.</p>

<p>Best wishes!</p>
",<linear-algebra>
"<p>Please help me in solving the recursion   $F(n)=K_0\frac{F(n-1)}{n-1}+K_1\frac{F(n-2)}{n-2}$, preferably using power series for the values of $F(n)$ in terms of $n$. Here $K_1$ and $K_2$  are constants. We are given  $F(0)=0,F(1)=3,F(2)=3/2$. Any methods of solution is welcome, even partial answers. We can discuss further. Thanks for taking time to read my question.</p>
",<linear-algebra>
"<p>Suppose the inner product on $P(R)$ is defined by $\langle p,q\rangle = \int^1_0 p(x)q(x)dx$.</p>

<p>Let $\phi$ be the linear functional on $P(R)$ defined by $\phi (p) = p(0)$ for each polynomial $p \in P(R)$. Prove that there does not exist $q \in P(R)$ such that $\phi (p) = \langle p, q\rangle$ for every $p \in P(R)$.</p>

<p>This is what I have so far:</p>

<p>Let $e_1,...,e_n$ be an orthonormal basis for $P(R)$.</p>

<p>$\phi (p) = \phi(\langle p, e_1\rangle e_1 + \dots + \langle p, e_n\rangle e_n) = \langle p, e_1\rangle \phi(e_1) + \dots + \langle p, e_n\rangle \phi(e_n)$</p>

<p>Thus, $\phi(e_1)e_1 + \dots + \phi(e_n) e_n = p(0)e_1 + \dots p(0) e_n$</p>

<p>I'm not sure where to go from here, help? Thank you!</p>
",<linear-algebra>
"<p>This came up in a practical problem involving a state change in a digital filter system.</p>

<p>Find $X$ given $A$ and $K$, where $A,X,K$ are all $n$ x $n$ square matrices:</p>

<p>$$
   X - A X A = K
$$
I couldn't find a direct way to do this, eventually I realized that it's just $n^2$ linear equations in $n^2$ unknowns, and it can be solved as follows:</p>

<ul>
<li>Restate so $X$ and $K$ are column vectors $\vec x$ and $\vec k$, each with $n^2$ elements</li>
<li>The term $A X A$ becomes $A_L A_R \vec x$, where $A_L, A_R$ are $n^2$ x $n^2$ matrices which perform the equivalent of multiplying by $A$ on the left and right in the original form</li>
<li>Find $\vec x = \left(I-A_L A_R \right)^{-1} \vec k$ and reorder to get $X$</li>
</ul>

<p>For $n=3$, for instance, if
$$
 A = \begin{bmatrix}
     a &amp; b &amp; c \\ d &amp; e &amp; f \\ g &amp; h &amp; i \end{bmatrix}
$$
then
$$
A_L = \begin{bmatrix}
  a &amp; 0 &amp; 0 &amp; b &amp; 0 &amp; 0 &amp; c &amp; 0 &amp; 0 \\
  0 &amp; a &amp; 0 &amp; 0 &amp; b &amp; 0 &amp; 0 &amp; c &amp; 0  \\
  0 &amp; 0 &amp; a &amp; 0 &amp; 0 &amp; b &amp; 0 &amp; 0 &amp; c  \\
  d &amp; 0 &amp; 0 &amp; e &amp; 0 &amp; 0 &amp; f &amp; 0 &amp; 0 \\
  0 &amp; d &amp; 0 &amp; 0 &amp; e &amp; 0 &amp; 0 &amp; f &amp; 0 \\
  0 &amp; 0 &amp; d &amp; 0 &amp; 0 &amp; e &amp; 0 &amp; 0 &amp; f \\
  g &amp; 0 &amp; 0 &amp; h &amp; 0 &amp; 0 &amp; i &amp; 0 &amp; 0 \\
  0 &amp; g &amp; 0 &amp; 0 &amp; h &amp; 0 &amp; 0 &amp; i &amp; 0 \\
  0 &amp; 0 &amp; g &amp; 0 &amp; 0 &amp; h &amp; 0 &amp; 0 &amp; i \end{bmatrix}
$$
and (in block form):
$$
A_R = \begin{bmatrix}
  A^T &amp; 0 &amp; 0 \\
   0 &amp; A^T &amp; 0 \\
   0 &amp;  0 &amp; A^T \end{bmatrix}
$$</p>

<p>Question is: is there any easier way to solve this? I'm now thinking that the relationship between the $n^2$ variables is such that you can't solve it without this kind of rewriting, but I'd be happy to be proven wrong. Is there a name for the procedure of restating the problem as above? Or a name for the type of the original equation? Perhaps it would be more natural with tensor notation - I'm not that familiar with that area.</p>

<p>One other note: the original equation can be be rewritten as the following two forms:</p>

<p>$$ 
X = A X A + K
$$
and (assuming $A$ is not singular)
$$
X = A^{-1} \left( X -  K \right) A^{-1}
$$
... and it seems to me that one of these (depending on the properties of $A$) may be usable as a iterator that will converge to the correct value of $X$. In some applications this may easier to work with than the full solution.</p>
",<linear-algebra>
"<p>I have a very stupid and simple question, which I do not have a clear idea on. One article that I read said, </p>

<blockquote>
  <p>""As a basic relationship in linear algebra states,
  the scalar product of vectors $x$ and $y$ in a base space of $A$ is
  $xAy$."" </p>
</blockquote>

<p>What does the 'base space' specifically represent? Is there any other term that indicates the same concept? What is the difference between ordinary scalar or inner product $xy$ and $xAy$? What role base space A does play? What are the impacts by converting $xy$ into $xAy$? Thank you so much in advance!</p>

<p>(I have uploaded the corresponding page below or you can have an access to the article via <a href=""http://www.sciencedirect.com/science/article/pii/S0378873310000031"" rel=""nofollow"">http://www.sciencedirect.com/science/article/pii/S0378873310000031</a> , in page 199, third paragraph.)</p>

<p><a href=""http://i.stack.imgur.com/tJMI5.jpg"" rel=""nofollow"">enter image description here</a></p>
",<linear-algebra>
"<ul>
<li>I want to show the following: For any square  matrix $A$ n $\times$ n of the form: $A=\begin{pmatrix} 
0 &amp; a_{1,2} &amp; . &amp; . &amp; . &amp; a_{1,n} \\ 
0 &amp; 0 &amp; a_{2,3} &amp; . &amp; . &amp; a_{2,n} \\ 
0 &amp; 0 &amp; 0 &amp; . &amp; .  &amp; .\\
. &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; a_{n-1,n} \\
. &amp; . &amp; . &amp; . &amp; . &amp; 0  
\end{pmatrix}$,</li>
</ul>

<p>$A^n=0$</p>

<p>In order to do that, I want to show that if you multiply such a matrix b times. All the coefficients $(a)_{ij}$ such that $i&lt;i+b-1$is equal to 0</p>

<ul>
<li><strong>My attempt</strong></li>
</ul>

<p>I am trying to show that $\forall b, \leq n$ $(A^b)_{i,i+b-1}= 0$</p>

<p>I will try to show this by induction</p>

<ul>
<li><strong>basis</strong> Let's show that $(A^2)_{i,i+1}=0$
By definition, $(A^2)_{i,i+1} = \Sigma^n_{k=0}a_{i,k}a_{k,i+1}$</li>
</ul>

<p>For $i \geq k$, $a_{i,k}=à \implies a_{i,k}a_{k,i+1} = 0$</p>

<p>For $i=k-1$, $i+1=k \implies i+1 \geq k \implies a_{k,i+1} = 0 \implies a_{i,k}a_{k,i+1} = 0$ And therefore, for $i &lt; k, a_{i,k}a_{k,i+1}=0$</p>

<p>Therefore, $(A^2)_{i,i+}=0$</p>

<p>We now have a matrix of the form: $A^2 = \begin{pmatrix} 
0 &amp; 0 &amp; a_{1,3} &amp; . &amp; . &amp; a_{1,n} \\ 
0 &amp; 0 &amp; 0 &amp; a_{2,4} &amp; . &amp; a_{2,n} \\ 
0 &amp; 0 &amp; 0 &amp; . &amp; .  &amp; .\\
. &amp; . &amp; . &amp; . &amp; . &amp; a_{n,n-2} \\
. &amp; . &amp; . &amp; . &amp; . &amp; 0 \\
. &amp; . &amp; . &amp; . &amp; . &amp; 0  
\end{pmatrix}$</p>

<ul>
<li><strong>Inductive step</strong> And here is where the struggle starts (I cannot finish this step).</li>
</ul>

<p>Let $(A^b)_{i,i+b-1}$ be TRUE, let's show that it implies that $(A^{b+1})_{i,i+b}$ is TRUE:</p>

<p>$(A^b)_{i,i+b-1}=0$</p>

<p>$\implies$ $(A)_{i,i+b-1}(A^b)_{i,i+b-1}=0$</p>

<p>$\implies \Sigma^n_{k=0} a_{i,i+b-1}(a^b)_{i,i+b-1}= 0$.</p>

<p><strong>EDIT: I think I have solved my problem, can you guys confirm</strong></p>

<p>We now have a multiplication of the form: $A \times A^b =\begin{pmatrix} 
0 &amp; a_{1,2} &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; a_{1,n} \\ 
0 &amp; 0 &amp; a_{2,3} &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; a_{2,n} \\ 
0 &amp; 0 &amp; 0 &amp; . &amp; .  &amp; . &amp; . &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; a_{n-1,n} \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; 0 \\
\end{pmatrix} \times  \begin{pmatrix} 
0 &amp; 0 &amp; . &amp; 0 &amp; a_{1,b+1} &amp; a_{1,b+2} &amp; . &amp; . &amp; . &amp; a_{1,n} \\ 
0 &amp; 0 &amp; 0 &amp; . &amp; 0 &amp; a_{2,b+2} &amp; a_{2,b+3} &amp; . &amp; . &amp; a_{2,n} \\ 
0 &amp; 0 &amp; 0 &amp; . &amp; .  &amp; 0 &amp; . &amp; . &amp; . &amp; .\\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; a_{n-b,n} \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; 0 \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
. &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\
0 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; 0 \\ 
\end{pmatrix}$</p>

<p>Let's call the first matrix $N$ and the second matrix $M$.</p>

<p>Let's show that $(NM)_{i,i+b} = 0$:</p>

<p>$(NM)_{i,i+b} = \Sigma_{k=0}^n n_{i,k}m_{k,i+b} = (0 \times \Sigma_{k=0}^i n_{k,i+b}) + (\Sigma_{k=n-i}^n a_{i,k} \times 0) = 0$</p>

<p>Therefore $(NM)_{i,i+b} = (AA^b)_{i,i+b} = (A^{b+1})_{i,i+b} = 0$</p>

<p><strong>I think I have succeeded to prove by induction that $(A^b)_{i,i+b-1} = 0$ But I am not certain</strong></p>

<p>Can someone confirm that my proof by induction is right?</p>
",<linear-algebra>
"<p>Let $T$ is a linear operator on a vector space V such that $dim(RangeT)=k$ , then how to prove  $T$ can have $at$ $most$ $k+1$ distinct eigen values ??</p>
",<linear-algebra>
"<p>How do you show that two matrices are similar? I know that if they are similar, then B=(v^-1)AV, but how do you find V^-1 and V? For example, the matrices {[1,2]T, [3,2]T} and {[0,4]T, [2,2]T}, how would you find the P and P^-1 (if they are similar)? Or would you have to show they are similar in a different way?</p>

<p>Thank you!</p>
",<linear-algebra>
"<p>Suppose we have a $(n+m)\times(n+m)$ matrix
$$A=\begin{pmatrix}
  0 &amp; B \\
  B^T &amp; C \\
 \end{pmatrix},$$
where $B$ is a $m\times n$ matrix and $C$ is a symmetric $n\times n$ matrix. Now suppose $B\ll C$ (in terms of eigenvalues), then matrix $A$ has two distinct sets of eigenvalues. $n$ of them are of order of the eigenvalues of $C$, the remaining three are suppressed by two powers of 
$$\theta=BC^{-1}.$$
One can block-diagonalize $A$ by expanding $\theta$. On one hand one obtains $m\times m$ matrix 
$$D=-\theta C\theta^T$$. On the other hand there is the $n\times n$ matrix 
$$E=C+\frac{1}{2}(\theta^\dagger\theta C+C^T\theta^T\theta^*).$$</p>

<p>Could anyone tell me how to do the block-diagonalization by expanding $\theta$?</p>
",<linear-algebra>
"<p>Let $A\in M_n({\Bbb F}_p )$. Suppose that $A^p = I_n$. Show that$(A-I_n)^p = 0$, and $A$ has an eigenvector $v\in {\Bbb F}^n_p$ with eigenvalue 1.</p>

<p>I know that $p$ divides the binomial coefficient $(^p_i)$ for $1\leq i\leq p-1$.
But what is the next?</p>

<p>Thnaks!</p>
",<linear-algebra>
"<p>Every basis of $\mathbb R^6$ can not be reduced to a basis of $5$-dimensional  subspace of $\mathbb R^6$ by removing one vector . Can anyone give an example for that?</p>
",<linear-algebra>
"<p>For any values of parameter $a$ solve the following system of linear equations:
$$\begin{cases} x+y+2z=1 \\ 2x+ay-z=4 \\ 3x+y+3z=1 \end{cases} $$</p>

<p>Calculating the value of determinant I found out, after I equalled it with zero, that $a$ has the value $-4\over 3$. So I thought that if it's required to solve this equation for any $a$ , then first of all I had to suppose that $D=0$, this means that $a={-4\over 3}$. For this value I found out that there is an infinite number of solutions for this system. But now my question is: what do I have to do with the case when $D\neq 0$? </p>

<p>Thank you!</p>
",<linear-algebra>
"<p>Let $X$ be a normed vector space and $Y \subset X$ a closed subspace. We consider the quotient $X / Y$ and equip it with the quotient norm. Then we may form the completion $\overline{X / Y}$.</p>

<p>We compare $\overline{X / Y}$ to the following space: denote by $\overline{X}$ the completion of $X$ and by $\overline{Y} \subset \overline{X}$ the closure of $Y \subset \overline{X}$. Now we form the quotient $\overline{X} / \overline{Y}$ with the quotient norm.</p>

<blockquote>
  <p>Are $\overline{X / Y}$ and $\overline{X} / \overline{Y}$ (naturally) isomorphic, i.e., is there a linear, bijective isometry between them?</p>
</blockquote>
",<linear-algebra>
"<p>I am asked to write a Matlab program to find the coefficients of the resulting polynomial which is the product of two other polynomials. However, I need someone to clarify the underlying concepts for me. In this post, I will use $P_a(x)=1+2x+3x^2$ and $P_b=1+2x+3x^2+4x^3$ as our examples for the two polynomials. Using matrix formulation (which is the requirement):
$$
a = \left(\begin{array}{c}
1\\
2\\
3\\
0\\
0\\
0\end{array} \right)
\qquad b = \left(\begin{array}{c}
1\\
2\\
3\\
4\\
0\\
0\\\end{array} \right)
$$
And we are looking for the vector $c$ that is the coefficients of our product polynomial. Since the degree of the resulting polynomial is $5$, we will have $6$ coefficients including the constant term. And I know that $c$ is a product of the circulant matrix of $a$ and the vector $b$:
$$
c = \left(\begin{array}{c}
c_0\\
c_1\\
c_2\\
c_3\\
c_4\\
c_5\end{array} \right)=
\left(\begin{array}{cccccc}
1&amp;0&amp;0&amp;0&amp;3&amp;2\\
2&amp;1&amp;0&amp;0&amp;0&amp;3\\
3&amp;2&amp;1&amp;0&amp;0&amp;0\\
0&amp;3&amp;2&amp;1&amp;0&amp;0\\
0&amp;0&amp;3&amp;2&amp;1&amp;0\\
0&amp;0&amp;0&amp;3&amp;2&amp;1\\
\end{array} \right)
\&gt;\dot\
\&gt;\left(\begin{array}{c}
1\\
2\\
3\\
4\\
0\\
0\\
\end{array} \right)
$$
and we denote the circulant matrix $A$. I know I can construct $A$ by using $FFT$: $$
A = F^{-1}\&gt;diag(F\,a)\&gt;F
$$
And here's what confuses me. From this point on, do I just multiply the matrix A by the vector b <strong>directly</strong>? I am required to implement the algorithm in $O(n\&gt;logn)$ time. And Let me repeat the requirement: <strong>1. Matrix formulation of the problem 2. Using FFT 3. Overall run time being $O(n\&gt;logn)$</strong> Any input is greatly appreciated. Thanks!</p>
",<linear-algebra>
"<p>Given two square matrices $A, B$, when is
$$\det(A+tB) = 0$$
for all $t\in \mathbb{R}$?</p>

<p>An easy sufficient condition is that $A$ and $B$'s kernels have nontrivial intersection. Per Henning's comment below, this is not also necessary.  Does there exist a nice necessary and sufficient characterization?</p>
",<linear-algebra>
"<p>Let $f \in \textrm{O}(n, \mathbb R)$. (O is the orthogonal group) Show:</p>

<p>i) If $f(W) \subset W$ for a subspace $W \subset \mathbb R^n$, then $f(W^\perp) \subset W^\perp$.</p>

<p>ii) If $f(\langle v \rangle ) \subset \langle v \rangle$ for a vector $0 \neq v \in \mathbb R$, then $v$ is an eigenvector of $f$ with value $1$ or $-1$.</p>
",<linear-algebra>
"<p>Find a projection $E$ wich projects $\mathbb{R}^2$ onto the subspace spanned by $(1,-1)$ along the subspace spanned by $(1,2)$.</p>

<p>What is the way to approach this problem? Almost to start! </p>

<p>Any suggestion is welcome, thanks.</p>
",<linear-algebra>
"<p>let $P_5 = \{ a_0 + a_1x + a_2x^2 + a_3x^3 + a_4x^4 + a_5x^5 \}$ be the vector space of polynomials of degree $\leq 5$ over $\mathbb{Q}$. Denote $D: P_5 \to P_5$ as the differentiation linear map, i.e. $D(\alpha) = \dfrac{d\alpha}{dx}$</p>

<p>1)Find the inverse of $D^4 + D^2 + Id$,</p>

<p>my answer: $Id - D^2$ as $D^6 - Id = (D^2-Id)(D^4 + D^2 + Id)$</p>

<p>2) find a unique solution to $\alpha \in P_5$ to the differential equation $\dfrac{d^4\alpha}{dx} + \dfrac{d^2\alpha}{dx^2} + \alpha = x^5 + 2x^3$</p>

<p>Could someone explain how I can use (1) to solve part (2)?</p>
",<linear-algebra>
"<p>Find the equation of the plane that passes through the line of intersection of the planes $4x - 2y + z - 3 = 0$ and $2x - y + 3z + 1 = 0$, and that is perpendicular to the plane $3x + y - z + 7 = 0$.</p>

<p>I have attached a picture and that is what I got.</p>

<p>Can someone please tell me if my answer is right because the answer at the back of the textbook is different but I am pretty confident with my solution. Thanks!</p>

<p><img src=""http://i.stack.imgur.com/5yp08.jpg"" alt=""enter image description here""></p>
",<linear-algebra>
"<p>Let the projector be the $N \times N$ matrix $A$. Let its rank be $r$. Let the dimension of the space for which it is the projector be $m$. Is $m==r$?</p>
",<linear-algebra>
"<p>Let $0$ denote the function ${T}$ that takes each element of some vector space to the additive identity of another vector space. Prove that $T$ is linear.</p>

<p>I just want to make sure I understand the basic properties of a linear map with this very simple function.</p>

<p>$ 0 \in\mathcal{L}(V,W)$ defined by $0v = 0$.</p>

<p>Additivity: $0(v+w) = 0v + 0w$ for all $v,w \in V$.</p>

<p>Homogeneity: $0(av) = a(0v)$ for all $a \in \mathbb{F}$ and all $v \in V$.</p>

<p>Therefore $T$ is linear.</p>

<p>Is this verification correct?</p>
",<linear-algebra>
"<p>$$\vec{v}^{t}\textbf{A}\vec{v} &gt; \textbf{0}\text{ and }\textbf{A}\vec{v} = \lambda\vec{v}\quad \Rightarrow \lambda&gt;\textbf{0}\quad(\mathbb{F}=\mathbb{R}) $$</p>

<p>proof:
$$\vec{v}^{t}\textbf{A}\vec{v} &gt; \textbf{0} \text{ and } \textbf{A}\vec{v}=\lambda\vec{v} $$
$$\Rightarrow\vec{v}^{t}\lambda\vec{v} &gt; \textbf{0} $$
$$\Rightarrow\lambda\vec{v}^{t}\vec{v} &gt; \textbf{0} $$
$$\Rightarrow\lambda\langle v, v\rangle&gt; \textbf{0} $$
$$\vec{v} \neq \vec{0}$$
$$\Rightarrow\langle v, v\rangle &gt; \textbf{0} $$
$$\Rightarrow\lambda &gt; \textbf{0}$$</p>
",<linear-algebra>
"<p>This question is somewhat simple: </p>

<p>If we write the transposition of a matrix like this:</p>

<pre><code>A_transposed = T.A
</code></pre>

<p>where T is the operator performing the transposition, can I find a matrix form for this operator, that is independent of the matrix A ? (In other words that would transpose any matrix the size of A? </p>
",<linear-algebra>
"<p>Find the equation of the plane tangent to the surface:
$$x^{\frac{1}{3}}+y^{\frac{1}{3}}+z^{\frac{1}{3}}=1$$ at the point:
$$P=\left(1,-1,1\right)$$
How to find it? I know i have to calculate a gradient which is:
$$\left(\frac{1}{3}x^{-\frac{2}{3}}, \frac{1}{3}y^{-\frac{2}{3}}, \frac{1}{3}z^{-\frac{2}{3}} \right)$$ but what should i do next? I think i need to substitute point into the gradient but how to substitute this point if i have $-1$ under the root?</p>
",<linear-algebra>
"<p>So, why do eigenvalues exclusively form the main diagonal in a diagonalizable matrix?</p>

<p>If we have $n\times n$ matrix ($n$ being a natural number) that is diagonalizable, why is it eigenvalues (exclusively eigenvalues) that make up the main diagonal? </p>
",<linear-algebra>
"<p>In the beginning of linear algebra courses, there are vectors in $\mathbb R^n$ and the dot product is introduced. We learn that if the dot product of two vectors is zero, then these vectors are called orthogonal and there is a right angle between them. Therefore, we understand perpendicularity from the definition of orthogonality. </p>

<p>In the last chapters, all previous notions are generalized as vector spaces and inner products are introduced. Then we learn vectors $\vec u$ and $\vec v$ are orthogonal if $\langle \vec u, \vec v \rangle=0$ in inner product spaces. Our previous knowledge guides us to seek a right angle between them. </p>

<p>Let $P$ be the vector space of first degree polynomials. Let $p(x)=ax+b$ and $q(x)=cx+d$. Define $\langle p(x), q(x) \rangle$ as</p>

<p>$$\langle p(x), q(x) \rangle=ac+bd$$</p>

<p>Then if we consider $p(x)=x-2$ and $q(x)=4x+2$, we find that $\langle p(x), q(x) \rangle=0$. But if we draw them, we see that there is $30.96$ degrees between them. So orthogonality in inner product spaces does not necessarily mean perpendicularity. If not perpendicularity, what should we understand from orthogonality in inner product spaces?</p>
",<linear-algebra>
"<p>$A$  is  an $n\times n$ matrix. Now  if  the  row-reduced  echelon  form  for  this  $A$  is   $E$  then  after  all  the  row  operations  we  have $\det(A)=M\det(E)$   where  $M$  is  a non-zero  scalar  from  the   field. 
 If  $E$  is  the  $n\times n$ identity  matrix  then  $\det(A) \neq 0$.
For  the  converse, $\det(A)$   is  non-zero, so  is  $M$. Then obviously $\det(E)\neq0$. How to  reach the  conclusion  that  $E$  is  the  identity matrix,from  here  just  need  a  little  help  with  that.</p>
",<linear-algebra>
"<p>The problem is to minimize the largest eigenvalue of a function of $x$.</p>

<p>objective:
$$ \min\{\lambda_{\max}(A(x))\}$$
where
$$A(x) = A_0+x_1A_1+x_2A_2+...x_nA_n$$ and all $A$ is positive semidefinite.</p>

<p>This problem can be solved by equivalent  SDP:
$$\text{minimize} \ \ t \\ \text{subject to} \ \ \ A(x)\leq tI$$
since
$$\lambda_{\max}(A(x)) \leq t \ \ \\ \text{iff} \ \ \ A(x)-tI \leq0$$
My question is why is that?</p>

<p>I can't find special property of eigenvalue for  positive semidefinite matrix where the above inequality holds</p>

<p>Can someone give me a brief proof? or point out where I can find the proof.</p>
",<linear-algebra>
"<p>Let $A\in \Bbb R^{n\times n}$ be a matrix such that $\mathrm{rank}(A) = n-1$ and consider the equation 
$$
  Ax = 0.
$$
Clearly, its solutions span a $1$-dimensional space, thus an additional assumption may lead to a unique solution. Let $a\in \mathbb R^n$ be a vector and consider a system of equations
$$ \tag{1}
\begin{cases}
  Ax &amp; = 0,
\\
 a\cdot x &amp; = 0
\end{cases}
$$
where $a\cdot x = \sum_i a_ix_i$ is the inner product. I have two questions: </p>

<ol>
<li><p>What are necessary and sufficient conditions on $a$ for $(1)$ to have the unique solution?</p></li>
<li><p>Can we rewrite $(1)$ in an equivalent matrix form, e.g. $(A+C)x = 0$ for some $C$.</p></li>
</ol>
",<linear-algebra>
"<p>Is the following true always for a matrix norm </p>

<p>$$\lVert AB\rVert \leqslant \lVert A\rVert \cdot \lVert B\rVert \text{ ?}$$</p>

<p>Related to this
 given $r$ is positive constant, $H$ is symmetric positive definite
 is the following true :</p>

<p>$$\lVert (rI - H)(rI + H)^{-1}\rVert &lt; 1 $$</p>

<p>or</p>

<p>$(rI - H)(rI + H)^{-1}$  has the spectral radius less than $1$ certainly?</p>

<p>Thank you.</p>
",<linear-algebra>
"<p>Let $A$ be any $n \times n$ matrix and $\| \cdot \|$ be the matrix norm induced by vector norm  on $\mathbb{R}^n$
(Euclidean
n-dimensional space). </p>

<p>If $\|I - A\| &lt; 1$, then show that $A$ is invertible 
and
derive the estimate</p>

<p>$\|A^{-1}\| &lt; \frac{1}{ 1 - \| I - A \|}$.</p>

<p>Similarly 
when can we expand $\frac{1}{ 1 - \| I - A \|}$ as a power series only is it if and only if the norm is less than $1$?</p>

<p>Thanks a lot!</p>
",<linear-algebra>
"<p>Line <em>m</em> goes through a point D(2, -4). Line <em>m</em> is parallel to line <em>l:</em> $5x+3y=-17$. Describe line <em>m</em> with an equation of type $ax+by=c$.</p>

<p>The solution should be $c=5*2+3*-4=-2$ so $\text{m: }5x+3y=-2$</p>

<p>The lines are parallel so $a=5$, $x$ and $y$ are known so: $5*2+b(-4)=c$. At this point I lack some information I should know to push that equation further. I suppose I should be able to fill either $b$ or $c$, but which one and why?</p>
",<linear-algebra>
"<p>Let $A_1, A_2, A_3, \ldots , A_m$ be positive semi-definite Hermitian matrices and then consider the polynomial $p(z,z_1,z_2,\ldots,z_m) = \det(z+z_1A_1 + z_2A_2 + \cdots+z_mA_m)$</p>

<p>Now Tao argues that if $z,z_1,z_2,\ldots,z_m$ have a positive imaginary part then the ""skew-adjoint part"" (what is this?) of $z+z_1A_1 + z_2A_2 + \cdots+z_mA_m $ is strictly positive definite and hence the quadratic form $\operatorname{Im} [ \langle (z+z_1A_1 + z_2A_2 + \cdots+z_mA_m)v, v \rangle   ]$ is non-degenerate and hence it follows that $z+z_1A_1 + z_2A_2 + \cdots+z_mA_m$ is non-singular.</p>

<p>Can someone kindly help understand what happened here? </p>
",<linear-algebra>
"<p>I'm losing my mind over this question.
For $H$ a Hilbert space, $A,B$ closed subspaces, and $B$ is of dimension $1$, I want to prove that $A+B$ is also closed.</p>

<p>I'm looking for a straightforward proof, with minimum use of high theorems (without Hahn-Banach, etc).</p>

<p>What I did so far was taking a converging sequence in A+B, and mark it as:
$x_k=a_k+\lambda_kv$, for $v$ the base of $B$.
I figured out that I could write the limit, $x$, as $x=P_Ax+P_{A^\perp}$ and do the same on $B$, and maybe work something from there.</p>

<p>I also thought that somehow it would be good to take advantage of the sequence $\lambda_k$ which is in $\mathbb{R}$ resp. $\mathbb{C}$. Maybe prove that it converges somehow..</p>

<p>Thanks so much</p>
",<linear-algebra>
"<p>I have a question on my assignment asking me to prove that if $A$ is an $n\times n$ orthogonal matrix, then $\det(A) = \pm1$. What I did so far is:</p>

<p>We know that $A\cdot A^t = I$ (since it is orthogonal), and that would mean that if $A$ is a matrix with elements $a,b,c,d$ then :</p>

<p>\begin{cases}a^2 + b^2 = 1\\
ac + bd = 0\\
c^2 + d^2 = 1\end{cases}</p>

<p>which implies that \begin{align}\det(A)&amp;= ad - bc\\
&amp;= -\frac{bd^2}{c} - cb\\
&amp;= -\frac{bd^2}{c} - \frac{c^2b}{c}\\
&amp;= -\frac{bd^2 + bc^2}{c}\\
&amp;= -\frac{b(d^2 + c^2)}{c}\\
&amp;= -\frac{b(1)}{c}\\
&amp;= -b/c\end{align}</p>

<p>I might be doing something wrong here, could someone please help me out understand this?</p>
",<linear-algebra>
"<p>Finding whether a given set is subspace of</p>

<p>$$ P_n$$</p>

<p>which is for $n\ge 0$ the set of all polynials of degree at most consist of all polynomial of the form</p>

<p>$$p(t)=a_0+a_1t^1+\cdots+a_nt^n$$</p>

<p>where coefficient and t are real numbers.</p>

<p>is </p>

<p>$$p(t)=a+t^2$$</p>

<p>where $a$ is a real number.</p>

<p>a subspace of $P_n$</p>

<p>I think no because the zero Vector of $P_n$ $0$ is not in</p>

<p>$$p(t)=a+t^2$$</p>

<p>if $t$ is zero then $a+0$ does not equal $0$?</p>
",<linear-algebra>
"<p>It's clear that for any field $\mathbb{F}$ any finite group $G$ can be embedded into $GL_{n}(\mathbb{F})$ for some $n$.</p>

<p>My question is about one modification of this result.
Let's fix positive integer $N$. Is it true that for any finite group $G$ there exist a field $\mathbb{F}$ such that $G$ is embeddable into $GL_N(\mathbb{F})$?</p>

<p><strong>UPD:</strong> For $N=1$ it's false. Let's consider $N&gt;1$.  </p>
",<linear-algebra>
"<p>What part of the definition of a vector space (see <a href=""https://en.wikipedia.org/wiki/Vector_space#Definition"" rel=""nofollow"">here</a>) requires it to be <strong>closed</strong> under addition and multiplication by a scalar in the field? I would understand if we defined a vector space as a group of vectors rather then a set but we don't, also non of the axioms require this to be a condition?</p>
",<linear-algebra>
"<p>Question goes: Law enforcement would like to know the time at which a person died. The investigator arrived on the scene at 8:15pm, which we will call $t$ hours after death. At 8:15 (i.e $t$ hours after death), the temp of the body was found to be $27.4°C$ (Degrees). One hour later, $t+ 1$ hours after death, the body was found to be $26.1°C$. Known constants are $T_s=21°C$, $T_o=36.8°C$.</p>

<p>At what time did the victim die?</p>

<p><strong>MY WORKING</strong></p>

<p>Formula: $T(t)=T_s+(T_o-T_s)e^{-kt}$</p>

<p><strong>1</strong>. $T(t)=T_s+(T_o-T_s)e^{-kt} \quad \rightarrow \quad  
27.4=21+15.8e^{-kt}$</p>

<p><strong>2</strong>.$T(t)=T_s+(T_o-T_s)e^{-kt} \quad \rightarrow \quad 26.1=21+15.8e^{-kt}$</p>

<ol>
<li><p>$27.4=21+15.8e^{-kt}\rightarrow  
6.4=15.8e^{-kt}\rightarrow  
\ln(6.4/15.8)=-kt\rightarrow  
-0.903=-kt$</p></li>
<li><p>$26.1=21+15.8e^{-kt} \rightarrow  
5.1=15.8e^{-k(t+1)} \rightarrow 
ln(5.1/15.8)=-k(t+1)  \rightarrow $</p></li>
</ol>

<p>$-1.131=-k(t+1)$</p>

<p>This is as far as I have got and I believe I should be doing simultaneous equations but am totally unsure if thats correct. Any help would be appreciated.</p>
",<linear-algebra>
"<p>How do I prove the statement: if there exist unique $u$ and $w$ such that for any $v$, $v=u+w$, then $V$ is the direct sum of $U$ and $W$? ($U,W,V$ are vector spaces, $u \in U, w \in W, v \in V$)</p>

<p>I have this vague feeling that I should negate the conclusion and show a contradiction occurs, but it's not so easy for me. How do I expand the logic? </p>
",<linear-algebra>
"<p>It may be that the title of my question is wrong but i am writing this question because i am struck while reading this paper  <a href=""https://projecteuclid.org/download/pdf_1/euclid.kjm/1250776060"" rel=""nofollow"">Brownian motion on rotational group</a> Where $^*\mathscr{f} $ is transpose of $\mathscr{f}$.</p>

<p>If $\mathcal{\gamma_1(=|\mathscr{f}|^2)\ge \gamma _2\ge\gamma _3} $  are the eigenvalues of  $\mathscr{f^*f}$ ,if the $p_1,p_2,p_3$ are the corresponding projections $(\gamma_1 p_1+\gamma_2p_2+\gamma_3p_3=\mathscr{f^*f})$,and if </p>

<p>$$\int_{S^2}do$$ </p>

<p>is the arithmetic average over the spherical surface $S^2$ ,then the </p>

<p>$$\int_{S^2}|p_1o|^2do=\int_{S^2}|p_2o|^2do=\int_{S^2}|p_3o|^2do=\frac{1}{3}$$and $$\int_{S^2 }of \ ^*fodo $$</p>

<p>where $o\ f\ ^*f\ o$ is the inner product of $o\in S^2 $ and $\mathcal{f}\in R^3$.
1. are f the skew hermitian matrix ?and where are these projections taken on?
2.what does it mean to have  inner product on two different basis  ?
any help will be appreciated</p>
",<linear-algebra>
"<blockquote>
  <p>The set of all polynomials in a single variable $x$ forms a vector space $P$ of infinite dimension. Differentiation is a
  linear transformation on this vector space: </p>
  
  <p>$\frac{d}{dx}: P → P, p(x) → p'(x)$.</p>
  
  <p>(a) What is the dimension of the kernel of $\frac{d}{dx}$
  as a linear transformation on $P$ ? </p>
  
  <p>(b) The linear transformation $\frac{d}{dx}
+ 2x$ acts on $P$ as $p(x) → p'(x) + 2xp(x)$. What is the dimension of its kernel?</p>
</blockquote>

<p>I do know what dimensions and kernels of matrices are but this question is confusing me and I don't really understand it. Would really appreciate some help.</p>
",<linear-algebra>
"<p>Show that reflecting $R_2$ across the line $y = x$ and then reflecting it across the $y$−axis is the same as rotating it counterclockwise by $90$ degrees.</p>

<p>I know how to prove this statement geometrically, but I'm assuming the question is asking me to prove this through rotational vectors, which I'm not sure how to prove.</p>
",<linear-algebra>
"<p>Let T : R2 → R2 be the linear transformation which rotates the plane clockwise by 45 degrees, then expands the plane by a factor of 2 in the direction of the x-axis, then finally rotates the plane counterclockwise by 45 degrees. </p>

<p>Find a standard matrix for T. </p>

<p>What does T do to the square whose vertices are (0, 0),(1, 1),(0, 2),(−1, 1)?</p>

<p>I'm struggling with this question at the moment. How would I find the standard matrix of T when I don't know the initial values of R2, before transformation?</p>

<p>For the transformation of the square, I'm not sure what the question is asking. Wouldn't the transformation of the square just be all four vectors rotated by 45 degrees, expanded towards the x-axis by a factor of 2 and then rotated counterclockwise by 45? How would I show that in terms of matrix T.</p>

<p>Help would be appreciated.</p>
",<linear-algebra>
"<p>Let $V$ be a matrix. </p>

<p>What conditions should we require so that we can find a random vector $X = (X_1, \dots, X_n)$ so that $V = Var(X)$? </p>

<p>Of course necessary conditions are:</p>

<ul>
<li>All the elements on the diagonal should be positive</li>
<li>The matrix has to be symmetric</li>
<li>$v_{ij} \le \sqrt{v_{ii}v_{jj}}$ (Because of $Cov(X_i, X_j) \le \sqrt{Var(X_i) Var(X_j)})$</li>
</ul>

<p>But I am sure these are not sufficient as I have a counterexample.</p>

<p>So what other properties we should require on a matrix so that it can be considered a covariance matrix? </p>
",<linear-algebra>
"<p>I have a collection of 3D points in the standard $x$, $y$, $z$ vector space. Now I pick one of the points $p$ as a new origin and two other points $a$ and $b$ such that $a - p$ and $b - p$ form two vectors of a new vector space. The third vector of the space I will call $x$ and calculate that as the cross product of the first two vectors.</p>

<p>Now I would like to recast or reevaluate each of the points in my collection in terms of the new vector space. How do I do that?</p>

<p>(Also, if 'recasting' not the right term here, please correct me.)</p>
",<linear-algebra>
"<p>Let $F:= \mathbb{F}_7[x]/(x^2+3x+1)$</p>

<ol>
<li>Is it a field?</li>
<li>Find all the roots in F of the polynom  $f (Y) := Y^2+[3]_{F}Y +[1]_{F} \in F[Y]$.</li>
</ol>

<p><strong>Attempt:</strong></p>

<ol>
<li>It is a field, because $x^2+3x+1$ is irreducible $\in \mathbb{F}_7[x]$. In fact it has no roots $\in \mathbb{F}_7$.</li>
<li>I suppose I can't just replace numbers from $0$ to $6$ in the place of the $Y$. What should you do to solve this problem?</li>
</ol>
",<linear-algebra>
"<p>My idea of proving every real symmetric matrix can be diagonalized is that, first prove two eigenvectors with different eigenvalues must be orthogonal, then I failed to prove that all the eigenvectors span the whole vector space.</p>

<p>To be specific, my question is, if $A$ is a real symmetric $n\times n$ matrix, let $p(t)=\det(tI-A)$ be the characteristic polynomial of $A$, and $\lambda$ be some eigenvalue of $A$, and $\lambda$ is a root of $p(t)$ of order $k$, then how to prove $\dim (\ker(\lambda I-A))=k$?</p>
",<linear-algebra>
"<blockquote>
  <p>\begin{align*}
  x - \alpha y &amp;= 1\\
  \alpha x - y &amp;= 1
  \end{align*}</p>
</blockquote>

<p>For which values of alpha does the system have an infinite number of solutions, no solutions and one solution.</p>

<p>Find the solution when it is unique.</p>

<p>My attempt:</p>

<p>$-\alpha \cdot \mathrm{eqn}_1 + \mathrm{eqn}_2$ resulting in $(\alpha^2 - 1)y = 1-\alpha$.</p>

<p>then we get $y = (1-\alpha)/(\alpha^2-1)$</p>

<p>so, $y = -1/(1+\alpha)$, but I am trying to proceed </p>
",<linear-algebra>
"<p>I'm having some trouble calculating the angle of an human joint in 3D using the Microsoft Kinect.</p>

<p>Here's an example of the angle of the elbow (using the shoulder and wrist joint):</p>

<p><a href=""http://i.stack.imgur.com/9tkQp.jpg"" rel=""nofollow"" title=""Example"">Image of example</a></p>

<p>Calculating angles between 0° and 180° is no problem, but when the person <strong>hyper</strong>extends his elbow my calculation returns 170° instead of 190°.</p>

<p>The calculation I'm using is as follows:</p>

<ol>
<li>$d = b - a$</li>
<li>$e = b - c$</li>
</ol>

<p>Where a, b and c are 3D-points and d and e are 3D-vectors.</p>

<p>My question is: <strong>How can I calculate the angle between $d$ en $e$ where the angle is between 0° and 360°?</strong></p>

<p>Thanks in advance!</p>
",<linear-algebra>
"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://math.stackexchange.com/questions/119904/units-and-nilpotents"">Units and Nilpotents</a>  </p>
</blockquote>



<p>Given $A^{2012}=0$ prove that $A+I$ is invertible and find an expression for $(A+I)^{-1}$ in terms of $A$. ($I$ is the identity matrix).</p>
",<linear-algebra>
"<p>From <a href=""http://en.wikipedia.org/wiki/Basis_%28linear_algebra%29#Definition"" rel=""nofollow"">Wikipedia</a>:</p>

<p>""A basis $B$ of a vector space $V$ over a field $K$ is a linearly independent subset of $V$ that spans (or generates) $V$.(1)</p>

<p>$B$ is a minimal generating set of $V$, i.e., it is a generating set and no proper subset of B is also a generating set.(2)</p>

<p>$B$ is a maximal set of linearly independent vectors, i.e., it is a linearly independent set but no other linearly independent set contains it as a proper subset.""(3)</p>

<p>I tried to prove (1) => (3) => (2), to see that these are equivalent definitions, can you tell me if my proof is correct:</p>

<p>(1) => (3):
Let $B$ be linearly independent and spanning. Then $B$ is maximal: Let $v$ be any vector in $V$. Then since $B$ is spanning, $\exists b_i \in B, k_i \in K: \sum_{i=1}^n b_i k_i = v$. Hence $v - \sum_{i=1}^n b_i k_i  = 0$ and hence $B \cup \{v\}$ is linearly dependent. So $B$ is maximal since $v$ was arbitrary. </p>

<p>(3) => (2):
Let $B$ be maximal and linearly independent. Then $B$ is minimal and spanning:</p>

<p>spanning: Let $v \in V$ be arbitrary. $B$ is maximal hence $B \cup \{v\}$ is linearly dependent. i.e. $\exists b_i \in B , k_i \in K : \sum_i b_i k_i = v$, i.e. $B$ is spanning. </p>

<p>minimal: $B$ is linearly independent. Let $b \in B$. Then $b \notin span( B \setminus \{b\})$ hence $B$ is minimal.</p>

<p>(2) => (1):
Let $B$ be minimal and spanning. Then $B$ is linearly independent:
Assume $B$ not linearly independent then $\exists b_i \in B, k_i \in K: b = \sum_i b_i k_i$. Then $B \setminus \{b\}$ is spanning which contradicts that $B$ is minimal.</p>
",<linear-algebra>
"<p>I have seen the statement ""Every finite dimensional vector space has a basis."" (<a href=""http://www.cs.xu.edu/math/math240/07s/06_Bases.pdf"" rel=""nofollow"">Here</a> on page 5)</p>

<p>I'm confused about what this tells me. It seems to tell me nothing: by definition, the dimension of a vector space is the number of elements in a basis of it. Then saying a vector space is finite dimensional is the same as saying that it has a basis.</p>

<p>Or are there any other definitions of dimension than the number of basis elements?</p>
",<linear-algebra>
"<p>Let $w_1, \dots, w_m \in \mathbb{C}^d$.</p>

<p>Condition (1) is:</p>

<p>$\sum_i |\langle v, w_i \rangle |^2 = \eta$ whenever $\|v\| = 1$.</p>

<p>Condition (2) is:</p>

<p>$\sum_i u_i u_i^* = I^d$, where $u_i = w_i / \sqrt{\eta}$</p>

<p><a href=""http://arxiv.org/pdf/1306.3969v3.pdf"" rel=""nofollow"">This paper</a> claims that the two conditions are equivalent (top of page 3, just beneath the statement of Corollary 1.3).  I can't figure out why that would be.  Can you help point me in the right direction?</p>
",<linear-algebra>
"<p>Show that the rank of $ n\times n$ symmetric tridiagonal matrix is at least $n-1$, and prove that it has $n$ distinct eigenvalues.</p>
",<linear-algebra>
"<p>For each $m \in \{0,1,2,3\} $  find a subspace of {0,1,2,3} of dimension $m$ and verify answers. I'm not sure what is meant by this or how to begin solving? </p>
",<linear-algebra>
"<p>Hey so I got a question about Vector spaces </p>

<blockquote>
  <p>Let $V=(8,\infty)$. For $u,v$ in $V$ and $a$ in $\mathbb R$ define vector addition by $u\boxplus v:= uv-8(u+v)+72$ and scalar multiplication by $a\boxdot u :=(u-8)^a +8$. It can be shown that $(V,\boxplus,\boxdot)$ is a vector space over the scalar field $\mathbb R$. Find the additive inverse of 16. </p>
</blockquote>

<p>So what I did was find the zero vector which is 9, and set u to 16, but that was the incorrect answer. So I'm confused as to how you find the additive inverse.</p>

<p>Thanks!  </p>
",<linear-algebra>
"<p>Is there a formal proper way of finding the line between two points?</p>

<p>By that I don't mean the line connecting the two points, I mean a line that runs the same distance away from point 1 and point 2.</p>

<p>To phrase it another way, I want to find the equation of a line that divides the plane into two equal parts, where each of the two points are the same distance from the line.</p>

<p>I drew a picture. In this picture, how do I find the purple line?</p>

<p><img src=""http://i.stack.imgur.com/gDe5v.jpg"" alt=""Line equidistant from two points""></p>

<p>It may or not be relevant, but I'm asking because I am trying to learn about Support Vector Machines.</p>
",<linear-algebra>
"<p><strong>Question:</strong> </p>

<blockquote>
  <p>Using the Lagrange's Multipliers method, find the points on the ellipse $x^2+2y^2=1$, that are situated in the longest and shortest distance from the line $x+y=2$. </p>
</blockquote>

<p>I know how to use Lagrange's Method but I do not know how to restrict this question to: 'find a functions max and min on a given set', thus you need to help me solely with that.</p>
",<linear-algebra>
"<p>I'm trying to figure this out: we're dealing with real numbers only here, with the standard scalar product defined as $&lt;\mathbf{v},\mathbf{w}&gt; = \mathbf{v}^T\mathbf{w}$, and we are told, unusually perhaps, that orthogonal matrices are defined as those for which $&lt;R\mathbf{v},R\mathbf{w}&gt; = &lt;\mathbf{v},\mathbf{w}&gt;$, and I need to show from this that the matrix R can be characterised by $R^TR=\mathbf{1}$ (and also that $det(R)=(+/-)1$ but this is fine).</p>

<p>Using the definition of the scalar product, it is not hard to get to $\mathbf{v}^TR^TR\mathbf{w}=\mathbf{v}^T\mathbf{w}$, which I feel is a good starting point, and seems to be close; whilst $R^TR=\mathbf{1}$ would make this work, it isn't the only matrix it can be. I also know $\mathbf{v}^TR^TR\mathbf{w}=\mathbf{w}^TR^TR\mathbf{v}$ but I'm not sure if that helps.</p>

<p>Thanks</p>
",<linear-algebra>
"<p>Find the equation of a plane tangent to the surface given by $$xyz+x^2-3y^2+z^3=14$$ at $$P=\left( 5,-2,3 \right)$$
In my opinion answer is: $$4x+27y+25z-41=0$$ If not please tell me what am i doing wrong.</p>
",<linear-algebra>
"<p>Is $\{(x,y,z) \in \mathbb{R}^3 :x^2+3y^2+12z^2 = 0\}$ a vector space?</p>

<p>My inclination is that the only real solution to $x^2+3y^2+12z^2=0$ is $(0,0,0)$, which is the trivial subspace of $\mathbb{R}^3$. However my true/false assignment is telling me that this is incorrect, that this set is in fact NOT a vector space. Why?</p>
",<linear-algebra>
"<blockquote>
  <p>Let $\{e_1,\ldots,e_n\}$ be an arbitrary basis in a finite dimensional inner product space $V$. Prove there exists vectors $\{f_1,\ldots,f_n\}$ such that $(e_i,f_j)=\delta_{ij}$.</p>
</blockquote>

<p>I tried using the the Gram-Schmidt process to obtain the $f_i$'s but the resulting $f_i$'s should apparently be uniquely determined and $(e_i,f_j)=\delta_{ij}$ won't hold.</p>

<p>What other methods could I try to obtain the $\{f_1,\ldots,f_n\}$? Perhaps we could use induction on $\dim(V)$ for a proof?</p>
",<linear-algebra>
"<p>Doing some reviewing and I'm not 100% sure if my thought-process is correct.</p>

<p>I have the following two vectors and need to prove they're a basis for $R^3$:</p>

<p>$$B=
        \begin{bmatrix}
        1 \\
        0 \\
        -1 \\
        \end{bmatrix},
  \begin{bmatrix}
        0 \\
        1 \\
        -1 \\
        \end{bmatrix}
$$</p>

<p>Now these two vectors are linearly independent, and now I have to prove they span $\Re^3$.</p>

<p>So I have two arbitrary scalars: $\alpha$ and $\beta$ that belong to R:</p>

<p>$$\alpha
        \begin{bmatrix}
        1 \\
        0 \\
        -1 \\
        \end{bmatrix}+\beta
  \begin{bmatrix}
        0 \\
        1 \\
        -1 \\
        \end{bmatrix}=
 \begin{bmatrix}
        x \\
        y \\
        z \\
        \end{bmatrix}
$$</p>

<p>Not much solving to do here</p>

<p>$$
        \begin{bmatrix}
        1 &amp; 0 &amp; x \\
        0 &amp; 1 &amp; y \\
        -1 &amp; -1 &amp; z \\
        \end{bmatrix}
$$</p>

<p>I am left with:</p>

<p>$\alpha = x$, $\beta=y$ and $-\alpha - \beta=z$</p>

<p>Now what I'm having difficulties understanding is how this basis can span all of $R^3$ with z being: $-\alpha - \beta=z$</p>
",<linear-algebra>
"<p>Let $V := R^n$ be a vector space and let $I \in O(n)$ be an operator satisfying $I^2 = -Id$. I want to show that the $span\{x,Ix\}$ is an invarient subspace of $I$.</p>

<p>Let $W = span\{x,Ix\}$. I need to show that $IW \subseteq W$. That is, if we transform $W$ by $I$, then we are still within $W$. But, my linear algebra is bad and i'm struggling to figure how to do this. Can anyone show me how to do this or offer advice?</p>
",<linear-algebra>
"<p>$f:G \rightarrow H$  is a group homomorphism</p>

<p>$g\in G$ is an element of order $n$.</p>

<p>(a) Prove that $f (g)$ is a final order and that the order of the element $f (g)$ parts n.</p>

<p>(b) What is the order of the element $g^{-1}$? What is the order of the element $g ^ m$ for $m\in \Bbb N$?</p>

<p>(c) Whether there is any injective homomorphism $(\Bbb Z_{12}, +) \rightarrow (\Bbb Z_{18}, +)$?</p>

<p>What $(\Bbb Z_{12}; +) \rightarrow (\Bbb Z_{24}; +)$? If so, find example.</p>

<p>i did this:</p>

<p>a) $f(g)$- is final order and parts n
$f(g)^n = f(e)=e$
$f(g)$ - is final order</p>

<p>b) $g^n = e \iff (g^-1)=e$
$g^{-1})^m=e$   for $m&lt;n    \implies  g^m=e$</p>

<p>So how can i do second (b) and (c) THANKS</p>
",<linear-algebra>
"<p><img src=""http://i.stack.imgur.com/KfInK.png"" alt=""I am trying to calculate B"">I wondered how to prove a theory I have about calculating a point having only a line and a circle the line is tangent to.
If  $$\Delta{y}^2$$$$\Delta{x}^2$$ Thus the distance between any two integer numbers is $$\sqrt{\Delta{y}^2+\Delta{x}^2}=m$$ After seeing this I decided to ""create"" my very own formula $$n*r=m$$ where r is the distance between the point on the line, m is the gradient and $$n\Delta{y}(initial)= \Delta{y}(new)$$ and $$n\Delta{x}(initial)=\Delta{x}(new)$$ thus the new point will be $$(x+\Delta{x}(new);y+\Delta{y}(new)) or (x-\Delta{x}(new);y-\Delta{y}(new))$$ I believe it will work for all lines and points with a distance. I am actually afraid to post this here but how can I prove my theory?
In this example I am trying to calculate B.</p>
",<linear-algebra>
"<p>I was trying to generate a direct formula for this series but I am not sure whether it is possible to do so.</p>

<p>$$1\ln(1) + 2\ln(2) + 3\ln(3) + 4\ln(4)+\dots+(n-1)\ln(n-1) + n\ln(n)$$</p>
",<linear-algebra>
"<p><strong><em>How exactly can I convert the below equation into the vector form?</em></strong> (i.e. V(i,j,k) form or $a*i+b*j+c*k$ form):
$$\frac{x-5}{-10}=\frac{y-3}{-6}=\frac{z-2}{-4}$$</p>

<p>I'm actually trying to find the angle between two 3D lines, but I only know how to find out angles between vectors, so I'm trying to convert the above equation to vector form so as to carry out what I need to.</p>
",<linear-algebra>
"<p>Given a matrix $M$, we can compute its singular value decomposition $M=U\Sigma V^*$ where $^*$ is the complex conjugate transpose. $U$ and $V$ are unitary, so $UU^*=I$, $VV^*=I$. Let's take the $i$-th column of $U$, $u_i$. I've experimented with a few examples and I found that the matrix $u_iu_i^*$ always has rank 1. What theorem/property is behind this? Or even, is there a short and sweet proof of it?</p>
",<linear-algebra>
"<p>What is the basis for the intersection of the plane $x-2y+3z=0$ with the $xy plane$ in $R^3$ ? Also can the basis and dimensions  of these planes separately be found  ??</p>
",<linear-algebra>
"<p>Let $f : \mathbb{C}^{n}\rightarrow \mathbb{C}^{n}$ be a linear operator with  a simple spectrum, furthermore, let $g : \mathbb{C}^{n}\rightarrow \mathbb{C}^n
$ be a linear operator such that  $f$ and $g$ commute.</p>

<p>Show that there is $P$ a polynomial such that $g=P(f)$.</p>

<p><strong>Remark:</strong> The spectrum is simple when the characteristic polynomial has not multiple roots.</p>
",<linear-algebra>
"<p>\begin{equation}
P = \begin{bmatrix}1 &amp; 1 &amp; 0 \\ 0 &amp; 1 &amp; 3 \\ 3 &amp; 0 &amp; 1
\end{bmatrix}
\end{equation}</p>

<p>a) P is the transition matrix from what basis B to the standard
basis S = {e1, e2, e3} for R3?</p>

<p>b) P is the transition matrix from the standard basis
S = {e1, e2, e3} to what basis B for R3?</p>

<p>My attempt:</p>

<p>For a), if PB=S (is this even right?), can we just multiply inverse of P both sides to get B?</p>
",<linear-algebra>
"<p>Some years ago I was in a lecture where I met for the first time the matrix representation of some differential and integral operators (once discretized). Back then, someone mentioned me that every linear operator has its matrix representation. I liked the idea a lot, and I even think it sounded to me like a theorem. Unfortunately I have not been able to find a reference where the same affirmation (together with some worked-out examples) would be. There is this book of Burden and Faires ""Numerical Analysis"", where certainly one can find examples of the equivalence of, say elliptic partial differential equations, with the explicit matrix of the discretized equation (using finite differences), chapter 12, eight edition. But I would hope to find some references with something more general (as I said, with some theorems behind), and also some examples, even if they are confined to finite differences only from which to extract the elements to construct the matrices. </p>
",<linear-algebra>
"<p>Let $A$ and $B$ be real $n\times n$ matrices with $ABA=A$. How can I prove
$\operatorname{tr} AB\le n$?</p>
",<linear-algebra>
"<p>Let $T$ be a diagonalizable operator on a vector space $V$. Prove that the operator
$$a_nT^n + a_{n-1}T^{n-1}+\cdots+a_1T+a_0 Id_V$$
 on $V$ is also diagonalizable for any scalars $a_1, a_1,\dots,a_n$.</p>

<hr>

<p>First off, what is $Id_V$? I've never seen this before.</p>
",<linear-algebra>
"<p>The question:</p>

<p><em>Suppose in a single period investment problem we may divide our wealth between n assets and that the return on the ith security is given by</em></p>

<p><em>$r_i = \alpha + \beta_i\theta + \epsilon_i,$ $i = 1, ..., n$,</em></p>

<p><em>where $\alpha,$ $\beta_i$ are constants, and $\theta$ and $\epsilon_i$ are random variables. $\theta$ represents some underlying common economic variable (such as overall economic growth) that influences the return on all the securities. The $\epsilon_i$ are random variables assumed to be uncorrelated with each other and with $\theta$. Suppose that the $\epsilon_i$ have zero mean and common variance $\sigma_\epsilon^2$.</em></p>

<p><em>Denote the mean and variance of $\theta$ by $\mu_\theta$ and $\sigma_\theta^2$.
Determine the covariance matrix $\Sigma$ of the returns, and verify that $\beta =(\beta_1, ..., \beta_n)$ is one of its eigenvectors.</em></p>

<p><em>What can you say about the other eigenvectors? Hence find the inverse $\Sigma^{-1}$. Find the portfolio that has minimal risk, and determine its variance and expected return.
Let $\beta_i$ = $1$ for all $i = 1,2,... ,n$. Treating $\mu_\theta$, $\sigma_\theta^2$ and $\sigma_\epsilon^2$ as fixed, let $n$ tend to infinity, and comment on the minimal variance portfolio. Interpret this in terms of diversification as an investment strategy.</em></p>

<p>Basically, I have done all of this question except everything after computing $\Sigma^{-1}$. I have the following formula for $\Sigma^{-1}$:</p>

<p>$\Sigma^{-1}$ = $aI  + b$, where $a = \displaystyle\frac{1}{\sigma_\epsilon^2}$ and $b = \left(\displaystyle\frac{1}{\sigma_\epsilon^2 + \sigma_\theta^2\beta^T\beta} - \displaystyle\frac{1}{\sigma_\epsilon^2}\right) \displaystyle\frac{1}{\beta^T \beta}$, </p>

<p>which I'm almost certain is correct. The problem is, that is pretty messy! So when I want to compute the minimal variance portfolio which is given by</p>

<p>$w = \displaystyle \frac{\Sigma^{-1}1}{1^T\Sigma^{-1}1}$, so I can compute the mean return. The variance is just $\displaystyle \frac{1}{1^T\Sigma^{-1}1}$, but this is still messy. </p>

<p>I get the motivation behind the question, which is that the $\epsilon_i$ represent risk due to unforeseen circumstances where as $\theta$ is risk felt by everyone in the market, and that by letting $n \rightarrow \infty$ I can in essence ""diversify away"" the contribution of risk due to the $\epsilon_i$, but I cannot show this explicitly, which is frustrating. </p>

<p>So, does anybody have smart ways of calculating the minimum variance portfolio, weights, variance etc? </p>
",<linear-algebra>
"<p>The way I always understood linear functionals on a vector space $V$ is to consider then as measuring objects which give projections when they are given vectors. Now I wanted to make this a little bit more precise and I did as follows: let $\omega \in V^\ast$ be a linear functional, $\omega \neq 0$. Since $\omega : V\to K$ where $K$ is the underlying field, and since $\omega$ is not null, we have that $\operatorname{Im}(\omega) = K$ since $K$ has dimension $1$.</p>

<p>Because of that, the rank-nullity theorem gives $\dim V = \dim \ker \omega + \dim K$ and this implies $\dim \ker \omega = \dim V - 1$. In that case $\ker \omega$ is a hyperplane and we know we can split $V$ as:</p>

<p>$$V = \ker \omega \oplus W,$$</p>

<p>now ne can show that there exists just one $w\in W$ such that $\omega(w) = 1$. To prove it, suppose there's another, say $\tilde{w}\in W$, then since $W$ is one-dimensional one has $\tilde{w} = kw$. Because of that we have $\omega(\tilde{w}) = k\omega(w)$ and since we suppose that both $w$ and $\tilde{w}$ are such that $\omega(w) = \omega(\tilde{w}) = 1$ one has $k =1 $ and $w = \tilde{w}$.</p>

<p>Given that, one can define the operator $P_W : V\to V$ given by $P_W(v) = \omega(v)w$ where $w$ is the unique vector with $\omega(w) = 1 $. This linear operator is a projection operator because $P_W^2 = P_W$.</p>

<p>In that case we can think of it in the following way: $P_W$ is a projection operator in the direction of $W$ with the property that $\omega$ is the object which measures the projections. So this unique $w$ defines ""one unit"" of $\omega$ and $\omega$ measure projections with respect to this $w$.</p>

<p>Is that the correct way to think about linear functionals? Is that way they can be considered measuring objects capable of giving projections?</p>
",<linear-algebra>
"<p>Show that $ B_1 = \{\textbf{Av}| \textbf{v} \in B\} $ is also a basis for $\mathbb{R}^n.$</p>

<p>I apologize for my informality, but I would really like some feedback as to whether I am using the correct reasoning. </p>

<p>To begin, I noticed that this set $ B_1$ should contain an $\textit{n}\times 1$ vector. So, this means that I need to prove that this vector is linearly independent and spans $\mathbb{R}^n$. </p>

<p>Since no component of $\textbf{v}$ is $0$ because it is linearly independent, and $\textbf{A}$ only has the trivial solution for a homogeneous system, then any $\textbf{Av}$ should be linearly independent. Because the dimensions of $\textit{B}_1$ are the same as $\mathbb{R}^n$, then it is a basis.</p>
",<linear-algebra>
"<ol>
<li>$\{(x,y): x^2+y^2=0, x,y\in\mathbb{C}\}$, is it a subspace of $\mathbb{C}^2$?</li>
</ol>

<p>I thought yes atleast a trivial subspace as $\{(0,0)\}$ , the answer says No!</p>

<ol start=""2"">
<li>$\{(x,y): x^2-y^2=0, x,y\in\mathbb{R}\}$, is it a subspace of $\mathbb{R}^2$?</li>
</ol>

<p>I thought yes as it is  just $y=x$ or $y=-x$ so one dimensional subspace of the plane, but answer says No!</p>

<p>3.$\{(x,y): xy=0, x,y\in\mathbb{R}\}$, is it a subspace of $\mathbb{R}^2$?</p>

<p>I thought yes atleast a trivial subspace as $\{(0,0)\}$ , the answer says No!</p>

<p>please help</p>
",<linear-algebra>
"<p><strong>Background:</strong> Many (if not all) of the transformation matrices used in $3D$ computer graphics are $4\times 4$, including the three values for $x$, $y$ and $z$, plus an additional term which usually has a value of $1$.</p>

<p>Given the extra computing effort required to multiply $4\times 4$ matrices instead of $3\times 3$ matrices, there must be a substantial benefit to including that extra fourth term, even though $3\times 3$ matrices <em>should</em> (?) be sufficient to describe points and transformations in 3D space.</p>

<p><strong>Question:</strong> Why is the inclusion of a fourth term beneficial? I can guess that it makes the computations easier in some manner, but I would really like to know <em>why</em> that is the case.</p>
",<linear-algebra>
"<p>Let $V$ be a finite dimensional real vector space and let $A:V\to V$ be a linear map such that $A^2=A$. Assume that $A\ne0$ and that $A\ne I$. Which of the following statements are true?</p>

<p>a. $ker(A)\ne0$</p>

<p>b. $V=ker(A)\oplus R(A)$</p>

<p>c. The map $I+A$ is invertible</p>

<p>(c) is true since eigen value of $A$ cann't be $-1$. (a) and (b) are too true??. Not sure about them</p>
",<linear-algebra>
"<p>In each of the following cases, describe the smallest subset of $C$ which contains all the eigenvalues of every member of the set $S$.</p>

<p>(a) $S=\{A\in M_n(C) | A=BB^*$ for some $B\in M_n(C)\}$</p>

<p>(b) $ S=\{A\in M_n(C)| A=B+B^*$ for some $B\in M_n(C)\}$</p>

<p>(c) $ S=\{A\in M_n(C)| A+A^*=0 \}$</p>

<p>For $\lambda$ as an eigen value of $B$ , (a) will have $\lambda\lambda^*=|\lambda|^2\ge0.$ Hence The smallest subset will be $\{x\in R|x\ge0\}$.</p>

<p>(b) will have the set as $R$. </p>

<p>(c)  will have the set as the set of purely imaginary numbers</p>
",<linear-algebra>
"<p>Consider an $m\times m$ non-negative matrix $A$ where elements of $A$ can take many different values e.g. they are functions of a variable z. Suppose $A$ is such that one of its eigenvalues is equal to one. Can we say anything about the properties of matrix $A$? </p>

<p>For example, a sufficient condition is that the sum of all columns to be one [plus irreducibility]. Under this condition, irrespective of the values of the elements of the matrix, one eigenvalue is always equal to one. My question: is there a simple necessary condition?</p>
",<linear-algebra>
"<p>If $V$ and $W$ are vector spaces and $T$ is a linear transformation such that $T:V\longrightarrow W$. Furthermore, if $T$ is onto then $\mathrm{rank}(T) = \dim(W)$ right? The range is on the entire codomain then. This is not a homework question I'm reviewing. I know it sounds stupid but I want to make sure I'm not missing something. Shouldn't this be the definition of onto? It's funny because this statement isn't mentioned anywhere in the text.</p>
",<linear-algebra>
"<p>If a man can row at a speed of x m/sec , and the water in the river flows at a speed of  y m/sec ,  then if then man wants to cross the river from one bank to the opposite bank, i.e. he would be moving horizontally in the river , neither upstream nor downstream , then what would be the effect on the speed of rowing of the man . What would be his speed in terms of x and y ?</p>
",<linear-algebra>
"<p>Looking for an elegant proof of $\det(\textbf{A}) = \det(\textbf{A}^{t})$ without Schur decomposition.</p>

<p><strong>Proof 1 with Schur decomposition</strong>
$$\textbf{A} = \textbf{P}^{t}\Delta\textbf{P} \implies\textbf{A}^{t} = (\textbf{P}^{t}\Delta\textbf{P})^{t} = \textbf{P}^{t}\Delta^{t}\textbf{P}$$
So, $\textbf{P}$  is unitary matrix, $\textbf{P}^{t}=\textbf{P}^{-1}$.
$$\det(\textbf{P})=\det(\textbf{P}^{t})= \det(\textbf{P}^{-1})$$
 $\Delta$ is upper triangular matrix.
$$\det(\Delta)=\det(\Delta^{t}) \implies   \det(\textbf{A})=\det(\textbf{A}^{t})$$ </p>
",<linear-algebra>
"<p>How to solve matrix equation $AXH+AHX−BH=0$? All matrices are square, $A$, $B$ known constant matrices and invertible, $H$ can take any value, $X$ represent the solution to be found. </p>

<p>I have seen about the Sylvester Equation like in this post <a href=""http://math.stackexchange.com/questions/39906/solving-a-matrix-equation-ax-xb-in-a-cas"">Solving a matrix equation $AX=XB$ in a CAS</a>, but I'm not sure how to apply it because of the presence of matrix H.</p>
",<linear-algebra>
"<p>I have that the set of vectors:</p>

<p>$$\vec u, \vec v, \vec w$$
is L.I.</p>

<p>This means that:</p>

<p>$$a_1\vec u + a_2\vec v + a_3\vec w = \vec 0 \implies a_1 = a_2 = a_3 = 0$$</p>

<p>I need to prove that the set:</p>

<p>$$(\vec u + \vec v + \vec w, \vec u - \vec v, 3\vec v)$$</p>

<p>Is also L.I.</p>

<p>What I did was to make a linear combination with the vectors, so:</p>

<p>$$k_1(\vec u + \vec v + \vec w) + k_2(\vec u - \vec v) + k_3(3\vec v) = \vec 0 \tag{1}$$</p>

<p>If this set is L.I., then $k_1 = k_2 = k_3 = 0$.</p>

<p>Expanding the sum:</p>

<p>$$k_1\vec u + k_1\vec v + k_1\vec w + k_2\vec u - k_2\vec v + 3k_3\vec v = \vec 0$$</p>

<p>So I have:</p>

<p>$$(k_1+k_2)\vec u + (k_1 -k_2 + 3k_3)\vec v + k_1 \vec w = \vec 0$$</p>

<p>By $(1)$ I have that this is a L.I. set iff 
$$(k_1+k_2) = (k_1 -k_2 + 3k_3) = k_1 = 0$$</p>

<p>Am I correct? How do I prove, then, that $(k_1+k_2) = (k_1 -k_2 + 3k_3) = k_1 = 0$?</p>
",<linear-algebra>
"<p>In general the inverse of a sparse matrix is dense. A notable (but trivial) exception from that rule are diagonal matrices. Is there any other (broad) class of sparse matrices whose inverse is also sparse?</p>
",<linear-algebra>
"<p>There exists a continuous function $f$ whose domain is $[2,5]$ and the range is $(3,4)$. We have to prove that there exists at least one point $p \in (2,5)$ such that $f(p)=p$.</p>

<p>Now this is easy to see intuitively. The values of $f$ increase more slowly than the values of $x$ do. In the beginning, $f(x)&gt;x$, in the end, $f(x)&lt;x$. So, there must be at least one point at which $f(x)=x$.</p>

<p>But how do I prove this rigourously, using some mathematical arguments?</p>
",<linear-algebra>
"<p>(Long time observer, first time asking a question, so excuse me if I get any of the rules wrong)</p>

<p>I am having trouble wrapping my head around this problem and presenting the proof. </p>

<p>If I know A, B is invertible, given: $X^{T}$$(BA)^{T}$$A^{T}$=$V^{T}$ I would like to show X = $A^{-1}$$B^{-1}$$A^{-1}$V </p>

<p>This is what I have so far</p>

<p>Given $X^{T}(BA)^{T}A^{T} = V^{T}$  and since we know $A, B$ is invertible then by property of transposes  $(AB)^{T}= B^{T}A^{T}$;</p>

<p>then $X^{T}A^{T}B^{T}A^{T} =V^{T}$;</p>

<p>$(XABA)^{T} = V^{T}$ by transposing both sides - ""involution"" </p>

<p>$XABA = V$ Then since $ABA $is invertible, it can be moved to the other side</p>

<p>as required I am left with $X = A^{-1}B^{-1}A^{-1}V$</p>

<p>Any comments or suggestions are appreciated!</p>
",<linear-algebra>
"<p>Can we define a vector space structure on $\mathbb {R}^n$ other than usual scalar multiplication and usual addition such that the dimension of $\mathbb {R}^n$ over $\mathbb {R}$ is not $n$ but some $m$ not equal to $n$?</p>
",<linear-algebra>
"<p>An ellipsoid centered at the origin is defined by the solutions $\mathbf{x}$ to the equation $\mathbf{x}^TM\mathbf{x} = 1$, where M is a positive definite matrix.</p>

<p>How can I see why M needs to be positive definite, based on the equation of an ellipse $Ax^2 + Bxy + Cy^2 = 1$ where $B-4AC &lt; 0$? It looks like the idea is to make $B-4AC &lt; 0$ equate to the requirement that all eigenvalues of $M$ are positive for a 2x2 matrix, but I can't seem to make it work.</p>

<p>Also, what other shapes can we represent with $\mathbf{x}^TM\mathbf{x} = 1$ when $M$ is not positive definite?</p>
",<linear-algebra>
"<p>There's a theorem in Linear Algebra which says that if ${\bf A}$ is an $m \times n$ matrix and $m &lt; n$, then the homogeneous system of linear equations ${\bf A}{\bf X}=0$ has a non trivial solution.</p>

<p>I read a proof but what happens if I have a matrix like ${\bf B}$, whereas</p>

<p>B =        \begin{pmatrix}
        1 &amp; 0 &amp; 0 &amp; 0\\
        0 &amp; 0 &amp; 0 &amp; 0\\
        0 &amp; 0 &amp; 0 &amp; 0\\
        \end{pmatrix}</p>

<p>For ${\bf B}$, $m &lt; n$ but the only solution for ${\bf B}$ is the trivial one, unless I am getting something wrong, which I suspect and I would appreciate a lot if you help me find what is it.</p>

<p>Thank you in advance.</p>
",<linear-algebra>
"<p><a href=""http://i.stack.imgur.com/podWE.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/podWE.png"" alt=""The problem:""></a></p>

<p><a href=""http://i.stack.imgur.com/LqSh7.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/LqSh7.png"" alt=""enter image description here""></a></p>

<p>I understand how to find the image($A$). The basis of Im($A$) would be the first two columns of the matrix $A$ (given the two leading 1's in ref are in the first and second columns). </p>

<p>So the $\text{Ker}(B) = [1,1,1,1],[1,2,3,4]$ </p>

<p>But I do not get how to find $B$ based on its kernel? Any ideas? </p>
",<linear-algebra>
"<blockquote>
  <p><strong>Problem Statement</strong>: Let $A=\begin{bmatrix} 2 &amp;&amp; 1 \\ 1 &amp;&amp; 2 \end{bmatrix}$. Find an orthonormal basis for $\mathbb{C}^2$ with respect to the Hermitian form $\bar{x}^TAy$.</p>
</blockquote>

<p>I am trying to figure out this proof, but not sure if what I am currently trying to do will help me. I was goin to start out by taking an arbitrary basis $B=\left\{v_1,v_2\right\}$ and write two arbitrary vectors in $\mathbb{C}^2$ as linear combinations of basis elements:
$$v=x_1v_1+x_2v_2$$ $$w=y_1v_1+y_2v_2$$</p>

<p>Then break down the Hermitian form $$\langle v,w \rangle=\langle x_1v_1+x_2v_2, y_1v_1+y_2v_2 \rangle=\bar{x_1}\langle v_1, y_1v_1+y_2v_2 \rangle+\bar{x_2}\langle v_2, y_1v_1+y_2v_2 \rangle=\bar{x_1}y_1\langle v_1, v_1 \rangle+\bar{x_1}y_2\langle v_1, v_2 \rangle+\bar{x_2}y_1\langle v_2, v_1 \rangle+\bar{x_2}y_2\langle v_2, v_2 \rangle=\sum_{i=1,j=1}^2 \bar{x}_iy_j\langle v_i, v_j\rangle=\bar{x}^TAy$$</p>

<p>Then I want to use the fact that we're given $A$ in order to define $v_1,v_2$, but I am confused with one thing:
In my notes, I have that $$a_{ij}=\langle v_i,v_j\rangle$$ but my notes also say for any orthonormal basis, that $$\langle v_i,v_j\rangle:=\begin{cases} 1\ \mathrm{if}\ i=j \\ 0\ \mathrm{if}\ i\neq j \end{cases}$$
which conflicts with the fact that $A=\begin{bmatrix} 2 &amp;&amp; 1 \\ 1 &amp;&amp; 2 \end{bmatrix}$ because that would mean that $$\langle v_i,v_j\rangle:=\begin{cases} 2\ \mathrm{if}\ i=j \\ 1\ \mathrm{if}\ i\neq j \end{cases}$$</p>

<p>Is there a more straightforward approach to this problem?</p>
",<linear-algebra>
"<p>Suppose that $X$ is $n\times K$ with full column rank and $y$ $n\times 1$. I understand that if $\beta$ satisfies the system $X\beta=y$, then $\beta=(X'X)^{-1}X'y$ (dimension $k\times 1$). But how do I verify the reverse direction
$$
\beta=(X'X)^{-1}X'y\implies X\beta=y?
$$
This question is self-contained but it is related to what I asked <a href=""http://math.stackexchange.com/q/983816/178464"">earlier</a>. Thank you for your help. </p>
",<linear-algebra>
"<p>Consider the following homogeneous equation where $A$ and $X$ are matrices.</p>

<p>$$AX = 0$$</p>

<p>I want to know whether there are non trivial solutions for this equations.
Now, if $A^{-1}$ exists, then I can multiply throughout by it and get $X = 0$, so if $A$ is invertible, only the trivial solution exists.
However, I do not understand why $A$ being non-invertible would imply that non-trivial solutions exists, shouldn't it just imply that no solutions exist?</p>
",<linear-algebra>
"<p>I have been given the quadratic form $$A(x,x) = 2x^2-\frac{1}{2}y^2-2xy-4xz$$ and been asked to diagonalize it, find the change of basis matrix, and find the new basis in which A is diagonalized.</p>

<p>I found the diagonalized version of A to be $$2\xi_1^2-\frac{1}{2}\xi_2^2-2\xi_3^2$$ where $\xi_1 = x-\frac{y}{2}-z$,$\xi_2=y$,$\xi_3=\frac{y}{2}+z$.  I was then able to calculate the change of basis matrix $$B = \begin{pmatrix} 0 &amp;0 &amp;-1\\0 &amp; 1 &amp; -\frac{1}{2} \\ 1 &amp; 2 &amp; 2 \end{pmatrix}$$</p>

<p>This leads to my question.  How do I find the new basis?  Do I just perform Gaussian elimination on the diagonalized version of A?  So the new basis would just be the stardard basis $$\begin{pmatrix} 1 &amp;0 &amp; 0\\0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{pmatrix}$$</p>
",<linear-algebra>
"<p>Let n=1,2,3,... and $i^2=-1$ and:</p>

<p>$$F=[e^{i\frac{2\pi kl}{n}}]_{k,l=0}^{n-1}\in\Bbb{C}^{n,n}$$</p>

<p>Find $F^HF$ and $F^{-1}$.</p>

<p>In this quite challenging (at least for me) problem I started from finding the matrix $F^HF$. In order to do that you need find $F^H$ first. I think the equation of this matrix is $F^H=[e^{-i\frac{2\pi kl}{n}}]_{k,l=0}^{n-1}$. So now let's think about matrix $F^HF$. To do that we need to know what is the k'th row of $F^H$. It looks like this:</p>

<p>$[e^{-i\frac{2\pi k0}{n}} e^{-i\frac{2\pi k1}{n}} e^{-i\frac{2\pi k2}{n}}... e^{-i\frac{2\pi k(n-1)}{n}}]$.</p>

<p>Now what about j'th column of $F$? According to definition it should look like that:
$[e^{i\frac{2\pi 0j}{n}} e^{i\frac{2\pi 1j}{n}} e^{i\frac{2\pi 2j}{n}}... e^{i\frac{2\pi (n-1)j}{n}}]^T$</p>

<p>So let's think about indices k,j of $F^HF$. It should be sum of multiplication of corresponding elements of k'th row of $F^H$ and j'th column of $F$. So the value at indices k,j of $F^HF$ should look like that:</p>

<p>$e^{-i\frac{2\pi k0}{n}}*e^{i\frac{2\pi 0j}{n}} + e^{-i\frac{2\pi k1}{n}}*e^{i\frac{2\pi 1j}{n}}+...+e^{-i\frac{2\pi k(n-1)}{n}}*e^{i\frac{2\pi (n-1)j}{n}}$</p>

<p>So we can write $F^HF$ down as:</p>

<p>$[\sum_{m=0}^{n-1}e^{\frac{i2\pi m}{n}(j-k)}]_{k,j=0}^{n-1}$</p>

<p>Have I done everything right? Or maybe I completely screwed up this part of the problem?</p>

<p>Also, how to proceed with finding $F^{-1}$?</p>
",<linear-algebra>
"<p>I want to find the projection from $\mathbb{R}^n$ onto a vector subspace of $\mathbb{R}^n$. Can I do this by adding the projections to each basis vector, even if the basis vectors are not orthogonal?
Specifically, projecting $x$ onto $V$, can I define the projection $$\operatorname{proj}_V(x) = \sum_i \frac {v_i\cdot x}{v_i\cdot v_i}v_i$$ for basis vectors $v_i$?</p>
",<linear-algebra>
"<p>Let $L_A$ :$R^{3}_{col}$ $\rightarrow$ $R^{3}_{col}$ , X $\rightarrow$ AX be operator of left multiplication by matrix $$A=\begin{bmatrix}1 &amp; 2 &amp; 1 \\ 0 &amp; 1 &amp; 1 \\ -1 &amp; 3 &amp;4\end{bmatrix}$$Find bases of :</p>

<p><strong>a.</strong> kernel Ker $L_A$</p>

<p><strong>b.</strong> image Im $L_A$</p>

<p><strong>c.</strong> Ker $L_A$ + Im $L_A$ and Ker $L_A$ $\cap$ Im $L_A$</p>

<p>I found the null space of $L_A$ as [1,-1,1] and the image as [1,0,-1] and [2,1,3].  But I couldn't think of something for part c.
Do we use the fact that dim Ker $L_A$ + dim Im $L_A$= dim $L_A$??</p>
",<linear-algebra>
"<p>Let $V$ be a real vector space equipped with a scalar product $\langle, \rangle$ (i.e. a positive definite symmetric bilinear form). </p>

<p>We say that an endomorphism $J: V \to V$ is an almost complex structure if $J^2=-Id.$ </p>

<p>$J$ is said to be compatible with the scalar product if $\langle J v, J w \rangle = \langle v, w \rangle. $</p>

<p>I'd like a very simple example of a scalar product and almost complex structure such that $J$ FAILS to be compatible with $\langle, \rangle.$   This is very basic -and hopefully trivial- but I can't find any counterexamples. </p>
",<linear-algebra>
"<p>H is an inner product space with inner product $( . , . )$ over the complex numbers, and $T∈L(H,H)$.  Let $R=T+T^*$, $S=T-T^*$ .   Supposing that T is normal and $T(\alpha)=(x+iy)\alpha$, how do I compute $(R∘S)(\alpha)$ in terms of $x,y,\alpha$ if I know that $R∘S=T∘T-T^*∘T^*$?  Thanks for any and all help!</p>
",<linear-algebra>
"<p>I have a problem with the following question.</p>

<p>For which $n$ does the following equation have solutions in complex numbers</p>

<p>$$|z-(1+i)^n|=z $$</p>

<p>Progress so far.</p>

<ol>
<li><p>Let $z=a+bi$.</p></li>
<li><p>Since modulus represents a distance, the imaginary part of RHS has to be 0. This immediately makes $b=0$.</p></li>
<li><p>If solutions are in the complex domain $|a-(1+i)^n|=a $ by 2., and $a$ is Real. </p></li>
<li><p>?</p></li>
</ol>

<p>I don't know where to go from here.  </p>
",<linear-algebra>
"<p>Define operatot $L$ :$R^{3}_{col}$$\rightarrow$ $R^{3}_{col}$ by equation $L(x_1,x_2,x_3)=(3x_1+x_3,-2x_1+x_2,-x_1+2x_2+4x_3)$.</p>

<p><strong>a.</strong> Find matrix L in the standard basis of $R^{3}_{col}$.</p>

<p><strong>b.</strong> and in the basis $f_1=(1,0,1)$, $f_2=(-1,2,1)$,$f_3=(2,1,1)$.</p>

<p><strong>c.</strong> Show that L is invertible and evaluate $L^{-1}(x_1,x_2,x_3)$.</p>

<p>Do we write the linear equation in the form $$A=\begin{bmatrix}3 &amp; 0 &amp; 1 \\ -2 &amp; 1 &amp; 0 \\ -1 &amp; 2 &amp;4\end{bmatrix}$$. And then from here by row reducing to row echelon matrix find the basis.Right??</p>
",<linear-algebra>
"<p>I have two vectors, 1 is the current direction of a moving object and the other is the new direction that I want that object to change to.</p>

<p>What I'm trying to achieve is to get the current direction to gradually change to the new direction but I'm not really sure how to do this. It would be similar a ship slowly turning I guess.</p>

<p>Could someone help me out please?</p>
",<linear-algebra>
"<p>Is it true that there are irreducible hyperbolic polynomials $p(x,y,z) \in \mathbb{R}[x,y,z]$, $p$ homogeneous of any degree? Are there even concrete examples for such polynomials?</p>

<p>I know that there are irreducible polynomials of any degree and that there are hyperbolic polynomials of any degree in this setting, but I do not see whether also polynomials which have both properties exist of any degree. </p>

<p>Thank you for your help.</p>

<p>Edit: By hyperbolic I mean the following: </p>

<p>$p$ is hyperbolic with respect to $\textbf{e} \in \mathbb{R}^3$ if $p(\textbf{e}) &gt; 0$ and for all vectors $\textbf{x} \in \mathbb{R}^3$ the univariate polynomial $t \mapsto p(\textbf{x} - t\textbf{e})$ has only real roots.</p>
",<linear-algebra>
"<p>If $X,Y,Z$ are three random variables with normal distribution, how can we define the joint distribution of $(X,Y,Z)$? Can we define a similar covariance tensor $\Sigma=Cov(X,Y,Z) = E[(X-E[X])(Y-E[Y])(Z-E[Z])]$? </p>

<p>Notice that knowing only the joint distribution of any two is not sufficient to know the joint distribution. We need the know the information of such a tensor $\Sigma$.</p>

<p>My question comes from in the following problem. Consider $\Sigma$ to be symmetric, $\Sigma_{ijk} = \Sigma_{ikj}=\Sigma_{jik}=\Sigma_{jki}=\Sigma_{kij}=\Sigma_{kji}$. </p>

<p>If 
$$\partial_t \Sigma_{ijk} + \Sigma_{sjk} \partial_{x_s} u_i  +\Sigma_{isk} \partial_{x_s} u_j +\Sigma_{ijs} \partial_{x_s} u_k = 0, $$
can we find a function $f=f(\Sigma)$ such that 
$$\partial_t f + \partial_{x_s} u_s = 0$$
Here we use that Einstein summation notation. And for $\sigma$ being a matrix, this is easy. Since if $\partial_t \sigma_{ij} + \sigma_{sj} \partial_{x_s} u_i+\sigma_{is} \partial_{x_s} u_j=0$, we have $f(\sigma)=\ln \det \sigma$ satisfying $f_t + 2 \nabla \cdot u = 0$.</p>

<p>These two questions are related, which I will not show here.</p>
",<linear-algebra>
"<blockquote>
  <p>Let $c_{00} (\mathbb{N})$ denote the space of finitely non-zero sequences, and let $(\beta_n)_{n \in \mathbb{N}} \subset \mathbb{F}$ be a sequence of scalars. Then the subsets
  $$X := \{(x_n)_{n \in \mathbb{N}} \in c_{00} (\mathbb{N}) \; | \; x_{2n} = 0, \; \forall n \in \mathbb{N} \}, \quad Y := \{(x_n)_{n \in \mathbb{N}} \in c_{00} (\mathbb{N}) \; | \; x_{2n-1} + \beta_n x_{2n} = 0, \; \forall n \in \mathbb{N} \}$$ are complementary subspaces of $c_{00} (\mathbb{N})$, that is, the subsets $X, Y$ are closed subspaces and $c_{00} (\mathbb{N})$ is the internal direct sum of $X$ and $Y$.</p>
</blockquote>

<p>It follows readily that $X$ and $Y$ are closed subspaces of $c_{00} (\mathbb{N})$. However, I do not succeed in showing that $c_{00} (\mathbb{N})$ is the internal direct sum of $X$ and $Y$.</p>
",<linear-algebra>
"<p>The theory of commutative absolutely flat rings (a.k.a. commutative von Neumann regular rings) is algebraic and furthermore it is the smallest variety containing all fields. Being a variety the category of (set-theoretic) models is arguably much better behaved than the category of fields.</p>

<blockquote>
  <p>How does linear algebra in modules over commutative absolutely flat rings compare
  to plain linear algebra, i.e. working with vector spaces? Is is it
  basically the same or are there some major differences?</p>
</blockquote>

<p>Let's ask some more concrete questions:</p>

<p>Let $R$ be an commutative absolutely flat ring: Are finitely generated $R$-modules always free? Assuming the axiom of choice: Are <em>all</em> $R$-modules free?</p>
",<linear-algebra>
"<blockquote>
  <p>How to prove that eigenvalues of a rotation matrix in $\text{SO}(3)$ are $e^{(i\theta)}$ , $e^{(−i\theta)}$?</p>
</blockquote>

<p>Here, $\theta$ is the angle of rotation and $i$ is $\sqrt{-1}$ . </p>

<p>Edit 1:</p>

<p>I have been able to prove that there is one eigenvalue of 1, and other two are complex conjugates of each other. This comes from the fact that the rotation matrix can be written as an exponential $R = e^\omega $ where R is the rotation matrix and $\omega$ is a skew symmetric matrix. I also was able to determine that the axis of rotation is the eigenvector corresponding to eigenvalue 1. What I'm not being able to prove is that how are the other eigenvalues related to the angle of rotation. </p>
",<linear-algebra>
"<p>I saw the following question a book of mine:</p>

<blockquote>
  <p>Show that an $n\times n$ orthogonal matrix has $n(n-1)/2$ independent parameters.</p>
</blockquote>

<p>I have no idea what an <em>independent parameter</em> is. Could you explain it to me?</p>
",<linear-algebra>
"<p>List all diagonalizable $2\times 2$ matrices over the a field $F$ consisting of two elements $0$ and $1$.</p>

<p>I want to try and do this using C++, but perhaps this isn't the place to ask. I have an idea as to how I'd do it.</p>
",<linear-algebra>
"<p>I have a differential equation $$N'_x(x)=G(x)N(x)$$ where $N, G$ are $2\times2$ matrices depending on $x$, and $G$ satisfies $\sigma G+G\sigma=0$, $\sigma$ is one half of the pauli matrix, i.e. $$\sigma=\begin{pmatrix}\frac{1}{2}&amp;0\\
0&amp;\frac{-1}{2}\end{pmatrix}$$ My question is:</p>

<blockquote>
  <p>Would $N^{\ast}\sigma N$ then be independent of $x$? Why or why not?</p>
</blockquote>
",<linear-algebra>
"<p>I would like to determine if the following map $T$ is a linear transformation:</p>

<p>\begin{align*}
T: P_{2} &amp;\to P_{2}\\
A_{0} + A_{1}x + A_{2}x^{2} &amp;\mapsto A_{0} + A_{1}(x+1) + A_{2}(x+1)^{2}
\end{align*}</p>

<p>My attempt at solving:</p>

<p>\begin{align}
T(p + q) &amp;= p(x+1) + q(x+1)\\
&amp;= \left[A_{0} + A_{1}(x+1) + A_{2}(x+1)^2\right] + \left[b_{0} + b_{1}(x+1) + b_{2}(x+1)^2\right]\\
&amp;= \left(A_{0} + b_{0}\right) + \left(A_{1} + b_{1}\right)(x+1) + \left(A_{2} + b_{2}\right)(x+1)^2\\
&amp;= T(p) + T(q)
\end{align}</p>

<p>Is this right so far? If not, what am I doing wrong?</p>
",<linear-algebra>
"<p>The question: </p>

<blockquote>
  <p>Show that if $A$, $B$, and $A+B$ are invertible matrices with the
  same size, then: $$A(A^{-1}+B^{-1})B(A+B)^{-1} = I$$</p>
</blockquote>

<p>I began by multiplying the first $A$:</p>

<p>$I+AB^{-1}B(A+B)^{-1}=I$</p>

<p>and then</p>

<p>$I + A(A+B)^{-1} = I$</p>

<p>At this point I'm not sure what to do. Should I just assume $A(A+B)^{-1} = 0$, or does that not work to prove this?</p>
",<linear-algebra>
"<p>Say I have the following <em>second</em> order 7 x 7 system of equations:</p>

<ul>
<li>$x_1'' = 10(x_2- x_1- 1)$ </li>
<li>$x_2'' = 10(x_3- 2x_2+ x_1)$</li>
<li>$x_3'' = 10(x_4- 2x_3+ x_2)$</li>
<li>$x_4'' = 10(x_5- 2x_4+ x_3)$</li>
<li>$x_5'' = 10(x_6- 2x_5+ x_4)$</li>
<li>$x_6'' = 10(x_7- 2x_6+ x_5)$</li>
<li>$x_7'' = 10(x_6- x_7)$.</li>
</ul>

<p>How would I convert this second order 7 x 7 system into a <em>first</em> order 14 x 14 system using the additional equations $v_j = x'_j$, where $j = 1, 2, 3, ..., 7$?</p>
",<linear-algebra>
"<p><strong>Problem:</strong> Solve the following system in function of the parameter $b$:</p>

<p>\begin{align*} \begin{cases} -bx + 2y - (2+b^2)z + bu &amp;= -2 \\ x -2y + bz -u &amp;= 0 \\ x + (2b-4)y + (2-b)z + (b-1)u &amp;= 2 \\ x -2by -(3b+2)z + (4b-5)u &amp;= 2b-4 \end{cases} \end{align*}</p>

<p><strong>Attempt at solution:</strong> We write down the augmented matrix of this system, and then apply the operations: $R_1 \leftrightarrow R_2, R_2 \rightarrow R_2 + b R_1$. This gives us the matrix: \begin{align*} \left(\begin{array}{cccc|c} 1 &amp; -2 &amp; b &amp; -1 &amp; 0 \\ 0 &amp; (2-2b) &amp; (2+b^2) &amp; 0 &amp; -2b \\ 1 &amp; (2b-4) &amp; (2-b) &amp; (b-1) &amp; 2 \\ 1 &amp; -2b &amp; -(3b+2) &amp; (4b+5) &amp; (2b-4) \end{array}\right) \end{align*} After that, we do $R_3 \rightarrow R_3 - R_1$ and $R_4 \rightarrow R_4 - R_1$: \begin{align*}\left(\begin{array}{cccc|c} 1 &amp; -2 &amp; b &amp; -1 &amp; 0 \\ 0 &amp; (2-2b) &amp; (2+2b^2) &amp; 0 &amp; -2b \\ 0 &amp; (2b-2) &amp; (2-2b) &amp; b &amp; 2 \\ 0 &amp; (-2b+2) &amp; (-4b-2) &amp; (4b-4) &amp; (2b-4) \end{array}\right) \end{align*} Now I want a leading $1$ at the position $a_{22}$. </p>

<p><strong>Case 1.</strong> Let $b = 1$. Then our matrix reduces to \begin{align*} \left(\begin{array}{cccc|c} 1 &amp; -2 &amp; 1 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; 4 &amp; 0 &amp; -2 \\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 \\ 0 &amp; 0 &amp; -6 &amp; 1 &amp; -2 \end{array}\right) \end{align*} From the second row we see than that $u =2$. Substituting this in the last equation gives $z = - \frac{2}{3}$. But from the second row $z = - \frac{1}{2}$, which is a contradiction. Does this mean I can conclude the system has no solutions in this case?</p>

<p>Then if $b \neq 1$, should I just proceed with Gauss-elimination untill I hit another case distinction?</p>
",<linear-algebra>
"<p>I was reviewing my homework and it seems I overlooked something crucial while proving some ring has no Invariant Basis Number property. This is exercise VI.1.12 in Aluffi's <em>Algebra: Chapter 0</em></p>

<p>The setup: $V$ is a $k$-vector space and let $R = \mathrm{End}_{k}(V)$.</p>

<ol>
<li>Prove that $\mathrm{End}_{k}(V\oplus V) \cong R^4$ as an $R$-module</li>
<li>Prove that $R$ doesn't satisfy the IBN property if $V = k^{\oplus \mathbb N}$.</li>
</ol>

<p>For the first, I used to the fact that $V \oplus V$ is both the product and coproduct (in $k$-Vect) of $V$ with itself to get the isomorphism. What I just realized is I only showed that the two are isomorphic as groups not $R$-modules. So what would be the $R$-module structure on $\mathrm{End}_{k}(V \oplus V)$? </p>

<p>For the second, I used the fact that $V = k^{\oplus \mathbb N}$ implies $V \cong V \oplus V$ which in turn implies $R = \mathrm{End}_{k}(V) \cong \mathrm{End}_{k}(V \oplus V)$. Again, I just realized that I only showed the latter two are isomorphic as groups. </p>

<p>It may be obvious (and maybe why my professor let it pass?) but I can't come up with a good $R$-module structure that makes the two group isomorphisms $R$-linear.</p>

<p><strong>Edit:</strong></p>

<p>Explicitly, these are the isomorphisms I'm dealing with. Let $\pi_j, i_j$ be the natural projection/inclusion maps of the $j$-th factor resp. and $\psi: k^{\oplus \mathbb N} \oplus k^{\oplus \mathbb N} \to k^{\oplus \mathbb N}$ the isomorphism given by $\psi(e_i, 0)=e_{2i-1}$ and $\psi(0, e_i)=e_{2i}$.</p>

<p>Then the first isomorphism $\mathrm{End}_k(V \oplus V)\to R^4$ is given by $\varphi \mapsto (\pi_1\varphi i_1,\pi_2\varphi i_1,\pi_1\varphi i_2,\pi_2\varphi i_2)$</p>

<p>The second isomorphism $R \to \mathrm{End}_k(V \oplus V)$ is given by $\alpha \mapsto \psi^{-1} \alpha \psi$</p>

<p>The composition doesn't seem to be $R$-linear if I use the obvious $R$-module structure on $R$ and $R^4$.</p>
",<linear-algebra>
"<p>No matter what I do I can't seem to get this in the proper form.  Here is the system:</p>

<p>$$
\left\{
\begin{aligned}
3x+3y+12z&amp;=6 \\
3x_1+x_2-2x_3&amp;=2 \\
2x_1+2x_2+x_3&amp;=10 \\
-x+2y+8z&amp;=4
\end{aligned}
\right.
$$</p>

<p>Here is my base matrix:</p>

<p>$$ \left[
      \begin{array}{cccc|c}
        3&amp;3&amp;12&amp;6\\
        1&amp;1&amp;4&amp;2 \\
        2&amp;5&amp;20&amp;10 \\
        -1&amp;2&amp;8&amp;4
      \end{array}
    \right]$$</p>

<p>$\left(\frac{1}{3}\right)R_1-&gt;R_1$</p>

<p>$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        1&amp;1&amp;4&amp;2 \\
        2&amp;5&amp;20&amp;10 \\
        -1&amp;2&amp;8&amp;4
      \end{array}
    \right]$$</p>

<p>$(-2)R_1+R_3-&gt;R_3$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        1&amp;1&amp;4&amp;2 \\
        0&amp;3&amp;12&amp;6 \\
        -1&amp;2&amp;8&amp;4
      \end{array}
    \right]$$</p>

<p>$R_2&lt;-&gt;R_4$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        -1&amp;2&amp;8&amp;4 \\
        0&amp;3&amp;12&amp;6 \\
        1&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_2&lt;-&gt;R_3$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        0&amp;3&amp;12&amp;6 \\
        -1&amp;2&amp;8&amp;4 \\
        1&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_3+R_4-&gt;R_4$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        0&amp;3&amp;12&amp;6 \\
        -1&amp;2&amp;8&amp;4 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_2&lt;-&gt;R_3$, $R_1+R_2-&gt;R_2$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        0&amp;-1&amp;-4&amp;-2 \\
        0&amp;3&amp;12&amp;7 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$\left(\frac{1}{3}\right)R_3-&gt;R_3$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;1&amp;4&amp;2\\
        0&amp;-1&amp;-4&amp;-2 \\
        0&amp;1&amp;4&amp;2 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_1&lt;-&gt;R_2$
$$ \left[
      \begin{array}{cccc|c}
        0&amp;-1&amp;-4&amp;-2\\
        1&amp;1&amp;4&amp;2 \\
        0&amp;1&amp;4&amp;2 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_1+R_2-&gt;R_2$
$$ \left[
      \begin{array}{cccc|c}
        0&amp;-1&amp;-4&amp;-2\\
        1&amp;0&amp;0&amp;0 \\
        0&amp;1&amp;4&amp;2 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_1&lt;-&gt;R_2$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;0&amp;0&amp;0 \\
        0&amp;-1&amp;-4&amp;-2 \\
        0&amp;1&amp;4&amp;2 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_2&lt;-&gt;R_3$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;0&amp;0&amp;0\\
        0&amp;1&amp;4&amp;2 \\
        0&amp;-1&amp;-4&amp;-2 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>$R_2+R_3-&gt;R_3$
$$ \left[
      \begin{array}{cccc|c}
        1&amp;0&amp;0&amp;0\\
        0&amp;1&amp;4&amp;2 \\
        0&amp;0&amp;0&amp;0 \\
        0&amp;1&amp;4&amp;2
      \end{array}
    \right]$$</p>

<p>I stopped here because I felt like I was going to be going into a circle and I felt like I've done way too many steps.  Please help!</p>

<p>Sorry for any mistakes, it took me awhile to type this up.</p>
",<linear-algebra>
"<p>Apparently, my previous question didn't get no satisfactory answer, when I asked for two equations having a fixed value for each, <em>not necessarily linear</em>. As XenoGraff states, WolframAlpha does the task, but counts permutations of values among variables, and is thus impractical to test any two equations.</p>

<p>Actually, I ask, is it possible that there could be a system of X equations that can be solved for more than X variables, all having whole number values, considering that these X equations have an unique solution?</p>

<p>As Gerry Myerson states in the previous thread, there is the unproven conjecture that $x^{5}+y^{5}=N$ will have only one solution for $x,y$ for a given $N$, which can be modified to satisfy $x^{5}+y^{5}=x+y$ for only one set of values for $x,y$.</p>

<p>So... are there any such equations? What about differential equations (I don't understand them, anyway) and multivariates? And Diophantine equations?</p>
",<linear-algebra>
"<p>I've sort of gotten a grasp on the Chain rule with one variable.  If you hike up a mountain at 2 feet an hour, and the temperature decreases at 2 degrees per feet, the temperature would be decreasing for you at $2\times 2 = 4$ degrees per hour.</p>

<p>But I'm having a bit more trouble understanding the Chain Rule as applied to multiple variables.  Even the case of 2 dimensions </p>

<p>$$z = f(x,y),$$ </p>

<p>where $x = g(t)$ and $y = h(t)$, so</p>

<p>$$\frac{dz}{dt} = \frac{\partial z}{dx} \frac{dx}{dt} + \frac{\partial z}{dy} \frac{dy}{dt}.$$</p>

<p>Now, this is easy enough to <em>""calculate""</em> (and figure out what goes where).  My teacher taught me a neat tree-based graphical method for figuring out partial derivatives using chain rule.  All-in-all, it was rather hand-wavey.  However, I'm not sure exactly how this works, intuitively.</p>

<p>Why, intuitively, is the equation above true?  Why <strong>addition</strong>?  Why not multiplication, like the other chain rule?  Why are some multiplied and some added?</p>
",<linear-algebra>
"<p>I just came back from an intense linear algebra lecture which showed that linear transformations could be represented by transformation matrices; with more generalization, it was later shown that affine transformations (linear + translation) could be represented by matrix multiplication as well.</p>

<p>This got me to thinking about all those other transformations I've picked up over the past years I've been studying mathematics.  For example, polar transformations -- transforming <code>x</code> and <code>y</code> to two new variables <code>r</code> and <code>theta</code>.</p>

<p>If you mapped <code>r</code> to the <code>x</code> axis and <code>theta</code> to the <code>y</code> axis, you'd basically have a coordinate transformation.  A rather warped one, at that.</p>

<p>Is there a way to represent this using a transformation matrix?  I've tried fiddling around with the numbers but everything I've tried to work with has fallen apart quite embarrassingly.</p>

<p>More importantly, is there a way to, given a specific non-linear transformation, construct a transformation matrix from it?</p>
",<linear-algebra>
"<p>If you have the following matrix can $k$ be any number?</p>

<p>\begin{pmatrix}
  1 &amp; 0 &amp; 0 \\
  0 &amp; k &amp; 0 \\
  0 &amp; 0 &amp; 1
 \end{pmatrix}</p>

<p>So this is obviously an assignment question, but I couldn't find a concrete answer anywhere.</p>

<p>I would just liketo double check my reasoning with other people (long distance learning, so no other students to chat too)</p>

<p>I say no, because $k$ cannot be zero. To my understanding, an elementary matrix can only be created using a single row operation on an Identity matrix. I can't think of any operation that would create a row of zeros from an Identity matrix.</p>

<p>Is my assumption correct: $k$ can be any number except for zero.</p>
",<linear-algebra>
"<blockquote>
  <p>Define D:$\wp_{2}$($\mathbb{R}$) $\mapsto$$\wp_{2}$($\mathbb{R}$)
  by $D(p)(x) = p'(x)$ , Find the matrix of $D$ with respect to the basis
  $\{1, 1+x, 1+x+x^2 \}$ </p>
</blockquote>

<p>I was thinking this would be the matrix $\left(\begin{array}[t]{ccc}
0 &amp; 1 &amp; 2\\
0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0
\end{array}\right)$ by differentiating each of the terms in the basis, 
but i have a feeling this is wrong? </p>
",<linear-algebra>
"<p>I am trying to teach my self some linear algebra in preparation for a module in machine learning. I am using Gilbert Strang's text Introduction to Linear Algebra and am having some difficulties.</p>

<p>My specific question is: How is the last equation below an example of integration by parts? It seems to be missing $x(t)y(t)$ to me and I don't know where that has gone.</p>

<p>The book gives the following equations leading up to it:</p>

<p>$$x^Ty = (x,y) = \int_{-\infty}^\infty x(t)y(t)~dt$$ </p>

<p>From what I can tell, the above is saying that the inner product of the vectors $x$ and $y$ is equivalent or at least approximate to taking the integral of those two vectors as functions. Does the $(x,y)$ just mean inner product? I don't really understand what a vector of a function even is though.</p>

<p>$$(Ax)^Ty = x^T(A^Ty)$$</p>

<p>This associative rule for matrix-vector multiplication was given as a more rigorous view of what the transpose of $A$ actually is. It then partnered it with this:</p>

<p>$$\int_{-\infty}^\infty\frac{dx}{dt}y(t)~dt = \int_{-\infty}^\infty x(t)\left(-\frac{dy}{dt}\right)~dt$$</p>

<p>I can see how the two equations relate to eachother and that this suggests that $A^T$ is anti-symmetric. Gil mentions integration by parts here which I can definitely recognise but it doesn't seem complete, there should be an extra $x(t)y(t)$ surely?</p>
",<linear-algebra>
"<p>Suppose T: $\mathbb{R}^{2}$$\rightarrow\mathbb{R}$$^{2}$ is linear
and has matrix $\begin{pmatrix}4&amp;9\\1&amp;1\end{pmatrix}$ with </p>

<p>respect to the standard basis of $\mathbb{R}$$^{2}$. What is the
matrix of T with respect to the basis </p>

<p>$\beta$= {(1,-1), (-3,2)} ?.</p>

<p>How do we approach these sort of problems, commutative diagram? And if so how would it look?</p>
",<linear-algebra>
"<p>Let $$\mathcal{B}=\left \{\frac{1}{\sqrt{2\pi}},\frac{\cos x}{\sqrt{\pi}},\frac{\sin x}{\sqrt{\pi}},\frac{\cos 2x}{\sqrt{\pi}},\frac{\sin 2x}{\sqrt{\pi}},\dots\right \}$$.
This is an orthonormal basis of $L^2(a,a+2\pi)$ since its elements are orthonormal and $\overline{\operatorname{span}_{\mathbb{R}}B}=L^2(a,a+2\pi)$. 
Is it true that these vectors are also linearly independent?</p>
",<linear-algebra>
"<p>Hy i have a small problem. I need to prove that a transformation $$\mathbb{R}_{3}[ x ] \rightarrow \Bbb{R}^{3}$$ $$\phi (p):= [p(-1), p(0), p(1)]^T$$  is linear. 
The $$\mathbb{R}_{3}[ x ]$$ vector space is a degree of max 3.
Then i need to find the basis of the transformation kernel and basis of the image.</p>

<p>How would i do that ?</p>

<p>Thanks</p>
",<linear-algebra>
"<p>I have a problem understanding getting the KERNEL and IMAGE of a linear transformation. We have the following transformation given: 
$$ \mathbb{R}_{2}[ x ] \rightarrow \mathbb{R}_{2}[ x ] $$
$$ (\phi (p))(x) = (x p(x+1))' - 2p(x) $$</p>

<p>We first have to find its matrix in basis $$ \{ 1, x, x^2 \} $$
which I know how to get. The transformation matrix result is:</p>

<p>$$ 
\begin{bmatrix}
 -1&amp; 1&amp; 1\\ 
 0&amp;  0&amp; 4\\ 
 0&amp;  0&amp; 1
\end{bmatrix}
 $$</p>

<p>How do I get the KERNEL and the IMAGE from it ?</p>

<p>Would really appretiate an explanation, not just the result.</p>

<p>THANKS !</p>
",<linear-algebra>
"<p>In $\mathbb{R}$3 we declare  an inner product as follows: $\langle v,u \rangle \:=\:v^t\begin{pmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 2 &amp; 0 \\0 &amp; 0 &amp; 3\end{pmatrix}u$  </p>

<p>we have operator $f \colon V \to V$ , $f\begin{pmatrix}x \\y \\z\end{pmatrix}\:=\begin{pmatrix}1 &amp; 2 &amp; 3 \\4 &amp; 5 &amp; 6 \\7 &amp; 8 &amp; 9\end{pmatrix}\begin{pmatrix}x \\y \\z\end{pmatrix}$</p>

<p>The question is : calculate $f^*$.  </p>

<p>So far, as i know, i need to find orthonormal basis $B$, and find $\left[f\right]_B^B$, and after that just do transpose to $\left[f\right]_B^B$.<br>
 is That correct?  it's a question from  test that i had and i didn't know how to answer it so i forwarding this to you. tnx!</p>
",<linear-algebra>
"<p>In $\mathbb{R}^3$ we declare an inner product as follows: $\langle v,u \rangle \:=\:v^t\begin{pmatrix}1 &amp; 0 &amp; 0 \\0 &amp; 2 &amp; 0 \\0 &amp; 0 &amp; 3\end{pmatrix}u$  </p>

<p>How can I find an orthonormal basis for this inner product space using the Gram–Schmidt process?</p>
",<linear-algebra>
"<blockquote>
  <p>Let matrix $A$ be
  $$\begin{bmatrix}
 -5&amp; 1&amp; 0&amp; 0\\
  a &amp;2&amp; 1 &amp;0\\
  0&amp; 1 &amp;1 &amp;1\\
  0 &amp;0&amp;1&amp; 0
\end{bmatrix}$$ 
  where $a$ is a constant between 1 and 3.</p>
  
  <p>Show that the dominant eigenvalue is real.</p>
</blockquote>

<p>Thanks a lot!!</p>
",<linear-algebra>
"<p>I have a system of linear equations as follows.</p>

<blockquote>
  <p>$$M(p) = 1+\frac{n-p-1}{n}M(n-1) + \frac{2}{n} N(p-1) + \frac{p-1}{n}M(p-1)$$
   $$N(p) = 1+\frac{n-p-1}{n}M(n-1) + \frac{p}{n}N(p-1)$$
   $$M(1) = 1+\frac{n-2}{n}M(n-1) + \frac{2}{n}N(0)$$
   $$N(0) = 1+\frac{n-1}{n}M(n-1)$$</p>
</blockquote>

<p>$M(p)$ is defined for $1 \leq p \leq n-1$.  $N(p)$ is defined for $0 \leq p \leq n-2$.  What is $M(n-1)$?</p>
",<linear-algebra>
"<blockquote>
  <p>Let $P \in \mathbb{R}_{n-1}[X]$ be a polynomial of degree $n-1 \geqslant 0$.</p>
  
  <ol>
  <li><p>Let $\mathbb{R}_{n-1}[X]$ be the vector space of polynomials with degree $\leqslant n-1$ over $\mathbb{R}$. Show that $(P(X),P(X+1),\ldots ,P(X+n-1))$ is a basis of $\mathbb{R}_{n-1}[X]$.</p></li>
  <li><p>Let $M_n = \begin{pmatrix} P(X) &amp; P(X+1) &amp; P(X+2) &amp; \ldots &amp; P(X+n) \\
				P(X+1) &amp; P(X+2) &amp; P(X+3) &amp; \ldots &amp; P(X+n+1) \\
				\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
				P(X+n) &amp; P(X+n+1) &amp; P(X+n+2) &amp; \ldots &amp; P(X+2n) \end{pmatrix}$.</p></li>
  </ol>
  
  <p>Show that $\det{M_n} = 0$ for every $X \in \mathbb{R}$.</p>
</blockquote>

<p>My thoughts on (1): $\mathbb{R}_{n-1}[X]$ is $n$-dimensional, because $(1,X, \ldots ,X^{n-1})$ is a basis of $\mathbb{R}_{n-1}[X]$. So it suffices to show that $(P(X),P(X+1),\ldots ,P(X+n-1))$  is a generating set/linearly independent. I tried proving it with induction and using the binomial theorem, but I am not getting anywhere.</p>

<p>My thoughts on (2): $\det{M_n} = 0$ implies that the columns are linearly dependent. (1) is probably useful here, but I don't even know how to start.</p>

<p>Any help is appreciated, thanks.</p>
",<linear-algebra>
"<p>There is a notation used in many sources (e.g. Wikipedia: <a href=""http://en.wikipedia.org/wiki/Exponential_family"" rel=""nofollow"">http://en.wikipedia.org/wiki/Exponential_family</a>) for the natural parameters of exponential family distributions which I do not understand, and I cannot find a description of.</p>

<p>With vector parameters and variables, the exponential family form has the dot product between the vector natural parameter, ${\boldsymbol\eta}({\boldsymbol\theta})$ and the vector sufficient statistic, ${\mathbf{T}}({\mathbf{x}})$, in the exponent. i.e. $e^{{\boldsymbol\eta}({\boldsymbol\theta})^{\top}{\mathbf{T}}({\mathbf{x}})}$. </p>

<p>However, many examples of these parameters for different distributions are vectors composed of matrices &amp; vectors. E.g. the multivariate Normal distribution has parameter $[\Sigma^{-1}\mu\space\space-\frac{1}{2}\Sigma^{-1}]$ and sufficient statistic $[\mathbf{x}\space\space\mathbf{xx^{\top}}]$.</p>

<p>So what are these ""vectors"" and moreover, how is the dot product between them defined? Does this notation have a name?</p>
",<linear-algebra>
"<p>How to prove the following?</p>

<p><strong>Lemma.</strong> Let $C=[A,A^{\star}]$. $A$ is normal iff $[A,C]=0$.</p>

<p>One direction is trivial. The other direction reduces to showing that $A^2 A^\star+A^\star A^2=0$ implies that $A$ is normal, but I don't see why that holds.</p>
",<linear-algebra>
"<p>I must construct this triangle:</p>

<p>Consider the triangle $ABC$. Take $D$ in the line of $BC$ such that $C$ is the mid point of $BD$ and take $Y$ in the line $AC$ such that the lines $AB$ and $BY$ are parallel. </p>

<p>I constructed the point $D$, but I don't see how I can construct the point $Y$ without this point being $A$. Does another possibility exists? I don't see how.</p>
",<linear-algebra>
"<p>I'm interested in the algorithm of LU decomposition in order to solve a LSE like $Ax=b$, where $A$ is a square matrix.</p>

<p>My question is: When I compute $PA=LU$ do I also need to interchange rows in $L$ whenever I interchange rows in $A$? It is clear, that I get $P$ by interchanging rows in $Id$ whenever I interchange rows in $A$, but is $L$ also affected?</p>

<p>Is it right that after that I need to solve $Lz = Pb$ and $Ux = z$?</p>
",<linear-algebra>
"<p><strong>I do not want the answer given to me, I just want assistance.</strong></p>

<p>Problem: <em>Marcus invests $750 in an account that pays 9.8% interest compounded annually. Write a function that describes the account balance, A, in terms of the number of years, t, that have passed.</em></p>

<p>I know that to find the account balance after one year: </p>

<p>A + (A * 0.098)
A + 0.098A (simplified)</p>

<p>But I don't know how to implement the time variable.</p>

<p>initial balance is $750
after one year 750 + (750*0.098) = 1348.5</p>

<p>after 2 years:
1348.5 + (1348.5*0.098) = 1480.653</p>
",<linear-algebra>
"<p>How to find inverse of an infinite lower triangular matrix all of whose diagonal entries are 1 and the entries of each column are given by coefficients of some power series rings?</p>
",<linear-algebra>
"<ol>
<li>How can one intuitively understand the definition of a bilinear map? Is there some way of looking at it geometrically? I found the following definition:</li>
</ol>

<p>Let $\mathit{A}$,$\mathit{B}$,$\mathit{C}$ be vector spaces. A map $f:\mathit{A}\times \mathit{B}\to C$ is said to be bilinear if for each fixed element $b\in \mathit{B}$, $f(.,b):\mathit{A}\to\mathit{C}$ is a linear map. Similarly, for each fixed element of $\mathit{A}$.</p>

<p>Matrix multiplication is an example of a bilinear map.Following my definition, I can prove that it is a bilinear map, but I don't understand the intuitive idea behind it. In my opinion, it is simply a linear map with one element fixed.</p>

<ol start=""2"">
<li>Is there some formal definition of a bilinear algorithm? I could find an explanation for it only in the context of matrix multiplication: <a href=""http://www.issac-symposium.org/2014/tutorials/ISSAC2014_tutorial_handout_LeGall.pdf"" rel=""nofollow"">http://www.issac-symposium.org/2014/tutorials/ISSAC2014_tutorial_handout_LeGall.pdf</a></li>
</ol>

<p>Kindly help me with these questions.
Thanks!</p>
",<linear-algebra>
"<p>Given that $A$ is a symmetric matrix, find $X$ that solves
$$\mathop {\min }\limits_X {\left\| {A - X{X^T}} \right\|_F}$$</p>

<p>I think that the problem can be solved using eigenvalue or singular value decomposition technics. $XX^T=A$ seems an obvious solution, but the problem is that $XX^T$ is positive semidefinite, while $A$ may not be, although they are both symmetric.</p>

<p>At this point I am thinking about taking eigenvalue decomposition of A, then replacing the negative eigenvalues in the middle diagonal matrix with 0-s (denote the resulting matrix by $\bar{A}$). But am having difficulties to understand why  $XX^T=\bar{A}$ gives the $X$ with minimal distance from A.</p>
",<linear-algebra>
"<p>I hope to solve this problem.</p>

<p>$$\min \quad \left\| CX \right\|_{1} $$
$$ \text{s.t.}\quad AX=b, X &gt;0 $$</p>

<p>where $C \in \mathbb{R}^{m \times m}$, $X \in \mathbb{R}^{m \times n}$, $A \in \mathbb{R}^{k \times m}$, $b \in \mathbb{R}^{k \times n}$. $C$ is known weight, $X$ is unknown matrix. My problem is how to calculate the proximal operator of $ \left\| CX \right\|_{1}$, I know, if without $C$ the proximal operator will be apply Shrinkage elementwise. </p>

<p>This problem will be easy if $x$ is a vector, we just need to solve a LP, but my $X$ is a matrix.</p>

<p>$$ \min \quad c^Tx $$ 
 $$ \text{s.t.}\quad Ax=b , x&gt;0 $$</p>

<hr>

<p>the overall problem I hope to solve is:
$$ \min \left\| CX \right\|_{1} + \lambda \left\| Y \right\|_{*} $$
$$ \text{s.t.}\quad AX+Y=b , X&gt;0 $$
Y has the same dimension with $b \in \mathbb{R}^{k \times n}$. X is known to be sparse.</p>
",<linear-algebra>
"<p>This is a question from a review package that is causing me some trouble.</p>

<p>Let $U,W$ be subspaces of a finite dimensional vector space. Show if $\dim(U+W) = 1+\dim(U \cap W)$, then $\{U+W,U\cap W\}=\{U,W\}$.</p>

<p>I know that for $\{U+W,U\cap W\}=\{U,W\}$ to be true, one of two cases must happen. Either $U\subseteq W$ or $W\subseteq U$, since $U \cap W$ must equal $U$ or $W$. However we can assume without loss of generality that either one is true. </p>

<p>I'm not sure how to show the implication (maybe through contraposition?). Any hints and help is greatly appreciated. </p>
",<linear-algebra>
"<p>In one book on differential equations and dynamical systems I read that if <strong>(1)</strong> $(A-\lambda I)^{k_j} \vec{v_j} = \vec{0}$ then <strong>(2)</strong> $(A-\lambda I)\vec{v_j} = V_j$ and $V_j\in \ker(A-\lambda I)^{k_j-1}$. But I don't see how (2) follows from (1). Can someone please explain?</p>
",<linear-algebra>
"<blockquote>
  <p>I've been given an $(n+1)\times(n+1)$ square matrix, which is written in the form of a block matrix with the following dimensions
  $$ \begin{bmatrix}
    (1\times1)       &amp; (n\times1)\\
    (n\times1)       &amp; (n\times n) 
\end{bmatrix} .$$
  I need to compute the determinant. </p>
</blockquote>

<p>I've tried to understand what is shown <a href=""https://en.wikipedia.org/wiki/Determinant#Block_matrices"" rel=""nofollow"">here</a> on how to solve this but I'm still confused. Can someone offer any insight as to how I would go about solving this? Also, is there any decomposition, factorization, etc I can take advantage of with the off diagonals being $(n\times 1)$ and $(1\times n)$? I feel like there's some simplification I can utilize. Thoughts?</p>
",<linear-algebra>
"<p>Let's say that we have linear subspaces $V$ and $W$ of $Y$.</p>

<p>What is the difference between the following sets:</p>

<ol>
<li>$V+W$ </li>
<li>$V\cup W$ </li>
<li>$V\oplus W$</li>
</ol>
",<linear-algebra>
"<p>Let $u = \left( \begin{matrix} 2 \\-5   \\1\end{matrix} \right)$</p>

<p><strong>Find an operator $T \in L(U) $ such that $T(u)=u $ and $T$ is self-adjoint.</strong> </p>

<p>I have to show that $T=T^*$ to have a self-adjoint operator T but I know how to start off.</p>

<p>I all I can think is $ T(u) = \left( \begin{matrix} 2 \\-5   \\1\end{matrix} \right)$</p>

<p>Any sort of help is appreciated! </p>

<p>Thanks!</p>
",<linear-algebra>
"<p>Given a % of change and the resulting value after the change, how do you calculate the original value?</p>

<p>For example, X increased by 50% = 150. In this case we can easily use guess and check to see that X = 100, but how do you calculate this given any percentage of change and any resulting value?</p>

<p>I though this formula should work, but it does not. As you can see my algebra is quite rusty, so maybe you can help.</p>

<pre><code>start = x
percentage of change = p
resulting value = r

x * (p / 100) + x = r
</code></pre>

<p>Which can be simplified to</p>

<pre><code>2(p / 100)x = r
</code></pre>

<p>which is equivalent to</p>

<pre><code>x = r / 2(p / 100)
</code></pre>

<p>But when I check my work, this is wrong:</p>

<pre><code>100 = 150 / 2(50 / 100)
100 = 150 / 2(.5)
100 = 150 / 1
100 = 150
</code></pre>
",<linear-algebra>
"<p>Suppose that A is an $n\times m$ matrix with $ n\neq m$.</p>

<p>Here's my reasoning.</p>

<p>Every nonpivot column corresponds to a free variable in the system Ax = 0. Each free
variable becomes a parameter, and each parameter is multiplied times a basis vector
of null(A). Therefore the number of nonpivot
columns equals nullity(A). Since rank(A) + nullity(A) = m, the nullity(A) must be greater than zero.</p>

<p>I'm not sure if I'm justified in stating the last sentence. Any suggestions or can you provide a different proof?</p>
",<linear-algebra>
"<p>Let Ax=b be a nonhomogenous system of linear equations with the unknown x ∈ |R^n. Assume that X1 ∈ |R^n and X2 ∈ |R^n are both solutions of the nonhomogenous system. Which ONE of the following statements must be true?</p>

<pre><code>a) x1 + x2 is a solution of Ax = 0
b) x1 - x2 is a solution of Ax = 0
c) x1 + x2 is a solution of Ax = b
d) x1 - x2 is a solution of Ax = b
</code></pre>

<p>I'm not really sure how to approach this question. My current approach is to make up a system of equations such that Ax = b is nonhomogenous.</p>

<p>So I have:</p>

<pre><code>5x - 2y = 1
8x - 3y = 2
</code></pre>

<p>The solutions of (x,y) are (1,2) for both equations.</p>

<pre><code>5(1) - 2(2) = 1
8(1) - 3(2) = 1
</code></pre>

<p>Not sure what to do next, if I plug in ( x , y ) for ( x1 , x2 ) in the answer choices, none of them seem to be consistent.</p>
",<linear-algebra>
"<p>Let $A$ be a non-negative irreducible $n\times n$ matrix. Then the function
$$f(t)=\rho(tA+(1-t)A^T)$$
is increasing on $[0,1/2]$, and is decreasing on $[1/2,1]$.</p>

<p>Here are the notations.</p>

<ol>
<li><p>$A$ is non-negative if any entry of $A$ is greater than or equal to $0$.</p></li>
<li><p>$A$ is irreducible if $A$ is not reducible; and $A$ is reducible if there exists a permutation matrix $P$ such that $$P^T AP=\begin{pmatrix}
B&amp;0\\
C&amp;D\end{pmatrix},$$ or equivalently, there exists a permutation $\sigma$ of $\{1,2,\cdots,n\}$ and a $1\leq k\leq n-1$ such that the sub-matrix of $A$ in rows $\sigma(1),\cdots,\sigma(k)$ and columns $\sigma(k+1),\cdots,\sigma(n)$ being $0$.</p></li>
<li><p>$A^T$ is the transpose of $A$.</p></li>
<li><p>$\rho(A)$ is the spectral radius of $A$, that is, the largest modulus of the eigenvalues of $A$.</p></li>
</ol>

<p>And now I have no idea on it. However, it is intuitively right. As there are more symmetry in the matrix, the spectral radius becomes larger.</p>
",<linear-algebra>
"<p>Let $A$ be a non-negative primitive matrix. Then $$\lim_{n\to\infty}\left[\frac{A}{\rho(A)}\right]^n=xy^T,$$
where $x, y$ are the Perron roots of $A$ and $A^T$ respectively, they satisfy $x^Ty=1$.</p>

<p>Here are the notations.</p>

<ol>
<li><p>$A$ is non-negative if any entry of $A$ is greater than or equal to $0$.</p></li>
<li><p>$A$ is primitive if $A$ is non-negative irreducible, and the number of eigenvalues of $A$ with modulus equal to $\rho(A)$ (the spectral radius of $A$) is $1$.</p></li>
<li><p>$A$ is irreducible if $A$ is not reducible; and $A$ is reducible if there exists a permutation matrix $P$ such that $$P^T AP=\begin{pmatrix}
B&amp;0\\
C&amp;D\end{pmatrix},$$ or equivalently, there exists a permutation $\sigma$ of $\{1,2,\cdots,n\}$ and a $1\leq k\leq n-1$ such that the sub-matrix of $A$ in rows $\sigma(1),\cdots,\sigma(k)$ and columns $\sigma(k+1),\cdots,\sigma(n)$ being $0$.</p></li>
<li><p>The Perron root $x$ of $A$ is an eigenvector $x$ corresponding to the eigenvalue $\rho(A)$, the entries of $x$ are positive.</p></li>
<li><p>$A^T$ is the transpose of $A$.</p></li>
</ol>

<p>It is easy to show that the limit exists. In fact, we could just use Jordan carnonical form to find there exists a invertible matrix $T$ such that 
$$T^{-1}AT=\begin{pmatrix}
\rho(A)&amp;0\\
0&amp;*\end{pmatrix},$$
and thus
$$
\lim_{n\to\infty}T^{-1}\left[\frac{A}{\rho(A)}\right]^nT
=\begin{pmatrix}
1&amp;0\\
0&amp;0\end{pmatrix}.$$
However, I could not prove that the limit if $xy^T$.</p>
",<linear-algebra>
"<p>I have some data points that define a curve and what i need to find is the slope of the lines definedby
line 1 = p1&amp;p2
line 2 = p1&amp;p3
line 3 = p1&amp;p4
.
.
.
line29= p1&amp;p30
line30= p2&amp;p3
line31= p2&amp;p4</p>

<p>linexxx = p29&amp;p30 being the last </p>

<p>and I as I go i need to determine if the lines above  <em>ONLY</em> have 2 points in common with the curve?
image below the blue line is valid the green isnt how can I tell this? </p>

<p><img src=""http://i.stack.imgur.com/Vw0k8.png"" alt=""enter image description here""></p>

<p>point 1     0.128250002 6.235036978 <br>
point 2     0.197718753 6.239911671 <br>
point 3     0.281734379 6.22376425  <br>
point 4     0.336656255 6.233513636 <br>
point 5     0.347343755 6.20761683  <br>
point 6     0.472625007 6.238083661 <br>
point 7     0.491625008 6.205484151 <br>
point 8     0.553968759 6.244786364 <br>
point 9     0.601765634 6.200609458 <br>
point 10    0.740703137 6.243263022 <br>
point 11    0.797703137 6.225287592 <br>
point 12    0.927140639 6.245091032 <br>
point 13    1.078546892 6.239911671 <br>
point 14    1.159890643 6.249661057 <br>
point 15    1.291703145 6.238997666 <br>
point 16    1.404812522 6.262457126 <br>
point 17    1.506937524 6.248747052 <br>
point 18    1.6057969   6.213405528 <br>
point 19    1.684765651 6.243263022 <br>
point 20    1.770859403 6.235341646 <br>
point 21    1.94037503  6.247833047 <br>
point 22    2.059125032 3.903410396 <br>
point 23    2.189453159 3.681916534 <br>
point 24    2.330468786 4.359194189 <br>
point 25    2.398453162 6.237169656 <br>
point 26    2.55728129  6.279213883 <br>
point 27    2.692656292 6.248747052 <br>
point 28    2.844359419 6.254840418 <br>
point 29    2.992203172 6.264589804 <br>
point 30    3.167062549 6.243263022 <br></p>
",<linear-algebra>
"<p>I've been researching for a while and trying to wrap my head around spanning of vector spaces completely (by visualizing them in R3) before moving on to Linear Independence, Basis' and anything else taught after subspaces. (had no calc 3 unfortunately =/)Based on what I'm reading, the span of a set of vectors is every possible linear combination of those vectors. After reading/looking at this figure:</p>

<p><img src=""http://i.stack.imgur.com/iMrLI.png"" alt=""enter image description here""></p>

<p>I think I understand it for two vectors in R3. It looks like any two arbitrary vectors (that arent scalar multiples of eachother) in R3 will span a never ending plane through the origin. </p>

<p>Does this mean that the span of any 3 arbitrary vectors in the vector space R3 will form a never ending 3d shape spanning all of R3? (as long as two of them arent scalar multiples of eachother (EDIT: or as long as they don't span R2?)</p>

<p>also, what if you have any random set of more than 3 vectors (that qualify as being in R3), is the span of that set also all of R3? (as long as the vectors dont end up being scalar multiples of eachother so that the set spans a line)(EDIT: or as long as they don't span R2?)</p>

<p>while I'm at it, does this mean that any 4 (or more?) vectors in R4 that arent multiples of any of the other 4 span all of R4? (EDIT:if not R2 or R3?)</p>

<p>if this is all true, I think I had a big epiphany and everything now makes sense to me..... such as being able to visualize linear dependent and linear independent.. I'm guessing that a set of vectors is L.D. if at least one vector can be written as a scalar multiple of another vector in the set. and LI means they are all unique, as in they have no scalar multiples of eachother, which also means that if the only solution to a set of 3 vectors in R3 is (0,0,0) or the homogeneous solution, or the trivial solution.... then that set spans R3? and the same applies to sets of vectors in R4, such as if there is 4 or 5 vectors satsifying R4 rules, and the only solution to their system of equations is homogeneous, they are LI, they span R4, and the 4x4 systems determinant is not equal to 0....</p>
",<linear-algebra>
"<p>I don't have a clue of what's going on. We haven't learn this in class so I need all the help possible. The more detailed of an explanation, the better. Thanks in advance. The only info I have is that this matrix is Orthogonal. Which means I know the answer, just don't know how to get it.</p>

<p>\begin{bmatrix}
       \cos\theta &amp; -\sin\theta \\
       \sin\theta &amp; \cos\theta 
     \end{bmatrix}</p>
",<linear-algebra>
"<p>Let $T:\mathbb{V} \rightarrow \mathbb{W}$ be an injective linear transformation and $S:\mathbb{W}\rightarrow \mathbb{V}$ be a surjective linear transformation with $\mathbb{V}$ and $\mathbb{W}$ finite-dimensional vector spaces.</p>

<p>How can I show that $S\circ T=id_{\mathbb{V}}$?</p>
",<linear-algebra>
"<p>I have this problem I'm working on, which I cannot entirely solve:</p>

<blockquote>
  <p>Let $V$ be a finite dimensional vectorspace over a field $K$ with a symmetric bilinear
  form $\langle \cdot, \cdot \rangle$. Define for every $v \in V$ the
  map $$ l_v : V \mapsto K: w \mapsto \langle v, w \rangle. $$ Consider
  now the map $$f: V \rightarrow V^{*}: v \mapsto l_v. $$ </p>
  
  <p>(i) Prove that $f$ is a linear map.</p>
  
  <p>(ii) Prove that $f$ is surjective if and only if $\langle \cdot, \cdot
 \rangle $ is non degenerate.</p>
</blockquote>

<p><strong>Attempt:</strong>
(i) Let $v, w \in V$ be vectors, and let $\lambda, \mu \in K$ be scalars. We need to prove that $$f(\lambda v + \mu w) = \lambda f(v) + \mu f(w). $$ This is equivalent to proving $$l_{\lambda v + \mu w} = \lambda l_v + \mu l_w. $$ Let $x \in V$ be another vector. Then the above equality is true since $$ l_{\lambda v + \mu w} (x) = \langle \lambda v + \mu w, x \rangle = \lambda \langle v, x \rangle + \mu \langle w, x \rangle = \lambda l_v(x) + \mu l_w(x). $$ This proves that $f$ is linear.</p>

<p>(ii) Suppose first that $f$ is surjective. To prove that $\langle \cdot, \cdot \rangle$ is non degenerate, we need to prove that $$ \forall w \in V: [ (\forall v \in V: \langle v, w \rangle = 0 ) \Rightarrow w = 0 ]. $$ So let $w \in V$ be arbitrary, and suppose that $\forall v \in V: \langle v, w \rangle = \langle w, v \rangle = 0$, since the bilinear form is symmetric. This means that $\forall v \in V : l_v(w) = l_w(v)= 0. $ </p>

<p>Now I'm not sure how to use the fact that $f$ is surjective to deduce from this that $w = 0$. I know that the linear functional $l_v \in V^{*}$. So since $f$ is surjective, I can take a $v \in V$ such that $f(v) = l_v$. But what can I conclude from this about $w$? </p>

<p>Help is appreciated.</p>
",<linear-algebra>
"<p>I'm trying to implement some determinant routines for some CUDA C++ code that I'm writing. The only issue is, my code is returning nan's and inf's! It turns out that it's my pivoting routine that's bad.</p>

<p>Right now, my overall approach has been ripped straight from rosetta code's C implementation (without those awful macros). But it seems like their pivoting routine isn't robust enough for my needs.</p>

<p>Right now, my pivoting routine will take a column, start at the row corresponding to the column index and then search for the column's largest element below the current row and only swap if a larger value is found.</p>

<p>This is far too naive in practice. Is there a more robust pivoting algorithm I can/should be using?</p>

<p>Here's a link to the source code test: <a href=""https://github.com/LeonineKing1199/cuda-stuff/blob/master/tests/matrix-tests.cu#L182"" rel=""nofollow"">https://github.com/LeonineKing1199/cuda-stuff/blob/master/tests/matrix-tests.cu#L182</a></p>

<p>If you look at that matrix, that's the properly permuted one. In my actual data,  the rows of the matrix can be in any order! I just needed a static test to make sure it would eventually work. So, what kind of pivoting algorithm would I need to help me produce that same matrix assuming the rows were permuted in any order?</p>

<p>Sorry if I haven't given enough information. My main matrix class is here: <a href=""https://github.com/LeonineKing1199/cuda-stuff/blob/master/include/math/matrix.hpp"" rel=""nofollow"">https://github.com/LeonineKing1199/cuda-stuff/blob/master/include/math/matrix.hpp</a></p>
",<linear-algebra>
"<blockquote>
  <p>Let $V$ be a vector space with dimension $n$ and let $T: V\rightarrow V$ satisfy $T^2=0$.</p>
  
  <p>(a) Prove Im$ T \subseteq$  Ker $T$ and $\dim($Ker$(T))\geq \frac{n}{2}$ (SOLVED BY ME)</p>
  
  <p>(b) Assume $n=3, T\neq 0$ Prove there exists a basis $B$ of $V$ such that $[T]_B=\begin{bmatrix}
0 &amp; 0 &amp; 1\\ 
 0&amp;0  &amp; 0\\ 
0 &amp;0 &amp; 0
\end{bmatrix}$</p>
</blockquote>

<p>My attempt at (b):</p>

<p>By (a) we know $\dim(\ker T)\geq \frac{3}{2}$, hence it's either 2 or 3. Assuming it's 3 we get $V=\ker T$ which contradicts $T\neq 0$. So $\dim(\ker T)=2$</p>

<p>Basically we can take a basis of $\ker T$, $(v_1,v_2)$. And we need to show there exists $v_3\in V$ such that $T(v_3)=v_1$ and $(v_1,v_2,v_3)$ is a basic of $V$ (ahm show theyr'e linearly independent). this is have I'm having trouble with.</p>

<p>Thanks in advance.</p>
",<linear-algebra>
"<p>$A\in \mathbb{C}^{n,n}$ is hermitian. Then:<br>
a. $A$ is congruent to some diagonal matrix $D\in\mathbb{R}^{n,n}$.<br>
b. if matrix $A$ is positively defined then all eigenvalues of $A$ are equal to $1$.<br>
c. if matrix $A$ is positively defined then $A$ is congruent to matrix $I_n$.  </p>

<p>b. not true, counterexample:<br>
$\left[ \begin{array}{ccc}
5 &amp; 0 \\
0 &amp; 3 
 \end{array} \right]$.  Of course this matrix is positively defined. Is also hermitian. However, eigenvalues are $5$ and $3$.  </p>

<p>c.  Yes, hermitian positively defined matrix $A$ has unique Cholesky decomposition: $A=LL^H$ where $L$ is lower triangular matrix with real positive (strictly) numbers on diagonal. Then $A=LI_nL^H$ and $L$ is nonsingular. Hence, c. is true.    </p>

<p>What about b, c?   Could you help me with a. ?</p>
",<linear-algebra>
"<p>I need to algebraically prove minimum for $\frac{1}{2}x^TAx-x^Tb$ using $r = A^{-1}x-b.$</p>

<p>I can write $x$ as $A(r+b)$ and whole expression as 
$$
\begin{align}
f(x) &amp;=\frac{1}{2}(r+b)^TA^3(r+b) - (r+b)^TAb \\
&amp;= \frac{1}{2}(Ar)^TA(Ar) - (Ar)^Tb \\
&amp; + \frac{1}{2}(Ab)^TA(Ab) - (Ab)^Tb \\
&amp; + (Ar)^TA^2b \\
\end{align}
$$</p>

<p>And i don't know how to proceed.</p>
",<linear-algebra>
"<p>I need some help here. It's hard to me to tackle this kind of question and I'm not used to write math proofs. I need to find values of $t$ that make $$\langle(x_1, x_2), (y_1, y_2)\rangle = x_1y_1 + tx_2y_2$$ an internal product in $\Bbb R^2$.</p>

<p>I have showed that for $3$ of the $4$ properties, t does not matter at all. But for the property that $$\langle u,u\rangle \gt 0, u \neq 0$$ I have $$x_1^2 +tx_2^2 \gt 0$$ and them $$t \gt -((x_1/x_2)^2)$$ The answer seems to be $t \gt 0$ and I'm lost in this. Could anyone give me the right direction to complete the proof?</p>
",<linear-algebra>
"<p><a href=""http://i.stack.imgur.com/NHbGu.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/NHbGu.png"" alt=""set of vectors, S""></a></p>

<p>Firstly, my apologies for attaching the question as an image. I was having a lot of problems typing it in MathJax.<br>
The question asks to show S is independent. From my understanding, I have to prove that:<br>
$$
\alpha{\begin{pmatrix}
 1\\
 1\\
 0\\
 \end{pmatrix}} +\beta{\begin{pmatrix}
 1\\
 0\\
 1\\
 \end{pmatrix}}= {\begin{pmatrix}
 0\\
 0\\
 0\\
 \end{pmatrix}}
$$if and only if $\alpha$ =$\beta$ = 0. I try to solve this using an augmented matrix, and reduce it to row-echelon form.
$$ \left[
    \begin{array}{cc|c}
      1&amp;1&amp;0\\
      1&amp;0&amp;0\\
      0&amp;1&amp;0
    \end{array}
\right] $$ $\underrightarrow{R2+R3}$
$$ \left[
    \begin{array}{cc|c}
      1&amp;1&amp;0\\
      1&amp;1&amp;0\\
      0&amp;1&amp;0
    \end{array}
\right] $$ $\underrightarrow{R3-R2}$
$$ \left[
    \begin{array}{cc|c}
      1&amp;1&amp;0\\
      1&amp;1&amp;0\\
      0&amp;0&amp;0
    \end{array}
\right] $$ But, 
$$ \left[
    \begin{array}{cc|c}
      0&amp;0&amp;0\\
    \end{array}
\right] $$ means that the system has infinitely many solutions. Therefore, $\alpha,\beta$ could have infinitely many solutions, and so the set $S$ is dependent.<br>
But the solution says that the set is independent because $\alpha,\beta = 0$. Could someone please assist me by pointing out where I have gone wrong? I would appreciate if you could stick to my method of solving (proving $\alpha,\beta=0$) because this is how I would like to approach it)</p>
",<linear-algebra>
"<p>I am doing some problems outside of class and have a couple of questions that I cannot figure out how to start. </p>

<ol>
<li>If $f$ and $g$ are independent polynomials and $h$ is a nonzero polynomial over $F$, show that $fh$ and $gh$ are independent.</li>
</ol>

<p>I think this is relatively intuitive, but cannot find a proof for it. </p>

<p>Since f and g are independent, that means they should each form a basis for the fields they are over (in terms of polynomials). Consequently, I believe multiplying by a nonzero polynomial is the equivalent of scaling it in the field. So, the only way they would be dependent is if $h=0$ or if $f=g=0$.</p>
",<linear-algebra>
"<p>Suppose I have a matrix $A$, not telling you what it looks like, and the set of eigenvalues associated with $A$ = $\{-1,-1,-1,4\}$</p>

<p>Suppose the geometric multiplicity of $-1$ is $2$, what would be the geometric multiplicity of $4$?</p>

<p>Possible answer could be $1$, $2$, since any more then our jordan form will blow up</p>

<p>Obviously here the algebraic multiplicity of $4$ is one. </p>

<p>Does it equal to the geometric multiplicity?</p>

<p>What is a condition to check when they are equal and how can I see that?</p>
",<linear-algebra>
"<p>a) Suppose $S = \{v_1, v_2, v_3, v_4, v_5\}$, where</p>

<p>$v_1 = \left(  
\begin{array}{c}
    1 \\
    -1 \\
    -1 \\
    2
  \end{array}
\right)$, 
$v_2 = \left(  
\begin{array}{c}
    1 \\
    -1 \\
    0 \\
    -1
  \end{array}
\right)$, 
$v_3 = \left(  
\begin{array}{c}
    5 \\
    -5 \\
    -2 \\
    1
  \end{array}
\right)$, 
$v_4 = \left(  
\begin{array}{c}
    1 \\
    -1 \\
    1 \\
    -4
  \end{array}
\right)$, 
$v_5 = \left(  
\begin{array}{c}
    0 \\
    0 \\
    3 \\
    -9
  \end{array}
\right)$,</p>

<p><strong>Without doing any row operations, explain why $S$ is a linearly dependent set</strong></p>

<p>I don't know how to start by just looking at it, all I can do is just Row Operation and see the leading columns then judge if it is linearly dependent or not. </p>

<p>Would someone please tell me how to judge if the set is a linearly dependent or independent set please?</p>

<p>Thank you.</p>
",<linear-algebra>
"<p>Solve the system: The last column is the vector b
$$
        \begin{bmatrix}
        1 &amp; 1 &amp; 4 &amp; -5 \\
        4 &amp; 3 &amp; -5 &amp; 8 \\
        \end{bmatrix}
$$</p>

<p>I reduced it down to</p>

<p>$$
        \begin{bmatrix}
        1 &amp; 0 &amp; -17 &amp; 23 \\
        0 &amp; 1 &amp; 21 &amp; -28 \\
        \end{bmatrix}
$$</p>

<p>Now I have to express it in terms of:</p>

<p>$$
        \begin{bmatrix}
        x1 \\
        x2 \\
        x3 \\
        \end{bmatrix}
$$</p>

<p>x3 is free and I believe the answer to be something along the lines of</p>

<p>$$
        \begin{bmatrix}
        1 &amp; 0\\
        0 &amp; 1\\
        ? &amp; ?\\
        \end{bmatrix}
$$</p>

<p>Not sure what the ? values are.</p>
",<linear-algebra>
"<p>I'm trying to write a Fortran subroutine to compute a QR factorization using the Householder method. To test my routine, I compute the factorization of the following matrix:
$$
A =
 \begin{pmatrix}
  12 &amp; -51 &amp; 4 \\
  6 &amp; 167 &amp; -68  \\
  -4 &amp; 24 &amp; -41 
 \end{pmatrix},
$$
which, if done correctly, will reduce to the following upper triangular matrix:
$$
R =
 \begin{pmatrix}
  14 &amp; 21 &amp; -14 \\
  0 &amp; 175 &amp; -70  \\
  0 &amp; 0 &amp; 35 
 \end{pmatrix}.
$$
However, the matrix I actually get is:
$$
R =
 \begin{pmatrix}
  -14 &amp; -21 &amp; 14 \\
  0 &amp; -175 &amp; 70  \\
  -0 &amp; 0 &amp; 35 
 \end{pmatrix},
$$
which looks almost correct, except for some strange sign changes. I've been staring at my subroutine all day trying to see where these sign changes are being introduced, but I can't identify the problem. </p>

<p>My algorithm is as follows:</p>

<p>$
for \:\: k \:=\: 1\: to\: n
$</p>

<p>$
\qquad x(k:m) = A(k:m,k)
$ </p>

<p>$
\qquad v(k:m) = \mathtt{sign}(x(k))||x(k:m)||_{2}e1 + x(k:m)
$</p>

<p>$
\qquad v(k:m) = v(k:m)/||v(k:m)||_{2}
$</p>

<p>$
\qquad A(k:m,k:n) = A(k:m,k:n) - 2vv^{\top}A(k:m,k:n)
$</p>

<p>To calculate the factor $2vv^{\top}A(k:m,k:n),$ I made another subroutine called outer_product to compute the outer product of $v$ with itself, i.e. $vv^{\top}$, and then matrix multiply the result into my submatrix $A(k:m,k:n)$. However, I'm not sure if this is legitimate - I suspect herein lies the problem.</p>

<p>I would really appreciate it if someone could glance at my code to see if there is any obvious reason for the incorrect sign changes: </p>

<pre><code>integer, parameter :: dp = selected_real_kind(15)

integer, intent(in) :: m, n
real(dp), dimension(m,n), intent(inout) :: A
real(dp), dimension(m,m), intent(out) :: Q

integer :: k
real(dp) :: two_norm
real(dp), dimension(m) :: x, e1
real(dp), dimension(m,n) :: v
real(dp), dimension(m,m) :: outprod_vv

v = 0.0_dp

do k=1,m
    Q(k,k) = 1.0_dp
end do

!Householder triangularization
do k=1,n

    e1(k) = 1.0_dp

    x(k:m) = A(k:m,k)
    v(k:m,k) = sign( sqrt(dot_product(x(k:m),x(k:m))), x(k) )* &amp;
        e1(k:m) + x(k:m)

    v(k:m,k) = v(k:m,k)/(sqrt(dot_product(v(k:m,k),v(k:m,k))))
    call outer_product(v(k:m,k), m-k+1, outprod_vv(k:m,k:m))

    A(k:m,k:n) = A(k:m,k:n) - &amp;
        2.0_dp*matmul(outprod_vv(k:m,k:m), A(k:m,k:n)) 

    !Form Q implicitly    
    Q(k:m,k:m) = Q(k:m,k:m) - 2.0_dp* &amp;
        matmul(outprod_vv(k:m,k:m), Q(k:m,k:m))

end do

Q = transpose(Q)
</code></pre>
",<linear-algebra>
"<p>What does it mean when someone says ""find a fundamental set of solutions for the system <strong>y'</strong> $=A$ <strong>y</strong>""?</p>

<p>That is, the system</p>

<p>$$ {\bf{y'}} =A {\bf{y}}. $$</p>
",<linear-algebra>
"<p>I've got a section in my textbook about non-parallel vectors, it says:</p>

<p>For two non-parallel vectors <strong>a</strong> and <strong>b</strong>, if $\lambda a + \mu b = \alpha a + \beta b$
then $\lambda  = \alpha $ and $\mu  = \beta $</p>

<hr>

<p>Okay I get that you can equate coefficients and solve for mu and lambda, but how are the two sides of the equation equal in the first place? How can you just equate two different vectors to each other like that? I'm just confused and i'm not entirely sure what about. I've tried googling but not much turns up.. I'd love it if someone could explain in basic terms what this equation is telling me.. (that non parallel vectors are equal?) as I've only just been introduced to this topic recently.. Thank you.</p>
",<linear-algebra>
"<p>The famous identity $\sin^2 x+\cos^2x =1$ can be written as follows:</p>

<blockquote>
  <p>The polynomials $P(x)=x^2$ and $Q(x)=1-x^2$ satisfy 
  $$P(\sin x)= Q(\cos x),\quad \text{for all }x\in\mathbb R$$</p>
</blockquote>

<p>What are other such pairs of polynomials. In other words, what is the sufficient and essential condition for two real polynomials $P(x)$ and $Q(x)$ to satisfy $P(\sin x)= Q(\cos x)$ for all $x$?</p>
",<linear-algebra>
"<p>This question is more general in the sense that I want to know how one finds a particular (say matrix) representation for any object. For the case of Grassmann numbers we have from <a href=""http://en.wikipedia.org/wiki/Grassmann_number#Matrix_representations"" rel=""nofollow"">Wikipedia the following representation</a>: </p>

<blockquote>
  <p>Grassmann numbers can always be represented by matrices. Consider, for example, the Grassmann algebra
  generated by two Grassmann numbers $\theta_1$ and
  $\theta_2$. These Grassmann numbers can be represented by
  4&times;4 matrices:</p>
  
  <p>$$\theta_1 = \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0\\ 1 &amp; 0 &amp; 0 &amp; 0\\ 0 &amp;
 0 &amp; 0 &amp; 0\\ 0 &amp; 0 &amp; 1 &amp; 0\\ \end{bmatrix}\qquad \theta_2 =
 \begin{bmatrix} 0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0\\ 1&amp;0&amp;0&amp;0\\ 0&amp;-1&amp;0&amp;0\\
 \end{bmatrix}\qquad \theta_1\theta_2 = -\theta_2\theta_1 =
 \begin{bmatrix} 0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0\\ 1&amp;0&amp;0&amp;0\\ \end{bmatrix}.
 $$</p>
</blockquote>

<p>How do one find these matrices? Do you guess them or is there a procedure? What about finding differenct matrix-representations for <strong>Dirac $\gamma$-matrices? How do you find them?</strong></p>
",<linear-algebra>
"<p>I am currently working no a linear algebra question and do not understand how to solve it. The questions gives:</p>

<pre><code>Four corners of a cube are (0,0,0), (2,0,0), (0,4,0) and (0,0,10).
</code></pre>

<p>I am asked to find:</p>

<pre><code>Find the remaining 4 corners.
Find the coordinates of the center point of the cube.
</code></pre>

<p>Can someone help me on the right path to this question?</p>

<p>How would find the other 4 corners of the cube? I dont understand how to find the width of the cube.</p>

<p>Thank you</p>
",<linear-algebra>
"<p>For an uknown 3x3 matrix $A$ we know that $\operatorname{tr} A = 0$, $\det(A) = 1/4$ and we also know that two eigenvalues are the same. Proove that $4A^3 = -3A - I$. Problem says to use Vieta to find characteristic polynomial and Cayley-Hamilton after.</p>

<p>I get that $2L_1 + L_3 = 0$ and $L_1^2 = 1/4$ but i do know how to proceed.</p>

<p>Thanks in advance</p>
",<linear-algebra>
"<p>I was just wondering, for a dynamic system does the origin always have to be an attractor, saddle point, or repellor?</p>

<p>Also if a matrix isn't diagonalizable then does that mean the origin cannot be a repellor the matrix?</p>
",<linear-algebra>
"<p>Determine if $\vec b$ is a linear combination of $\vec a_1,\vec a_2,\vec a_3$.</p>

<p>$\vec a_1 = \left[\begin{array}{c}
1     \\
-2   \\
0    \\
\end{array}\right], \vec a_2 = \left[\begin{array}{c}
0    \\
1   \\
2    \\
\end{array}\right], \vec a_3=\left[\begin{array}{c}
5     \\
-6   \\
8    \\
\end{array}\right], \vec{b} = \left[\begin{array}{c}
2    \\
-1   \\
6    \\
\end{array}\right]$</p>

<p>Okay, so I made my constants $x_{1}, x_{2}, x_{3}$ for $\vec a_1, \vec a_2,\vec a_3$, respectively. I end up getting the following consistent system:
$$\left[\begin{array}{cccc}
1 &amp; 0 &amp; 5 &amp; 2    \\
0 &amp; 1 &amp; 4 &amp; 3   \\
0 &amp; 0 &amp; 0 &amp; 0    \\
\end{array}\right]$$
Which has the general solution: $$
\begin{cases}
x_{1} = 2 - 5x_{3} \\
x_{2} = 3 - 4x_{3} \\
x_{3} = \text{free}.
\end{cases}
$$
So $\vec b$ is equal to infinitely  many linear combinations of $\vec a_1,\vec a_2,\vec a_3$, right? Why does my book say that $\vec b$ is <em>not</em> a linear combination of these three vectors? Must the constants be a unique solution?</p>
",<linear-algebra>
"<p>It is clear that a reordering of the elements in a chosen basis for an n-dimensional vector space induces a permutation on n elements, and conversely such a permutation corresponds to a re-ordering of the basis.</p>

<p>I am wondering why the signature of a permutation is associated to the orientation induced by a basis. I understand that one can do this technically - but is there an intuitive way to understand why this is so? </p>

<p>That is, is there a way to understand  why the concept of orientation, as experienced in 1,2 and 3 dimensional Euclidean space, is formalized (and thereby generalized to abstract vector spaces of arbitrary finite dimension) using the signature? </p>
",<linear-algebra>
"<p>Suppose $A$ is a $2\times2$ matrix. How do I prove that, if $\det(A) &lt; 0$, then $A$ is a diagonalizable matrix over $\mathbb{R}$?</p>
",<linear-algebra>
"<p>I want to find the least squares solution to $\boldsymbol{Ax}=\boldsymbol{b}$ where $\boldsymbol{A}$ is a highly sparse square matrix.<br>
I found two methods that look like they might lead me to a solution: <a href=""http://en.wikipedia.org/wiki/QR_decomposition"" rel=""nofollow"">QR factorization</a>, and <a href=""http://en.wikipedia.org/wiki/Singular_value_decomposition#Pseudoinverse"" rel=""nofollow"">singular value decomposition</a>. Unfortunately, I haven't taken linear algebra yet, so I can't really understand most of what those pages are saying. I can calculate both in Matlab though, and it looks like the SVD gave me a smaller squared error. Why did that happen? How can I know which one I should be using in the future?</p>
",<linear-algebra>
"<p>I have a problem which is interesting: given a real matrix $A_{n\times n}$, when this matrix has a largest real eigenvalue which is strictly bigger than 1. If possible, can you give some conditions that can  guarantee this  statement? Equivalently, this statement says that the entropy of the subshift of finite type is strictly bigger than 0. </p>
",<linear-algebra>
"<blockquote>
  <p>Let $\mathbb{F}$ be an arbitrary field and $A\in M_{n\times n}(\mathbb{F})$ such that $$tr(A)=0$$
  Now show that there exists $P$,$Q$ $\in M_{n\times n}(\mathbb{F})$ such that $$A=PQ-QP$$</p>
</blockquote>

<p>It is so natural because we know that $tr(XY-YX)=0$ for all $X,Y$.</p>

<p>I know some proof by induction and canonical forms. Can somebody say another easy and elementary way.</p>
",<linear-algebra>
"<p>Let $A$ and $B$ be two real $n\times n$ matrices s.t. $AB=BA$. We now that $\det(A^2+B^2) \geq 0$. Is  the similar question true for $n$ matrices which commute with each other? If not, how do I generalize this question?</p>
",<linear-algebra>
"<blockquote>
  <p>Let $V$ be a vector space where dot product is defined. Then the following is true:
  $$\forall x, y\in V \quad \langle x,y\rangle^2  \leq \langle x,x \rangle\langle y,y \rangle$$
  <strong>Proof:</strong></p>
  
  <p>Consider the following linear combination: $z=\langle x,y\rangle x - \langle x,x\rangle y$. Let's find the dot product $\langle z,z\rangle \ge 0$: 
  $$\langle z,z\rangle = \langle \langle x,y\rangle x - \langle x,x\rangle y,\langle x,y\rangle x - \langle x,x\rangle y\rangle\\
\stackrel{?}= \langle x,x\rangle \left[ \langle x,x\rangle\langle y,y\rangle - \langle x,y\rangle^2\right] \ge0.
$$</p>
</blockquote>

<p>This is a proof from my linear algebra course book. I fail too see how the transition which I marked with $?$ was made. I will appreciate a clear explanation.</p>
",<linear-algebra>
"<p>Let $T,S: V\to V$ be 2 linear transformations, and $\ker(T)=\{0\}$.
I need to prove why $\ker(T\circ S)=\ker(S)$.<br>
I have no idea how to prove it.</p>
",<linear-algebra>
"<p>Let $A$ be a matrix so $A=A^2$.</p>

<p>I need to show that the eigenvalues of $A$ are only $1$ or $0$.</p>

<p>I tried some ways but none of them help.</p>
",<linear-algebra>
"<p>Let $x_{k+1} = Bx_k + c$
where $B$ is $n \times n$ matrix $c$ is a vector.</p>

<p>Assume $\|B\| \le \beta &lt;1$</p>

<p>$\|x_k - x_{k-1}\| \le \varepsilon$ for some $k$</p>

<p>Show that $\| x - x_k\| \le \dfrac{\beta\varepsilon}{1 - \beta}$</p>

<p>Thanks a lot...</p>
",<linear-algebra>
"<p>Naturally we can describe graphs via tables of ""yes there is an edge"" or ""no there is not"" between each pair of vertices, so the definition of an adjacency matrix is easily understood.  Thinking of these tables as <em>matrices</em>, however, adds structure - specifically, an interpretation as a linear operator.  Why do we look at them in this light?  Is it just for application - for example, efficiently obtaining a lot of data about a graph by computing its spectrum?  Or is there also an intuitive geometric (or algebraic) motivation behind the adjacency matrix?</p>

<p>For example, the $2$-path <img src=""http://i.stack.imgur.com/bSdoS.png"" alt=""2-path""> has adjacency matrix
 $$\mathcal{A}(P_2)=\left(\begin{array}{cc} 0 &amp; 1\\1 &amp; 0\end{array}\right)$$ which acts on a $2$-dimensional vector space by flipping the coordinates, $(x,y)\mapsto (y,x)$.  Can we somehow intuitively connect this action to the $2$-path?  What about for other simple graphs?</p>
",<linear-algebra>
"<p>Write vector 
u = $$\left[\begin{array}{ccc|c}2 \\10 \\1\end{array}\right]$$</p>

<p>as a linear combination of the vectors in S. Use elementary row operations on an augmented matrix to find the necessary coefficients. </p>

<p>S = {
$v1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , v2\left[\begin{matrix}4\\2\\1\end{matrix}\right],  
v2\left[\begin{matrix}5\\4\\1\end{matrix}\right]
$ }. If it is not possible, explain why?</p>

<hr>

<p>This is what i have so far: </p>

<p>S = {
$v1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , v2\left[\begin{matrix}4\\2\\1\end{matrix}\right],  
v3\left[\begin{matrix}5\\4\\1\end{matrix}\right].
v4\left[\begin{matrix}2\\10\\1\end{matrix}\right]
$ } 
<br><br>
$c1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , c2\left[\begin{matrix}4\\2\\1\end{matrix}\right],  
c3\left[\begin{matrix}5\\4\\1\end{matrix}\right].
c4\left[\begin{matrix}2\\10\\1\end{matrix}\right]
=\left[\begin{matrix}0\\0\\0\end{matrix}\right]$</p>

<p>$c1$$\left[\begin{matrix}1\\2\\2\end{matrix}\right] , c2\left[\begin{matrix}4\\2\\1\end{matrix}\right],  
c3\left[\begin{matrix}5\\4\\1\end{matrix}\right].
c4\left[\begin{matrix}2\\10\\1\end{matrix}\right]$</p>

<p>$
\begin{bmatrix}
1 &amp; 4 &amp; 5 &amp; 2\\
2 &amp; 2 &amp; 4 &amp; 10\\
2 &amp; 1 &amp; 1 &amp; 1\\
\end{bmatrix}
$</p>

<p>Now i don't know how to do this. Help will greatly be appreciated.</p>

<p>Thanks</p>
",<linear-algebra>
"<p>I am stuck with the following linear algebra problem:</p>

<p>given a basis $ \{e_{1} ... e_{n}\} $, I can define products $ \{p_{1} ... p_{m} \} $ as linear combination of the basis and the products itself:</p>

<p>$$
p_{i} = \sum_{j=1}^m u_{ij} p_{j} + \sum_{j=1}^n a_{ij}e_{ij} 
$$</p>

<p>My goal is to represent each product only as a combination of the basis. </p>

<p>$$
p_{i} = \sum_{j=1}^n s_{ij} e_{j} 
$$</p>

<p>any hint for a solution?</p>
",<linear-algebra>
"<p>I have the following question :</p>

<p>Let $A_{n \times n}$ that implies : $A^2-2A+I=0$ </p>

<ul>
<li>Proof $1$ is an eigevalue of $A$</li>
</ul>

<p>I don't really know how to approach this this what I manage to do (its not much though):</p>

<ul>
<li>$A(A-2I)=-I$ </li>
</ul>

<p>We know that if $\lambda$ is an eigenvalue then $Ax=\lambda x$ $(x \neq 0)$</p>

<p>$$A(A-2I)=I$$
Can I say now that since $Ax=\lambda x$ and Let $x=(A-2I)$ but $x$ is vector, not a matrix.</p>

<p>I don't really understand what to do next.</p>

<p>Any help will be be dearly appreciated, Thanks.</p>
",<linear-algebra>
"<p>Let $A$ be a linear operator which acts on the vector space $V=\langle x_1,x_2, \ldots,x_n\rangle$. Suppose we know its eigenvalues - $\lambda_1, \lambda_2, \ldots, \lambda_n.$</p>

<p>Now consider the vector space $V^{(2)} \subset {\rm Sym}^2 V$ generated by elements $x_i x_j, i&lt;j,$ $\dim V^{(2)}=\binom{n}{2}.$ Let us extend the operator $A$ on $V^{(2)}$    by linearity and by $A(x_i x_j)=A(x_i)A(x_j)$. Denote the extension by $A^{(2)}$  <em>and suppose that $A^{(2)}$ is an injective endomorphism of $V^{(2)}$.</em></p>

<p><strong>Question.</strong> What is the eigenvalues of the $A^{(2)}?$ 
My first answer    was that the set of eigenvalues consists of elements  $\lambda_i \lambda_j, i&lt;j$ but simple examples with small $n$  show  that is wrong answer.</p>

<p><strong>Edit.</strong> We may assume  that $A$ is  a permutation of the basis vectors.</p>
",<linear-algebra>
"<p>Let's say I am using the $Ham(2,11)$ code - the hamming code for which r = 2 and q=11 (length of alphabet). Can I detect errors caused by the transposition of 2 letters? If not, why?</p>
",<linear-algebra>
"<p>Assuming I have a $[n,k,d]$ linear code, how can it be shown that $d \leq n-k+1$ ?</p>
",<linear-algebra>
"<p>Suppose $A$ is a $m \times n$ matrix and $V$ is a $m \times 1$ matrix with both $A$ and $V$ having rational entries and suppose the system $AX=V$ has a solution in $\mathbb{R}^n$. Then the equation has a solution with rational entries.</p>

<p>Is the above statement true or false?</p>
",<linear-algebra>
"<p>$A$ is invertible, but it does not say that $A$ is symmetric.
By $B^T$ I mean that $B$ is transposed.</p>
",<linear-algebra>
"<p>Not all matrix with positive eigenvalues is positive definite, i.e. $\mathbf{x}^\mathsf{T}A\mathbf{x}&gt;0$ for all non zero vector $\mathbf{x}$. For example consider matrix</p>

<p>$$A = \begin{bmatrix} 1 &amp; -3 \\ 0 &amp; 1 \end{bmatrix}.$$</p>

<p>How to prove that if we add symmetry into hypothesis then the assertion is true? That is, a symmetric matrix with positive eigenvalues is positive definite. </p>
",<linear-algebra>
"<p>If $A\in L(X)$ then prove that $A^{-1}$ is linear and invertible.</p>

<p><strong>Proof:</strong> Since $A$ is invertible then $A$ is injective and surjective. We know that $A^{-1}$ defines by $A^{-1}(Ax)=x$. </p>

<p><em>Remark:</em> Also we can prove that $A(A^{-1}x)=x$. Indeed, if $x\in X$ and $A$ is surjective then exists $y\in X$ such that $x=Ay$. Hence $A(A^{-1}x)=A(A^{-1}Ay)=Ay=x$.</p>

<p>$A^{-1}(x+y)=A^{-1}(Ax_0+Ay_0)=A^{-1}A(x_0+y_0)=x_0+y_0=A^{-1}x+A^{-1}y$ and<br>
$A^{-1}(\alpha x)=A^{-1}(\alpha Ax_0)=A^{-1}A(\alpha x_0)=\alpha x_0=\alpha A^{-1}x$. Hence $A^{-1}$ is linear operator.</p>

<p>We have to prove that $A^{-1}$ is invertible, i.e. $A^{-1}$ is injective and surjective. </p>

<p>If $A^{-1}x=A^{-1}y$ then $x=Ax_0$ and $y=Ay_0$ for some $x_0,y_0\in X$. Then $A^{-1}Ax_0=A^{-1}Ay_0$ $\Rightarrow$ $x_0=y_0$ $\Rightarrow$ $x=y$.</p>

<p>For any $x\in X$ exists $y\in X$ such that $x=A^{-1}y$. For example, we can take $y=Ax$.</p>

<p>Thus, $A^{-1}$ is invertible and $\exists$ $(A^{-1})^{-1}$. We have prove that $(A^{-1})^{-1}=A$. Let $A^{-1}=F$ and $F^{-1}=G$.</p>

<p>For any $x\in X$ we have: $Gx=F^{-1}x$ since $F$ is invertible $\Rightarrow$ surjective then $\exists $$y\in X$ such that $x=Fy.$ Hence $$Gx=F^{-1}x=F^{-1}Fy=y=Ax.$$ Thus $(A^{-1})^{-1}=A.$</p>

<p>Please can anyone check my solution? I would be very grateful for any answer.</p>
",<linear-algebra>
"<p>The following is a discussion on the following second differential equation</p>

<p>$$  \frac{dy^2}{dx} - y = 0 $$</p>

<p>So, let us introduce the following, convention and definition, represent the derivative operator by</p>

<p>$$ D = \frac{d}{dx} $$</p>

<p>So that our first equation can be represented as</p>

<p>$$ \left( D^2 - 1 \right) y(x) = 0 $$</p>

<p>My professor discussed on the idea, on ""factoring"" (The space on which the derivative acts?) the operators, his discussion lead to the apparent nonuniqueness of factoring as either</p>

<p>$$ (D - \tanh(x))(D + \tanh(x)) y = 0 \quad \textrm{and } \quad (D - 1)(D + 1) = 0 $$</p>

<p>My first question is that how is that  $ (D - \tanh(x))(D + \tanh(x))  ``="" (D^2 - 1) $ and how is unfoiled?, because I think is not commutative because</p>

<p>$$ f(x)\frac{d}{dx}y \neq \frac{d}{dx}f(x)y(x) $$</p>

<p>One is first derive something and multiply by $f$, and the other is derive the product.</p>

<p>Second, my professor move to the general</p>

<p>$$ y'' + a(x)y' + b(x) = 0 $$</p>

<p>or</p>

<p>$$ \big(D^2 + a(x)D + b(x)\big)y(x) = 0 $$</p>

<p>and he wants to factor the above as</p>

<p>$$ \big( D + A(x)\big)\big(D + B(x)\big)y(x) = 0 $$</p>

<p>So then he unfoils the above equation as:</p>

<p>$$ \big( D^2 + AD + AB + B' + BD \big)y = 0 $$</p>

<p>and he argues that the terms $B'$ and $BD$ comes from the fact that either we take the derivative of $B$ or take the derivative and multiply by B (Why that doesn't apply to the terms $ AD  $ and we should add $ A'$ ? </p>

<p>Also, do you happen to know where to find reference on solving ODEs by this approach? Thanks</p>
",<linear-algebra>
"<p>Find the jordan form of the matrix
$$A = \begin{pmatrix}
 1 &amp; 1 &amp; 2 &amp; 2\\
 1 &amp; -2 &amp; -1 &amp; -1\\
 -2 &amp; 1 &amp; -1 &amp; -1\\
1 &amp; 1 &amp; 2 &amp; 2 \\
\end{pmatrix}$$</p>

<p>Hint: check that the matrix A has a single eigenvalue, and $trace(A) = 0$.</p>

<p>How can I check that the matrix has a single eigenvalue without using the determinant?</p>

<p>That's what I managed to do so far:</p>

<p>Let $G$ the Jordan form of $A$, using the hint that there is a single eigenvalue:
$$trace(A) = 0\implies 0 = trace(A) = trace(G) = 4a \implies a = 0$$</p>

<p>meaning that theres a single eigenvalue which is zero.</p>

<p>Then the characteristic polynomial is $A^4$  and the minimal polynomial is $A^3$.</p>

<p>Then:
$G = diag\{J_3(0), J_1(0)\}$</p>

<p>But how do I prove that there is a single eigenvalue?</p>
",<linear-algebra>
"<p>What is  an example of  a  $C^{*}$  algebra with an idempotent $e$ such that $e$ is  not Murray-von Neumann equivalent to $e^{*}$?</p>
",<linear-algebra>
"<p>For any $i \in \{1,2,3\}$, let:</p>

<ul>
<li>$w_i \in [0,1]$ is an <em>unknown</em> number such that $\sum_{i \in \{1,2,3\}} w_i = 1$.</li>
<li>$t$ is a <em>known</em> number in $[0,1]$. Suppose that $t = 0.8$.</li>
<li>$f_i$ is also a <em>known</em> number in $[0,1]$. Suppose that $f_1 = 0.2$, $f_2=0.6$, and $f_3=0.5$.</li>
</ul>

<p>Which are related by the equation: $t = \sum_{i \in \{1,2,3\}} w_i \times f_i$. It is assumed that there is only one true solution, and the rest are not true (maybe close estimations).</p>

<p>The questions are: </p>

<ul>
<li>What are the possible values of $w_1, w_2, w_3$ that satisfy the equation above? </li>
<li>How to find them?</li>
<li>In case too many possible values exist, what is the best method in estimating the values of $w_1,w_2,w_3$?</li>
</ul>

<h2>A brute-force by Python</h2>

<p>By brute-forcing answers, I found these which give a priority to ensuring that the sum $w_1+w_2+w_3=1$ (without ensuring that $t=0.8$):</p>

<pre><code>w1, w2, w3, w1+w2+w3, t
0, 0.01, 0.99, 1.0, 0.501
0, 0.02, 0.98, 1.0, 0.502
0, 0.02, 0.99, 1.01, 0.507
0, 0.03, 0.98, 1.01, 0.508
0, 0.03, 0.99, 1.02, 0.513
0, 0.04, 0.98, 1.02, 0.514
0, 0.04, 0.99, 1.03, 0.519
0, 0.05, 0.98, 1.03, 0.52
0, 0.05, 0.99, 1.04, 0.525
0, 0.06, 0.98, 1.04, 0.526
0, 0.06, 0.99, 1.05, 0.531
0, 0.07, 0.98, 1.05, 0.532
0, 0.07, 0.99, 1.06, 0.537
0, 0.08, 0.98, 1.06, 0.538
0, 0.08, 0.99, 1.07, 0.543
0, 0.09, 0.98, 1.07, 0.544
0, 0.09, 0.99, 1.08, 0.549
0, 0.1, 0.98, 1.08, 0.55
0, 0.1, 0.99, 1.09, 0.555
0, 0.11, 0.98, 1.09, 0.556
</code></pre>

<p>There are solutions, but none of them correspond to a $t$ that is close enough to 0.8.</p>

<p>Here I gave a high priority to ensuring that $t=0.8$ (while ignoring the sum = 1):</p>

<pre><code>w1, w2, w3, w1+w2+w3, t
0, 0.51, 0.99, 1.5, 0.801
0, 0.55, 0.94, 1.49, 0.8
0, 0.6, 0.88, 1.48, 0.8
0, 0.65, 0.82, 1.47, 0.8
0, 0.7, 0.76, 1.46, 0.8
0, 0.75, 0.7, 1.45, 0.8
0, 0.8, 0.64, 1.44, 0.8
0, 0.85, 0.58, 1.43, 0.8
0, 0.9, 0.52, 1.42, 0.8
0, 0.95, 0.46, 1.41, 0.8
0.01, 0.53, 0.96, 1.5, 0.8
0.01, 0.58, 0.9, 1.49, 0.8
0.01, 0.63, 0.84, 1.48, 0.8
0.01, 0.68, 0.78, 1.47, 0.8
0.01, 0.73, 0.72, 1.46, 0.8
0.01, 0.78, 0.66, 1.45, 0.8
0.01, 0.83, 0.6, 1.44, 0.8
0.04, 0.82, 0.6, 1.46, 0.8
0.06, 0.83, 0.58, 1.47, 0.8
0.07, 0.81, 0.6, 1.48, 0.8
0.09, 0.82, 0.58, 1.49, 0.8
0.1, 0.8, 0.6, 1.5, 0.8
0.11, 0.83, 0.56, 1.5, 0.8
0.12, 0.81, 0.58, 1.51, 0.8
0.13, 0.54, 0.9, 1.57, 0.8
0.13, 0.59, 0.84, 1.56, 0.8
0.13, 0.64, 0.78, 1.55, 0.8
0.13, 0.69, 0.72, 1.54, 0.8
0.13, 0.74, 0.66, 1.53, 0.8
0.13, 0.79, 0.6, 1.52, 0.8
0.14, 0.52, 0.92, 1.58, 0.8
0.14, 0.57, 0.86, 1.57, 0.8
0.14, 0.62, 0.8, 1.56, 0.8
0.14, 0.67, 0.74, 1.55, 0.8
0.14, 0.72, 0.68, 1.54, 0.8
0.14, 0.77, 0.62, 1.53, 0.8
0.14, 0.82, 0.56, 1.52, 0.8
0.15, 0.5, 0.94, 1.59, 0.8
0.15, 0.55, 0.88, 1.58, 0.8
0.15, 0.6, 0.82, 1.57, 0.8
0.15, 0.65, 0.76, 1.56, 0.8
0.15, 0.7, 0.7, 1.55, 0.8
0.15, 0.75, 0.64, 1.54, 0.8
0.15, 0.8, 0.58, 1.53, 0.8
0.16, 0.53, 0.9, 1.59, 0.8
0.16, 0.58, 0.84, 1.58, 0.8
0.16, 0.63, 0.78, 1.57, 0.8
0.16, 0.68, 0.72, 1.56, 0.8
0.16, 0.73, 0.66, 1.55, 0.8
0.16, 0.78, 0.6, 1.54, 0.8
0.16, 0.83, 0.54, 1.53, 0.8
0.17, 0.51, 0.92, 1.6, 0.8
0.17, 0.56, 0.86, 1.59, 0.8
0.17, 0.61, 0.8, 1.58, 0.8
0.17, 0.66, 0.74, 1.57, 0.8
0.17, 0.71, 0.68, 1.56, 0.8
0.17, 0.76, 0.62, 1.55, 0.8
0.17, 0.81, 0.56, 1.54, 0.8
0.18, 0.54, 0.88, 1.6, 0.8
0.18, 0.59, 0.82, 1.59, 0.8
0.18, 0.64, 0.76, 1.58, 0.8
0.18, 0.69, 0.7, 1.57, 0.8
0.18, 0.74, 0.64, 1.56, 0.8
0.18, 0.79, 0.58, 1.55, 0.8
0.19, 0.52, 0.9, 1.61, 0.8
0.19, 0.57, 0.84, 1.6, 0.8
0.19, 0.62, 0.78, 1.59, 0.8
0.19, 0.67, 0.72, 1.58, 0.8
0.19, 0.72, 0.66, 1.57, 0.8
0.19, 0.77, 0.6, 1.56, 0.8
0.19, 0.82, 0.54, 1.55, 0.8
0.2, 0.5, 0.92, 1.62, 0.8
0.2, 0.55, 0.86, 1.61, 0.8
0.2, 0.6, 0.8, 1.6, 0.8
0.2, 0.65, 0.74, 1.59, 0.8
0.2, 0.7, 0.68, 1.58, 0.8
0.2, 0.75, 0.62, 1.57, 0.8
0.2, 0.8, 0.56, 1.56, 0.8
0.21, 0.53, 0.88, 1.62, 0.8
0.21, 0.58, 0.82, 1.61, 0.8
0.21, 0.63, 0.76, 1.6, 0.8
0.21, 0.68, 0.7, 1.59, 0.8
0.21, 0.73, 0.64, 1.58, 0.8
0.21, 0.78, 0.58, 1.57, 0.8
0.21, 0.83, 0.52, 1.56, 0.8
0.22, 0.51, 0.9, 1.63, 0.8
0.22, 0.56, 0.84, 1.62, 0.8
0.22, 0.61, 0.78, 1.61, 0.8
0.22, 0.66, 0.72, 1.6, 0.8
0.22, 0.71, 0.66, 1.59, 0.8
0.22, 0.76, 0.6, 1.58, 0.8
0.22, 0.81, 0.54, 1.57, 0.8
0.23, 0.54, 0.86, 1.63, 0.8
0.23, 0.59, 0.8, 1.62, 0.8
0.23, 0.64, 0.74, 1.61, 0.8
0.23, 0.69, 0.68, 1.6, 0.8
0.23, 0.74, 0.62, 1.59, 0.8
0.23, 0.79, 0.56, 1.58, 0.8
0.24, 0.52, 0.88, 1.64, 0.8
0.24, 0.57, 0.82, 1.63, 0.8
0.24, 0.62, 0.76, 1.62, 0.8
0.24, 0.67, 0.7, 1.61, 0.8
0.24, 0.72, 0.64, 1.6, 0.8
0.24, 0.77, 0.58, 1.59, 0.8
0.24, 0.82, 0.52, 1.58, 0.8
0.25, 0.5, 0.9, 1.65, 0.8
0.25, 0.55, 0.84, 1.64, 0.8
0.25, 0.6, 0.78, 1.63, 0.8
0.25, 0.65, 0.72, 1.62, 0.8
0.25, 0.7, 0.66, 1.61, 0.8
0.25, 0.75, 0.6, 1.6, 0.8
0.25, 0.8, 0.54, 1.59, 0.8
0.26, 0.53, 0.86, 1.65, 0.8
0.26, 0.58, 0.8, 1.64, 0.8
0.26, 0.63, 0.74, 1.63, 0.8
0.26, 0.68, 0.68, 1.62, 0.8
0.26, 0.73, 0.62, 1.61, 0.8
0.26, 0.78, 0.56, 1.6, 0.8
0.26, 0.83, 0.5, 1.59, 0.8
0.27, 0.51, 0.88, 1.66, 0.8
0.27, 0.56, 0.82, 1.65, 0.8
0.27, 0.61, 0.76, 1.64, 0.8
0.27, 0.66, 0.7, 1.63, 0.8
0.27, 0.71, 0.64, 1.62, 0.8
0.27, 0.76, 0.58, 1.61, 0.8
0.27, 0.81, 0.52, 1.6, 0.8
0.28, 0.54, 0.84, 1.66, 0.8
0.28, 0.59, 0.78, 1.65, 0.8
0.28, 0.64, 0.72, 1.64, 0.8
0.28, 0.69, 0.66, 1.63, 0.8
0.28, 0.74, 0.6, 1.62, 0.8
0.28, 0.79, 0.54, 1.61, 0.8
0.29, 0.52, 0.86, 1.67, 0.8
0.29, 0.57, 0.8, 1.66, 0.8
0.29, 0.62, 0.74, 1.65, 0.8
0.29, 0.67, 0.68, 1.64, 0.8
0.29, 0.72, 0.62, 1.63, 0.8
0.29, 0.77, 0.56, 1.62, 0.8
0.29, 0.82, 0.5, 1.61, 0.8
0.3, 0.5, 0.88, 1.68, 0.8
0.3, 0.55, 0.82, 1.67, 0.8
0.3, 0.6, 0.76, 1.66, 0.8
0.3, 0.65, 0.7, 1.65, 0.8
0.3, 0.7, 0.64, 1.64, 0.8
0.3, 0.75, 0.58, 1.63, 0.8
0.3, 0.8, 0.52, 1.62, 0.8
0.31, 0.53, 0.84, 1.68, 0.8
0.31, 0.58, 0.78, 1.67, 0.8
0.31, 0.63, 0.72, 1.66, 0.8
0.31, 0.68, 0.66, 1.65, 0.8
0.31, 0.73, 0.6, 1.64, 0.8
0.31, 0.78, 0.54, 1.63, 0.8
0.31, 0.83, 0.48, 1.62, 0.8
0.32, 0.51, 0.86, 1.69, 0.8
0.32, 0.56, 0.8, 1.68, 0.8
0.32, 0.61, 0.74, 1.67, 0.8
0.32, 0.66, 0.68, 1.66, 0.8
0.32, 0.71, 0.62, 1.65, 0.8
0.32, 0.76, 0.56, 1.64, 0.8
0.32, 0.81, 0.5, 1.63, 0.8
0.33, 0.54, 0.82, 1.69, 0.8
0.33, 0.59, 0.76, 1.68, 0.8
0.33, 0.64, 0.7, 1.67, 0.8
0.33, 0.69, 0.64, 1.66, 0.8
0.33, 0.74, 0.58, 1.65, 0.8
0.33, 0.79, 0.52, 1.64, 0.8
0.34, 0.52, 0.84, 1.7, 0.8
0.34, 0.57, 0.78, 1.69, 0.8
0.34, 0.62, 0.72, 1.68, 0.8
0.34, 0.67, 0.66, 1.67, 0.8
0.34, 0.72, 0.6, 1.66, 0.8
0.34, 0.77, 0.54, 1.65, 0.8
0.34, 0.82, 0.48, 1.64, 0.8
0.35, 0.5, 0.86, 1.71, 0.8
0.35, 0.55, 0.8, 1.7, 0.8
0.35, 0.6, 0.74, 1.69, 0.8
0.35, 0.65, 0.68, 1.68, 0.8
0.35, 0.7, 0.62, 1.67, 0.8
0.35, 0.75, 0.56, 1.66, 0.8
0.35, 0.8, 0.5, 1.65, 0.8
0.36, 0.53, 0.82, 1.71, 0.8
0.36, 0.58, 0.76, 1.7, 0.8
0.36, 0.63, 0.7, 1.69, 0.8
0.36, 0.68, 0.64, 1.68, 0.8
0.36, 0.73, 0.58, 1.67, 0.8
0.36, 0.78, 0.52, 1.66, 0.8
0.36, 0.83, 0.46, 1.65, 0.8
0.37, 0.51, 0.84, 1.72, 0.8
0.37, 0.56, 0.78, 1.71, 0.8
0.37, 0.61, 0.72, 1.7, 0.8
0.37, 0.66, 0.66, 1.69, 0.8
0.37, 0.71, 0.6, 1.68, 0.8
0.37, 0.76, 0.54, 1.67, 0.8
0.37, 0.81, 0.48, 1.66, 0.8
0.38, 0.54, 0.8, 1.72, 0.8
0.38, 0.59, 0.74, 1.71, 0.8
0.38, 0.64, 0.68, 1.7, 0.8
0.38, 0.69, 0.62, 1.69, 0.8
0.38, 0.74, 0.56, 1.68, 0.8
0.38, 0.79, 0.5, 1.67, 0.8
0.39, 0.52, 0.82, 1.73, 0.8
0.39, 0.57, 0.76, 1.72, 0.8
0.39, 0.62, 0.7, 1.71, 0.8
0.39, 0.67, 0.64, 1.7, 0.8
0.39, 0.72, 0.58, 1.69, 0.8
0.39, 0.77, 0.52, 1.68, 0.8
0.39, 0.82, 0.46, 1.67, 0.8
0.4, 0.5, 0.84, 1.74, 0.8
0.4, 0.55, 0.78, 1.73, 0.8
0.4, 0.6, 0.72, 1.72, 0.8
0.4, 0.65, 0.66, 1.71, 0.8
0.4, 0.7, 0.6, 1.7, 0.8
0.4, 0.75, 0.54, 1.69, 0.8
0.4, 0.8, 0.48, 1.68, 0.8
0.41, 0.53, 0.8, 1.74, 0.8
0.41, 0.58, 0.74, 1.73, 0.8
0.41, 0.63, 0.68, 1.72, 0.8
0.41, 0.68, 0.62, 1.71, 0.8
0.41, 0.73, 0.56, 1.7, 0.8
0.41, 0.78, 0.5, 1.69, 0.8
0.41, 0.83, 0.44, 1.68, 0.8
0.42, 0.51, 0.82, 1.75, 0.8
0.42, 0.56, 0.76, 1.74, 0.8
0.42, 0.61, 0.7, 1.73, 0.8
0.42, 0.66, 0.64, 1.72, 0.8
0.42, 0.71, 0.58, 1.71, 0.8
0.42, 0.76, 0.52, 1.7, 0.8
0.42, 0.81, 0.46, 1.69, 0.8
0.43, 0.54, 0.78, 1.75, 0.8
0.43, 0.59, 0.72, 1.74, 0.8
0.43, 0.64, 0.66, 1.73, 0.8
0.43, 0.69, 0.6, 1.72, 0.8
0.43, 0.74, 0.54, 1.71, 0.8
0.43, 0.79, 0.48, 1.7, 0.8
0.44, 0.52, 0.8, 1.76, 0.8
0.44, 0.57, 0.74, 1.75, 0.8
0.44, 0.62, 0.68, 1.74, 0.8
0.44, 0.67, 0.62, 1.73, 0.8
0.44, 0.72, 0.56, 1.72, 0.8
0.44, 0.77, 0.5, 1.71, 0.8
0.44, 0.82, 0.44, 1.7, 0.8
0.45, 0.5, 0.82, 1.77, 0.8
0.45, 0.55, 0.76, 1.76, 0.8
0.45, 0.6, 0.7, 1.75, 0.8
0.45, 0.65, 0.64, 1.74, 0.8
0.45, 0.7, 0.58, 1.73, 0.8
0.45, 0.75, 0.52, 1.72, 0.8
0.45, 0.8, 0.46, 1.71, 0.8
0.46, 0.53, 0.78, 1.77, 0.8
0.46, 0.58, 0.72, 1.76, 0.8
0.46, 0.63, 0.66, 1.75, 0.8
0.46, 0.68, 0.6, 1.74, 0.8
0.46, 0.73, 0.54, 1.73, 0.8
0.46, 0.78, 0.48, 1.72, 0.8
0.46, 0.83, 0.42, 1.71, 0.8
0.47, 0.51, 0.8, 1.78, 0.8
0.47, 0.56, 0.74, 1.77, 0.8
0.47, 0.61, 0.68, 1.76, 0.8
0.47, 0.66, 0.62, 1.75, 0.8
0.47, 0.71, 0.56, 1.74, 0.8
0.47, 0.76, 0.5, 1.73, 0.8
0.47, 0.81, 0.44, 1.72, 0.8
0.48, 0.54, 0.76, 1.78, 0.8
0.48, 0.59, 0.7, 1.77, 0.8
0.48, 0.64, 0.64, 1.76, 0.8
0.48, 0.69, 0.58, 1.75, 0.8
0.48, 0.74, 0.52, 1.74, 0.8
0.48, 0.79, 0.46, 1.73, 0.8
0.49, 0.52, 0.78, 1.79, 0.8
0.49, 0.57, 0.72, 1.78, 0.8
0.49, 0.62, 0.66, 1.77, 0.8
0.49, 0.67, 0.6, 1.76, 0.8
0.49, 0.72, 0.54, 1.75, 0.8
0.49, 0.77, 0.48, 1.74, 0.8
0.49, 0.82, 0.42, 1.73, 0.8
0.5, 0.5, 0.8, 1.8, 0.8
0.5, 0.55, 0.74, 1.79, 0.8
0.5, 0.6, 0.68, 1.78, 0.8
0.5, 0.65, 0.62, 1.77, 0.8
0.5, 0.7, 0.56, 1.76, 0.8
0.5, 0.75, 0.5, 1.75, 0.8
0.5, 0.8, 0.44, 1.74, 0.8
0.51, 0.53, 0.76, 1.8, 0.8
0.51, 0.58, 0.7, 1.79, 0.8
0.51, 0.63, 0.64, 1.78, 0.8
0.51, 0.68, 0.58, 1.77, 0.8
0.51, 0.73, 0.52, 1.76, 0.8
0.51, 0.78, 0.46, 1.75, 0.8
0.51, 0.83, 0.4, 1.74, 0.8
0.52, 0.51, 0.78, 1.81, 0.8
0.52, 0.56, 0.72, 1.8, 0.8
0.52, 0.61, 0.66, 1.79, 0.8
0.52, 0.66, 0.6, 1.78, 0.8
0.52, 0.71, 0.54, 1.77, 0.8
0.52, 0.76, 0.48, 1.76, 0.8
0.52, 0.81, 0.42, 1.75, 0.8
0.53, 0.54, 0.74, 1.81, 0.8
0.53, 0.59, 0.68, 1.8, 0.8
0.53, 0.64, 0.62, 1.79, 0.8
0.53, 0.69, 0.56, 1.78, 0.8
0.53, 0.74, 0.5, 1.77, 0.8
0.53, 0.79, 0.44, 1.76, 0.8
0.54, 0.52, 0.76, 1.82, 0.8
0.54, 0.57, 0.7, 1.81, 0.8
0.54, 0.62, 0.64, 1.8, 0.8
0.54, 0.67, 0.58, 1.79, 0.8
0.54, 0.72, 0.52, 1.78, 0.8
0.54, 0.77, 0.46, 1.77, 0.8
0.54, 0.82, 0.4, 1.76, 0.8
0.55, 0.5, 0.78, 1.83, 0.8
0.55, 0.55, 0.72, 1.82, 0.8
0.55, 0.6, 0.66, 1.81, 0.8
0.55, 0.65, 0.6, 1.8, 0.8
0.55, 0.7, 0.54, 1.79, 0.8
0.55, 0.75, 0.48, 1.78, 0.8
0.55, 0.8, 0.42, 1.77, 0.8
0.56, 0.53, 0.74, 1.83, 0.8
0.56, 0.58, 0.68, 1.82, 0.8
0.56, 0.63, 0.62, 1.81, 0.8
0.56, 0.68, 0.56, 1.8, 0.8
0.56, 0.73, 0.5, 1.79, 0.8
0.56, 0.78, 0.44, 1.78, 0.8
0.56, 0.83, 0.38, 1.77, 0.8
0.57, 0.51, 0.76, 1.84, 0.8
0.57, 0.56, 0.7, 1.83, 0.8
0.57, 0.61, 0.64, 1.82, 0.8
0.57, 0.66, 0.58, 1.81, 0.8
0.57, 0.71, 0.52, 1.8, 0.8
0.57, 0.76, 0.46, 1.79, 0.8
0.57, 0.81, 0.4, 1.78, 0.8
0.58, 0.54, 0.72, 1.84, 0.8
0.58, 0.59, 0.66, 1.83, 0.8
0.58, 0.64, 0.6, 1.82, 0.8
0.58, 0.69, 0.54, 1.81, 0.8
0.58, 0.74, 0.48, 1.8, 0.8
0.58, 0.79, 0.42, 1.79, 0.8
0.59, 0.52, 0.74, 1.85, 0.8
0.59, 0.57, 0.68, 1.84, 0.8
0.59, 0.62, 0.62, 1.83, 0.8
0.59, 0.67, 0.56, 1.82, 0.8
0.59, 0.72, 0.5, 1.81, 0.8
0.59, 0.77, 0.44, 1.8, 0.8
0.59, 0.82, 0.38, 1.79, 0.8
0.6, 0.5, 0.76, 1.86, 0.8
0.6, 0.55, 0.7, 1.85, 0.8
0.6, 0.6, 0.64, 1.84, 0.8
0.6, 0.65, 0.58, 1.83, 0.8
0.6, 0.7, 0.52, 1.82, 0.8
0.6, 0.75, 0.46, 1.81, 0.8
0.6, 0.8, 0.4, 1.8, 0.8
0.61, 0.53, 0.72, 1.86, 0.8
0.61, 0.58, 0.66, 1.85, 0.8
0.61, 0.63, 0.6, 1.84, 0.8
0.61, 0.68, 0.54, 1.83, 0.8
0.61, 0.73, 0.48, 1.82, 0.8
0.61, 0.78, 0.42, 1.81, 0.8
0.61, 0.83, 0.36, 1.8, 0.8
0.62, 0.51, 0.74, 1.87, 0.8
0.62, 0.56, 0.68, 1.86, 0.8
0.62, 0.61, 0.62, 1.85, 0.8
0.62, 0.66, 0.56, 1.84, 0.8
0.62, 0.71, 0.5, 1.83, 0.8
0.62, 0.76, 0.44, 1.82, 0.8
0.62, 0.81, 0.38, 1.81, 0.8
0.63, 0.54, 0.7, 1.87, 0.8
0.63, 0.59, 0.64, 1.86, 0.8
0.63, 0.64, 0.58, 1.85, 0.8
0.63, 0.69, 0.52, 1.84, 0.8
0.63, 0.74, 0.46, 1.83, 0.8
0.63, 0.79, 0.4, 1.82, 0.8
0.64, 0.52, 0.72, 1.88, 0.8
0.64, 0.57, 0.66, 1.87, 0.8
0.64, 0.62, 0.6, 1.86, 0.8
0.64, 0.67, 0.54, 1.85, 0.8
0.64, 0.72, 0.48, 1.84, 0.8
0.64, 0.77, 0.42, 1.83, 0.8
0.64, 0.82, 0.36, 1.82, 0.8
0.65, 0.5, 0.74, 1.89, 0.8
0.65, 0.55, 0.68, 1.88, 0.8
0.65, 0.6, 0.62, 1.87, 0.8
0.65, 0.65, 0.56, 1.86, 0.8
0.65, 0.7, 0.5, 1.85, 0.8
0.65, 0.75, 0.44, 1.84, 0.8
0.65, 0.8, 0.38, 1.83, 0.8
0.66, 0.53, 0.7, 1.89, 0.8
0.66, 0.58, 0.64, 1.88, 0.8
0.66, 0.63, 0.58, 1.87, 0.8
0.66, 0.68, 0.52, 1.86, 0.8
0.66, 0.73, 0.46, 1.85, 0.8
0.66, 0.78, 0.4, 1.84, 0.8
0.66, 0.83, 0.34, 1.83, 0.8
0.67, 0.51, 0.72, 1.9, 0.8
0.67, 0.56, 0.66, 1.89, 0.8
0.67, 0.61, 0.6, 1.88, 0.8
0.67, 0.66, 0.54, 1.87, 0.8
0.67, 0.71, 0.48, 1.86, 0.8
0.67, 0.76, 0.42, 1.85, 0.8
0.67, 0.81, 0.36, 1.84, 0.8
0.68, 0.54, 0.68, 1.9, 0.8
0.68, 0.59, 0.62, 1.89, 0.8
0.68, 0.64, 0.56, 1.88, 0.8
0.68, 0.69, 0.5, 1.87, 0.8
0.68, 0.74, 0.44, 1.86, 0.8
0.68, 0.79, 0.38, 1.85, 0.8
0.69, 0.52, 0.7, 1.91, 0.8
0.69, 0.57, 0.64, 1.9, 0.8
0.69, 0.62, 0.58, 1.89, 0.8
0.69, 0.67, 0.52, 1.88, 0.8
0.69, 0.72, 0.46, 1.87, 0.8
0.69, 0.77, 0.4, 1.86, 0.8
0.69, 0.82, 0.34, 1.85, 0.8
0.7, 0.5, 0.72, 1.92, 0.8
0.7, 0.55, 0.66, 1.91, 0.8
0.7, 0.6, 0.6, 1.9, 0.8
0.7, 0.65, 0.54, 1.89, 0.8
0.7, 0.7, 0.48, 1.88, 0.8
0.7, 0.75, 0.42, 1.87, 0.8
0.7, 0.8, 0.36, 1.86, 0.8
0.71, 0.53, 0.68, 1.92, 0.8
0.71, 0.58, 0.62, 1.91, 0.8
0.71, 0.63, 0.56, 1.9, 0.8
0.71, 0.68, 0.5, 1.89, 0.8
0.71, 0.73, 0.44, 1.88, 0.8
0.71, 0.78, 0.38, 1.87, 0.8
0.71, 0.83, 0.32, 1.86, 0.8
0.72, 0.51, 0.7, 1.93, 0.8
0.72, 0.56, 0.64, 1.92, 0.8
0.72, 0.61, 0.58, 1.91, 0.8
0.72, 0.66, 0.52, 1.9, 0.8
0.72, 0.71, 0.46, 1.89, 0.8
0.72, 0.76, 0.4, 1.88, 0.8
0.72, 0.81, 0.34, 1.87, 0.8
0.73, 0.54, 0.66, 1.93, 0.8
0.73, 0.59, 0.6, 1.92, 0.8
0.73, 0.64, 0.54, 1.91, 0.8
0.73, 0.69, 0.48, 1.9, 0.8
0.73, 0.74, 0.42, 1.89, 0.8
0.73, 0.79, 0.36, 1.88, 0.8
0.74, 0.52, 0.68, 1.94, 0.8
0.74, 0.57, 0.62, 1.93, 0.8
0.74, 0.62, 0.56, 1.92, 0.8
0.74, 0.67, 0.5, 1.91, 0.8
0.74, 0.72, 0.44, 1.9, 0.8
0.74, 0.77, 0.38, 1.89, 0.8
0.74, 0.82, 0.32, 1.88, 0.8
0.75, 0.5, 0.7, 1.95, 0.8
0.75, 0.55, 0.64, 1.94, 0.8
0.75, 0.6, 0.58, 1.93, 0.8
0.75, 0.65, 0.52, 1.92, 0.8
0.75, 0.7, 0.46, 1.91, 0.8
0.75, 0.75, 0.4, 1.9, 0.8
0.75, 0.8, 0.34, 1.89, 0.8
0.76, 0.53, 0.66, 1.95, 0.8
0.76, 0.58, 0.6, 1.94, 0.8
0.76, 0.63, 0.54, 1.93, 0.8
0.76, 0.68, 0.48, 1.92, 0.8
0.76, 0.73, 0.42, 1.91, 0.8
0.76, 0.78, 0.36, 1.9, 0.8
0.76, 0.83, 0.3, 1.89, 0.8
0.77, 0.51, 0.68, 1.96, 0.8
0.77, 0.56, 0.62, 1.95, 0.8
0.77, 0.61, 0.56, 1.94, 0.8
0.77, 0.66, 0.5, 1.93, 0.8
0.77, 0.71, 0.44, 1.92, 0.8
0.77, 0.76, 0.38, 1.91, 0.8
0.77, 0.81, 0.32, 1.9, 0.8
0.78, 0.54, 0.64, 1.96, 0.8
0.78, 0.59, 0.58, 1.95, 0.8
0.78, 0.64, 0.52, 1.94, 0.8
0.78, 0.69, 0.46, 1.93, 0.8
0.78, 0.74, 0.4, 1.92, 0.8
0.78, 0.79, 0.34, 1.91, 0.8
0.79, 0.52, 0.66, 1.97, 0.8
0.79, 0.57, 0.6, 1.96, 0.8
0.79, 0.62, 0.54, 1.95, 0.8
0.79, 0.67, 0.48, 1.94, 0.8
0.79, 0.72, 0.42, 1.93, 0.8
0.79, 0.77, 0.36, 1.92, 0.8
0.79, 0.82, 0.3, 1.91, 0.8
0.8, 0.5, 0.68, 1.98, 0.8
0.8, 0.55, 0.62, 1.97, 0.8
0.8, 0.6, 0.56, 1.96, 0.8
0.8, 0.65, 0.5, 1.95, 0.8
0.8, 0.7, 0.44, 1.94, 0.8
0.8, 0.75, 0.38, 1.93, 0.8
0.8, 0.8, 0.32, 1.92, 0.8
0.81, 0.53, 0.64, 1.98, 0.8
0.81, 0.58, 0.58, 1.97, 0.8
0.81, 0.63, 0.52, 1.96, 0.8
0.81, 0.68, 0.46, 1.95, 0.8
0.81, 0.73, 0.4, 1.94, 0.8
0.81, 0.78, 0.34, 1.93, 0.8
0.81, 0.83, 0.28, 1.92, 0.8
0.82, 0.51, 0.66, 1.99, 0.8
0.82, 0.56, 0.6, 1.98, 0.8
0.82, 0.61, 0.54, 1.97, 0.8
0.82, 0.66, 0.48, 1.96, 0.8
0.82, 0.71, 0.42, 1.95, 0.8
0.82, 0.76, 0.36, 1.94, 0.8
0.82, 0.81, 0.3, 1.93, 0.8
0.83, 0.54, 0.62, 1.99, 0.8
0.83, 0.59, 0.56, 1.98, 0.8
0.83, 0.64, 0.5, 1.97, 0.8
0.83, 0.69, 0.44, 1.96, 0.8
0.83, 0.74, 0.38, 1.95, 0.8
0.83, 0.79, 0.32, 1.94, 0.8
0.84, 0.52, 0.64, 2.0, 0.8
0.84, 0.57, 0.58, 1.99, 0.8
0.84, 0.62, 0.52, 1.98, 0.8
0.84, 0.67, 0.46, 1.97, 0.8
0.84, 0.72, 0.4, 1.96, 0.8
0.84, 0.77, 0.34, 1.95, 0.8
0.84, 0.82, 0.28, 1.94, 0.8
0.85, 0.5, 0.66, 2.01, 0.8
0.85, 0.55, 0.6, 2.0, 0.8
0.85, 0.6, 0.54, 1.99, 0.8
0.85, 0.65, 0.48, 1.98, 0.8
0.85, 0.7, 0.42, 1.97, 0.8
0.85, 0.75, 0.36, 1.96, 0.8
0.85, 0.8, 0.3, 1.95, 0.8
0.86, 0.53, 0.62, 2.01, 0.8
0.86, 0.58, 0.56, 2.0, 0.8
0.86, 0.63, 0.5, 1.99, 0.8
0.86, 0.68, 0.44, 1.98, 0.8
0.86, 0.73, 0.38, 1.97, 0.8
0.86, 0.78, 0.32, 1.96, 0.8
0.86, 0.83, 0.26, 1.95, 0.8
0.87, 0.51, 0.64, 2.02, 0.8
0.87, 0.56, 0.58, 2.01, 0.8
0.87, 0.61, 0.52, 2.0, 0.8
0.87, 0.66, 0.46, 1.99, 0.8
0.87, 0.71, 0.4, 1.98, 0.8
0.87, 0.76, 0.34, 1.97, 0.8
0.87, 0.81, 0.28, 1.96, 0.8
0.88, 0.54, 0.6, 2.02, 0.8
0.88, 0.59, 0.54, 2.01, 0.8
0.88, 0.64, 0.48, 2.0, 0.8
0.88, 0.69, 0.42, 1.99, 0.8
0.88, 0.74, 0.36, 1.98, 0.8
0.88, 0.79, 0.3, 1.97, 0.8
0.89, 0.52, 0.62, 2.03, 0.8
0.89, 0.57, 0.56, 2.02, 0.8
0.89, 0.62, 0.5, 2.01, 0.8
0.89, 0.67, 0.44, 2.0, 0.8
0.89, 0.72, 0.38, 1.99, 0.8
0.89, 0.77, 0.32, 1.98, 0.8
0.89, 0.82, 0.26, 1.97, 0.8
0.9, 0.5, 0.64, 2.04, 0.8
0.9, 0.55, 0.58, 2.03, 0.8
0.9, 0.6, 0.52, 2.02, 0.8
0.9, 0.65, 0.46, 2.01, 0.8
0.9, 0.7, 0.4, 2.0, 0.8
0.9, 0.75, 0.34, 1.99, 0.8
0.9, 0.8, 0.28, 1.98, 0.8
0.91, 0.53, 0.6, 2.04, 0.8
0.91, 0.58, 0.54, 2.03, 0.8
0.91, 0.63, 0.48, 2.02, 0.8
0.91, 0.68, 0.42, 2.01, 0.8
0.91, 0.73, 0.36, 2.0, 0.8
0.91, 0.78, 0.3, 1.99, 0.8
0.91, 0.83, 0.24, 1.98, 0.8
0.92, 0.51, 0.62, 2.05, 0.8
0.92, 0.56, 0.56, 2.04, 0.8
0.92, 0.61, 0.5, 2.03, 0.8
0.92, 0.66, 0.44, 2.02, 0.8
0.92, 0.71, 0.38, 2.01, 0.8
0.92, 0.76, 0.32, 2.0, 0.8
0.92, 0.81, 0.26, 1.99, 0.8
0.93, 0.54, 0.58, 2.05, 0.8
0.93, 0.59, 0.52, 2.04, 0.8
0.93, 0.64, 0.46, 2.03, 0.8
0.93, 0.69, 0.4, 2.02, 0.8
0.93, 0.74, 0.34, 2.01, 0.8
0.93, 0.79, 0.28, 2.0, 0.8
0.94, 0.52, 0.6, 2.06, 0.8
0.94, 0.57, 0.54, 2.05, 0.8
0.94, 0.62, 0.48, 2.04, 0.8
0.94, 0.67, 0.42, 2.03, 0.8
0.94, 0.72, 0.36, 2.02, 0.8
0.94, 0.77, 0.3, 2.01, 0.8
0.94, 0.82, 0.24, 2.0, 0.8
0.95, 0.5, 0.62, 2.07, 0.8
0.95, 0.55, 0.56, 2.06, 0.8
0.95, 0.6, 0.5, 2.05, 0.8
0.95, 0.65, 0.44, 2.04, 0.8
0.95, 0.7, 0.38, 2.03, 0.8
0.95, 0.75, 0.32, 2.02, 0.8
0.95, 0.8, 0.26, 2.01, 0.8
0.96, 0.53, 0.58, 2.07, 0.8
0.96, 0.58, 0.52, 2.06, 0.8
0.96, 0.63, 0.46, 2.05, 0.8
0.96, 0.68, 0.4, 2.04, 0.8
0.96, 0.73, 0.34, 2.03, 0.8
0.96, 0.78, 0.28, 2.02, 0.8
0.96, 0.83, 0.22, 2.01, 0.8
0.97, 0.51, 0.6, 2.08, 0.8
0.97, 0.56, 0.54, 2.07, 0.8
0.97, 0.61, 0.48, 2.06, 0.8
0.97, 0.66, 0.42, 2.05, 0.8
0.97, 0.71, 0.36, 2.04, 0.8
0.97, 0.76, 0.3, 2.03, 0.8
0.97, 0.81, 0.24, 2.02, 0.8
0.98, 0.54, 0.56, 2.08, 0.8
0.98, 0.59, 0.5, 2.07, 0.8
0.98, 0.64, 0.44, 2.06, 0.8
0.98, 0.69, 0.38, 2.05, 0.8
0.98, 0.74, 0.32, 2.04, 0.8
0.98, 0.79, 0.26, 2.03, 0.8
0.99, 0.52, 0.58, 2.09, 0.8
0.99, 0.57, 0.52, 2.08, 0.8
0.99, 0.62, 0.46, 2.07, 0.8
0.99, 0.67, 0.4, 2.06, 0.8
0.99, 0.72, 0.34, 2.05, 0.8
0.99, 0.77, 0.28, 2.04, 0.8
0.99, 0.82, 0.22, 2.03, 0.8
</code></pre>

<p>It seems that the closest solution is:  $w_1=0$, $w_2=0.9$, $w_3=0.52$, which gives the sum $w_1+w_2+w_3=1.42$, and maintains $t=0.8$.</p>

<p>Among all of the runs, I found this: $w_1=0$, $w_2=0.29$, $w_3=0.99$, $w_1+w_2+w_3=1.28$, $t=0.669$ --- is this a good compromise? (just an example).</p>

<p>Is there a mathematical method in identifying a solution that generally minimizes the error $((w_1+w_2+w_3) - 1)^2 + (t-0.8)^2$. Feel free to propose an alternative error metric.</p>

<h2>A preliminary guess of mine:</h2>

<p>How about finding $w_1,w_2,w_3$ that minimize $\Big(\big((w_1+w_2+w_3) - 1\big)^2 + (t-0.8)^2\Big)$?</p>

<p>Is there a way to do this using calculus?</p>
",<linear-algebra>
"<p>I have a cyclotomic field $\mathbb{Q}(\zeta_3)$, and want to know how I can find a minimal polynomial of  $\zeta_{10}$, and $\zeta_{12}$. I have determined that both the polynomials should be of degree 2. I am trying to use a basis argument for both of them. That is: </p>

<p>$(x+m\zeta_3)(x+n\zeta_3)=0$; here $x=\zeta_{10}$</p>

<p>$(y+p\zeta_3)(y+q\zeta_3)=0$; here $y=\zeta_{12}$</p>

<p>and trying to find some explicit restrictions for m,n and p,q. But I am having trouble doing that.</p>
",<linear-algebra>
"<p>Suppose I have a vector field $F(x)=Ax$ where $A$ is a matrix. How can I express $Sx$ without $A$ (use $F$ instead)? Here $S=\dfrac{A+A^T}2$ is symmetric part of $A$.</p>
",<linear-algebra>
"<p>Find all matrices similar <em>only</em> to themselves, i.e., $PTP^{-1}=T$ for any invertible $P$.</p>

<p>My attempt: $PT = TP$.</p>

<p>Am I going about this correctly? If so, how do I find all matrices that are commutative (where $P$ is invertible)?</p>
",<linear-algebra>
"<p>Let $S$ and $W$ be subsets of a vector space $V$. Show that if $S$ is a subset of $W$, then $\mathrm{span}(S)$ is a subspace of $\mathrm{span}(W)$.</p>

<p>Ok I'm finally understanding what each of these things mean.. but I'm running out of ideas on how to actually show it without using arbitrary vector space examples like Rn. Since sets and subspaces are kind of new its hard to figure out how to write correct proofs with them. </p>

<p>I can see how if $V = \mathbb{R}^3$ and $S = \{(1,1,1)\}$ and $W = \{(1,1,1), (1,1,2)\}$ then $\mathrm{span}(S) = R1(line), span(W) = R2(plane)$ and that R1 is a subspace of R2... im just having trouble showing this officially when all vector spaces and sets are completely in general....</p>

<p>I could get as far as writing out $S=(u_1,u_2,\dots,u_n)$, $u_i \in V$ and same with $W$, but I can't really figure out what else to do in this sort of proof.</p>
",<linear-algebra>
"<p>I know that if $L$ is a linear transformation from $V$ to $W$ where $V,W$ are finite dimensional, then we can conclude that the dimension of image (rank) of $L$ is same as that of its transpose, i.e., $L^t$.</p>

<p>But what happens when: $\dim V,\dim W=\infty$, or just one of them has infinite dimension? If there is any difference in the above statement, why such difference arises?</p>
",<linear-algebra>
"<p>I had a question on some h/w that asked if $row(A)=col(A)$ then $A = A^t$.</p>

<p>I answered false and found somewhere that if $A^t = - A$ then $row(A) = col(A)$</p>

<p>does this go the other way as well? </p>
",<linear-algebra>
"<p>Given two vectors $\vec{u}$, $\vec{v}$ indexed by $2^X$ for some finite set $X$, define $\vec{u} \star \vec{v}$ as the vector of similar type whose dimension indexed by $S \subseteq X$ is:
$$\sum_{\begin{array}{c}S = A \cup B\\[-2em]A\cap B = \emptyset\end{array}} u_A v_B\enspace.$$
<strong>Question:</strong> Does this operation have a name?  Am I seeing that from the wrong point of view?</p>

<p>As a concrete example, if $X = {1, 2}$, $\vec{u} = (x, x_1, x_2, x_{12})$ and $\vec{v} = (y, y_1, y_2, y_{12})$, where the components are, in order, indexed by $\emptyset, \{1\}, \{2\}, \{1, 2\}$, then:
$$\vec{u} \star \vec{v} = (xy,\quad x_1y + xy_1, \quad x_2y + xy_2, \quad x_{12}y + x_1y_2 + x_2y_1 + xy_{12})\enspace.$$</p>
",<linear-algebra>
"<p>Here's the entire question: Let $A$ be an 8 x 5 matrix of rank 3, and let $b$ be a nonzero vector in $N(A^T)$.</p>

<p><strong>a) Show that the system $Ax = b$ must be inconsistent.</strong>
Gonna take a wild stab at this one... If the rank is 3, that means the dimension of the column space is 3. But $A$ has 5 columns, so they are not all linearly independent and therefore $Ax = b$ is inconsistent.</p>

<p><strong>b) How many least squares solutions will the system $Ax = b$ have? Explain.</strong></p>

<p>On previous problems, I found the best least squares linear fit, where the approximation of $x$ was a vector that contained sometimes regular numbers, and sometimes variables. Does this mean that there must be either 1 linear solution or infinite (because you can always find an approximation)? In the example that apparently had an infinite number of least squares solutions, it appeared that one row of $A^TA$ was a constant multiple of another row, leading to a row of zeros in reduced row echelon form. From this problem I know that $A^TA$ is a 5x5 matrix, but I don't think I can prove that any rows are a scalar multiple of other rows, so I'm guessing I have to use some other means of figuring this out.</p>

<p>Sorry if I sound like I have no idea what I'm talking about. Just wanted to try out the problem to my best ability before asking about it.</p>
",<linear-algebra>
"<p>Sorry but I'm not a mathematician, so please bear with me.
I understand what idempotence is from a communications point of view, and am wondering if it is correct to include linear algebraic formulas as being idempotent.  After all, if <code>y= 2x</code>, then for a given input X, you always get the same result.  Is <code>y=2x</code> idempotent?</p>

<p>Cheers,
Nap</p>
",<linear-algebra>
"<p>Find the coordinates of bivector u⊗v with the respect to cannonical basis and basis M = ((1,2),(1,3)), u = (1,1) v=(1,-2). Please help, does it even have the solution? After the tensor multiplication the matrix is singlular?</p>
",<linear-algebra>
"<p>To prove: </p>

<blockquote>
  <p>A symmetric matrix has only real eigenvalues.</p>
</blockquote>

<p>For this I took a symmetric matrix $A$, an eigenvalue $k$ and an eigenvector $X$. </p>

<p>$AX=kX$</p>

<p>Taking $X$ transpose on both sides</p>

<p>$X'AX=X'kX$</p>

<p>Taking transpose</p>

<p>$(X'AX)'=(X'kX)'$</p>

<p>After solving<br>
$X'AX=k'X'X$<br>
$X'kX=k'X'X$<br>
$(k-k')X'X=0$<br>
$k-k'=0$<br>
$k=k'$</p>

<p>$k$ is equal to the transpose of $k$. How does it prove that $A$ symmetric matrix has only real eigenvalues?</p>
",<linear-algebra>
"<p>Cayley Hamilton Theorem states that if $A$ is an $n \times n$ matrix over the field $F$ then $p(A) = 0$.</p>

<p>We note that $p(\lambda) = \det(\lambda I - A)$. Hence, why can't we just substitute $\lambda$ with $A$ and directly prove Cayley-Hamilton Theorem by saying that $p(A) = p(\lambda) = \det(\lambda I - A) = \det(AI - A) = 0$?</p>
",<linear-algebra>
"<p>$x_1 - x_2 - 2x_3 + x_4 = 0 \\
-3x_1 + 3x_2 + x_3 - x_4 = 0 \\
2x_1 - 2x_2 + x_3 = 0$</p>

<p>How do I solve this system of equations? I know this is a homogenous system. 
By applying elementary row operations, I get the following:</p>

<p>$x_1 - x_2 + 1/5x_4 = 0$</p>

<p>$x_3 - \frac{2}{5}x_4 = 0$</p>
",<linear-algebra>
"<p>Let's assume that $V$ and $W$ are vector spaces over a field $\mathbb{K}$, $\lambda\in\mathbb{K}$, $\lambda\neq0$.</p>

<p>$S: V\rightarrow W$ and $T: W\rightarrow V$ are linear maps. Prove, that</p>

<p>$\lambda$ is an eigenvalue of $TS\iff\lambda$ is an eigenvalue of $ST$</p>

<p>What can be stated about the eigenvalues of the maps $TS$ and $ST$?
Would it also be correct if $\lambda=0$?</p>

<p>Proof:</p>

<ol>
<li>$\lambda$ is an eigenvalue of $TS\Rightarrow\lambda$ is an eigenvalue of $ST$ </li>
</ol>

<p>$TSv=\lambda v$ that is $S(TSv)=S(\lambda v)$ that is $ST(Sv)=\lambda (Sv)$</p>

<ol start=""2"">
<li>$\lambda$ is an eigenvalue of $ST\Rightarrow\lambda$ is an eigenvalue of $TS$</li>
</ol>

<p>$STw=\lambda w$ that is $T(STw)=T(\lambda w)$ that is $TS(Tw)=\lambda (Tw)$</p>

<p>I do not understand why the statement is proven by finding two eigenvectors. What would happen if we couldn't construct $Sv$ out of $v$ and $Tw$ out of $w$?
I also don't understand how to answer the latter two questions.</p>
",<linear-algebra>
"<p>I'm having trouble finding the Eigen values for this matrix:</p>

<p>$$ A =\begin{pmatrix} 0&amp;1&amp;-2 \\ 1&amp;3&amp;0 \\ -2&amp;0&amp;5 \end{pmatrix} $$</p>

<p>I did $A - \lambda I $ and ended up with this matrix:</p>

<p>$$ A - \lambda I =\begin{pmatrix} -\lambda&amp;1&amp;-2 \\ 1&amp;3-\lambda&amp;0 \\ -2&amp;0&amp;5-\lambda \end{pmatrix} $$</p>

<p>I then took the determinant and got $ -\lambda^3 + 8 \lambda^2 - 10\lambda - 17 $, but I don't know what I can do from here. The above polynomial is not factorable. How would I find the Eigen values? </p>
",<linear-algebra>
"<p>The second order equation </p>

<p>$\frac{d^2\vec{x}}{dt^2} = A\vec{x}\ + \vec{g}(t)$</p>

<p>models an earthquake's effect on a 7-story building. Let $x_j(t)$ be the displacement of the $j$th floor with respect to its equilibrium position. The ground moves with displacement $g(t)$. </p>

<p>Here</p>

<p>$\vec{x} = 
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_7
\end{pmatrix}$</p>

<p>$\vec{g}(t) = 
\begin{pmatrix}
g(t)\\
0\\
\vdots\\
0
\end{pmatrix}$ . </p>

<p>A <em>second</em> order $7\times7$ system in $x_j(t)$ is given by</p>

<ul>
<li>$x_1'' = 10(x_2- x_1- 1)$ </li>
<li>$x_2'' = 10(x_3- 2x_2+ x_1)$</li>
<li>$x_3'' = 10(x_4- 2x_3+ x_2)$</li>
<li>$x_4'' = 10(x_5- 2x_4+ x_3)$</li>
<li>$x_5'' = 10(x_6- 2x_5+ x_4)$</li>
<li>$x_6'' = 10(x_7- 2x_6+ x_5)$</li>
<li>$x_7'' = 10(x_6- x_7)$.</li>
</ul>

<p>Write the above second order system as a <em>first</em> order $14\times14$ system using the additional equations $v_j = x'_j$.</p>
",<linear-algebra>
"<p>Need some help and hints on how to prove this one:</p>

<p>Let $F=\mathbb{R}$ or $\mathbb{C}$, and $_FV=M_{n,1}(F)$. Let $A \in M_n(F)$ be Hermitian (i.e $A^* = \bar{A}^T=A$) and $f(x,y)=x^*Ay$, for all $x,y \in V$. Show that $f$ is a Hermitian form, and that $f$ is an inner product on $_FV$ if and only if all the eigenvalues of $A$ are positive. </p>

<p>I already proved that $f$ is a Hermitian form. </p>

<p>Can I have some help on the if and only if part? Thanks a lot. </p>
",<linear-algebra>
"<p>It's an exercise of the book Linear Algebra Done Right.
I'm not clear about how to prove these problems, would you please offer me some suggestion about how to improve this kind of ability, thanks a lot.</p>
",<linear-algebra>
"<blockquote>
  <p>I'm asked to find the equation of plane satisfying the given conditions:</p>
  
  <ul>
  <li>Passing through the line given by:
  \begin{cases}
x+y=2 \\
y-z=3
\end{cases}</li>
  <li>Perpendicular to the plane:
  $$
2 x+3 y+4 z=5
$$
  Knowing that the normal to the plane is 
  $2 i+3 j+4 k$</li>
  </ul>
</blockquote>

<p>I would have hade no problems finding this out if I was given the point. However I am not able to figure it out.
My first tought was to find the point where these lines intersect and then use this point to create the plane with these coinditions, 
$$
x+y-2=y-z-3\Rightarrow z=-x-1
$$</p>

<p>Which I could have expected since I am dealing with tree variables. </p>

<p>Now how could I solve this?</p>

<p>Answer should be $x+6 y-5 z=17$</p>
",<linear-algebra>
"<p>I am having struggle with this question.</p>

<p>suppose I have two unitary matrices.</p>

<p>Is their sum is  normal ?</p>

<p>I am try to give an example to show it is not true and I can not find.</p>

<p>I try to proof and I reach this results:</p>

<p>$T$,$S$ are unitary then:</p>

<p>$(T+S)(T+S)^{*}=(T+S)^{*}(T+S)$</p>

<p>$(T+S)(T^{*}+S^{*})=(T^{*}+S^{*})(T+S)$</p>

<p>$(T+S)T^{*}+(T+S)S^{*}=(T^{*}+S^{*})T+(T^{*}+S^{*})S$</p>

<p>$TT^{*}+ST^{*}+TS^{*}+SS^{*}=T^{*}T+S^{*}T+T^{*}S+S^{*}S$</p>

<p>$ST^{*}+TS^{*}=S^{*}T+T^{*}S$</p>

<p>and I can not continue from here.</p>

<p>I can not tell is this equation is true or not.</p>

<p>and BTW </p>

<p>matrix is unitary iff matrix is symmetric ?</p>

<p>Thanks in advanced !!</p>
",<linear-algebra>
"<p>Given a transition matrix of a Markov chain, $P$, I want to solve the left eigenvector of $P$, namely a row vector $\alpha$ such that
$$
\alpha P = \alpha
$$</p>

<p>I know the algorithm to solve a linear equation takes $O(n^3)$, using LU decomposition.</p>

<p>I wonder if there is any faster algorithm?</p>

<p>By the way, I may not need the exact solution, approximate one is OK.</p>
",<linear-algebra>
"<p>I asked this on mathoverflow as well and apologies for cross-posting. I am trying to compute this so-called bending energy matrix. The bending energy of a thin plate in 3D is given by:</p>

<p>$$
BE = \int_0^{X}\int_0^{Y}\int_0^{Z} \sum_{d=1}^{3}\left\{\left(\frac{\partial^2u_d}{\partial x^2}\right)^2+\left(\frac{\partial^2u_d}{\partial y^2}\right)^2+\left(\frac{\partial^2u_d}{\partial z^2}\right)^2 + 2\left[\left(\frac{\partial^2u_d}{\partial x \partial y}\right)^2 + \left(\frac{\partial^2u_d}{\partial x \partial dz}\right)^2 + \left(\frac{\partial^2u_d}{\partial y \partial z}\right)^2\right]\right\}dx dy dz
$$</p>

<p>Now, I am trying to embed this so that the above equation would be equal to </p>

<p>$$
BE = u'\Sigma u
$$</p>

<p>where u would be a vector that defines the components of the field $u_d$ in the bending energy equation. Now, I need to find this matrix $\Sigma$ so that the above equality can be specified. This should be a large and sparse matrix. However, I have been struggling all day to formulate the form of this matrix and what it's entries should be. I was hoping someone here might give me some pointers on how to construct this matrix.</p>

<p>I would really appreciate any help you can give me.</p>
",<linear-algebra>
"<p>I just started studying vectors in linear algebra, and I didn't understand the idea of the geometric description of a vector.</p>

<p><strong>Why do we treat the vector entries as coordinates?</strong> </p>

<p>As far as I understand, the entries of a column vector are the coefficients of the same variable of different equations. If I'm right on the previous sentence then why do we use them as a coordinate $(x,y)$ (for $\Bbb R^2$) and why do we treat them as different entries?</p>

<p>My English is poor, so if you didn't understand me let me try it with an example.</p>

<p>Let's say there are two equations with two variables.</p>

<p>$$\begin{align}
2x + 3y &amp;= 4 \\
x + 5y &amp;= 15 
\end{align}$$</p>

<p>$$\left[\begin{matrix} 
2 &amp; 3 &amp;|&amp; 4 \\
1 &amp; 5 &amp;|&amp; 15 
\end{matrix}\right]$$</p>

<p>If I take $(2, 1)$ as a column vector, $2$ is a run and $1$ is a rise. This is what I didn't understand as far as $2$ and $1$ are the same $x$ value, why do we use one as a run and the other as a rise? </p>

<p>Thank you.</p>

<p>correction<br />
 i just changed the 2nd entire from 1 to 3 just to make it more clear and understandable </p>
",<linear-algebra>
"<p>I'm learning Linear Algebra using MIT's Open Courseware <a href=""http://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2005/"">Course 18.06</a></p>

<p>Quite often, the professor says ""... assuming that the matrix is invertible ..."".</p>

<p>Somewhere in the lecture he says that using a determinant on an $n \times n$ matrix is on the order of $O(n!)$ operations, where an operation is a multiplication and a subtraction.</p>

<p><strong>Is there a more efficient way?</strong>  If the aim is to get the inverse, rather than just determine the invertibility, what is the most effecient way to do this?</p>
",<linear-algebra>
"<p>In my AP chemistry class, I often have to balance chemical equations like the following:</p>

<p>$$ \mathrm{Al} + \text O_2 \to \mathrm{Al}_2 \mathrm O_3 $$</p>

<p>The goal is to make both side of the arrow have the same amount of atoms by adding compounds in the equation to each side.</p>

<p>A solution:</p>

<p>$$ 4 \mathrm{Al} + 3 \mathrm{ O_2} \to 2 \mathrm{Al}_2 \mathrm{ O_3} $$</p>

<p>When the subscripts become really large, or there are a lot of atoms involved, trial and error is impossible unless performed by a computer. What if some chemical equation can not be balanced?  (Do such equations exist?) I tried one for a long time only to realize the problem was wrong.</p>

<p>My teacher said trial and error is the only way. Are there other methods?</p>
",<linear-algebra>
"<p>What is the dimension of the space of $\{A\ {^t\!A}: A\in M(n\times n,\mathbb{R})\}$? I think it should be $n(n+1)/2$ if one knows already the dimension of the special orthogonal group, but I would love to derive the latter from the former.</p>
",<linear-algebra>
"<p>I have an operator</p>

<p>$$h: \mathbb R^4 \to \mathbb R^3\text{ given by } h(x, y, u, v) = (2x + 3y - u + 2v, x - 5y + 6v, 2y + u + v)$$</p>

<p>What is the easiest way to proove that this operator is linear?</p>

<p>I looked over on wiki etc., but I didn't really find the way to prove it mathematically.</p>
",<linear-algebra>
"<p>Homework problem.</p>

<p>Let $a_1, a_2, ..., a_n$ and $b_1,b_2,...,b_n$ be sets of real numbers.  Show that: 
$$ \left(\sum_{k=1}^n a_kb_k\right)^2 \leq \left(\sum_{k=1}^n ka_k^2\right) \left(\sum_{k=1}^n\frac{b_k^2}{k}\right)$$</p>

<p>for all $n \geq 1$.</p>

<hr>

<p>The hint given to us was not to prove this with induction, but to think of the problem ""in linear algebra terms"".</p>

<p>I've pondered this for a few days now, and come up with this: You can think of the $a$'s as a vector $\langle a_1,...,a_n\rangle$, and the $b$'s as a vector $\langle b_1,...,b_n\rangle$ and then the problem can be rephrased as inner products:
$$\langle A,B\rangle\langle A,B\rangle  \space \leq \space \langle A,A\rangle\langle K^{-1}B,B\rangle\;,$$</p>

<p>where $A$ and $B$ are defined above and $KA$ is $\langle 1a_1, 2a_2, ..., na_n\rangle$ and $K^{-1}B$ is 
$\langle 1b_1, \frac{1}{2}b_2,...,\frac{1}{n}b_n\rangle$.</p>

<p>I'm aware of the similarity with the Cauchy-Schwarz inequality, but can't figure out how to manipulate what I have any further.</p>

<p>Any insights are appreciated.</p>
",<linear-algebra>
"<p>This is a question from Serre's Exercise book in Matrix theory. I don't even know how to start. Any help would be appreciated. </p>

<p>Assume that the characteristic of the field $k$ is not equal to 2. Given $M \in GL_n(k)$, show that the matrix
\begin{align}
\begin{pmatrix}0_n &amp; M^{-1} \\ M &amp; 0_n\end{pmatrix}
\end{align}</p>

<p>is diagonalizable. Find its eigenvectors and eigenvalues. More generally, show that every involution ($A^2=I$) is diagonalizable. </p>
",<linear-algebra>
"<p>I've been working through ""Groups and Symmetry"" (Armstrong) and came across this problem in chapter 9 which I can't figure out. Any hints/help would be greatly appreciated!</p>

<p>Show that every $2\times 2$ unitary matrix has the form</p>

<p>$$
\left(\begin{array}{c c}
w &amp; z \\
-e^{i \theta} z^{*} &amp; e^{i \theta} w^{*}
\end{array}\right)
$$</p>

<p>for some $\theta\in\mathbb{R}$ and $w,z\in\mathbb{C}$. (A matrix is said to be <em>unitary</em> if it is invertible with its adjoint as the inverse. The symbol ""*"" denotes complex conjugate.)</p>
",<linear-algebra>
"<p>Hi could you help me with the following:</p>

<blockquote>
  <p>Show that for a symmetric positive definite matrix $B$, $$b_{ij} + b_{jk} + b_{ki} \leqslant b_{ii} + b_{jj} + b_{kk}$$ holds for any $1 \leqslant i,j,k \leqslant n$ with $b_{ij}$ being the entry at $(i,j)$ of matrix $B$.</p>
</blockquote>

<p>Thanks a lot.</p>
",<linear-algebra>
"<p>For any non-zero $\mathbf{y}\in\mathbb{R}^\mathcal{l}$  one half-space through origin is defined by $H_{\mathbf{y}}^{\leq}(\mathbf{0})=\left\{\mathbf{x}\in\mathbb{R}^{\mathcal{l}}:\mathbf{y}\cdot \mathbf{x}\leq\mathbf{0} \right\}$. I want to show $\mathbf{a}\in H^{\leq}_{\mathbf{y}}(0)$ iff $\mathbf{a}=\mathbf{x}-s\mathbf{y}$, where $\mathbf{x}$ is orthogonal to $\mathbf{y}$ and $s\geq 0$.</p>

<p>$\Leftarrow$ side is straightforward, but I couldn't prove $\Rightarrow$ side. I defined $x_i=a_i$ if $ y_i=0$ and $x_i=0$ if $y_i\neq 0$, but couldn't come up with something.Thanks for any help.</p>
",<linear-algebra>
"<p>Suppose $A$ is a $m\times n$ matrix. Show that $\mbox{rank}\,A=m$ if and only if there exists a $n\times m$ matrix $B$ such that $AB=I_m$.<br>
I have proved the case  $AB=I_m$ eventuates $\mbox{rank}\,A=m$, but the main part, the inverse, I couldn't establish.<br>
Would be grateful for your help.</p>
",<linear-algebra>
"<p>If $A$ and $B$ are two positive definite matrices such that $A - B$ is nonnegative definite, is it true that $B^{-1} - A^{-1}$ is positive definite?</p>

<p>The doubt came to me when working with confidence regions in multivariate statistics that are usually obtained as hyper ellipsoids.</p>
",<linear-algebra>
"<blockquote>
  <p>Example: Find all quadratic polynomials that are orthogonal to the function $e^x$ with respect to the $L^2$ inner product on the interval $[0,1]$.</p>
  
  <p>Solution: $p(x)=a((e-1)x-1)+b(x^2-(e-2)x)$ for any $(a,b\in\mathbb{R})$.</p>
</blockquote>

<p>The textbook didn't give any further explanations or steps.  Can someone please help me understand how to get this result??</p>
",<linear-algebra>
"<p>Given $\{U_i\}_{i\in\mathbb N}=\{U_1,U_2,U_3,...\}$ an infinite family of subspaces of $V$ is $\bigcap_{i\in\mathbb N}U_i$ a subspace of V?</p>

<p>I know that it's right for $n$ subspaces with a pretty simple proof, but I don't know how to deal with the infinity.</p>
",<linear-algebra>
"<p>How to find a matrix $A$ when you are given some parameters and the basis for the null space?</p>

<p>The problem I've been scratching my head over is this. The basis  for the null space of $A-4I$ is</p>

<p>$$\left\{
\begin{pmatrix} 1\\0\\0\end{pmatrix}, 
\begin{pmatrix} 1\\1\\1\end{pmatrix}
\right\}
$$</p>

<p>We also know that the matrix $A$ is square.</p>

<p>I'm pretty confused about what to do; this seems to be implying that there are two free variables but I'm not sure how. </p>
",<linear-algebra>
"<p>Let's say I have 2 linear codes, $C_1 = [n,k_1]$ and $C_2 = [n,k_2]$, and I have the parity check matricies $H_1,H_2$ for them. I use the Plotkin construction to create the code $C$ out of them (for every $u\in C_1$, $v\in C_2$, $(u|u+v)\in C$). How can I construct the parity check matrix $H$ of $C$?</p>
",<linear-algebra>
"<p>Assume that we have $6$ vectors in $\mathbb R^4$ such that every two of them is independent. can we generate $\mathbb R^4$ with them?</p>
",<linear-algebra>
"<p>I came across such an exercise:</p>

<blockquote>
  <p>Let $V$ be a linear space over $K$ such that $\dim V = n$. Show that for any $\alpha_1, \alpha_2, \dots, \alpha_m$ with $ m &gt; n + 1$ there exist
  $a_1, \dots, a_m \in K$ such that 
  $\sum_{i=1}^m a_i \triangleright \alpha_i = 0_V$
  and
  $\sum_{i=1}^m a_i = 0_K$</p>
</blockquote>

<p>But then the answer is trivial: it is $a_1 = \dots = a_m = 0$. What could've this exercise been meant to be?</p>
",<linear-algebra>
"<p>How do I prove or disprove if $\{1, \cos x, \cos 2x,..., \cos nx\}$ is linearly independent?</p>

<p>I tried solving the problem using the definition of linear independence,</p>

<p>$\sum_{k=0}^n a_k\cos kx = 0$</p>

<p>$\Rightarrow a_k =0 $</p>

<p>but I am not able to prove/disprove it.</p>
",<linear-algebra>
"<p><a href=""http://i.stack.imgur.com/MDMX0.png"" rel=""nofollow"">Question</a></p>

<p>I have some methodological questions with this exercise:</p>

<blockquote>
  <p><strong>1.</strong> You are given that the transition matric $P_{\mathcal C,\mathcal B}$ from a basis $\mathcal B=\{b_1,\ b_2,\ b_3\}$ to a basis $\mathcal C=\{c_1,\ c_2,\ c_3\}$ is
  $$\frac12\begin{bmatrix}0&amp;-1&amp;1\\-1&amp;1&amp;1\\1&amp;0&amp;0\end{bmatrix}$$</p>
  
  <p>$(a)$ Compute the vector $u=b_1+b_2+2b_3$ as a linear combination of the vectors in $\mathcal C$ and from this write down $[u]_\mathcal C$ (i.e. the coordinates of the vector $u$ w.r.t. $\mathcal C$)</p>
  
  <p>$(b)$ Calculate $P_{\mathcal B,\mathcal C}$</p>
  
  <p>$(c)$ Suppose
  $$\begin{matrix}c_1=(1,2,3),&amp;c_2=(1,2,0),&amp;c_3=(1,0,0)\end{matrix}$$
  Compute $P_{\mathcal S,\mathcal B}$, where $\mathcal S$ is the standard basis, and from this read off the explicit for of the vectors $b_1,\, b_2,\,b_3$.</p>
</blockquote>

<p>$1.a)$
The question asks to find $u$ in terms of $\mathcal C$. To do this do I simply use the transition matrix on $u$ in the form of $(1, 1, 2)$ to get $\frac12(1, 2, 1)$?<br>
If so, what would $[u]_\mathcal C$ in this case? Have we just found it?</p>

<p>$1.c)$ Do I just use the formula here? (the transition matrix one, I can't quite recall it on the spot).</p>

<p>Thanks</p>
",<linear-algebra>
"<p>The solution to a linear algebra problem I'm working on reads: </p>

<blockquote>
  <p>$$\det(A-\lambda I) = \det\begin{pmatrix}-\lambda &amp; 1 &amp; 0 \\ 0 &amp; -\lambda &amp; 1\\ 1 &amp; -1 &amp; 1-\lambda\end{pmatrix} = -\lambda(-\lambda(1-\lambda)+1)+1$$
  This may be written as $\lambda^2(1-\lambda)+(1-\lambda) = (\lambda^2+1)(1-\lambda)$.</p>
</blockquote>

<p>I understand how the determinant is calculated, but am struggling to understand the algebraic manipulation at the end. By my math, $-\lambda(-\lambda(1-\lambda)+1))+1$ simplifies to 
 $-\lambda(-\lambda + \lambda^2 +1)+1$, which simplifies to $(\lambda^2 - \lambda^3 - \lambda)+1$ </p>

<p>What am I misunderstanding here? </p>
",<linear-algebra>
"<p>Suppose there is a square binary matrix (Adjacency matrix of a graph), $A$.</p>

<p>I got that, the matrices, $A^2$ and $A^3$ are distinct but the set of eigenvalues are same for $A^2$ and $A^3$. It is to be noted that the set of eigenvalues of $A$ is different from the same of $A^2$ and $A^3$. Other powers of $A$ are same as $A^3$. </p>

<p>What does the above result interpret?</p>

<p>Please let me know. </p>

<p>Thanks in advance!  </p>
",<linear-algebra>
"<p>Is strict/weak negative/positive definiteness/semidefiniteness of matrices preserved under matrix addition?</p>

<p>I tried to do this for 2x2 matrix but even this wasn't easy. (I tried to use the principal minors definition of definiteness)</p>
",<linear-algebra>
"<p>This is the problem: Let $A$ be a real symmetric $n \times n$ matrix with non negative entries. Prove that $A$ has an eigenvector with non-negative entries</p>

<p>I looked at the answer key and don't quite understand it. In the expression containing max, why should it correspond to the eigenvalue $\lambda_0$? I thought that this may be because if Ax is parallel to x, then the dot product between $Ax$ and $x$ is maximised, but is it not possible that it still attains a large value if $A$ transforms $x$ in a way that scales x by so much that Ax is large enough to make $\langle Ax,x\rangle$ large even though they may not be parallel?</p>

<p>Solution(as in answer key):</p>

<p>Let $\lambda_0$ be the largest eigenvalue of $A$. We have</p>

<p>$$\lambda_0 = \max{\{\langle Ax, x\rangle\mid x\in\mathbb{R}^n,\|x\| = 1\}}$$</p>

<p>and the maximum it attains precisely when $x$ is an eigenvector of $A$ with 
eigenvalue $\lambda_0$. Suppose $v$ is a unit vector for which the maximum is attained, and let $u$ be the vector whose coordinates are the absolute values of the coordinates of $v$. Since the entries of $A$ are nonnegative, we have</p>

<p>$$\langle Au,u \rangle \ge \langle Ax,x\rangle =\lambda_0$$ implying that $\langle Au,u\rangle = \lambda_0$, so that $u$ is an eigenvector of $A$ for the eigenvalue $\lambda_0$.</p>
",<linear-algebra>
"<p>I have tried it in the following manner. We know that $$rank(XY)\ge rank(X)+rank(Y)-n$$ where $X$ and $Y$ are two matrices of order $n$ and also $$rank(XY) \le \min{\{rank(X),rank(Y)\}}$$ If we take $X=Y=A^3$ then using the above two inequalities we obtain $1\le rank(A^6)\le 2$ i.e. rank of $A^6=1 \text{ or } 2$. Is it correct at all? I have failed to obtain a definite rank of $A^6$ by the above process. Is it ok?</p>
",<linear-algebra>
"<p>Let $W$ be a linear subspace  of $\mathbb{R}^n$ of dimension at most $n-1$. Determine which of the following hold:<br>
(1) $W$ is nowhere dense.<br>
(2) $W$ is closed.<br>
(3) ${\mathbb{R}^n}\setminus W$ is connected.<br>
(4) $\mathbb{R}^n\setminus W$ is not connected.    </p>

<p>""$W$ is closed"" is  equivalent to saying that $W'=\mathbb{R}^n\setminus W$ is open. Now by hypothesis $W'$ is of dimension at least 1. In standard basis, this would mean that $W'$ would consist of the <em>n</em>-th column vectors whose at least one component is non-zero. Take any such $t$ in $W'$, take $\epsilon=||t||$, then i guess it is very clear that $B(t;\epsilon)$ is a subset of $W''$. So this proves that $t$ is an interior point of $W$. 
I can not do the parts. Any hint will be well appreciated. </p>
",<linear-algebra>
"<p>What can we conclude about a square matrix $A$ if we know the following?</p>

<ol>
<li>The characteristic polynomial of the matrix is $f(t)=(t-3)^4(t-2)^3$ and</li>
<li>$(A-2I)(A-3I)^2=0$</li>
</ol>

<p>Extracting information from 1) is easy but I don't know what to conclude from 2). Maybe something about the minimal polynomial? </p>
",<linear-algebra>
"<p>Find all values of $t$ such that 
$$
        \begin{pmatrix}
        t &amp; 7 \\
        3 &amp; t \\
        \end{pmatrix}
$$
is not a basis for $\mathbb{R}^2$</p>

<p>$$
        \begin{pmatrix}
        t &amp; 0 \\
        1 &amp; t \\
        \end{pmatrix}
$$
is a basis for $\mathbb{R}^2$</p>

<p>For first question I know i have to show that its either linear dependent or it's span is not $\mathbb{R}^2$ but I'm not sure how I would go about showing that. I'm new to linear algebra please help me as much as possible sorry.</p>
",<linear-algebra>
"<p>From the structure of this all i getting is that</p>

<blockquote>
  <p>If $V$ an n-dimensional vector space with an ordered basis
  $\beta=(x_1,x_2,x_3,\dots,x_n)$ among them (say) first $k$-vectors
  form the basis for $W$.Let $\beta*=\{f_1,f_2,f_3,\ldots,f_n\}$ be the
  dual basis for $V$, then the functional elements of$\beta* $
  corresponding to last $n-k$ vectors of $\beta$ forms the basis for
  $W^0$</p>
</blockquote>

<p>Showing $\{f_{k+1},f_{k+2},f_{k+3},\ldots,f_n\}$ is a basis for $W^0$ .</p>

<p>We know that,inorder to show $T=\{f_{k+1},f_{k+2},f_{k+3},\ldots,f_n\}$ to be a basis for $W^0$ it is sufficient to show that $\operatorname{span} T=W^0$.Since $T$ is a subset of $\beta*  $ so $T $ is linearly independent. Since $W^0$ is a subset of $V*$,every element of $f$ in $W^0$ we could write as a linear combinations of elements of $\beta*$.</p>

<p>From,here I lost the track.</p>
",<linear-algebra>
"<p>I am doing a self study in linear algebra and I am trying to solve the problem bellow. </p>

<blockquote>
  <p>Suppose $P$ is the projection matrix onto the subspace $\mathbf{S}$ and $Q$ is the projection onto the orhogonal complements $\mathbf{S}^{\perp}$. What are $P+Q$ and $PQ$? Show that $P-Q$ is its own inverse. </p>
</blockquote>

<p>Given $P$ and $Q$ and a vector $b$ we take the projection of $b$ onto $\mathbf{S}$ and $\mathbf{S}^{\perp}$ by $p = Pb$ and $q = Qb$ respectively. </p>

<ul>
<li><p>Geometrically, we may say that $b$ equals $p+q$, but is it possible to prove this algebraically?</p></li>
<li><p>Given $b=p+q$, we have $b = p + q = Pb+ Pq = (P+Q)b$. It seems that $P+Q=I$ but how we prove this algebraically? [taking $(I-P-Q)b=0 \implies P+Q=I$ give me doubts.]</p></li>
<li><p>Finally, applying $PQ$ in $b$ geometrically (think in 3D), $b$ projects in zero vector. How can we prove this algebraically? [taking $PQb=0\implies PQ = 0$ again does not fill very right to me.]</p></li>
</ul>

<p>Any help would by priceless.</p>

<p>Thanks.</p>
",<linear-algebra>
"<p>What is exact relationship between matrix R and input matrix A in QR factorization? Say, R gives the structure of A or R is a representation of A. How? We have Q'A = R. Does it mean A is projected to the subspcae of Q? If so, is R a representation of A in another space?</p>
",<linear-algebra>
"<p>If $A$ is a $ \displaystyle  10 \times 10 $ matrix such that $A^{3} = 0$ but $A^{2}  \neq 0$ (so A is nilpotent) then I know that $A$ is not invertible, but why does at least one eigenvalue of $A$ have to be equal to zero? How would one show that all eigenvalues of $A$ are equal to zero?</p>
",<linear-algebra>
"<p>$A$ is a $ \displaystyle  10 \times 10 $ matrix such that $A^{3} = 0$ but $A^{2}  \neq 0$ and therefore, by definition, $A$ is nilpotent. Is there a non-zero vector that lies in both the column space and null space of $A$? This would mean that the $\text{col}(A) \cap \text{nul}(A) \neq {0}$, right?</p>
",<linear-algebra>
"<p>Find the eigenvectors of
$$
A = \begin{bmatrix} 0 &amp; 2 \\ 1 &amp; 1 \end{bmatrix}.
$$
I know you can solve $ \det(A - \lambda I) = 0 $ to find the eigenvalues of $ A $, but I keep getting no free variables. However, I thought this was impossible, but I know this problem works.</p>
",<linear-algebra>
"<p>I'm going through Spivak's Calculus on Manifolds, and I'm currently working on Problem 2-13 part (b). The problem statement is</p>

<blockquote>
  <p>If $f,g: \mathbb{R} \rightarrow \mathbb{R}^{n}$ are differentiable and $h: \mathbb{R} \rightarrow \mathbb{R}$ is defined by $h(t) = \langle f(t),g(t)\rangle$, show that
  $h'(a) = \langle f'(a)^T,g(a) \rangle + \langle f(a),g'(a)^T \rangle.$ </p>
</blockquote>

<p>What's confusing me is that $f'(a)$ is a vector in $\mathbb{R}^{n}$ and taking its transpose yields a vector in the dual space to $\mathbb{R}^{n}$, so taking the inner product with $g(a)$, which is a vector in $\mathbb{R}^{n}$ yields a scalar in $\mathbb{R}$, corresponding to the left term in the equation. Whereas the right term in the equation is an outer product, so it sends a vector and a dual vector to a linear map in Hom($\mathbb{R}^{n}$), since the vector $f(a)$ is $n\times 1$ and the dual vector $g'(a)^T$ is $1\times n$ so their product is $n\times n$. How can these two terms be added to yield a scalar in $\mathbb{R}$?</p>
",<linear-algebra>
"<p>If we're given a $ \displaystyle  2 \times 2 $  Markov Matrix (so all entries are non-negative and columns add to 1) <strong>M</strong>$(a,b)$ such that $$M = M(a,b) := \begin{bmatrix}1 - a &amp; b\\a &amp; 1 - b \end{bmatrix}$$ where $a$ and $b$ are $ 0 ≤ a ≤ 1, 0 ≤ b ≤ 1$, how could you define $N := 1 - M$? I'm confused how to show that if $⟨µ,u⟩$ is an eigenpair for <strong>M</strong>, why is $⟨1 − λ,u⟩$ in an eigenpair for <strong>N</strong>?</p>
",<linear-algebra>
"<p>I know that generally, for any square matrix $ B $, $ \text{rank}(B^{2}) $ is less than or equal to $ \text{rank}(B) $, but I’m having trouble with this proof.</p>
",<linear-algebra>
"<p>I want to find possible solution satisfying both the equation:</p>

<p>$\sum_{i=1}^{n} f_i^{2} = n$</p>

<p>$\sum_{i=1}^{n} f_i=0$</p>

<p>As the number of equations less than number of variables can we just comment on some properties that solutions will have like the following below:</p>

<p>For example a possible solution to the above equation set can be(I got it by hit and trail and intuition) </p>

<p>$f_i$ =
   \begin{cases} 
      \sqrt{\frac{|\overline{A}|}{|A|}} &amp; v_i \in A \\
      \sqrt{\frac{|A|}{|\overline{A}|}} &amp; v_i \in \overline{A} \\
   \end{cases}</p>

<p>where $A$ is any set and $|A|=n$</p>
",<linear-algebra>
"<p>I am stuck on the following problem :  </p>

<blockquote>
  <p>Let $v_1 = (1, 0); v_2 = (1,-1) \space\text{and} \space v_3 = (0, 1).$ How many linear transformations
  $T \colon \Bbb R^2 \to \Bbb R^2$ are there such that $Tv_1 = v_2; Tv_2 = v_3$ and $Tv_3 = v_1?$ The options are as follows:   </p>
  
  <p>(A) $3!$<br>
  (B) $3$<br>
  (C) $1$<br>
  (D) $0$  </p>
</blockquote>

<p>What I observed that $v_2=v_1-v_3$ and so $T(v_2)=T(v_1)-T(v_3) \implies v_3=v_2-v_1$.<br>
But I do not know how to progress from here. Any idea?</p>
",<linear-algebra>
"<p>Consider the strictly convex quadratic function $f(x) = \frac{1}{2}x^tPx - q^tx + r,$ where $P \in \mathbb{R}^{n \times n}$ is a positive definite matrix, $q \in \mathbb{R}^n$ and $r \in \mathbb{R}.$ Let $\mathcal{H} := \{H: H \text{ is a }k- \text{dimensional subspace in } \mathbb{R}^n\}.$ Clearly, the restriction of $f$ to any $H \in \mathcal{H}$ is again a strictly convex function. For any $H \in \mathcal{H},$ we will use $x_H$ to denote the <em>unique</em> optimal point of the following problem</p>

<p>\begin{equation*}
\underset{x \in H}{\text{min.}} \;  f(x).
\end{equation*}</p>

<p>Now consider the map, $\psi(H) = x_H.$ </p>

<p>Prove / Disprove: The map $\psi$ is bijective.</p>

<p>Remark: It is assumed that $P$ is invertible and $q \neq \mathbf{0}.$</p>
",<linear-algebra>
"<p>As a follow-up of <a href=""http://math.stackexchange.com/questions/1011644/which-rings-containing-a-field-are-as-a-vector-space-over-the-field-is-i"">this question</a>, I would like to ask, what are the $2$-dimensional algebras over $\mathbb R$, $\mathbb Q$, or any arbitrary field? Can we classify them?</p>
",<linear-algebra>
"<p>Problem: If $A$ and $B$ are positive semidefinite matrices such that $A^2 = B^2$, show $A = B$, where $A, B$ are $n$-by-$n$ matrices.</p>

<p>This problem is taken out of Linear Algebra (4th edition) by Friedberg, Insel, and Spence. </p>

<p>EDIT: Problem is in Section $6.4$ number $17(d)$</p>

<p>Before posting my question, I looked at this specific question on the website: <a href=""https://math.stackexchange.com/questions/889963/a-b-in-lx-is-positive-semidefinition-hermitian-operators-and-a2-b2-then"">$A,B\in L(X)$ is positive semidefinition hermitian operators and $A^2=B^2$, then $A=B.$</a></p>

<p>This seems like the answer draws from materials outside this textbook. I am not familiar with the square root of a matrix as denoted in that thread. This is not a homework question, but I suspect that there should be a shorter and simpler proof (whether there is one or not) that does not draw materials outside of this textbook.</p>

<p>However, I am stumped as to show how given the hypothesis above (been at it for an hour), how I can deduce that $A = B$. I also attempted to show the contraposition but I am uncertain as how to proceed other than using the fact that there exists an orthonormal basis $\beta$ for $\mathbb{R}^n$ consisting of eigenvectors of $A$ since it is symmetric. </p>

<p>I would appreciate it if anyone can point me in the right direction. </p>
",<linear-algebra>
"<blockquote>
  <p>Let a linear transformation $T:\mathbb{R}^3\to \mathbb{R}^3$ defined as $T(v_1, v_2, v_3) = (v_1, v_3 - 2v_2, -v_3)$. Calculate $f(T)$ where $f(X) = -X^2 + 2 \in \mathbb{R}[X]$</p>
</blockquote>

<p>I'm not so sure how to evaluate $f(T)$. I'll be glad for an explanation.</p>

<p>Maybe $T(v)$ can be viewed as the polynomial $v_1 + (v_3 - 2v_2)x -v_3x^2$?</p>
",<linear-algebra>
"<p>Let $A, B \in \mathbb{R}^{m,n}$, with SVDs $A = U_A \Sigma_A V_A^T$ and $B = U_B \Sigma_B V_B^T$. I want to show that
$$
  || \Sigma_A - \Sigma_B ||_F \leq || A - U B V^T ||_F
$$ 
where $U, V$ are arbitrary unitary matrices of appropriate dimension (this exercise comes from <a href=""http://math.ecnu.edu.cn/~jypan/Teaching/books/SVD.pdf"" rel=""nofollow"">http://math.ecnu.edu.cn/~jypan/Teaching/books/SVD.pdf</a>, 17.5, problem 5). </p>

<p>I know that by unitary invariance of the Frobenius norm we have $ || B ||_F = || U B V ||_F$ for any appropriately sized $U, V$. Thus, I can show that
$$
  || \Sigma_A - \Sigma_B ||_F = || A - U_A U_B^T B V_B V_A^T ||_F
$$</p>

<p>How do I relate this to $U$,$V$? </p>
",<linear-algebra>
"<blockquote>
  <p>Let $A$ be some matrix over $\mathbb{Q}$ (then it's also over
  $\mathbb{R}$). Suppose $A$ is invertible over $\mathbb{R}$ (that is,
  $A^{-1}$ is over $\mathbb{R}$). Prove that $A^{-1}$ is also over
  $\mathbb{Q}$.</p>
</blockquote>

<p>I know that I have to prove that $A^{-1}$ contains no irrational numbers but I fail to do so. I would appreciate any suggestions.</p>
",<linear-algebra>
"<p>My task is to figure out determinant of following matrix depending on $n$.  I want to solve it without altering the rows!
$$
A^{n,n} = \begin{vmatrix} 
0  &amp;    &amp; ... &amp; 0  &amp; -1\\
   &amp;    &amp;     &amp; -1 &amp; 0 \\
   &amp;    &amp; ... &amp;    &amp;   \\
0  &amp; -1 &amp;     &amp;    &amp;   \\
-1 &amp; 0  &amp; ... &amp;    &amp;0  \\
 \end{vmatrix}
$$</p>

<p>This will be $\pm1$ depending on following two things.</p>

<ol>
<li>Multiplication of $-1$. This is simply $-1^n$</li>
<li>The number of left-down positions - 1. For $1\times1$ det is simply $(+1)*(-1)$. For $2\times2$ it's $(-1)*(1)$ and so on.</li>
</ol>

<p>So the rule should look like so:</p>

<p>$$det(A) = -1^n \times -1^{ld_positions - 1}$$</p>

<p>Now the only complicated thing was to figure out the nuber of left-down positions to know the second part. I believe it's $$\sum_{0}^{n-1}n$$ where $n$ is the matrix dimension.</p>

<p>But my solution did not pass my test calculations, so it must be wrong.</p>
",<linear-algebra>
"<p>Let $(\theta,A\theta)=\theta_i A_{ij}\theta_j$ where $A$ is some $(2\times2)$ antisymmetric matrix. </p>

<p>I want to generalize the following </p>

<p>$$I(A) =\int d\theta_1d\theta_2~ \exp\Bigg[\frac{1}{2}(\theta,A\theta)\Bigg]=\int d\theta_1d\theta_2~ (1+\theta_1\theta_2A_{12}) = A_{12}=\sqrt{\det A}$$</p>

<p>to the $n$-tuple case. </p>

<p>Let now $$A:=\begin{bmatrix}
0    &amp; 1       &amp; \;     &amp; \;    \\
\;-1    &amp; 0     &amp;  &amp; \;    \\     
\;     &amp; \;      &amp; 0 &amp; 1     \\
\;     &amp; \;      &amp; -1 &amp; 0     \\
\;     &amp; \;      &amp; \;     &amp; \, &amp;\ddots   \\
\end{bmatrix}.$$</p>

<p>I evaluate, I get the following 
$$I(A) = \int d\theta_n\dots d\theta_1\,\exp\Bigg[\frac{1}{2}(\theta,A\theta)\Bigg]\\ = 
\int d\theta_n\dots d\theta_1\, (\theta_1\theta_2+\theta_3\theta_4+\cdots)\\=0$$</p>

<p>The answer should be $$I(A) = 1.$$ </p>

<p>In the above I use (perhaps incorrectly?) </p>

<p>$$\int d\theta_n\dots d\theta_1\, \theta_n\dots \theta_1\, = 1 $$
and
$$\int d\theta_n\dots d\theta_1= 0. $$</p>

<p>Where do I err? </p>

<p>EDIT: I think I know how to fix this: it is the last term in the expansion of the exponential the contributes. All other terms give zero (just like the one above). I will add the solution later. </p>
",<linear-algebra>
"<blockquote>
  <p>Let $A \in M_2(\mathbb R)$ be a matrix which is not a diagonal matrix . Which of the following statements are true??</p>
  
  <p>a. If $tr(A)=-1$ and $detA=1$, then $A^3=I$.</p>
  
  <p>b. If $A^3=I$, then $tr(A)=-1$ and $det(A)=1$.</p>
  
  <p>c. If $A^3=I$, then $A$ is diagonalizable over $\mathbb R$.</p>
</blockquote>

<p>For (a), it is clear that $A$ will satisfy $\lambda^2+\lambda+1=0$ giving $A^2+A+I=0$. Multiplying $A$ through out gives $A^3+A^2+A=0\implies A^3=-A^2-A=I$</p>

<p>For (b), the only possibilities of eigen values are $1, \omega, \omega^2$. Now if the eigen values are only $1$ and $1$ then $A$ will satisfy $(\lambda-1)^2=0$. We already know that $A^3=I$. From these two facts it is not difficult to see that $A=kI$. Hence the only possible eigen values can be $\omega, \omega^2$. Hence (b) is true.</p>

<p>For(c), $A$ is definitely diagonalizable over $\mathbb C$. Is there any condition which would force a matrix to be diagonalizable over $\mathbb R$ when it is already diagonalizable over $\mathbb C$?</p>
",<linear-algebra>
"<p>In my linear algebra class, we just talked about determinants. So far I’ve been understanding the material okay, but now I’m very confused. I get that when the determinant is zero, the matrix doesn’t have an inverse. I can find the determinant of a $2\times 2$ matrix by the formula. Our teacher showed us how to compute the determinant of an $N \times N$ matrix by breaking it up into the determinants of smaller matrices, and apparently there is a way by summing over a bunch of permutations. But the notation is really hard for me and I don’t really know what’s going on with them anymore. Can someone help me figure out what a determinant is, intuitively, and how all those definitions of it are related?</p>
",<linear-algebra>
"<p>In least-squares approximations the normal equations act to project a vector existing in N-dimensional space onto a lower dimensional space, where our problem actually lies, thus providing the ""best"" solution we can hope for (the orthogonal projection of the N-vector onto our solution space).  The ""best"" solution is the one that minimizes the <strong>Euclidean distance (two-norm)</strong> between the N-dimensional vector and our lower dimensional space. </p>

<p>There exist other norms and other spaces besides $\mathbb{R}^d$, what are the analogues of least-squares under a different norm, or in a different space?</p>
",<linear-algebra>
"<p>The notion (rank-2) ""tensor"" appears in many different parts of physics, e.g. stress tensor, moment of inertia tensor, etc.</p>

<p>I know mathematically a tensor can be represented by a 3x3 matrix. But I can't grasp its geometrical picture — unlike scalar (a number) and vector (an arrow with direction and magnitude) which I can easily see what's going on.</p>

<p>How to visualize a tensor?</p>
",<linear-algebra>
"<p>I asked this question on Stack Overflow but it was closed as ""not programming related"". So I think this is probably the best place for it...</p>

<hr>

<p>I read over the wikipedia <a href=""http://en.wikipedia.org/wiki/Linear_programming"">article</a>, but it seems to be beyond my comprehension. It says it's for optimization, but how is it different than any other method for optimizing things?</p>

<p>An answer that introduces me to linear programming so I can begin diving into some less beginner-accessible material would be most helpful.</p>
",<linear-algebra>
"<p>The absolute value of a $2 \times 2$ matrix determinant is the area of a corresponding parallelogram with the $2$ row vectors as sides.</p>

<p>The absolute value of a $3 \times 3$ matrix determinant is the volume of a corresponding parallelepiped with the $3$ row vectors as sides.</p>

<p>Can it be generalized to $n-D$? The absolute value of an $n \times n$ matrix determinant is the volume of a corresponding $n-$parallelotope?</p>
",<linear-algebra>
"<p>I know there is a double covering map between SU(2) and SO(3) but I have no idea how I would go about proving this or showing this. </p>

<p>can someone point me in the right direction please?</p>
",<linear-algebra>
"<p>Let's suppose we have  a function $Y=A\cdot t^B$ and the values for $Y$ are $30,60,90,120,150$ and the values for $t$ are respectively $0.974, 1.331, 1.718, 1.971, 2.356$. Can you find $A$ and $B$ with the method of linear regression? I have to do a lab work and this is a very small part of it, which does not count but I still have to do it and I have never done linear regression, I need this now? please?</p>
",<linear-algebra>
"<p>Let's look at the well-known definition of orthogonal vectors:</p>

<blockquote>
  <p>Let $V$ be a vector space. Two vectors $x, y \in V$ are <em>orthogonal</em> to each other when the following condition is fulfilled: $$\langle x,y \rangle = 0$$</p>
</blockquote>

<p>Let me explain, where I see a contradiction in this definition.</p>

<p>As I understand the inner product $\langle\cdot \rangle$ of two vectors is not unambiguous (like a norm of a vector, for instance). It can be defined in many ways for different vector spaces, it just needs to satisfy <a href=""http://mathworld.wolfram.com/InnerProduct.html"" rel=""nofollow"">some properties</a>.</p>

<p>As inner product can be defined differently, it can presumably take different values for the same two vectors, depending on which 'version' of the inner product is applied to these vectors. Just analogically with the definition of the norm. Let me illustrate this on an example:</p>

<blockquote>
  <p>Consider a vector $x=(1,-3,2)^T$. Its <a href=""http://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm"" rel=""nofollow"">Euclidean norm</a> is $\lVert x\rVert_2=\sqrt {14} = 3.74\dots$, its <a href=""http://en.wikipedia.org/wiki/Norm_(mathematics)#Maximum_norm_.28special_case_of:_infinity_norm.2C_uniform_norm.2C_or_supremum_norm.29"" rel=""nofollow"">Maximum norm</a> is $\lVert x\rVert_\infty = \max(\lvert 1 \rvert, \lvert -3 \rvert, \lvert 2 \rvert) =3.$ Clearly these two norms are not equal.</p>
</blockquote>

<p>So I guess the same generalization is applied to the definition of the inner product: for some vector space it can be defined in many different ways and thus can take different values upon the applied definition. So theoretically in some special case it might happen so, that the value of an inner product of two vectors, that was defined in one way can be equal $0$ which will imply that the vectors are orthogonal, while the value of an inner product defined in another way applied to the same two vectors will not be equal $0$, which will imply that the vectors are not orthogonal. There's an obvious contradiction here. </p>

<p>There's no such contradiction in the definition of norm as there's a theorem that specifies equivalence of two norms:</p>

<blockquote>
  <p>For some finite-dimensional vector space $V$ let $\lVert x\rVert$ and $\lVert x \rVert '$, $x\in V$ be two different norms. Then $$\exists c \ge 0 \; \forall x \in V: \quad \frac 1c \lVert x\rVert ' \leq \lVert x\rVert \leq c\lVert x\rVert '$$</p>
</blockquote>

<p>Clearly when some norm evaluates for some definite vector to $0$ (thus the vector has 'zero length') any other norm will evaluate to $0$ according to the equivalence theorem. So it cannot happen so, that a zero vector can have a 'non-zero length'. </p>

<p>Therefore the one way to solve this contradiction is to prove that for some vector space $V$ an inner product is defined unambiguously. Another way would be to set an analogical theorem for inner products stating their equivalence, from which will follow, that if some inner product is evaluated to zero, all other will do so. Let's again consider Euclidean space, for which the inner product (also called dot product) is defined as $$\langle x,y \rangle = a_xa_y + b_xb_y,$$ where $a_i$ and $b_i$ are the corresponding coordinates of the vectors $x$ and $y$. This is really an inner product as it fulfills the properties. However has it been proved that in the Euclidean space there can't exist any other prescription, which, when applied to two vectors will fulfill the properties of the inner product and thus be another type of the inner product for this space? Or does some theorem exist that states equivalence of two inner products?</p>
",<linear-algebra>
"<p>I'm using Linear Algebra by Jim Hefferon (freely available, links below with solution).</p>

<p>I'm having trouble understanding Exercise 1.18 on page 117.</p>

<p>1.18 Decide if each is a basis for $P_2$.
(a) $(x^2 + x - 1, 2x + 1, 2x - 1)$</p>

<p>First, I try to prove that it spans $P_2$ $(ax^2 + bx + c)$. However, I do not understand how to set up the matrix. I usually do not have any trouble when there are column vectors given to me and I simply have to row-reduce using Gauss' Method, however whenever given equations with variables I have trouble. </p>

<p>Can someone walk through this step by step? That would be really helpful. I'm trying to teach myself Linear Algebra so there may be many missing gaps of knowledge.</p>

<p>Book: <a href=""http://joshua.smcvt.edu/linearalgebra/book.pdf"" rel=""nofollow"">http://joshua.smcvt.edu/linearalgebra/book.pdf</a></p>

<p>Answer Key: <a href=""http://joshua.smcvt.edu/linearalgebra/jhanswer.pdf"" rel=""nofollow"">http://joshua.smcvt.edu/linearalgebra/jhanswer.pdf</a></p>
",<linear-algebra>
"<p>I've just seen a proof of the statement: ""Given $\alpha$ in a commutative ring $K$ there is a unique alternating multilinear function $f$ with $f(Id)=\alpha$.""</p>

<p>The determinant is defined as the unique $f$ such that $f(Id)=1$. I don't understand why for each alternating multilinear function $f$ we have $$f(A)=\det(A)f(Id)$$</p>

<p>I would appreciate if anyone could explain me why this is true. Thanks in advance.</p>
",<linear-algebra>
"<p>How to find the linear transformation $T: \mathbb R^3 \to \mathbb R^3$ such that the set of all vectors satistfying $4x_1-3x_2+x_3=0$ is</p>

<p>a) Null space of $T$</p>

<p>b) Range of $T$</p>

<p>I'm not able to approach this problem.
Any help would be appreciated.</p>
",<linear-algebra>
"<blockquote>
  <p>Let $A$ be a square matrix of order $3$. Prove that 
  $$
\operatorname{adj}(A) 
= \tfrac{1}{2} \bigl[ 
(\operatorname{tr} A)^2 - \operatorname{tr}(A^2) \bigr] I_3 
- [\operatorname{tr} A] A + A^2
$$ 
  where $\operatorname{tr}A$ is the trace of $A$.</p>
</blockquote>

<p>To begin, I'm not quite sure how to start proving this. I've considered using brute force, but I suspect there should be a much more elegant way of doing it. Is there a need to prove this for both $(i,i)$ and $(i,j)$ entries? I'd appreciate an explanation that is not too complex. Thanks in advance!</p>
",<linear-algebra>
"<p>Given that $A$ is a complex square matrix of order $n$, $\lambda$ is an eigenvalue of $A$ with geometric and algebraic multiplicity $1$, and $x,y$ are entrywise nonzero vectors such that $Ax=\lambda x$ and $y^*A=\lambda y^*$. Show that every proper principal submatrix of $\lambda I-A$ has nonzero determinant.</p>

<p>I know that $\operatorname{adj}(\lambda I -A) = \gamma xy^*$ where $\gamma$ is nonzero, hence has only nonzero entries. So I know every principal submatrix of  $\lambda I-A$ of size $n-1$ has nonzero determinant. I don't know how to prove this is true for smaller principal submatrices.</p>

<p>Any help is appreciated.</p>
",<linear-algebra>
"<p>Assume that the following is used:</p>

<p>$$ 
A = \begin{pmatrix}
 0&amp;  1&amp;\\
 2&amp;  3&amp;\\ 
 4&amp;  5&amp;\\
 6&amp;  7&amp;\\
 8&amp;  9&amp; 
\end{pmatrix}
$$</p>

<p>Then calculating the Coveriance matrix, which, gives me:</p>

<p>$$ 
C = \begin{pmatrix}
 40&amp;  40&amp;\\
 40&amp; 40&amp;\\ 
\end{pmatrix}
$$</p>

<p>Then using the following:</p>

<p>$$
det = (a+b) \cdot (a+b)-4 \cdot(a \cdot b - c \cdot c),
$$</p>

<p>where in this case, $a = 40, b = 40, c = 40$ gives the answer:</p>

<p>$$
\lambda_{1} = 80, \\
\lambda_{2} = 0,
$$</p>

<p>These are therefore the correct Eigen values. However, using this formula, if I have the following:</p>

<p>$$ 
A = \begin{pmatrix}
 -4&amp;  -2&amp;\\
 -1&amp;  -3&amp;\\ 
 4&amp;  5&amp;\\
 6&amp;  7&amp;\\
 8&amp;  9&amp; 
\end{pmatrix},
$$</p>

<p>where the Covariance matrix is given: </p>

<p>$$
C = \begin{pmatrix}
 99.2&amp;  103.4&amp;\\
 103.4&amp; 116.8&amp;\\ 
\end{pmatrix},
$$</p>

<p>gives the Eigenvalues as: </p>

<p>$$
\lambda_{1} = 218.119 \\
\lambda_{2} = -15.5189
$$</p>

<p>When the actual values are:
$$
\lambda_{1} =211.774 \\
\lambda_{2} = 4.226
$$</p>

<p>Could anyone tell me where I am calculating this wrong please?</p>

<p>EDIT:</p>

<p>For
$\lambda_{1} = (a + b + det)/2 \\
 \lambda_{2} = (a + b - det)/2
$</p>
",<linear-algebra>
"<p>I have a 10x10 symmetrical variance-covariance matrix, such that the variances for 10 vectors are on the main diagonal and the covariance between all vectors are on the off-diagonals.</p>

<p>I want to quantify the amount of variance in total. I can easily take the matrix trace as the sum of the eigenvalues on the main diagonal.</p>

<p>However, the matrix can be split into meaningful (biologically meaningful, in my case) sub-matrices: 4 submatrices, 5x5 each, in each corner of the original matrix. If I then want to quantify the variation within each sub-matrix  using the matrix trace, I run into some trouble with the top-right/bottom-left sub-matrices. These are formed of covariance estimates and are therefore not necessarily positive. My question is, what is the correct way to calculate the matrix trace here? If I sum the eigenvalues, I will have some negative values subtracting from the total, so should I use absolute values? Is the matrix trace the best method to use here or is there a more appropriate way of summarising the amount of variance in the sub-matrices?</p>

<p>Any guidance would be gratefully received.</p>

<p>Thanks,</p>

<p>Fiona </p>
",<linear-algebra>
"<ol start=""2"">
<li>Find matrix of a given linear transformation L->M in given new bases:</li>
</ol>

<p>a) $L =&lt; e_1,e_2,e_3 &gt;, M =&lt; g_1,g_2 &gt;, f(e_1) = g_1 − 2g_2, f(e_2) = g_1 + g_2, f(e_3) = 2g_1 + 3g_2,
$
$
\bar e_1 = 2e_1 − e_3, \bar e_2 = e_2 + e_3,  \bar e_3 = e_1 − e_2, \bar g_1 = g_1 + 2g_2, \bar g_2 = 2g_1 − g_2
$$</p>

<p>I know similar questions have been asked but they didn't help me because I am unsure if my data matches the data in the other answers. Please note there is a difference between $e_1$ and $\bar e_1 $.</p>
",<linear-algebra>
"<p>What is the dimension of a kernel with the basis {[0,0,0]}?</p>

<p>I'm confused because the definition of the dimension is number of vectors in a basis. So there is 1 vector here which is [0,0,0]. </p>

<p>Why does my professor say that the dimension of kernel is zero? He mentioned something about the zero vector space.</p>
",<linear-algebra>
"<p>Let $T$ be the Toeplitz operator on $\ell_p$ with symbol $\alpha(\lambda)=a/2\cdot \lambda-(a+1/2)+\lambda^{-1}$, where $a$ is complex. I want to solve the following </p>

<p>$$
Tx=y
$$</p>

<p>for $x\in \ell_p$ and $y=(1,q,q^2,\ldots),|q|&lt;1$. Therefore, I (Wiener-Hopf) Factorized the symbol:</p>

<p>$$\alpha(\lambda)=\alpha_{-}(\lambda)\alpha_{+}(\lambda)=(1/2-\lambda^{-1})(a\lambda-1).$$</p>

<p>So, $T^{-1}=T_{\alpha_+^{-1}}T_{\alpha_-^{-1}}$ (only if $a\not=1/\lambda$, not sure if this is true though. When is $T$ one-sided invertible?). Hence (again, not sure),</p>

<p>$$T^{-1}= 2a(S-2I)^{-1}(I-aS_{backw.})^{-1}$$</p>

<p>Now, I want tot compute $T^{-1}y$ to obtain $x$, but I got stuck here. Any hints? My second question is: How is the spectrum of $T$ defined, is there an easy way to compute $\sigma(T)$?</p>
",<linear-algebra>
"<p>I am asked to determine all faces of the $n$-dimensional hypercube
$$C_n = \left\lbrace x\in\mathbb R^n \;|\;\forall i\in\lbrace1\ldots n\rbrace : |x_i|\leq1\right\rbrace $$</p>

<p>I already know that the the $k$-dimensional faces of $C_n$ are defined by $n-k$ equalities $|x_i|=1$. <br/>
So in total there are $2^{n-k}{n\choose k}$ of those $k$-dimensional faces.</p>

<p>I understand how those faces look like and for a fixed particular $n$ I would be able to write them all down one by one, but I am struggeling to write down some general expression for all faces.</p>
",<linear-algebra>
"<p>Find a basis of subspace $ U_1 +U_2 $ of a vector space $V$. $ U_1, U_2 \subseteq V$:
$V = \mathbb R[t]$, $U_1 = \{ f \mid t^2-4t+3 \text{ divides } f \}, U_2 = \{g \mid t^2-5t+4 \text{ divides } g\}. $</p>
",<linear-algebra>
"<p>I don't understand this really good and couldnt find anything helpful on internet. I only found in book the following: A is the matrix with reflection over line through the origin with direction vector $\left(\cos(\frac{\alpha }{2} ) , \sin(\frac{\alpha }{2} ) \right) ^{T}$
$A=\begin{pmatrix} \cos(\alpha ) &amp; \sin(\alpha ) \\ \sin(\alpha )  &amp; -\cos(\alpha )    \end{pmatrix}$
I'm not sure how to connect this from book to solve it, because there is another direction vector.</p>
",<linear-algebra>
"<p>How to prove that $\det(A^{T}A) \neq 0$ if coloumns of $A$ are linearly independent, without using Cauchy-Binet formula? $A$ is real matrix.</p>
",<linear-algebra>
"<p>How do I prove this proposition:</p>

<blockquote>
  <p>If $A$, $B$ are similar matrices then for every $\lambda$ $\in$
  $\mathbb{R}$ the matrices $A-\lambda I$ and $B-\lambda I$ are similar.</p>
</blockquote>

<p>Now, from what I was given I know $A = P^{-1} B P$ and $B = PBP^{-1}$, and</p>

<p>I need to show that $A-\lambda I = M^{-1} (B-\lambda I)M$ and I am done.</p>

<p>I can't see how to solve this, I hope someone does,</p>

<p>Thank you in advance.</p>
",<linear-algebra>
"<p><strong>For 5 months!</strong> I have been struggling to solve the following equations analytically without numeric method (i,e, Newton method):</p>

<blockquote>
  <p><strong>Main equation:</strong></p>
  
  <p>$$
 \biggl(M^2-\cfrac{\mathbf{x^{\text{T}}}M^2\mathbf{x}}{\mathbf{x^{\text{T}}}\mathbf{x}}E\biggr)\mathbf{x}=\mathbf{1}
$$</p>
  
  <p><strong>Constraint equations:</strong></p>
  
  <p>$$
\begin{cases}
 \mathbf{x^{\text{T}}1}=0 \\
\\
\mathbf{x^{\text{T}}x}=u 
\end{cases} $$</p>
  
  <p>where $\{M,E\}\in\mathbf{R}^{n \times n}$ and $\{\mathbf{1},\mathbf{x}\}\in\mathbf{R}^n$ are defined, then $M$ is an arbitrary symmetric matrix, $E$
  is an identical matrix,
  $\mathbf{1}$ is all one vector, $\mathbf{x}$ is a
  variable vector and $u\in\mathbf{R}$ is a scalar.
  Furthermore, as a knowledge, the below equation form is called <a href=""https://en.wikipedia.org/wiki/Rayleigh_quotient"" rel=""nofollow"">Rayleigh
  quotient</a> $R(M^2,\mathbf{x})$:</p>
  
  <p>$$R(M^2,\mathbf{x}):=\cfrac{\mathbf{x^{\text{T}}}M^2\mathbf{x}}{\mathbf{x^{\text{T}}}\mathbf{x}}$$</p>
</blockquote>

<p>Now, we attempt to estimate the $\mathbf{x}$. Does the analytic solution or method exist? My ability is shortage but, I guess that this problem has a beautiful solution. Also, main equation is a simultaneous cubic equation. Theoretically, this is solvable. Just, this is my theme question.</p>

<p>Furthermore, same question is already asked on <a href=""http://mathoverflow.net/questions/238431/explicit-solution-to-a-rayleigh-quotient-equation"">math overflow</a>. Then answerers provided worthful information which may be solution to clue. </p>
",<linear-algebra>
"<p>The property states, ""A square matrix A is invertible iff it can be written as the product of elementary matrices""</p>

<p>I'm confused on the part of the theorem where they're trying to show that if A is invertible, then it can be written as the product of elementary matrices.</p>

<p>This is that section of the proof:</p>

<p>""Assume A is invertible. You know the system of linear equations represented by Ax=0 has only the trivial solution. But this implies that the augmented matrix [A 0] can be rewritten in the form [I 0] (using elementary row operations corresponding to E<sub>1</sub>,E<sub>2</sub>,...,E<sub>k</sub>). So, E<sub>k</sub>,...,E<sub>2</sub>,E<sub>1</sub>A I and it follows that A = E<sub>1</sub>-1E<sub>2</sub>-1...E<sub>k</sub>-1 . A can be written as the product of elementary matrices.""</p>

<p>I just don't get how knowing that Ax=0 has only the trivial solution implies that [A 0] can be written in the form [I 0]. Wasn't it already obvious that A can be rewritten as I since it's invertible? And obviously if there's a 0 matrix adjoined A to it it's going to stay the zero matrix no matter what row operations are done on it? What's the point of doing that?</p>

<p>I'm just generally confused on this proof</p>
",<linear-algebra>
"<p>Compute $det(OE[A](t))$ with $tr(A)=0$, where $OE[A]$ is defined <a href=""http://en.wikipedia.org/wiki/Ordered_exponential"" rel=""nofollow"">here</a>.</p>

<p>Attempt: First I computed the following derivative $\frac{d}{dt}det(OE[A](t))=tr((OE[A])^{-1}\frac{d}{dt}OE[A])det(OE[A](t))$. Since the ordered exponential can be written as a product of infinitesimal exponentials, I can use the product rule and simplify:
$tr((OE[A])^{-1}\frac{d}{dt}OE[A])=tr(\Delta t \sum_{i=1}^\infty A(t_i))=\Delta t \sum_{i=1}^\infty tr(A(t_i))=0$. Hence the determinant is independent on the parameter $t$ and now I choose $t=0$ (it's arbitrary) and get $det(OE[A](t))=1$.</p>

<p>Are there built errors? Are there formulas for the determinant of a ordered exponentials?
Hints would be greatly appreciated.</p>
",<linear-algebra>
"<blockquote>
  <p>Suppose $A,B$ are $n\times n$ positive definite. Then which of the followings are positive definite:</p>
  
  <ol>
  <li><p>$A+B$</p></li>
  <li><p>$ABA$</p></li>
  <li><p>$A^2+I$</p></li>
  <li><p>$AB$</p></li>
  </ol>
</blockquote>
",<linear-algebra>
"<p>I would like if someone could look over my proof. It feels odd to me.</p>

<blockquote>
  <p>Let $W$ be a subspace of a vector space $V$ over a field $F$. Prove that $v + W = \{v + w \mid w \in W\}$ is a subspace of $V$ if and only if $v \in W$.</p>
</blockquote>

<p><strong>Proof:</strong> ($\Rightarrow$) Suppose $v + W$ is a subspace of $V$. Note that $v = v + 0_W \in v + W$ and as a result $(-1)v = -v \in v + W$ as $v + W$ is a subspace. Therefore, $-v = v + w$ for some $w \in W$. Solving for $w$, we get $w = -2v$ and since $W$ is a subspace $(\frac{-1}{2})w = (\frac{-1}{2})(-2v) = v \in W$ as desired. </p>

<p>($\Leftarrow$) Suppose $v \in W$. As $W$ is a subspace, $-v \in W$ and therefore $0 = v + (-v) \in v + W$. If $x, y \in v + W$ then $x = v + w$ and $y = v + w'$ for some $w,w' \in W$. Then $x + y = (v + w) + (v + w') = v + (v + w + w') \in v + W$ as $v,w,w'\in W$ and $W$ is a subspace. Furthermore, note that $cx = c(v + w) = cv + cw = v + (cv + cw - v) \in v + W$ as $v,w \in W$ and $W$ is a subspace. Therefore, $v + W$ is a subspace of $V$ as it contains the $0$ element and it is closed under addition and scalar multiplication. $_\Box$</p>

<p>I'd appreciate any feedback. Thank-you. </p>
",<linear-algebra>
"<p>I am trying to optimize the output of a given neural network with a single hidden layer.  To accomplish this, I intend to find solve for all combinations of inputs where the derivative of the neural network = 0 and select the input vector with the highest (or lowest, depending on the problem) neural network output.  It uses the activation function</p>

<p>$$
H_i,_j = \frac{1}{(1 + e^{-t})}
$$</p>

<p>where </p>

<p>$$
t = X_i\theta_j
$$</p>

<p>for a given input vector i and hidden node j. </p>

<p>The activation values of each hidden node are multiplied by a separate weight matrix to produce the outputs.  The output k of a given input vector i is the product of the hidden node activation values i and the weight vector k.</p>

<p>$$
O_i,_k = H_iW_k
$$</p>

<p>Could someone please explain the steps I would use to create the derivative formula for an input vector of arbitrary length, an arbitrary number of hidden nodes, a single hidden node layer, and a given output k?  Thank you so much.</p>
",<linear-algebra>
"<p>I've a task to find the distance in $E^4$ between:</p>

<p>$L = [1,2,-1,4] + \text{lin}((1,2,-1,0))$</p>

<p>and</p>

<p>$M = [2,3,1,5] + \text{lin}((2,1,0,2))$</p>

<p>My efforts to find the correct solution:</p>

<p>Let</p>

<p>$\alpha_{1}=(1,2,-1,0)
 , \alpha_{2}=(2,1,0,2)$</p>

<p>$p_{0}=[2,3,1,5]$</p>

<p>$p\in R^{4}
  p\in L$</p>

<p>$p=(1+t,2+2t,-1-t,4)$</p>

<p>$\overrightarrow{p_{0}p}=(t-1,2t-1,2t,-1)$</p>

<p>$f(t)=
 d(p,M)$</p>

<p>Then (W is a determinant of grammian matrix)</p>

<p>$f(t)=\sqrt{\frac{W(\alpha_{2},\overrightarrow{p_{0}p})}{W(\alpha_{2})}}$</p>

<p>$&lt;\alpha_{2},\alpha_{2}&gt;=3$</p>

<p>$W(\alpha_{2})=3$</p>

<p>$W(\alpha_{2},\overrightarrow{p_{0}p})=\det\left(\begin{array}{cc}
&lt;\alpha_{2},\alpha_{2}&gt; &amp; &lt;\alpha_{2},\overrightarrow{p_{0}p}&gt;\\
&lt;\overrightarrow{p_{0}p},\alpha_{2}&gt; &amp; &lt;\overrightarrow{p_{0}p},\overrightarrow{p_{0}p}&gt;
\end{array}\right)$</p>

<p>$&lt;\overrightarrow{p_{0}p},\overrightarrow{p_{0}p}&gt;=\sqrt{(t-1)^{2}+(2t-1)^{2}+4t^{2}+1}=\sqrt{t^{2}-2t+1-4t^{2}+1+4t^{2}+1}=\sqrt{t^{2}-2t+1}=\sqrt{(t-1)^{2}}=t-1$</p>

<p>$&lt;\alpha_{2},\overrightarrow{p_{0}p}&gt;=&lt;\overrightarrow{p_{0}p},\alpha_{2}&gt;=\sqrt{2t-2+2t-1-2}=\sqrt{4t-5}$</p>

<p>$W(\alpha_{2},\overrightarrow{p_{0}p})=\det\left(\begin{array}{cc}
3 &amp; \sqrt{4t-5}\\
\sqrt{4t-5} &amp; \mid t-1\mid
\end{array}\right)=\\3\cdot\mid t-1\mid-\mid4t-5\mid=\begin{cases}
-3t+3+4t-5 &amp; \iff t&lt;1\\
3t-3+4t-5 &amp; \iff t\in[1,\frac{5}{4})\\
3t-3-4t+5 &amp; \iff t&gt;\frac{5}{4}
\end{cases}=\begin{cases}
t-2 &amp; \iff t&lt;1\\
7t-8 &amp; \iff t\in[1,\frac{5}{4})\\
-t+2 &amp; \iff t\geq\frac{5}{4}
\end{cases}$</p>

<p>From that I noticed, that distance can be negative, and I don't know how to fix it</p>

<p>Could you help me to point, where I made an error in reasoning?</p>

<p>Thanks in advance for all advices! </p>
",<linear-algebra>
"<p>We are given the following problem:</p>

<blockquote>
  <p>Let $S$ be the set of all functions $y$ that satisfy the following differential equation
  $$2\dfrac{d^2y}{dx^2}  - 3\dfrac{dy}{dx} + y = 0.$$
  Show that $S$ is a subspace of the vector space $A$, where $A$ is the set of all functions $f : \mathbb{R} \rightarrow \mathbb{R}$.</p>
</blockquote>

<p>I do not know how to approach the problem.</p>
",<linear-algebra>
"<p>So I was able to figure out the first part of this problem, but I have no concept of how it relates to Schur complements, so I'm not sure (no pun intended) how to proceed. The question is as follows:</p>

<p>Consider $2x^2 + 2xy + 2y^2 + z^2 + 2xz$. Write the symmetric matrix representing this quadratic form. Now, express this as a sum of squares by using this symmetric matrix and Schur complements.</p>

<p>I determined the symmetric matrix representation as:</p>

<p>$$\begin{bmatrix}
        2 &amp; 1 &amp; 1 \\
        1 &amp; 2 &amp; 1 \\
        1 &amp; 0 &amp; 1 \\
        \end{bmatrix} $$</p>

<p>And that's as far as I've gotten. Any help would be much appreciated.</p>
",<linear-algebra>
"<p>If we're given a $ \displaystyle  2 \times 2 $  Markov Matrix (so all entries are non-negative and columns add to 1) <strong>M</strong>$(a,b)$ such that $$M = M(a,b) := \begin{bmatrix}1 - a &amp; b\\a &amp; 1 - b \end{bmatrix}$$ where $a$ and $b$ are $ 0 ≤ a ≤ 1, 0 ≤ b ≤ 1$, I know that $λ1 = 1$ is an eigenvalue for $M(a,b)$, but how would I find a corresponding eigenvector $u_{1}(a, b)$ such that when normalized, $e^{T}u1(a, b) = 1$? There should also be a second eigenvalue / eigenvector right?</p>
",<linear-algebra>
"<p>Let T:P2->P2 be a linear transformation and A be the matrix of the linear transformation. Prove that if det(A) does not equal 0 then T is one-to-one.</p>

<p>I know that for T to be 1-1 then the kernel is a zero vector and therefore A would reduce to an identity matrix I'm just not sure how to tie that into the determinant?</p>
",<linear-algebra>
"<p>Define a function $ T: P_{3} \to \text{M}_{2 \times 2} $ by
$$
  T \! \left( a_{0} + a_{1} x + a_{2} x^{2} + a_{3} x^{3} \right)
= \begin{pmatrix} a_{3} &amp; a_{0} \\ a_{2} &amp; a_{1} \end{pmatrix}.
$$
I know how to show that $ T $ is a linear transformation, i.e.,
$$
T(\vec{u} + k \cdot \vec{v}) = T(\vec{u}) + k \cdot T(\vec{v}).
$$
I also know how to show that $ T $ is an isomorphism (one-to-one and onto), but how do I find the matrix representation of $ T $ with respect to the standard bases of $ P_{3} $ and $ \text{M}_{2 \times 2} $?</p>
",<linear-algebra>
"<p>I am not seeing why a subspace must include $ 0 $. From what I am told, this inclusion means that the subspace is not “empty”, but I cannot see how the inclusion of $ 0 $ does this. For instance, can you not have a subspace of $ \Bbb{R}^{2} $ that is a line represented by $ y = x + 1 $, which will not intersect $ (0,0) $ of $ \Bbb{R}^{2} $ at all? This subspace appears to exists and to be an element of $ \Bbb{R}^{2} $ by addition and scalar multiplication, but unless I am mistaken, it does not satisfy this $ 0 $-vector inclusion requirement (unless this means it is NOT a subspace, but then that is not making sense).</p>
",<linear-algebra>
"<p>I'm having a bit trouble with this excercise:</p>

<p><strong>The problem:</strong><br>
Let there be a polynomial $f(x)=a_1x^{t_1} + a_2x^{t_2} + ... + a_nx^{t_n}$
Where $t_1, t_2, ..., t_n$ are not-negative integers.
The polynomial has a root $b$ which occurs $n$ times.
Prove that $b = 0$.</p>

<p><strong>What I have so far:</strong><br>
I can presume that $a_1, a_2, ..., a_n \neq 0$<br>
If $n = 1$, then it's obviously true.<br>
If $n = 2$, I tried using this:<br>
$f^{(n-1)}(b) = 0
\\
f^{(n)}(b) = 0$<br>
So:<br>
$f(b) = a_1b^{t_1} + a_2b^{t_2} = 0
\\
f'(b) = t_1*a_1b^{(t_1-1)} + t_2*a_2b^{(t_2-1)} = 0
\\
f''(b) = (t_1-1)t_1*a_1b^{(t_1-2)} + (t_2-1)t_2*a_2b^{(t_2-2)} \neq 0$<br>
Why can't $b \neq 0$ be true?</p>
",<linear-algebra>
"<p>Let $V$ be a vector space of dimension $n$ over $\mathbb{F}_q$, and let $U$ be a subspace of dimension $k$. I want to compute the number of subspaces $W$ of $V$ of dimension $m$ such that $W\cap U=0$.</p>

<p>I know why the number of subspaces of $V$ that contain $U$ and have dimension $m$ is $\binom{n-k}{m-k}_q$, but I don't understand why $q^{km}\binom{n-k}{m}_q$ is number of these subspaces?</p>
",<linear-algebra>
"<p>how to prove $\|(A^HA)^k\| =||A||^{2k}$ using singular value decomosition. $A^H$ is a hermitian matrix. $A$ element of $C^{p\times q}$, for every positive integer $k$.</p>
",<linear-algebra>
"<p>Suppose one has the following system of linear equations
$$(A + \Delta A) x = b$$
where $A$ and $\Delta A$ are large sparse matrices and $\Delta A$ is ""small"" compared to $A$, furthermore vector $x$ is unknown (the solution) and vector $b$ is known. </p>

<p>The system needs to be solved many times, in which only $\Delta A$ varies (the perturbation). Therefore it is relatively cheap to obtain $A^{-1}$ and may be considered to be known.</p>

<p>The best solution I've found thus far is to apply the Neumann series expansion on the inverse
$$A(I + A^{-1} \Delta A) x = b$$
$$A(I + P) x = b$$
$$\Rightarrow \quad x = (I + P)^{-1} A^{-1} b = \lbrace I - P + P^2 - P^3 + \cdots \rbrace A^{-1} b$$</p>

<p>Does any one know of a better alternative, preferably a method that doesn't require a series expansion?</p>
",<linear-algebra>
"<p>I have a polynomial $p_a(x,y)= x^2F(a)+y^2G(a)-xH(a)-I(a)$ where $F(a)$, $G(a)$, $H(a)$ and $I(a)$ some real fuctions of $a$ are. Which conditions must satisfy $a$ so that I can factorize the polynomial $p_a(x,y)$ in lineal real factors?</p>
",<linear-algebra>
"<p>If both roots of the equation $(a-b)x^2+(b-c)x+(c-a)=0$ are equal, prove that $2a=b+c$.</p>

<blockquote>
  <p><strong>Things should be known:</strong></p>
  
  <ul>
  <li><p>Roots of a Quadratic Equations can be identified by:</p>
  
  <p>The roots can be figured out by: 
  $$\frac{-b \pm \sqrt{d}}{2a},$$ 
  where
  $$d=b^2-4ac.$$</p></li>
  <li><p>When the equation has equal roots, then $d=b^2-4ac=0$.</p></li>
  <li><p>That means $d=(b-c)^2-4(a-b)(c-a)=0$</p></li>
  </ul>
</blockquote>
",<linear-algebra>
"<p>My question is: </p>

<p>How to solve this equation:</p>

<p>$ax²+by²+cxy=0$</p>

<p>with respect to $x$ and $y$ in the same time. Here $a,b,c$ are real constants.</p>
",<linear-algebra>
"<p>How would one go about proving that there is no embedding of a vector space into it's dual that is independent of a choice of basis? Thanks</p>
",<linear-algebra>
"<p>Given the matrix $A= \begin {pmatrix} 1 &amp; 1 &amp;1  \\ -1 &amp; 1 &amp; 0 \\ 0 &amp; 2 &amp;1 \end{pmatrix}$.</p>

<p>(i) Determine the orthogonal projection $p:\mathbb{R}^3 \rightarrow \mathbb{R}^3$ on $Im(A)$</p>

<p>(ii) Calculate an orthonormal basis of $(\ker(A))^{\perp}$</p>

<p>(iii) Determine the pseudoinverse $A^+$ of $A$</p>

<p>I was wondering about the sequence of the subtasks. Normaly, i would do (iii) first and then (i) &amp; (ii) using that $AA^+:\mathbb{R}^m\rightarrow Im (A)$ and $A^+A:\mathbb{R}^n\rightarrow(\ker(A))^{\perp}$. So my question is: Is there a way to to (i) and (ii) without determine the pseudoinverse?</p>
",<linear-algebra>
"<p>$T:P(\mathbb{R}) \mapsto P(\mathbb{R})$ defined by</p>

<p>$(Tp)(x) = x^2p(x)$</p>

<p>Verify that multiplication by $x^2$ is a linear map.</p>

<p>Additivity: $x^2(p+q) = x^2p+x^2q$</p>

<p>Homogeneity: $x^2(ap) = a(x^2p)$</p>

<p>Is this a correct verification?</p>
",<linear-algebra>
"<p>Let $$V=\{x \in \mathbb{R} : x&gt;0\}$$</p>

<p>For $x,y,a \in \Bbb{R}$, define $x\oplus y=xy$ and $x\odot a=x^a$. Is $V$ a vector space under these operations? Justify your answer.</p>
",<linear-algebra>
"<p>I know that normally for commutators that [A,B]=-[B,A] where A and B are operators.  But under what conditions does [A,B]=[B,A]?</p>
",<linear-algebra>
"<blockquote>
  <p>Can someone give me some examples of unit vectors that's in the same direction as vector, let's say $v=(1,2,-3)^T$ for:</p>
</blockquote>

<p>(i) Euclidean norm</p>

<p>(ii) Weighted norm $||V||^2=2V_1^2+v_2^2+\frac13v_3^2$</p>

<p>(iii) The 1 norm</p>

<p>(iv) The infinite norm</p>

<p>(v) The norm based on their inner product $2v_1w_1-v_1w_1-v_2w_1+2v_2w_2-v_2w_3-v_3w_2+2v_3w_3$.</p>

<p>Ty.</p>
",<linear-algebra>
"<p>How can I show that for a particle in an infinite square well in a stationary state, that the expectation value $\langle[\hat{H},\hat{O}]\rangle=0$ where $\hat{H}$ is the Hamiltonian operator and $\hat{O}$ is an arbitrary operator?</p>
",<linear-algebra>
"<p>I'm studying for an exam and I don't understand how my prof finds the basis for eigenspaces using the matrix representation of a linear map. Once I find an eigevalue then how do I find the basis for its eigenspace. I've attached a screenshot of the part that I don't understand (from an example). Can someone please explain it to me in detail? Thanks<img src=""http://i.stack.imgur.com/78iPC.png"" alt=""![enter image description here"">]<a href=""http://i.stack.imgur.com/78iPC.png"" rel=""nofollow"">1</a></p>
",<linear-algebra>
"<blockquote>
  <p>Even if an isomorphism between two linear spaces $L$ and $M$ over a field $\mathbb{K}$ exists, it is defined uniquely only in two cases:</p>
  
  <ol>
  <li><p>$L=M=\{0\}$ and</p></li>
  <li><p>$L$ and $M$ are one-dimensional, while $\mathbb{K}$ is a field consisting of two elements.</p></li>
  </ol>
</blockquote>

<p>How can I show this fact? Does anyone have any hints?</p>
",<linear-algebra>
"<p>I would like to generate a semi-unitary matrix, i.e., $UU^T=~I$ where U is a non-square matrix whose number of rows is bigger than its number of columns.
I tried it by solving the optimization problem $\min_U\|UU^T - I\|_F$ but didn't work at all since the problem is not convex. </p>
",<linear-algebra>
"<p>I have a question:
Suppose I have a  $n\times n$ matrix:
$$
        \begin{bmatrix}
        1 &amp; 1 &amp;...&amp; 1 \\
        1 &amp; 1 &amp;...&amp;1 \\
        \vdots&amp;\vdots &amp;\ddots &amp; \vdots&amp;\\
        1 &amp; 1 &amp; ...&amp;1 \\
        \end{bmatrix}
$$
,then is there a easy way to compute the eigenvalues of the matrix?</p>

<p>How can I compute this matrix eigenvalue?</p>
",<linear-algebra>
"<p>Find a system of linear equations whose solution set is the line in the 3 dimensional space.</p>

<p>$$  \begin{pmatrix} x \\ y \\ z \\  \end{pmatrix} =\begin{pmatrix} t-4 \\ t-10 \\ 2t-20 \\  \end{pmatrix}$$</p>

<p>What i tried</p>

<p>The question is asking to link the variables $x$ $y$ and $z$ into a single equation while removing the $t$. I first equate $x=t-4$,$y=t-10$ and $z=2t-20$. Then i tried combining the equations to form $$x=y+6$$ and $$z=2x-12$$ and $$z=2y$$. However i could not combine all these 3 variable into a single equation. Could anyone explain. Thanks</p>
",<linear-algebra>
"<p>I have a panel 1200 pixels wide, and am filling in smaller subpanels to fill the length. Each sub-panel is of a different color ($p$ = purple, $g$ = green, etc). It's for a navigation bar on a website, each subpanel corresponds to a link to another page.</p>

<p>In one situation, there's three purple subpanels, a yellow subpanel, two blue subpanels, one each of green and red subpanels; in another situation, there's 3 purple subpanels, a yellow ($e$ for clarification) subpanel, two more purple subpanels, and a green subpanel. The width of each subpanel (irrespective of situation) must add up to 1200px. </p>

<p>Lastly, $e$ is always the middle panel, with some panels to the left or right. The combined widths of the left must equal the combined widths of the right in both situations.</p>

<p>Since all subpanel widths must add up to 1200px irrespective of situation, two equations can be made:</p>

<p>$$\begin{align}
3p + e + 2b + g + r &amp;= 1200 \\
3p + e + 2p + g &amp;= 1200
\end{align}$$</p>

<p>Since $e$ is in the center of the panel, subpanels to the left and right of $e$ in the above equations must equal. This gives me two more:</p>

<p>$$\begin{align}
3p &amp;= 2b + g + r \\
3p &amp;= 2p + g
\end{align}$$</p>

<p>I think I can derive two more equations, but I'm not sure their relevance. Since $e$ is in the middle ($e$'s midpoint is in the middle of the panel), the left and right halves each contain half of $e$:</p>

<p>$$\begin{align}
3p + \frac{1}{2}e &amp;= \frac{1}{2}e + 2b + g + r \\
3p + \frac{1}{2}e &amp;= \frac{1}{2}e + 2p + g
\end{align}$$</p>

<p>Alternatively:</p>

<p>$$\begin{align}
3p + \frac{1}{2}e &amp;= 600 \\
\frac{1}{2}e + 2b + g + r &amp;= 600 \\
\frac{1}{2}e + 2p + g &amp;= 600
\end{align}$$</p>

<p>I want to find <em>any solution</em> given a couple of rules:</p>

<p>$$\begin{align}
p &amp;&gt; 150\\
b &amp;&gt; 100 \\
e &amp;&gt; 90 \\
g &amp;&gt; 80 \\
r &amp;&gt; 60
\end{align}$$</p>

<p>As long as any solution fits those minimum values, things are kosher.</p>

<p>Forming a matrix and bringing to RREF yields three free variables, which is why I'm stumped.</p>

<p>The illustrate the first situation, $3p + e + 2b + g + r = 1200$:</p>

<p><a href=""http://i.stack.imgur.com/hOst4.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/hOst4.png"" alt=""enter image description here""></a></p>
",<linear-algebra>
"<p>In $P_2 = {ax^2 + bx + c: a,b,c \in\mathbb{R}}$, why do coefficients in $ax^2$ form reduce with coefficients in $bx$ or $c$?</p>

<p>For example, lets look at the set {$x^2 + x - 1, 2x + 1, 2x - 1$}</p>

<p>If we wanted to check to see if its independent, we would rewrite this as </p>

<p>$C_1* (x^2 + x -1) + C_2 * (2x +1) + C_3 * (2x-1) = \vec{0} $</p>

<p>In matrix form this would be,</p>

<p>\begin{bmatrix}
    1 &amp; 0 &amp; 0 \\
    1 &amp; 2 &amp; 2 \\
    -1 &amp; 1 &amp; -1
\end{bmatrix}</p>

<p>When we apply Gaussian Elimination, in my head, when eliminating the $1$ found in the 1st column 2nd row and $-1$ in the 1st column 3rd row, it seems like I am subtracting x with $x^2$ and -1 with $x^2$ respectively. I understand that the first column all belongs to $C_1$. </p>

<p>I understand that we are subtracting $C_1$ from $C_1$ but why can we simply ignore the $x^2$ and $x$. I guess my problem with how this is all set up is that I'm relating it back to high school algebra where you had equations like.</p>

<p>$x + y = 5$</p>

<p>$x -y = 10$ </p>

<p>And you solved for $x$. Why can we ignore the $x$ or $x^2$?</p>

<p>Thanks.</p>
",<linear-algebra>
"<p>Let $A$ and $B$ be two real symmetric matrices in $M_n(\mathbb{R})$. I would like to learn about necessary and sufficient conditions for knowing when $B \in \overline{GL_n(\mathbb{R})\cdot A}$; where:
$$
GL_n(\mathbb{R})\cdot A:=\{(g^{-1})^{T} A g^{-1} : g \in GL_n(\mathbb{R})\},
$$
$(g^{-1})^{T}$ is the transpose of $g^{-1}$ and $\overline{GL_n(\mathbb{R})\cdot A}$ is the closure of $GL_n(\mathbb{R})\cdot A$ with respect to the usual topology of $M_n(\mathbb{R})$.</p>

<p>Help would be appreciated!</p>

<p>Thanks!</p>
",<linear-algebra>
"<blockquote>
  <p>If $V=\mathbb R[x]_k=\{\sum\limits_{i=1}^ka_ix^i:a_i\in\mathbb R, \forall i\}$ is a vector space of dimension $k+1$ over $K=\mathbb R$ and $\mathcal B=\{1,x,\dots,x^k\}$ is a basis of $V$. The dual space of $V$ is the vector space $V^*=\{F:V\to K:F\ \text{is linear}\}$ for all $i$;  $x^{i^*}$ is defined as $x^{i^*}(x^j)=\delta_{ij}=\begin{cases} 1 &amp; \textrm{if i=j}\\0 &amp; \textrm{else}\end{cases}$, let $\mathcal B^*=\{x^{i^*}:i\in\{0,\dots,k\}\}$ and $F:V\to K$ with $p(x)\mapsto\int_0^1 p(x)$ write $F$ as a linear combination of elements of $\mathcal B^*$</p>
</blockquote>

<p>First $\mathcal B^*$ is basis of $V^*$ because $Hom(V,K)\simeq Mat_{1\times(k+1)}(K)$ with $f\mapsto(f(1),\dots f(x^k)$(I can show it is injective and with our chosen $x^{i^*}$ also surjective, and since $x^{i^*}$s are linearly independent it is a basis, but how can I write $F$; </p>

<p>$F=\lambda_01^*+\lambda_1x^*+\dots\lambda_kx^{k^*}$, for examples if $p(x)=x$ then $\int_0^1 p(x)=\frac12$, so $F=\frac12x^*$ and in general $\lambda_k=\frac{1}{k+1}$, am I correct ?</p>
",<linear-algebra>
"<blockquote>
  <p>Given $A_{n\times n},B_{n\times n} \in \mathbb C$ then:</p>
  
  <ol>
  <li><p>if $A$ is unitary and the characteristic polynomial $f_A(x)=f_B(x)$ then $B$ is also unitary. </p></li>
  <li><p>if $A$ is normal and $f_A(x)=f_B(x)$ then $B$ is also normal. </p></li>
  <li><p>if $A$ is unitary and $f_A(x)=f_B(x)$ and $m_A(x)=m_B(x)$ then $A$ is similar to $B$. </p></li>
  </ol>
</blockquote>

<p>(1) if the matrix is unitary then the eigenvalues are $\pm 1$ but why would having eigenvalues that are  $\pm 1$ mean that the matrix is unitary? So that's probably false.</p>

<p>(2) similar reasoning, why would the normal property carry over because of the same eigenvalues? Probably false.</p>

<p>(3) since $A$ is diagonalizable because its unitary, then it has a similar diagonal matrix, and since $B$ has the same diagonal matrix then it has to be similar to it? How does the minimal polynomial help here?</p>
",<linear-algebra>
"<p>I am going through Linear Algebra right now, we are using the book Elementary Linear Algebra by Andrilli. In one of the theorems he uses this notation without really introducing it. Here is the theorem:</p>

<p>Theorem 2.3 Let <strong>AX</strong>=<strong>B</strong> be a system of linear equations. If [<strong>C</strong>|<strong>D</strong>] is row equivalent to [<strong>A</strong>|<strong>B</strong>], then the system <strong>CX</strong>=<strong>D</strong> is equivalent to <strong>AX</strong>=<strong>B</strong>.</p>

<p>I just don't know how to read the [C|D] notation there. I want to say C divides D because that's the symbol for integer division, but I know that's not right.</p>

<p>Thanks!</p>
",<linear-algebra>
"<p>The notion of <a href=""https://en.wikipedia.org/wiki/Linear_independence""><em>linear independence</em></a> is very well-known and well-understood.</p>

<p>However, is there a way to generalize the definition to other types of independence -- such as perhaps ""quadratic independence"", ""polynomial independence"", ""harmonic independence"", etc.?</p>

<p>(Sorry if <a href=""/questions/tagged/linear-algebra"" class=""post-tag"" title=""show questions tagged &#39;linear-algebra&#39;"" rel=""tag"">linear-algebra</a> isn't a good tag; I couldn't think of a better one.)</p>
",<linear-algebra>
"<p>Could anyone help me with this proof without using determinant? I tried two ways. </p>

<blockquote>
  <p>Let $A$ be a matrix. If $A$ has the property that each row sums to zero, then there does not exist any matrix $X$ such that $AX=I$, where $I$ denotes the identity matrix. </p>
</blockquote>

<p>I then get stuck. The other way was to prove by contradiction, and I failed too. </p>
",<linear-algebra>
"<p>Let $A$ be a $n\times n$ matrix. Choose the correct option.</p>

<p>a) if $A^2 =0$ then $A$ is diagonalisable.</p>

<p>b) if $A^2 =I$ then $A$ is diagonalisable.</p>

<p>c) if $A^2 - A =0$ then $A$ is diagonalisable.</p>

<p>Now... for a) I tried by transforming them using minimal polynomial (cayley Hamilton) so it should satisfy $t^2 =0$ hence the eigen values are $0$ (twice) hence its not convertible and not diagonalisable... similar arguments for b) and c).</p>

<p>But I am not sure whether I am right... please guide me..</p>
",<linear-algebra>
"<p>How to find a matrix $A$ over $\mathbb{R}$ such that $A^2-5A+6I=0$ is diagonalisable??</p>

<p>I tried by considering $A$ a particular matrix and putting the values in the given equation but it gives nine equations in nine variables and those too very complex .. is there any other way ??</p>
",<linear-algebra>
"<p>I am quite confused at the notation used for $J$ in the following question: </p>

<blockquote>
  <p>For a normed space $\Omega$, let $\Omega^{**}$ denote the dual space of the dual space. Let $J: \Omega \rightarrow \Omega^{**}$ be defined by $\langle x^*, J(x)\rangle   =  \langle x, x^*\rangle  = x^*(x)$. Show that $J$ is a linear isometry.</p>
</blockquote>

<p>Where, for $x^* \in \Omega$, $\langle x, x^*\rangle = x^*(x)$. </p>

<p>I am confused on how to decipher the notation for $J$. I am normally use to things like $J(x) = $ fill in the blank. Could someone please explain this notation? </p>
",<linear-algebra>
"<p>I have here a linear transformation $T : P_3(\mathbb{R})\rightarrow P_3(\mathbb{R}) $ defined by:</p>

<blockquote>
  <p>$ T(at^3 + bt^2 + ct + d) = (a-b)t^3 + (c-d)t $</p>
</blockquote>

<p>I'm very very new in this subject and I'm not going well with polynomials. I need find the $ Kernel $ and the $ Image $ of the transformation. Look what I've been thinking:</p>

<blockquote>
  <p>$Ker(T) =  \{ T(p) = 0 / p \in P_3\} $</p>
  
  <p>$ T(at^3 + bt^2 + ct + d) = (a-b)t^3 + (c-d)t = 0 $</p>
  
  <p>$(a-b) = 0 \ ;\ \ (c-d) = 0 \ ;\ \ a = b \ ; \ \ c = d $</p>
  
  <p>$ Ker(T) = \{ at^3 + at^2 + ct +c\ /\ a,c \in \mathbb{R} \} $</p>
</blockquote>

<p>And what about the $ Image $? I know that $Im(T) = \{ T(p) / p \in P_3 \}$, but how can I show it? And how can I test if a polynomial such as $ p(t) = t^3 + t^2 + t -1 \in Im(t)$?</p>
",<linear-algebra>
"<p>Let $T: V\rightarrow W$ be a linear mapping. Let $M$ be a linear subspace such that $M \subset ker(T)$. Let $Q$ be the quotient mapping  $Q:V \rightarrow V/M$ then I have to show there is unique mapping $S: V/M \rightarrow W$ such that $T=SQ$. </p>

<p>Thoughts: Is it a reasonable choice to choose $S(x+M) = T(x+m) = T(x)$ for any coset $x+M$ and any $m \in M$ since $M \subset ker (T)$ or have I drifted down the wrong path.</p>
",<linear-algebra>
"<p>Let $A\in\{0,1\}^{m\times n}$ where $m \gg n$. Take this matrix to be over $\mathbb{R}^{m\times n}$ (not the binary field). What is the probability that said matrix will have full rank? Is there some condition on the difference between $m$ and $n$ so that the probability approaches $1$? </p>
",<linear-algebra>
"<p>Assume $A$ is a $m \times n$ matrix. We want to see whether the linear system $Ax=b$ has any solution for $x$ given $b$. One way to check this is:</p>

<p>""This linear system of equation has a solution if the b is contained in the column space of A.""</p>

<p>1- Does anybody know a good reference for this?</p>

<p>2- How can we check that b is within the column space of A using projection? Any reference for this method?</p>
",<linear-algebra>
"<p>I was given as an assignment to diagonalize the following matrix:</p>

<blockquote>
  <p>$\left(\begin{array}{cc}
\cos\theta &amp; -\sin\theta\\
\sin\theta &amp; \cos\theta
\end{array}\right)$ </p>
</blockquote>

<p>I started by finding the eigenvectors and got:</p>

<blockquote>
  <p>$v_{1}=\left(\begin{array}{c}
1\\
-i
\end{array}\right)$,  $v_{2}=\left(\begin{array}{c}
1\\
i
\end{array}\right)$</p>
</blockquote>

<p>then I normalized the vectors and composed a unitary matrix:</p>

<blockquote>
  <p>$U=(v_1|v_2)=\frac{1}{\sqrt{2}}\left(\begin{array}{cc}
1 &amp; 1\\
-i &amp; i
\end{array}\right)$</p>
</blockquote>

<p>The problem is in the final step:</p>

<blockquote>
  <p>$U^{*}AU=\left(\frac{1}{\sqrt{2}}\left(\begin{array}{cc}
1 &amp; i\\
1 &amp; -i
\end{array}\right)\right)\left(\begin{array}{cc}
\cos\theta &amp; -\sin\theta\\
\sin\theta &amp; \cos\theta
\end{array}\right)\left(\frac{1}{\sqrt{2}}\left(\begin{array}{cc}
1 &amp; 1\\
-i &amp; i
\end{array}\right)\right)$</p>
</blockquote>

<p>This doesn't produce a diagonal matrix.</p>

<p>Is there any mistake in these stages?</p>

<p>Many thanks.</p>
",<linear-algebra>
"<p><strong>Question:</strong> 
Let V be the vector space of all functions $\Bbb R\to \Bbb R$.
Show that $V=U \oplus W$
for $U=${$f | f(x)=f(-x) \forall x$}$, $W={$f | f(x)=-f(-x) \forall x$}</p>

<p><strong>What I did</strong>:</p>

<p>I did prove that $U \cap W$={$0$}. But proving that any function from R to R can be displayed as a sum of odds and evens wasn't a success. I tried saying that for $v \in V, w \in W: v=v-w+w$ and proving that $v-w \in U$ but that didn't work (That trick worked with some linear transformations we saw, but this isn't a linear transformation).</p>
",<linear-algebra>
"<p>I'm not clear about the sum(and direct sum) operator on subspaces, how to use them? please offer me some applications about this operation, really really appreciate it.</p>
",<linear-algebra>
"<p>Let $V$ be the space of $n \times 1$ matrices over $F$ and let $W$ be the space of all $m \times 1$ matrices over $F$. Let $A$ be a fixed $m \times n$ matrix over $F$ and $T$ be a linear transformation $T : V\to W$ defined by $T(X)=AX$. Prove that $T$ is the zero transformation if and only if $A$ is the zero matrix.</p>

<p><strong>My try</strong></p>

<ol>
<li>I am thinking that if I set the matrix $A=0$, that is, the $0$ matrix, then the $0$ matrix times any other matrix will be zero. I don't know whether this is right or not. Can someone please check it?</li>
<li>What if I had to prove the converse of this? How am I supposed to prove that?</li>
</ol>
",<linear-algebra>
"<p>The $U_e$ and $U_o$ denote the set of all real-valued even/odd function on $\mathbb R$ respectively.</p>
",<linear-algebra>
"<p>I'm interested in algorithms to compute matrix multiplications. Is the Coppersmith-Winograd algorithm similar to the Strassen algorithm ?</p>

<p>I have two other questions:</p>

<p>1) Are the multiplications done at the end of the recursion, like the Strassen one ?</p>

<p>2) The $O(n^{2.38})$ refers to the number of multiplications or to any basic operations (additions...) ?</p>

<p>Thank you</p>
",<linear-algebra>
"<p>\begin{array}{rrrrr|r}
    b &amp; a &amp; a &amp; \cdot \cdot \cdot &amp; a \\
    a &amp; b &amp; a &amp; \cdot \cdot \cdot &amp; a \\
    a &amp; a &amp; b &amp; \cdot \cdot \cdot &amp; a \\
    \cdot &amp; \cdot &amp; \cdot &amp; \space &amp; \cdot\\
\cdot &amp; \cdot &amp; \cdot &amp; \space &amp; \cdot\\
a &amp; a &amp; a &amp; \cdot \cdot  \cdot &amp; b
  \end{array}</p>

<p>I have the above matrix $A\in M_{n\times n}(F)$ where $F$ is a field and $n\geq1$, $a,b\in F$.</p>

<p>I'm trying to find out how to use row operations to make it into an upper triangular matrix in order to figure out the determinant. But I'm not sure how I would approach it.</p>
",<linear-algebra>
"<p>Page 2 (506), line 18 of
<a href=""http://www-personal.umich.edu/~orosz/articles/NonlinScipublished.pdf"" rel=""nofollow"">http://www-personal.umich.edu/~orosz/articles/NonlinScipublished.pdf</a></p>

<p>says that ""The presence of translational symmetry in the nonlinear equations gives rise to a relevant zero eigenvalue in the linearized system at any of the trivial solutions"".</p>

<p>It looks like a general statement, but I don't see why (is it trivial?). Where can I find a precise statement about this and on what conditions does that hold? (I'd like to know the proof too.) Thank you.</p>
",<linear-algebra>
"<p>Let $A$ be a Hermitian matrix of size $n$ such that $A^5+A^3+A=3I_n$ , then is it true that $A=I_n$ ? What I got is if $a$ is an eigenvalue then $a^5+a^3+a-3=0=(a-1)(a^4+a^3+2a^2+2a+3)$ this doesn't seem to get anywhere , Please help . </p>
",<linear-algebra>
"<p><strong>Given data in the problem</strong> </p>

<ol>
<li>${\psi'(t)}_{3 \times 3}=A_{3 \times 3}\psi(t)_{3 \times 3}, \psi(0)_{3 \times 3}=R^{cl}_{3 \times 3}  \\
\phi'(t)_{3 \times 3}=t\hspace{.1cm}B_{3 \times 3} \phi(t)_{3 \times 3},\phi(0)_{3 \times 3}=R^{cl}_{3 \times 3}   \tag 1$</li>
<li>$A,B ,R^{cl}$ are constant matrices. A,B are skew symmetric matrices. $R^{cl}$ is a rotation matrix</li>
<li>We know the solutions of  equation (1). Implies we know about what is  $\psi(t)=e^{At}R^{cl}_{3 \times 3},\phi(t)=e^{B\frac{t^2}{2}}R^{cl}_{3 \times 3} \tag 2 $</li>
</ol>

<p><strong>Question</strong></p>

<p>What is the  the solution of $ R'(t)=AR(t)+tBR(t)\tag 3$ in closed form?</p>
",<linear-algebra>
"<p>If $T:\mathbb C^n \to \mathbb C^n$ is a linear transform such that $\ker(T-aI)=\ker(T-aI)^n , \forall a\in \mathbb C$ , then is $T$ diagonalizable ? </p>
",<linear-algebra>
"<p>Consider $\mathbb{R}^3$ with the standard inner product. Let $W$ be the plane spanned by $(1,1,1)$ and $(1,1,-2)$. Let $U$ be the linear operator defined as: $U$ is rotation through the angle $\theta$, about the straight line through the origin which is orthogonal to $W$. How to find the matrix of $U$ in the standard ordered basis.</p>
",<linear-algebra>
"<p>I have taken a Quadratic form and performed simultaneous row and column operations on it.  I started with $$A = \begin{pmatrix} 1 &amp; 2 &amp; 1\\2 &amp; 3 &amp; 4\\ 1 &amp; 4 &amp; 5\end{pmatrix}$$ and diagonalized to $$D = \begin{pmatrix} 1 &amp; 0 &amp; 0\\0 &amp; -1 &amp; 0 \\ 0 &amp; 0 &amp; 8\end{pmatrix} S = \begin{pmatrix} 1 &amp; 0 &amp; 0\\-2 &amp; 1 &amp; 0 \\ -5 &amp; 2 &amp; 1\end{pmatrix}$$ where $SAS^T = D$.  How do I find the new basis for the diagonalized version of A?</p>

<p>EDIT:  I typed A incorrectly.</p>
",<linear-algebra>
"<p>I am currently working on a Computer Algebra System and was wondering for suggestions on methods of finding roots/factors of polynomials. I am currently using the Numerical Durand-Kerner method but was wondering if there are any good non-numerical methods (primarily for simplifying fractions etc).</p>

<p>Ideally this should work for equations in multiple variables.</p>
",<linear-algebra>
"<p>I understand what the Hodge dual is, but I can't quite wrap my head around the dual space of vector space. They seem very similar, almost the same, but perhaps they are unrelated. </p>

<p>For instance, in R^3, the blade a^b gives you a subspace that's like a plane, and the dual is roughly the normal to the plane.</p>

<p>Is there a similarly simple example for the dual space of a vector space, or is there a way to describe the vector space dual in terms of the Hodge dual?</p>
",<linear-algebra>
"<p>I have a simple linear operator:</p>

<p>$$\begin{align}g: \Bbb{R^4} &amp;\to \Bbb{R^3}\\g (x, y, u , v) &amp;= ( x + u, x + v, y + u)\end{align}$$</p>

<blockquote>
  <p>How would I determine the image of this linear operator?</p>
</blockquote>

<p>I thought that putting in the vectors</p>

<blockquote>
  <p>$(0, 0, 0, 1) \implies \dots$<br>
  $(0, 0, 1, 0) \implies \dots$<br>
  $\;\;\;\vdots$</p>
</blockquote>

<p>would yield the $4$ vectors of the image.</p>

<p>But the solution says there are only $3$ vectors in the image of this operator.</p>

<p>What am I missing?</p>
",<linear-algebra>
"<p>First I'll define what I talk about:</p>

<p>A <strong>bilinear form</strong> on a vector space V is a mapping:</p>

<p>$F: V \times V \rightarrow \mathbb{R}, (a,b) \mapsto F(a,b)$</p>

<p>which is linear in every argument:</p>

<p>$a, b, c \in V$ and $\lambda, \mu \in \mathbb{R}$:</p>

<ul>
<li>$F(\lambda a + \mu b, c) = \lambda F(a, c) + \mu F(b, c)$</li>
<li>$F(a, \lambda b + \mu c) = \lambda F(a, b) + \mu F(a, c)$</li>
</ul>

<p>If I get an expression which could be a bilinear form, I check those two. This can be quite long.</p>

<p>A bilinear form F is <strong>symmetric</strong>, if:</p>

<p>$\forall a, b \in V: F(a, b) = F(b, a)$</p>

<p>Now my <strong>question</strong>:
If I know that a mapping is symmetric, can I make the checks for bilinearity shorter? Something like that:</p>

<p>$F(\lambda a + b, c) = \lambda F(a, c) + F(b, c)$?</p>

<p>If it is not possible, do you have counterexamples where it doesn't work?</p>
",<linear-algebra>
"<p>Let $V$ a vector space and $W$ be its linear subspace. Give an example of a linear map that satisfies $\mathrm{im}(f)=W$ and $\ker(f) \oplus \mathrm{im}(f)=V$, but $f^2 \neq f$.</p>

<p>Would $f(v)=2v$ be the right example? Since the kernel of it is $0$ and the map itself is surjective so the condition $\ker(f)\oplus \mathrm{im}(f)=V$ satisfied, also $W=V$ in this case and $f^2 \neq f$ is also satisfied.</p>

<p>A next question is once I restrics $f$ to $W$ then will it be always isomorohism. My approach was that the map will be surely surjective, since $f:W\rightarrow W$ and $W=\mathrm{im}(f)$. But the injectivity I check by using the given $\ker(f)\oplus \mathrm{im}(f)=V$. Can someone help me on that. I am stuck. Please.</p>
",<linear-algebra>
"<p>I want to show the following:</p>

<p>Let $A,B \subseteq \mathbb{R}^n$ disjoint, nonempty, closed and convex sets. Then there exists a $h \in \mathbb{R}^n$, such that $A$ and $B$ gets separated in the following way:
$$
 \langle b, h \rangle \le \langle a, h \rangle \quad \forall a \in A, b \in B.
$$
I have the following proof: Consider $C := B - A$, which is convex too. Because $A$ and $B$ are disjoint, it must be that $0 \notin C$. (*) Then there exists a $h$ such that $\langle c, h \rangle \le 0$ for all $c \in C$ or $\langle c, h \rangle \ge 0$ for all $c \in C$. WLOG let $\langle c, h \rangle \le 0$, then
$\langle b - a, h \rangle \le 0$, which means $\langle b, h \rangle - \langle a, h \rangle \le 0$, i.e. $$\langle b, h \rangle \le \langle a, h \rangle.$$</p>

<p>But (*) uses the fact that: For every convex set $X$ and a point $u \notin X$, there exists a $h$ such that $\langle u, h \rangle = 0$ and $\langle x, h \rangle \le 0$  for all $x \in X$ or $\langle x, h \rangle \ge 0$  for all $x \in X$.</p>

<p>Which I feel is geometrically true because the Elements $h$ could be identified with hyperplanes, but I am not sure how to proof this?</p>
",<linear-algebra>
"<p>Show that the set of functions $1,x,x^2,x^3...x^n...$ is linearly independent on any interval $[a,b]$.</p>

<p>If $$c_1+xc_2+x^2c_3+x^3c_4...=0$$ we should show $$c_i=0,\quad i=1,2, \ldots$$
how could I start?</p>

<p>My second question: is it linearly independent on $C[0,1]$?</p>
",<linear-algebra>
"<p>Suppose I have an infinite set $U$ and let $M$ be the linear subspace of all real-valued functions $\nu$ on $2^U$ such that $\nu(\emptyset) = 0$. Here the sum of two such functions (and the product of such a function by a scalar) is taken simply; i.e. pointwise. </p>

<p>Now fix a member $\bar{\nu} \in M$. I am interested in linear transformations $T_{\bar{\nu}}$ of $M$ into itself of the following form: $$(T_{\bar{\nu}}\nu)(S) = \bar{\nu}(S)(\nu(S)), \quad S \subset U.$$ I would like to know if these types of transformations (or equivalent) have been studied before and, if so, under what name. Note I am not requiring that $\bar{\nu}$ be a measure, or even an outer measure, but would be willing to start there.</p>

<p>Thanks in advance for any pointers.</p>
",<linear-algebra>
"<p>I am taking linear algebra, and have learned about the vector dot product and cross product.  Is there a vector product defined by :
    $(a_1, a_2, \dots ,a_n)\times (b_1, b_2, \dots,b_n) = (a_1b_1, a_2b_2, \dots ,a_nb_n)$ ?
If so, what it it called?</p>
",<linear-algebra>
"<p>Suppose $v_1, \dots v_m$ is a linearly independent list in V. Show that there exists $w \in V$ such that $\langle w, v_j \rangle &gt; 0$ for all $j \in {1, \dots ,m}$.</p>

<p>I understand this question is saying given a linearly independent list, there is $w \in V$ such that the vector $w$ is not orthogonal to any $v$ in that linearly independent set. I'm also confused as to why it is significant that the inner product be greater than zero and instead of just $\neq 0$. Can someone give me a hint on how to do this problem?</p>

<p>I know that $\langle v, v \rangle &gt;0$ for all $v$ not equal to zero, and since $v_1, \dots v_m$ is linearly independent, then none of the $v_j$ will be zero, but it is impossible to have w equal to all $v_j$?</p>
",<linear-algebra>
"<p>It is said that the kernel of a isogeny is finite because it is discrete and complex tori are compact.</p>

<p>I have some questions about this.</p>

<p>1.</p>

<p>Following is my reason for the kernel is discrete.</p>

<p>Suppose
$$
\varphi:\mathbb{C}/\Lambda\rightarrow\mathbb{C}/\Lambda'
$$</p>

<p>is an isogeny. Then there exists $m\in \mathbb{C}$ such that $m\Lambda=\Lambda'$. So the kernel of $\varphi$ is $\left(\frac{1}{m}\Lambda'\right)/\Lambda$. Intuitively, it is discrete, I think. But I don't know how to reason it. </p>

<p>There is a hint in the book I'm reading saying that if the kernel is not discrete, complex analysis shows that the map is zero.</p>

<p>Can anyone tell me why?</p>

<p>2.</p>

<p>Why can we deduce finiteness from discreteness and compactness?</p>
",<number-theory>
"<p>Ok,this problem might appear a bit trivial but I have some doubts..If it's not a burden take a look and comment!</p>

<p>Let $F$ be a finite field of characteristic equal to $p$ and $ƒ(x)=x^p-α$ $∊F[x]$.Show that $ƒ(x)$ either has one root of multiplicity equal to $p$ or that $ƒ(x)$ is irreducible over $F$.</p>

<p>My answer:</p>

<p>Let $x₁$ and $x₂$ be two different roots of $ƒ(x)$.
Then it follows: $x₁^p-α=x₂^p-α=0$ ⇒$x₁^p-x₂^p=0$ 
But since the characteristic of the field is $p$,it derives from Euler's theorem that in general: $(b-c)^p=b^p-c^p$ for any $b,c∊F$</p>

<p>Thus:$(x₁-x₂)^p=0⇒x₁=x₂$.
So $ƒ(x)$ can have no distinct roots.
If $α$ is a root,then indeed $(x-α)^p=0$ and $α$ is a root of multiplicity equal to $p$.
Since no other factorization of $ƒ(x)$ exists-that is one that does not include the $(x-α)$ factor,it follows that if $α$ is not a root,then $ƒ(x)$ is irreducible.</p>
",<number-theory>
"<p>I have 2 numerical series like this:</p>

<p>$$
144 + 25 + 27 + 29 + 31 + \cdots
$$</p>

<p>$$
133 + 3 + 5 + 7 +9 +11+13+\cdots
$$</p>

<p>Is there a efficient way to find the common sum of these patterns?</p>

<p>solution for this case:
$$
144 + 25 + 27 = 133 +3+5+\cdots+15$$</p>
",<number-theory>
"<p>I am <a href=""https://github.com/gazman-sdk/quadratic-sieve"" rel=""nofollow"">developing</a> the <a href=""https://en.wikipedia.org/wiki/Quadratic_sieve"" rel=""nofollow"">quadratic sieve</a> algorithm and I reached a new bottle neck: The matrix processing.</p>

<p>I been reading quit a lot about this topic and I found many solutions</p>

<ul>
<li><a href=""https://en.wikipedia.org/wiki/Gaussian_elimination"" rel=""nofollow"">Gaussian elimination</a>: This perhaps the most common approach for this problem. It's running time is $O(N^3)$ above GF (2)</li>
<li><a href=""https://en.wikipedia.org/wiki/Method_of_Four_Russians"" rel=""nofollow"">Method of Four Russians</a>: It optimize the classic <strong>Gaussian elimination</strong> to $O(\frac{N^3}{\log{N}})$ by partition the matrix into small square blocks of size $\log N$. You can read more about it <a href=""https://martinralbrecht.files.wordpress.com/2011/11/ple.pdf"" rel=""nofollow"">here</a>(section 3.2). </li>
<li><a href=""https://en.wikipedia.org/wiki/Block_Wiedemann_algorithm"" rel=""nofollow"">Block Wiedemann algorithm</a> is used to parallelize the matrix of several machines in order to speed up the process. Here is a good example of such <a href=""https://www.cs.umd.edu/~gasarch/TOPICS/factoring/fastgauss.pdf"" rel=""nofollow"">implementation over GAPP</a>, they use a little bit different algorithm for that.</li>
</ul>

<p>But non of the above was good enough for me, so I created my own <a href=""https://github.com/gazman-sdk/quadratic-sieve/blob/master/src/com/gazman/factor/matrix/HashMatrix.java"" rel=""nofollow"">algorithm</a>.</p>

<p>It's based on <strong>Gaussian elimination</strong>, but instead of searching for pivot in each column I represent the columns with <a href=""https://en.wikipedia.org/wiki/Hash_table"" rel=""nofollow"">hash table</a> and <a href=""https://en.wikipedia.org/wiki/Linked_list"" rel=""nofollow"">linked list</a>(This is how HashMap in java is implemented) where each node of the list is stored in <strong>hash table</strong> and contain the pivot index of the original matrix. So I can iterate over all the pivots of column. </p>

<p>When I start to perform <strong>Gaussian elimination</strong> it only takes $O(1)$ to find the first pivot of a column, once I find such pivot I store it in another <strong>hash  table</strong>, so I want pick it for the next column I will operate. Next I iterate over the reset of the pivot of the first column and xor the rows to eliminate all the reset of the pivot of the first column. I do the same trick with the rows, so I got duplicate representation of the matrix, by hash-table rows and columns. This allows me to speed up the row xoring process as I only iterate over the row pivot. The xor operation itself of each node is $O(1)$ but it evolves 2 hash operations and it's by it self quite expensive. And so this is the part where this algorithm fails. Even so that I estimate it's performance as sub-quadratic on average. </p>

<p>I ended up <a href=""https://github.com/gazman-sdk/quadratic-sieve/blob/master/src/com/gazman/factor/matrix/BitMatrix.java"" rel=""nofollow"">implementing</a> classic <strong>Gaussian elimination</strong> using bit operators and it allows me to process square matrices with up to $10,000$ lines in minutes time. But it's not fast enough for me, my goal is factoring 100 digits number, this require around 1M square matrix size, and this impossible with my current approach. </p>

<p>Is there any other methods that I missed? What is the best non parallel method to solve this problem?</p>
",<number-theory>
"<p>Let $M$ be a finitely generated $\mathbb Z_ \ell$-module, where $\ell$ is a prime number and $\mathbb Z_\ell$ is the ring of $\ell$-adic integers. Let $T$ be its torsion submodule. Is $T$ finite?</p>

<p>In other words, let $f$ be a non-zero element of $\mathbb Z_ \ell$. Is the quotient module $\mathbb Z_ \ell/ (f)$ finite?</p>

<p>My guess is that we can assume $f= \ell^n$ for some non-negative $n$ (as units don't change the quotient). Then the quotient module $\mathbb Z_ \ell/ (f)$ should just be $\mathbb Z/(\ell^n)$. Is that correct? What triviality am I missing?</p>
",<number-theory>
"<p>Problem: Find all $n\in \mathbb{N}$ such that $f(x)=x^n+4$ is reducible in $\mathbb{Z}[x]$.</p>

<p>It seems $n=4k$ is the only one (the factorization follows easily from Sophie Germain's identity in this case), but I can't prove it. I can prove, however, that if $f(x)=g(x)h(x)$ for non-constant integer polynomials $g(x),h(x)$, then their constant terms, say $a_0,b_0$, must satisfy $a_0=b_0=\pm 2$.</p>

<p>EDIT: I am still looking for an elementary solution which only uses olympiad tools. It will be viewed in higher regard than one which uses linear/abstract algebra. Thanks!</p>
",<number-theory>
"<p>What is an efficient algorithm to find the first number $n$ such that $n^2 \equiv -1 \mod p$ for a prime $p$, if such an $n$ exists?</p>

<p>Is there anything better than the brute-force approach up to $p-1 \over 2$?</p>

<p>I know this is simple to find for primes of the form $n^2+1$ because $n^2 \equiv -1 \mod (n^2+1)$, resulting in $n$, but is there a fast way for a generic case $n$?</p>

<p>Ex:</p>

<ul>
<li>$p = 29, n = \pm 12$ </li>
<li>$p = 37, n = \pm 6$ </li>
<li>$p = 41, n = \pm 9$</li>
<li>$p = 53,
   n = \pm 23$</li>
</ul>
",<number-theory>
"<p>I don't know whether the books metioned in <a href=""http://math.stackexchange.com/questions/329/best-ever-book-on-number-theory"">Best ever book on Number Theory</a> are beyond undergraduate/high-school-olympiad level.</p>

<p>Please recommend your favourite.</p>
",<number-theory>
"<blockquote>
  <p>Let $e(x)$ be the number of $1$'s in the ternary representation of $x$. Let $A(n)$ be the number of integers $x$ with $0 \leq x \leq 3^n$ such that $3 \mid e(x)$. Prove that $A(n)$ can be expressed in the form $$A(n) = \sum_{\mu \geq 0} c_n^{3 \mu} 2^{n-3\mu}.$$</p>
</blockquote>

<p>I thought that $A(n) = 2^n+\binom{n}{3}2^{n-3}+\cdots$ by a counting argument. Where does the $c_n^{3 \mu}$ come from?</p>
",<number-theory>
"<p>How can I prove that $ 10200300040000100004000300201$ is not a perfect square ? This number is divisible with $3$ only one time. Is it a good reason and it is enough ? </p>

<p>thanks :)</p>
",<number-theory>
"<p>Whether a Prime number greater than can be written as sum of a Prime number and $2^n$?</p>

<p>$P_2 = P_1 +  2^N$</p>

<p>Some Examples of this
<Br/>
$3=2+2^0$<br/><br/>
$5=3+2^1$<br/>
<Br/>$1021=509+2^9$</p>
",<number-theory>
"<p>I just read this paragraph: (written by G. H. Hardy, on Ramanujan)</p>

<blockquote>
  <p>I remember once going to see him when he was lying ill at Putney. I
  had ridden in taxi cab number 1729 and remarked that the number seemed
  to me rather a dull one, and that I hoped it was not an unfavorable
  omen. ‘No,’ he replied, ‘it is a very interesting number; it is the
  smallest number expressible as the sum of two cubes in two different
  ways.’</p>
</blockquote>

<p><em>Was Ramanujan right?</em></p>

<p><em>What are other numbers having such property (expressible as the sum of two cubes in two different ways)?</em></p>

<p><em>Are there infinite number of them?</em></p>

<p>And, on the other hand:</p>

<p><em>What if the word ""cubes"" is replaced by ""5-degree power""? Would such numbers exist? If yes, what would be the smallest?</em></p>

<hr>

<p>Another SO question related to 1729: <a href=""http://math.stackexchange.com/questions/487537/proof-that-1729-is-the-smallest-taxicab-number"">Proof that 1729 is the smallest taxicab number</a></p>
",<number-theory>
"<p>Prove that a natural number with at least 2 digits cannot be written like a sum with the the power of digits equal $2$. </p>

<p>What I want to say: $$\overline{ab}\neq a^2+b^2.$$</p>

<p>What I have done: </p>

<p>$$10a+b=a^2+b^2 $$ or 
$$a(a-10)=b(1-b).$$
$b(1-b)=2k$ so $a(a-10)=2k$ and this is possible only when $a=2q.$ </p>

<p>so: $2 \cdot 8 =b(b-1)$ or $4\cdot 6=b(b-1)$ and this is not possible. 
final conclusion for the number $\overline{ab}$ is ok, but what can I do for number formatted with $3,4, \ldots$ digits ? </p>

<p>thanks :) </p>
",<number-theory>
"<p>Bertrand's postulate states that for any integer $n&gt;3$, there's always a prime $p$ between $n$ and $2n-2$.
That result sets a reasonable 'lower bound' on how often we can expect primes to show up, and there are even better estimates (such as the Prime Number Theorem, although that one is an asymptote, and doesn't prove any result for a specific $n$).</p>

<p>However, the proofs of the results above aren't obvious; I'm looking for elementary claims, ones that follow straight from the definition of a prime number (and perhaps some algebraic manipulation).</p>

<p>For example, the following can be deduced using Euclid's method for the infinitude of primes:</p>

<p><strong>Claim:</strong> Let $M_n$ Denote the product of all primes smaller than or equal to $n$. Then, if $n \geq 2$, there's at least one prime $p$ such that $n&lt;p\leq M_n+1$.</p>

<p><strong>Proof:</strong> If $M_n+1$ is prime, we're done.</p>

<p>Otherwise, $M_n+1$ is divisible by a smaller prime $p$. Then $p$ can't divide $M_n$. but $M_n$ is divisible by all numbers smaller than or equal to $n$, so we get $n&lt;p&lt;M_n+1$, just as we wanted. $\square$</p>

<p>In particular, this shows that there's always a prime number between $n$ and $n!$.</p>

<p>Now, how can we lower this bound? Is there any elementary proof for how there's always a prime between $n$ and $n^2$? What about, say,  $n$ and $1000^n$?</p>
",<number-theory>
"<p>I was given that $\sum_{p\le x} \frac{1}{p}$ = $\log\log x$+O(1). </p>

<p>I need to show that $\sum_{pq\le x} \frac{1}{pq} = (\log \log x)^2 + O(\log \log x)$.</p>

<p>Here we go:</p>

<p>Break the sum into two sums: $\sum_{p\le x} \frac{1}{p}\sum_{q\le \frac{x}{p}} \frac{1}{q}$</p>

<p>Using what I was given: $(\log \log x +O(1))(\log \log \frac{x}{p} +O(1))$</p>

<p>Log Rules: $(\log \log x +O(1))(\log( \log x - \log p) +O(1))$</p>

<p>Algebra: $\log \log x \cdot \log(\log x - \log p) + O(\log \log x)$ </p>

<p>From here I am lost. Any ideas?</p>
",<number-theory>
"<p>I know there exist (even) residues that do not appear in the sequence <a href=""http://oeis.org/A005277"" rel=""nofollow"">(A005277)</a>, but of those that do, do any appear only once?</p>
",<number-theory>
"<p>Show that $\displaystyle\sum_{d|n}\mu(d)\phi(d)=0$ using only Dirichlet Convolution propertys (without multiplicative function concepts).</p>

<p>I suspect you have to use that  $1\ast \mu=I$ and $f\ast 1=id$ where $1(n)=1$, $id(n)=n$ for all $n$ and $I$ is the unity in the set of arithmetic function.
but not how to use this.</p>

<p>Note: $\phi$ and $\mu$ are Euler and Mobiüs function respectly.</p>
",<number-theory>
"<p>Let $K$ be a non-archimedean field of characteristic zero and $||.||:K\to \mathbb{R}_{\geq 0}$ be its absolute value.</p>

<p>Define the Washnitzer Algebra as: $$W_n=\{\sum_{u\in \mathbb{Z}_{\geq 0}^n} \in K[[X]]: \text{ there exists some } \rho&gt;1 \text{ such that } ||a_u||\rho^{|u|}\to 0 \text{ as } |u|\to \infty\}$$ where $|u|=u_1+u_2+\ldots+u_n$ for $u=(u_1,\ldots,u_n)$ and $\rho\in\mathbb{R}$.</p>

<p>Now, set $n=1$, so $$W_1=\{\sum_{n\in\mathbb{Z_{\geq 0}}}\in K[[X]]: \text{ there exists some } \rho&gt;1 \text{ such that } ||a_n||\rho^{n}\to 0 \text{ as } n\to \infty\} $$</p>

<p>The question is the derivation map on $W_1$ defined as $$\partial: W_1\to W_1, \qquad \sum_{n\in\mathbb{Z_{\geq 0}}} a_n X^n \to \sum_{n\in\mathbb{Z_{&gt; 0}}} na_nX^{n-1}$$ is a surjection or not?</p>

<p>Essentially, one needs to show that if $\sum_{n\in\mathbb{Z_{\geq 0}}}a_nX^n$ is in $W_1$ then we also have $\sum_{n\in\mathbb{Z_{&gt; 0}}} \frac{a_{n-1}}{n}X^n $ (which is the formal integration of the initial series) is also in $W_1$.</p>

<p>More spesifically, if there exists some $\rho&gt;1$ such that $$\lim_{n\to\infty} ||a_n|| \rho^n=0$$ then is there a $\rho_1$ such that  $$\lim_{n\to\infty} ||{\frac{a_{n-1}}{n}}||{\rho_1}^n=0$$ ??</p>
",<number-theory>
"<p>I'm a little stuck with the proof of a theorem I'm trying to understand. The theorem is as follows:</p>

<p>""For odd prime $p$, suppose for $\alpha \in Q_{p}$ (the p-adic rationals) that $|\alpha|_p=1$. Then $\exists\beta\in Q_p$ such that $\alpha=\beta^2\iff \exists\gamma\in Z/pZ$ such that $|\alpha-\gamma^2|_p&lt;1$.""</p>

<p>The proof is:</p>

<p>Suppose $\exists\gamma\in Q_p$ such that $|\alpha -\gamma^2|_p&lt;1$, (i.e. $\beta^2\equiv\alpha(modp)$ is soluble).
Now we construct a sequence $(\beta_n)$ by letting $\beta_1=\gamma$ and defining $\beta_n$ to satisfy:</p>

<p>$|\beta_n^2-\alpha|_p&lt;\frac{1}{p^n}$ and $|\beta_{n+1}-\beta_n|&lt;\frac{1}{p^n}$</p>

<p>If we take $\beta_n$ as given, then we take $\beta_{n+1}=\beta_n+\delta_n$, so that $\beta_{n+1}^2=\beta_n^2+2\beta_n\delta_n+\delta_n^2$, and it is sufficient to take $\delta_n=\frac{\alpha-\beta_n^2}{2\beta_n}$.</p>

<p>Conversely, the necessity is obvious if we choose $\gamma=\beta^2$. $\square$</p>

<p>I feel that I'm missing something which is stopping me from understand this. $|\beta_{n+1}-\beta_n|_p&lt;\frac{1}{p^n}\implies p^n$ divides $(\beta_{n+1} - \beta_n)$, and if $\beta_{n+1}=\beta_n+\delta_n$ then this must mean $\delta_n$ is divisible by $p^n$.</p>

<p>If we take $\delta_n$ as $\delta_n=\frac{\alpha-\beta_n^2}{2\beta_n}$, then this gives us $\beta_{n+1}^2=\alpha+\delta_n^2 \implies \beta_{n+1}^2-\alpha=\delta_n^2$. Then we have shown that $\beta_{n+1}-\alpha$ is divisible by $p^{n+1}$, i.e. that $|\beta_{n+1}^2-\alpha|_p&lt;\frac{1}{p^{n+1}}$.</p>

<p>Was this the aim of the proof? An inductive argument on the terms of $(\beta_n)$? If not what is it that I have misunderstood in this theorem? Many thanks in advance for any replies, I would love to understand this.</p>
",<number-theory>
"<p>Let $k$ be the completion of an algebraic number field at a prime divisor $\mathfrak{p}$. We note that $k$ is locally compact. Let $k^{+}$ be the additive group of $k$ which is a locally compact commutative group.</p>

<p>Tate's Thesis Lemma 2.2.1 states that</p>

<blockquote>
  <p>If $\xi \rightarrow \chi(\xi)$ is one non-trivial character of $k^{+}$, then for each $\eta \in k^{+}$, $\xi \rightarrow \chi(\eta\xi)$ is also a character. The correspondence $\eta \leftrightarrow \chi(\eta\xi)$ is an isomorphism, both topological and algebraic, between $k^{+}$ and its character group.</p>
</blockquote>

<p>The proof of this lemma is divided up into 6 steps, one step is to show that the characters $\chi(\eta\xi)$ are everywhere dense in the character group. Tate writes</p>

<blockquote>
  <p>$\chi(\eta\xi) = 1$, all $\eta \implies k^{+}\xi \neq k^{+} \implies \xi = 0$. Therefore the characters of the form $\chi(\eta\xi)$ are everywhere dense in the character group.</p>
</blockquote>

<p>My question is: How does he get from showing that the $\xi = 0$ to the the result that the $\chi(\eta\xi)$ are everywhere dense?</p>
",<number-theory>
"<p>META: I wrote the explanation for this problem assuming a monospace font... it might be easier to read if you copy and paste it into a text file and view it separately.
Or, if you know how, feel free to edit it to have a monospace font with automatic line breaks because I don't know how.</p>

<p>Let 4 variables $a,b,c,d$ be rationals in $[0,1]$ which, when multiplied by $255$, become integers. (That is, $a,b,c,d\in \{\frac{x}{255}\mid 0\leq x\leq 255,\ x\in\mathbb{Z}\}$.  Examples of valid values are $1/255$, $2/255$, $3/255$, etc.</p>

<p>The variables are related in one equation. I want to prove that there are no solutions to this equation, by which I mean there are no valid values for the 4 variables that will satisfy the equation.
$$\frac{ac + (1-a)bd}{a+(1-a)b} = \frac{1}{2}$$</p>

<p>Now I'm going to redefine $a,b,c,d$ to be non-negative integers in the domain $[0,255]$. The equation will still hold if I add the denominator $255$ to the variables.
$$\begin{align*}
\frac{\frac{a}{255}\;\frac{c}{255} + \left(1-\frac{a}{255}\right)\frac{b}{255}\;\frac{d}{255}}{\frac{a}{255} + \left(1 - \frac{a}{255}\right)\frac{b}{255}} &amp;= \frac{1}{2}\\
\frac{\frac{ac}{255^2} + \frac{(255-a)bd}{255^3}}{\frac{a}{255}+\frac{(255-a)b}{255^2}} &amp;= \frac{1}{2}\\
\frac{\quad\frac{255ac + (255-a)bd}{255^3}\quad}{\frac{255a + (255-a)b}{255^2}}&amp;=\frac{1}{2}\\
\frac{255 ac + (255-a)bd}{255^3}\;\frac{255^2}{255a+(255-a)b} &amp;= \frac{1}{2}\\
\frac{255ac + (255-a)bd}{255(255a + (255-a)b)}&amp;=\frac{1}{2}\\
\frac{255 ac + (255-a)bd}{255^2a + 255(255-a)b}&amp;=\frac{1}{2}.
\end{align*}$$</p>

<p>$a,b,c,d$ are non-negative integers in the domain $[0,255]$.  Is it possible to prove that there are no solutions to this equation?</p>

<p>One way to determine this is to test all ($255^4=4228250625$) possible combinations, however I'm looking for a more compelling proof.</p>

<p>Both the numerator and denominator will each evaluate to a non-negative integer value. That being said, a part of the set of possible evaluated fractions will look like this:
$$\frac{1}{2}, \frac{2}{4}, \frac{3}{6},\frac{4}{8},\frac{5}{10},\frac{6}{12},\frac{7}{14},\frac{8}{16},\frac{9}{18},\frac{10}{20},\ldots$$</p>

<p>The denominator must evaluate to an even number.</p>

<p>Here are some of the rules of parity (even or odd) arithmetic:</p>

<p>Addition/subtraction:</p>

<pre><code>      Even Odd
     __________
Even |Even Odd
Odd  |Odd  Even
</code></pre>

<p>Multiplication:</p>

<pre><code>      Even Odd
     __________
Even |Even Even
Odd  |Even Odd
</code></pre>

<p>The denominator has only two variables $a$ and $b$ that I need to worry about. Let's consider the possible cases of parity and see which combinations result in an even number.</p>

<p>$$255^2a + 255(255-a)b$$
$$(\mathrm{Odd})a + (\mathrm{Odd})((\mathrm{Odd})-a)b$$</p>

<pre><code>$a$: Even; $b$: Even
(Odd)(Even) + (Odd)((Odd)-(Even))(Even)
(Even) + (Odd)(Odd)(Even)
(Even) + (Odd)(Even)
(Even) + (Even)
(Even)

a: Odd; b: Even
(Odd)(Odd) + (Odd)((Odd)-(Odd))(Even)
(Odd) + (Odd)(Even)(Even)
(Odd) + (Even)(Even)
(Odd) + (Even)
(Odd)

a: Even; b: Odd
(Odd)(Even) + (Odd)((Odd)-(Even))(Odd)
(Even) + (Odd)(Odd)(Odd)
(Even) + (Odd)(Odd)
(Even) + (Odd)
(Odd)

a: Odd; b: Odd
(Odd)(Odd) + (Odd)((Odd)-(Odd))(Odd)
(Odd) + (Odd)(Even)(Odd)
(Odd) + (Even)(Odd)
(Odd) + (Even)
(Odd)
</code></pre>

<p>Therefore, the denominator is only even when both $a$ and $b$ are even. Let's see the parity of the numerator with $a$ and $b$ both being even.</p>

<p>$$255ac + (255-a)bd$$</p>

<pre><code>(Odd)(Even)c + ((Odd)-(Even))(Even)d
(Even)c + (Odd)(Even)d
(Even)c + (Even)d
(Even) + (Even)
(Even)
</code></pre>

<p>Therefore, the numerator must be an even number as well, reducing the set of possible evaluated fractions to those with even numerators:
$$\frac{2}{4},\frac{4}{8},\frac{6}{12},\frac{8}{16},\frac{10}{20},\ldots$$</p>

<p>.. this is the furthest I could go with my insular analysis. Are there any other rules I could use to reduce the set of possible evaluated fractions down to 0?</p>
",<number-theory>
"<p>Prove that for any primitive Pythagorean triple (a, b, c), exactly one of a and b must be a multiple
of 3, and c cannot be a multiple of 3.</p>

<p><strong>My attempt:</strong></p>

<p>Let a and b be relatively prime positive integers.</p>

<p>If $a\equiv \pm1 \pmod{3}$ and $b\equiv \pm1 \pmod{3}$, </p>

<p>$c^2=a^2+b^2\equiv 1+1\equiv 2 \pmod{3}$</p>

<p>This is impossible as the only quadratic residues modulo 3 are 0 and 1.</p>

<p><em>So far, so good.</em></p>

<p>If one of a, b is $\equiv 0 \pmod{3}$ and the other is $\equiv \pm1 \pmod{3}$,</p>

<p>$c^2=a^2+b^2\equiv 0+1\equiv 1 \pmod{3}$</p>

<p><em>This is the part I don't understand. Just because $c^2\equiv 1\pmod{3}$ doesn't mean that $c^2$ must be a perfect square. For example, $a=12$ and $b=13$ satisfy the above conditions but $c^2=a^2+b^2=313$, which isn't a perfect square.</em></p>
",<number-theory>
"<p>I've been trying to figure this out and it's been getting on me myself.  I know that $3$ is not just a prime number, but also a triangular number.  I'll now add a sequence:</p>

<p>Prime numbers: $2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97, 101, 103, 107$
Triangular numbers: $1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 66, 78, 91, 105, 120, 136, 153, 171, 190, 210, 231, 253, 276, 300$</p>

<p>Anyway, let's cut to the chase.  Does this sequence help anything about which prime numbers are also triangular numbers?  Now I want to know from you, yeah, you.  How many prime numbers can also be triangular numbers?  I don't think it's probable.  If you have serious, stupendous answers, I would be glad to accept <em>one</em> of them.</p>
",<number-theory>
"<p>Given prime number $p\equiv 1 \pmod 4$. Prove  if $a∈F_p^×$ is a quadratic
residue then the congruence $$x^4 ≡ a \pmod p$$ has either no solutions or four solutions.
Give examples of each case.</p>
",<number-theory>
"<p>When is $f(k):=8k^2+8k+1$ a square for $k\in\mathbb Z_{\geq 0}$?</p>

<p>How do I begin on this? I see $f(k)$ is a square for $k=0,2$, but I do not know where to go from here.</p>
",<number-theory>
"<p>I'm writing a primality testing program and want to test numbers like $3^{3^{3^{100}}} + 4
\mod 7$.</p>

<p>I use Euler's theorem to do this, so that the exponents get smaller. Instead of calculating $a^{b^c} \mod n$, I calculate $a^{b^{c \mod n_2}\mod n_1} \mod n$, where $n_1 = \phi(n)$ and $n_2 = \phi(\phi(n))$. This works sometimes, however, sometimes the exponent and the modulo is not relativly prime and Euler's theorem can not be used. Like in the above example with $3^{3^{3^{100}}} + 4
\mod 7$, it works in the first step, but not in the second, because $\gcd(3,6)\neq 1$.</p>

<p>One mathematician told me many years ago that it doesnt matter, because if you calculate $a^{(b \mod n_1)+n_1} \mod n$, it will still equal $a^b \mod n$. Even in the cases where $\gcd(a, n)\neq 1 $. I have tested this and it seems to work.</p>

<p>My questions are:
Under what circumstances does this work? Are there any exceptions, like $a = 0$ or $1$ or anything?</p>

<p>What theorems does this trick rely upon? What is the formal proof that it works? I don't want to use math that I dont understand in my program and I dont really understand why this works.</p>

<p>The main idea is that the number that should be tested is large, and the modulo is small. However, if anybody could give any hints of how to do it when the modulo is large, like ($2^{300}+1$) it would also be appreciated. It might be good for further, more advanced testing. It is enough to know that $a^b \mod n = c $ with a certain probability, if $n$ is large. I feel that this should be possible to do in some way, but I don't know how.</p>

<p>Thanks for any help.</p>

<p>Note that I am looking for mathematical tricks, not programming, so please don't ask me to use Java's BigInteger package or anything like that.</p>

<p>Note 2: Before anybody complains that I am writing mod wrong. I am referring to the binary operation modulo, not modular congruence.</p>

<p><a href=""http://en.wikipedia.org/wiki/Modulo_operation"" rel=""nofollow"">http://en.wikipedia.org/wiki/Modulo_operation</a></p>

<p>They basically work the same, but the notation is a bit different.</p>
",<number-theory>
"<p>$$\varphi(n)=\sum_{d\mid n}d\cdot \mu\left(\frac{n}{d}\right)=n\sum_{d\mid n}\frac{\mu(d)}{d}$$</p>

<p>This describes the totient function in terms of the Möbius function. I understand what the Möbius function does but I don't understand this derivation at all. Is there an easy way to understand why this is so?</p>

<p>I'm not looking for some lengthy mathematical proof, but rather an intuitive understanding. I've seen plenty of papers showing the proof, but I just don't get what's happening.</p>
",<number-theory>
"<blockquote>
  <p>If $a$ and $b$ are positive integer then what is length of digits of $a^b$?</p>
</blockquote>

<p>I have worked so far and formula works fine.</p>

<p>To find the exact length of digits of $a^b$ where $a\gt 0, b\gt 0$:</p>

<p>Number of Digits are = $\lfloor 1+b\ log(a) \rfloor$</p>

<p>Either it is true of any positive integers $a$ and $b$ or it's accuracy is limited?</p>

<p>Tell me if is there any other method to find the number of digits of $a^b$?</p>
",<number-theory>
"<p>I came across this question while looking at powers of 2 and investigating number theory. I found it quite interesting, unfortunately I would say that my skills in number theory are far too primitive to solve this. My question is, is this true?
$$i \in \mathbb{N} \land \exists j \in \mathbb{N}, 2j = i \implies \exists x \in \mathbb{N}, 3x + 1 = 2^i$$</p>

<blockquote>
  <p>$\frac{2^i - 1}{3}$ is an integer only when $i$ is even</p>
</blockquote>

<p>I think it has something to due with the fact that $2^1 = 2$ is 1 away from $3$ and $2^2 = 4$ is $1$ away from 3 but in the other direction. I know this has to do with modular arithmetic (or at least I'm guessing) and the fact that I guess when you subtract $2^i$ by $1$ and $i$ is even, then you bring it down one mod cycle to become divisible by $3$, and then multiplying $2^i$ by $4$ doesn't affect the mod cycle of $2^i - 1$ against $3$ (i.e. its divisibility by $3$)</p>
",<number-theory>
"<p>Let $f: \mathbb{N}\rightarrow \mathbb{R}$ be a function potentially taking negative values. Suppose I knew that $$cN \leq \left|\sum_{n= 1}^{N}f(n)\right|$$ for some absolute constant $c$. Then by the Pigeonhole Principle, there is at least one integer $n_0 \in [1, N]$ such that $|f(n_0)| \geq c/2$. Can I say more? Do I know that a positive proportion of the integers $n \in [1, N]$ satisfy $|f(n)|\geq c/2$?</p>
",<number-theory>
"<p>In the extant books of Diophantus, are considered in the system of equations.  Of interest is the non-linear system of Diophantine equations.  Some simple systems from his book manages to solve it.</p>

<p>For example, from 3 books tasks 10, 11. 4 books   task 19. The solution presented in this.  <a href=""http://www.artofproblemsolving.com/community/c3046h1057324_the_system_is_almost_linear_diophantine_equations"" rel=""nofollow"">http://www.artofproblemsolving.com/community/c3046h1057324_the_system_is_almost_linear_diophantine_equations</a></p>

<p>But here's an example of a decision like almost linear system of 3 task book 6.  Gives another view of the representation of solutions.  <a href=""http://www.artofproblemsolving.com/community/c3046h1055253_the_system_of_equations_15"" rel=""nofollow"">http://www.artofproblemsolving.com/community/c3046h1055253_the_system_of_equations_15</a></p>

<p>But there is in the books such a group of tasks which are of the same type. The conditions are very similar.  It is conceivable that they can be solved one way.</p>

<p>For example with 2 books tasks 20, 21.  This system was considered Sierpinski. He found a private decision. And in the book of Diophantus referred to the decision according to his formulas don't work.  It turned out that the formulas of the solutions may be several.
<a href=""http://www.artofproblemsolving.com/community/c3046h1046718__4"" rel=""nofollow"">http://www.artofproblemsolving.com/community/c3046h1046718__4</a></p>

<p>Now interested in the question itself.</p>

<p>In the 2nd book is very much the same type of systems that can be described as follows. I wrote to look for solutions in integers.</p>

<p>$$\left\{\begin{aligned}&amp;X^2\pm{(X+Y)q}=Z^2\\&amp;Y^2\pm{(X+Y)q}=R^2\end{aligned}\right.$$</p>

<p>$$\left\{\begin{aligned}&amp;XY+(X+Y)q=Z^2\\&amp;XY-(X+Y)q=R^2\end{aligned}\right.$$</p>

<p>$$\left\{\begin{aligned}&amp;(X+Y)^2\pm{Xq}=Z^2\\&amp;(X+Y)^2\pm{Yq}=R^2\end{aligned}\right.$$</p>

<p>The system is very similar and right of common and more simple approach to their solution.  Because then there are systems with a large number of equations.</p>
",<number-theory>
"<p>We are interested in the following statement: </p>

<p>For each $n&gt;1$ and $x&gt;2$ there is at least one prime $p$ satisfying $x&lt;p&lt;n x$.</p>

<p>For $n=2$ we get precisely the Bertrand's postulate which is true. As corollaries, the statements for arbitrary $n\geqslant 2$ are true. However, I am interested maybe there exist independent proofs (prossibly short and elementary) for some of the cases $n&gt;2$.</p>

<p>Thanks for your help.  </p>
",<number-theory>
"<p>Is there a prime number $p$ that $p &gt; 2$, and in which $p$ is a never a factor of any Carmichael number $C_n$:</p>

<p>(p ∤ $C_n$)</p>

<p>Extended this to all numbers $m$, instead of just $p$, will prove the $p$ problem too. (m ∤ $C_n$)</p>

<p>After a quick glance at some Carmichael number factors, $p$ must be greater or equal to $53$.</p>
",<number-theory>
"<p>Working needs to be shown
$\sqrt{\sqrt{5}+3}+\sqrt{\sqrt{5}-2}$
My guess is to multiply by  $\sqrt{\sqrt{5}+3}-\sqrt{\sqrt{5}-2}$ then we have a rational number but is it enough to prove the rationality of a number?</p>
",<number-theory>
"<p>Prove that $(\mathbb Z/m\mathbb Z)^\times$ is cyclic <strong>if and only if</strong> there is a primitive root modulo $m$.</p>

<p>if $g$ is a primtive root modulo $m$ so indeed  $(\mathbb Z/m\mathbb Z)^\times$ is cyclic by definition , but I don't understand why the other direction is right:</p>

<p>if  $(\mathbb Z/m\mathbb Z)^\times$ is cyclic then it has a generator g, but how do we know that $\gcd (m,g)=1$ and that $g\in (\mathbb Z/m\mathbb Z)^\times$  ?</p>
",<number-theory>
"<p>Find all positive integers $a, b, c$ such that $a^2+1$ and $b^2+1$ are both primes and
$$(a^2+1)(b^2+1)=c^2+1$$</p>

<p>What I have done:</p>

<p>It is obvious that $a^2+1$ and $b^2+1$ cannot be both 2, so assume, WLOG, that $a^2+1=2$. Then, $a=1$
$$2(b^2+1)=c^2+1 \Rightarrow 2b^2+1=c^2 \Rightarrow b=2, c=3$$
Hence, we have 2 sets of solutions $(a, b, c)=(2, 1, 3); (1, 2, 3)$
If both $a^2+1$ and $b^2+1$ are odd primes, then $a, b, c$ are all even numbers.</p>

<p>I tried using Fermat's Infinite Descent by letting $a=2a_1, b=2b_1, c=2c_1$ but I seem to reach a dead end. Can anyone help me?</p>

<p>Thank you!</p>
",<number-theory>
"<p>I just wondered about the following question:</p>

<p>Suppose that we are given a homogeneous second-order recurrence relation, $x_{n+2}+ax_{n+1}+bx_n=0$ for all $n\in\mathbb{N}$. </p>

<p>Can we choose integers $a$, $b$, $x_0$ and $x_1$ in such a way that the resulting sequence $(x_n)$ will contain <em>infinitely many</em> primes?</p>

<p>Does anybody know anything about this? Or any ideas? I couldn't come up with much, having done some years of maths at uni.</p>
",<number-theory>
"<p>I would like to know whether or not a prime number $ p $ can be written in the form
$$
p = 3A + 2B,
$$
where $ A $ and $ B $ are positive integers.</p>
",<number-theory>
"<p>The Weyl equidistribution theorem states that the sequence of fractional parts ${n \xi}$, $n = 0, 1, 2, \dots$ is uniformly distributed for $\xi$ irrational. </p>

<p>This can be proved using a bit of ergodic theory, specifically the fact that an irrational rotation is uniquely ergodic with respect to Lebesgue measure. It can also be proved by simply playing with trigonometric polynomials (i.e., polynomials in $e^{2\pi i k x}$ for $k$ an integer) and using the fact they are dense in the space of all continuous functions with period 1.  In particular, one shows that if $f(x)$ is a continuous function with period 1, then for any $t$,  $\int_0^1 f(x) dx = \lim \frac{1}{N} \sum_{i=0}^{N-1} f(t+i \xi)$. One shows this by checking this (directly) for trigonometric polynomials via the geometric series.  This is a very elementary and nice proof.</p>

<p>The general form of Weyl's theorem states that if $p$ is a monic integer-valued polynomial, then the sequence ${p(n \xi)}$ for $\xi$ irrational is uniformly distributed modulo 1.  I believe this can be proved using extensions of these ergodic theory techniques -- it's an exercise in Katok and Hasselblatt.  I'd like to see an elementary proof.</p>

<p>Can the general form of Weyl's theorem be proved using the same elementary techniques as in the basic version?</p>
",<number-theory>
"<p>I am so excited to learn finding integer solutions of the equation $x^2 -y^5 = x-y$. I just found few solutions by plugging various integers in place of $x$ and $y$. But, I need a permanent method or approach to find all most all OR as much as we can ""integer"" solutions of the cited above equation.
Kindly help.
with regards
Pokwishi</p>
",<number-theory>
"<p>Below is a proof that the cyclotomic polynomial $\Phi_n(x)=\prod_{d|n}(x^d-1)^{\mu(n/d)}$ using Möbius inversion. However, it requires that we take the log of a polynomial, which (to my knowledge) is not necessarily well defined. Is there any way to fix this, or rigorously define a logarithm of a polynomial?</p>

<blockquote>
  <p>We have $x^n-1=\displaystyle\prod_{d|n}\Phi_d(x)$. Taking the log of
  both sides yields:
  $$\log\left(\prod_{d|n}\Phi_d(x)\right)=\sum_{d|n}\log(\Phi_d(x))=\log(x^n-1).$$
  By Möbius inversion we have \begin{align*}\log(\Phi_n(x)) &amp;=
\log(x^n-1)*\mu \\ &amp;=\sum_{d|n}\log(x^d-1)\mu\left(\frac{n}{d}\right)\\ &amp;=\sum_{d|n}\log\left[(x^d-1)^{\mu(n/d)}\right] \\ &amp;=\log\left(\prod_{d|n}(x^d-1)^{\mu(n/d)}\right), \end{align*} and
  exponentiating both sides gives the desired result.</p>
</blockquote>
",<number-theory>
"<p>I am supposed to have proved the following congruence identity:
$$
1^{n} + 2^{n} + \cdots + (p - 1)^{n} \equiv 0 ~ (\text{mod} ~ p).
$$
This is apparently meant to help me solve the problem stated in the title. I have noticed that
$$
y^{2} \equiv x^{2} - D ~ (\text{mod} ~ p)
$$
has solutions if and only if $ x^{2} - D $ is a quadratic residue modulo $ p $. I have applied the Legendre symbol to show that it is equivalent to $ (x^{2} - D)^{(p - 1) / 2} ~ (\text{mod} ~ p) $. I know that I have to apply the Binomial Theorem to this, but I am not sure how. Even if I were sure, I still would not know how to produce $ (p - 1) $ solutions.</p>

<p>I can get the number $ p - 1 $ based on the fact there are $ (p - 1) / 2 $ squares in the group $ (\Bbb{Z} / p \Bbb{Z})^{*} $ and that there are two solutions for each one, but that is not the point. :/</p>
",<number-theory>
"<p>How to prove$\forall n\ge5 $ </p>

<blockquote>
  <p>$$\phi(n)\ge \frac{n}{6\log \log (n)} $$ </p>
</blockquote>

<p>$\phi$ is Euler function</p>

<p>Thanks in advance </p>
",<number-theory>
"<p>I found this interesting conjecture, but maybe I'm not the first to state it. I have tested it for the first $10^4$ positive integers, but that is not a proof. Can anybody prove or disprove this conjecture?</p>

<blockquote>
  <p><strong>Every positive integer can be written as the sum of 1 square number, 1 pentagonal number, and 1 hexagonal number.</strong></p>
</blockquote>

<p><strong>Note</strong>:</p>

<p>Square numbers are generated by the formula, $S_{n}=n^{2}$.
The first ten square numbers are:</p>

<pre><code>0, 1, 4, 9, 16, 25, 36, 49, 64, 81,...
</code></pre>

<p>Pentagonal numbers are generated by the formula, $P_{n}=\frac{1}{2}n(3n-1)$.
The first ten pentagonal numbers are:</p>

<pre><code>0, 1, 5, 12, 22, 35, 51, 70, 92, 117,...
</code></pre>

<p>Hexagonal numbers are generated by the formula, $H_{n} = n(2n-1)$.
The first ten Hexagonal numbers are:</p>

<pre><code>0, 1, 6, 15, 28, 45, 66, 91, 120, 153,...
</code></pre>

<hr>

<p>Here are the solutions for the first 10 positive integers.</p>

<p>Numbers    =     Square + Pentagon + Hexagon </p>

<pre><code>1    =    0    +    1    +    0
2    =    1    +    1    +    0
3    =    1    +    1    +    1
4    =    4    +    0    +    0
5    =    0    +    5    +    0
6    =    1    +    5    +    0
7    =    1    +    5    +    1
8    =    1    +    1    +    6
9    =    4    +    5    +    0
10   =    9    +    1    +    0
</code></pre>
",<number-theory>
"<p>As is well-known, $Z[\sqrt{-5}]$ is not a ufd because $6$ has more than one prime factorization in this ring: $6=2\cdot 3$ and $6=(1+\sqrt{-5})(1-\sqrt{-5})$. But both of these prime factorizations have the same number $(2)$ of prime factors...Am I correct that in $Z[\sqrt{-29}], 30=2\cdot 3\cdot 5$ and $30=(1-\sqrt{-29})(1+\sqrt{-29})$ are prime factorizings of $30$ that have different numbers of factors? </p>

<p>Also would $Z[\sqrt{-2309}]$ give as distinct prime factorizations $2310=2\cdot 3 \cdot 5\cdot 7\cdot 11=(1+\sqrt{-2309})(1-\sqrt{-2309})$? ($2309$ is a prime number). </p>

<p>What about $Z[\sqrt{-30029}]$, would that give $30030=2\cdot 3\cdot 5\cdot 7\cdot 11\cdot 13=(1+\sqrt{-30029})(1-\sqrt{-30029})$ as distinct prime factorizations?($30029$ is prime)...Does this show the number of primes in distinct prime factorizings be different? I'm worried that the norms will cause some of my ""primes"" to be nonprimes. Thanks.</p>
",<number-theory>
"<p>For any prime $p &gt; 2 $ and any positive integer a with GCD$(a, p) = 1$ the
Legendre symbol for $a$ and $p$ is</p>

<p>$$\text{Leg}\Big[\dfrac{a}{p} \Big] = 1 \text{ if } a\text{ is a quadratic residue modulo }p,$$</p>

<p>$$\text{Leg}\Big[\dfrac{a}{p} \Big] = -1 \text{ if } a\text{ is a quadratic nonresidue modulo }p,$$</p>

<p><b>Jacobi Symbol</b></p>

<p>Let $n = p_1^{k_1}\cdot p_2^{k_2}\cdot ... \cdot p_l^{k_l}$ be the factorization of an odd integer $n \geq 3$. For all positive integers $a$ with GCD$(a, n) = 1$, the Jacobi symbol of $a$
and $n$ is</p>

<p>$$
	\text{Jac}\Big[\frac{a}{n}\Big] = \prod_{i=1}^{l}\Big(\text{Leg}\Big[\frac{a}{p_i}\Big]\Big)^{k_i} =   \prod_{i=1}^{l}\Big(\text a^{\tfrac{p_i-1}{2}} \text{ mod } p_i \Big)^{k_i}.
$$  </p>

<hr>

<p>It has been hours I am trying to prove the following equations but I failed:</p>

<p>(1) Jac $\Big[\dfrac{2}{n} \Big] = -1 \text{ forall } n \text{ mod } 8 \in \{3, 5\}$ AND Jac $\Big[\dfrac{2}{n} \Big] = 1 \text{ forall } n \text{ mod } 8 \in \{1, 8\}$</p>

<p>(2) Jac $\Big[\dfrac{a}{n} \Big] = (-1)^{\tfrac{a-1}{2}.\tfrac{n-1}{2}}.\text{ Jac } \Big[\dfrac{n}{a} \Big]$ for all odd $a$</p>
",<number-theory>
"<p>I need some help in solving the following problem:</p>

<p>Suppose $K$ is a number field and $K=\mathbb{Q}(\theta)$ where $\theta\in\mathfrak{O}_K$, the ring of integers of $K$. Now among the elements in $\mathfrak{O}_K$ of the form $$\frac{1}{d}(a_0+\cdots+a_i\theta^i)$$($0\ne a_i$; $a_0,\ldots a_i\in\mathbb{Z})$, where $d$ is the discriminant of $K$, pick one with the minimum value of $|a_i|$ and call it $x_i$. Do this for $i=1,\ldots ,n=\dim_{\mathbb{Q}}K$. I need to show that $\lbrace x_1,\ldots ,x_n\rbrace$ is an integral basis for $K$.</p>

<p>Here are my thoughts: If we can show that the discriminant of $\lbrace x_1,\ldots ,x_n\rbrace$, which can be shown to be a $\mathbb{Q}$-basis, is less than or equal to $d$ then we are done; I tried to show this but have not succeed. I have to somehow use the fact that $|a_i|$ is the minimum which I am unable to see how. Any help will be appreciated.</p>
",<number-theory>
"<p>I am wondering if there is a way to prove the following statement, which bears some resemblance to Zsigmondy's Theorem. I am not sure if the statement is true, but it seems as though it should be. </p>

<p>For any prime $p$, there is some positive integer $n$ such that $p^{2^n}-1$ is divisible by two distinct primes that do not divide $p^k-1$ for any $k\in\{1,2,\ldots,2^n-1\}$. </p>

<p>Indeed, Zsigmondy's Theorem guarantees that $p^{2^n}-1$ will be divisible by some prime that does not divide $p^k-1$ for any $k\in\{1,2,\ldots,2^n-1\}$ no matter the choice of $n$ (except, perhaps, if $n=1$ and $p$ is a Mersenne prime). 
One might also phrase the problem as follows: </p>

<p>Let $p$ be a prime. Show that there is some positive integer $n$ such that $ord_{q_1}(p)=ord_{q_2}(p)=2^n$ for distinct primes $q_1$ and $q_2$.  </p>
",<number-theory>
"<p>Prove that the prime factors of $510510^{510510} + 1$ are greater than or equal to 19.</p>

<p>Here is my (incomplete) proof that I need help with:<br>
1. The prime factors of 510510 are 2, 3, 5, 7, 11, 13 and 17.<br>
2. Since $510510^{510510}$ is a multiple of 510510, the prime factors of $510510^{510510}$ are also 2, 3, 5, 7, 11, 13 and 17.<br>
3. ...  </p>

<p>How can I show that none of these prime factors are factors of $510510^{510510} + 1$? In other words, how can I show that the smallest possible prime factor of $510510^{510510} + 1$ must be greater than the largest prime factor of $510510^{510510}$, which is 17?</p>
",<number-theory>
"<p>This question is inspired by the announcement of a proof that ""fake twin"" primes, i.e. pairs of consecutive primes differing by at most K, are -in infinite number- where K is a fixed integer which can be taken = 70,000,000.</p>

<p>It is not known OTOH whether the above would hold where K would be  = 2 instead (the twin prime conjecture).</p>

<p>Anyway, my question may be (hopefully is)  much simpler : it's been known after Viggo Brun that the sum of reciprocals of actual ""twin primes"" is finite (whether the number of terms in this sum is finite or infinite). Does the sum of reciprocals of the ""fake twins"" as defined above (for K=70 million) also converge ? Does it follow from an easy extension of Brun's method ?</p>

<p>I should make it clear I am no expert, just curious... I do not have accesss to a proof of Brun's original theorem, even a sketchy one.</p>
",<number-theory>
"<p>Prove (or disprove) the following statement:
For any positive integers $x,y,t$,</p>

<p>$\displaystyle\sum_{i=1}^{t(y+1)-1} \frac{1}{t(xy+x-1)-x+i}$</p>

<p>is an increasing function of $t$.</p>

<p>My attempts:
The statement appears to be true numerically.
Tried some obvious bounds to compare the sums for consecutive values of $t$ but didn't find one that was strong enough to prove the statement. </p>
",<number-theory>
"<p>It is quite well-known that,</p>

<p>$$1^2+2^2+\dots+24^2 = 70^2$$</p>

<p>Not so well-known is,</p>

<p>$$15^3+16^3+\dots+34^3 = 70^3$$</p>

<p>The formula for the sum of $m$ consecutive <em>squares</em> starting with $a^2$ is,</p>

<p>$$F(a,m) = (m/6)(6a^2-6a+6am+1-3m+2m^2)$$</p>

<p>while the sum of $n$ consecutive <em>cubes</em> starting with $b^3$ is,</p>

<p>$$F(b,n) = (n/4)(2b+n-1)(2b^2-2b+2bn-n+n^2)$$</p>

<p><strong><em>Question</em></strong>: Is the only solution in <em>positive integers</em> to the simultaneous equations,</p>

<p>$$F(a,m) = x^2$$</p>

<p>$$F(b,n) = x^3$$</p>

<p>given by $a,m;b,n;x = 1,\,24;\,15,\,20;\,70$? (I have searched within a relatively small range, but didn't find any new solution.)</p>
",<number-theory>
"<p>Consider a set of integers $Q$ such that the set of all positive integers $\mathbb{Z}$ is equivalent to the span of ever possible power tower</p>

<p>$$a_1^{a_2^{\ldots a_N}}$$ involving $a_i \in Q$.</p>

<p>In simpler terms. Take the integers, remove all square numbers, cube numbers, fourth powers, fifth powers, etc... And this remaining set is $Q$.</p>

<p>What is the density of $Q$ compared to positive $\mathbb{Z}$? Does it obey a theorem similar to the prime number theorem for primes? Are there infinity many numbers $x$, in $Q$ such that both $x$ and $2x$ are members of $Q$? Is there a formula for the elements of $Q$?</p>

<p>This is basically analogous to prime numbers except now it deals with exponents as opposed to multiplication.</p>
",<number-theory>
"<p>The Chebyshev functions are defined as $\psi(x) = \sum_{p^m \leq x} \log n$ and $\theta(x) = \sum_{p\leq x} \log p$, where $p$ is a prime, $m\geq 1$ is an integer and $n=p^m$ in $\psi(x)$. It is known that there exist positive constants $c_{1}$ and $c_2$ such that
                                                                                 $c_{1}x &lt; \psi(x) &lt; c_{2} x$ and $ \frac{1}{2}c_{1}x &lt; \theta(x) &lt; c_{2} x$.     By these bounds we find that
                                                                                  $  \dfrac{ \psi(x) - x}{\psi(x) - \theta(x)} &lt; \dfrac{(c_{2} -1)x}{(c_2 - c_1)x}  = \dfrac{c_{2} -1 }{c_2 - c_1} &lt;\infty $. </p>

<p>Hence invoking the well known result  $\mid \psi(x) - \theta(x) \mid = O(x^{\frac{1}{2}}\log x)$, it then follows that $\mid \psi(x) - x \mid = O(x^{\frac{1}{2}}\log x)$ ?</p>

<p>EDIT: It is also known that $\theta (x)$ tends to $x$ as $x$ tends to $\infty$. Surely, an impication of this is $\dfrac{ \psi(x) - x}{\psi(x) - \theta(x)}  &lt; \infty$ since $\psi(x) \neq \theta(x)$.</p>
",<number-theory>
"<p>Given large $n\in\Bbb N$ is there many $a,b\in(n,2n)$ with $\gcd(a,b)=1$ and $q,r\in(n^4,2n^4)$ with $\gcd(a,bq)=\gcd(ar,b)=1$ and $c,d\in(n^3,2n^3)$ with $-n&lt;-x=q\bmod c,-y=r\bmod d&lt;0$ with $\gcd(a,x)=1$ and $\gcd(b,y)=1$ such that if $u=ca^{-1}{b^2}\bmod q, v=db^{-1}{a^2}\bmod r$ then ${n}^{2+\beta}&lt;u,v&lt;{n}^{2+\beta'}$ at a fixed $0&lt;\beta&lt;\beta'\ll1$?</p>
",<number-theory>
"<p>I am looking for a fast pairing function which maps two integers (cartesian coordinates) to a single unique integer. In other words, 
$$
\mathbb{Z} \times \mathbb{Z} \rightarrow \mathbb{Z},
$$
thats has a one-to-one-correspondence (bijection). </p>

<p>I have found Cantor's pairing function, 
$$
f(x,y) = \frac{(x+y)\times(x+y+1)}{2}+y,
$$
which is,
$$
\mathbb{N} \times \mathbb{N} \rightarrow \mathbb{N},
$$
and a bijection, but since it is from $\mathbb{N} \times \mathbb{N}$ it only works for coordinates where x and y are both positive or zero.</p>

<p>Additionally I have found the 'elegant pairing function' written by <a href=""http://szudzik.com/ElegantPairing.pdf"" rel=""nofollow"">Matthew Szudzik</a> which suffers from the same issue.</p>

<p>There should be a way to modify Cantor's, or Szudzik's method so that it spirals around (0,0) as opposed to saw toothing through all of the positive integer pairs. </p>

<p>Is such a function already defined?</p>

<p>If not is there a good place to start in creating one?</p>

<p>What about,
$$
\mathbb{Z} \times \mathbb{Z} \rightarrow \mathbb{N}.
$$</p>
",<number-theory>
"<p>Is this a solution for the problem: $\ a^3 + b^3 = c^3\ $ has no nonzero integer solutions?    </p>

<p>Suppose $\ a^3 + b^3 = c^3,\ a,b,c \in \mathbb Z^*,\ $then:<br>
$c^3 - b ^ 3 = (c - b)((c - b) ^ 2 + 3cb) = a ^ 3 \quad (1)$<br>
<a href=""http://mathrefresher.blogspot.com/2005/05/coprime-numbers-xn-yn-zn.html"" rel=""nofollow"">We can assume that all variables are coprime</a>, because $\ c - b\ $ divides $\ 3cb,\ a\ $ and $\ c - b\ $ doesnt divides $\ c,\ b,\ $ so<br>
  $c - b = 3 \quad (2),$<br>
from $(1)\ $ and $\ (2)\ $   get $\ 3 (3 ^ 2 + 3 c(c - 3)) = 3^{3}x ^{3},\ c ^ 2 - 3c + 3 = 3x ^3$,<br>
here we see $3$ divides $\ c,\ $and we know $3$ divides $a$, this conflict by assuming.</p>

<p>Edit:</p>

<p>As Nishant commented: ""I don't see why $\ c−b\ $ divides $\ 3cb$...""<br>
Divide both side of  $(1)\ $ by $\   (c - b)\ $ get  $\ 3cb = (c - b)^{2}(x^{3} - 1)$    </p>

<p>Update:</p>

<p>If$~(c−b)~$ is a single prime or a product of distinct primes or $~(c−b)~\nmid~a~$ and $~(c−b)~$ isn't a cubic number, then $~(c−b)~$ contains factor$~m~$ of $~a,~$divide both side of  $(1)\ $ by $\ (c - b):$</p>

<p>$(c - b) ^ 2 + 3cb = (c-b)^{2}x^3 \quad (2),$ </p>

<p>from $~(2)~$ if $~m=3~$ or not, we can get $~c~$ or $~b~$ contains factor $~m,~$this conflict by assuming.  </p>

<p>If $~(c−b)=1~$ then $~3c^2-3c+1=a^3,~$</p>

<p>from  <a href=""http://www.wolframalpha.com/"" rel=""nofollow"">Wolframalpha</a> get:</p>

<p>$$
c = \dfrac{3- \sqrt{3}\sqrt{4a^{3}-1}}{6} \\
c = \dfrac{\sqrt{3}\sqrt{4a^{3}-1}+3}{6}
$$
There's no integer solution (As Steven Stadnicki commented,$~\sqrt{3}\sqrt{4a^{3}-1}~$ isn't an integer, lack of proof)(update: this solved by Jack D'Aurizio see: <a href=""http://math.stackexchange.com/questions/884565/how-to-prove-sqrt3-sqrt4a3-1-isnt-an-integer"">How to prove $~\sqrt{3}\sqrt{4a^{3}-1}~$ isn&#39;t an integer?</a>).</p>

<p>(If$~(c−b)~$ is a cubic number, the problem left: $~(c - b) ^ 2 + 3cb=x^3~$has no nonzero integer solutions for $~c,~b$)</p>
",<number-theory>
"<p>Find all integer solutions to $x^2=2y^4+1$.</p>

<hr>

<p><strong>What I tried</strong> </p>

<p>The only solutions I got are $(\pm 1 ,0)$, I rewrote the question as : is $a_{n}$ a perfect square for $n&gt;0$ were </p>

<p>$$a_0=0,\quad a_1=2, \quad a_{n+2}=6a_{n+1}-a_n.$$ 
I tried taking $\pmod{4}$ and $\pmod{12}$ but that lead me nowhere.</p>
",<number-theory>
"<p>let $P$ be a non-zero prime ideal of $O_K$, where $K$ is a number field(i.e. the degree $[K:\mathbb{Q}]$ is finite) then $O_K/P$ is finite. I'm working through a proof for this claim, however there is some group theory used in the proof which I don't understand.</p>

<p>Choose $\alpha\in P$, such that $\alpha\neq 0$, then $N=|Nm(\alpha)|=\alpha\Pi_{i=2}^d \phi_i(\alpha)$ where $\phi_i$ are the embeddings for $\alpha$, letting $\phi_1$ be the identity map. So $N=\alpha\beta$, where both $\alpha,\beta\in O_k$. Therefore $N\in P$, by definition of an ideal. So $\langle N\rangle\subseteq P$ and $(O_K/P)\subseteq (O_K/\langle N\rangle)$.</p>

<p>I understand everything up until this point, but now $O_K\cong \mathbb{Z}^d$, where $d$ is the degree of the minimal polynomial of $\alpha$. But I don't understand where this result comes from. Next the proof says that $(O_K/\langle N\rangle)\cong(\mathbb{Z}/N\mathbb{Z})^d$, which is finite, hence $(O_K/P)\subseteq (O_K/\langle N\rangle)$ must also be finite. Since $\langle N\rangle = NO_K$, does this mean that $\langle N\rangle\cong N\mathbb{Z}^d$, and then can we jump to the conclusion that $(O_K/\langle N\rangle)\cong(\mathbb{Z}/N\mathbb{Z})^d$.</p>
",<number-theory>
"<p>Consider an integer$L$ written in Base 2B which digits</p>

<p>$$a_n a_{n-1} a_{n-2} ... a_1 B$$</p>

<p>Where $a_i$ are arbitrary constants such that $9 \le a_i &lt; 2B$.</p>

<p>I am attempting to prove that the square of this integer $L$ will have ending digits equal to</p>

<p>$$\frac{B}{2}, 0$$
if B is even and:</p>

<p>$$\frac{B-1}{2}, B $$
when B is odd</p>

<p>and furthermore the leading chunk of digits will be equivalent to those of</p>

<p>$$\frac{L}{2m} \left(\frac{L}{2m} + 1\right)$$</p>

<p>Where the division is integer division (no remainders)</p>

<p>Naturally I opted to begin by considering $L^2$ modulo $(2B)^2$ to generate a solution to the initial clause involving even and odd $B$.</p>

<p>I proceeded by noting</p>

<p>$$L^2 = B^2 + 2Ba_1(2B)+ ... + a_n^2(2B)^{2n} $$</p>

<p>When expanding the multiplicands</p>

<p>Thus we can consider</p>

<p>$$B^2 + (2B)a_1 \mod 4B^2$$</p>

<p>So my first challenge is to absorb the $a_1$ and have it dissappear when considering this expression $\mod 4B^2$ No clue how to do this</p>

<p>Any suggestions where to take it from here?</p>
",<number-theory>
"<p>Something I have been struggling with!</p>

<blockquote>
  <p>Let $m$ be a squarefree odd integer, and let $(a, m) = 1$. Show that $x^2 ≡ a \pmod m$ has a solution if and only if the Jacobian Symbol $(a/p) = 1$, for all primes $p|m.$</p>
</blockquote>
",<number-theory>
"<p>I wrote this equation, that is a way to represent the Sieve of Eratosthenes:</p>

<p>$-1+\sum\limits_{i=2}^{\infty} ( 2 \left \lfloor \frac {x}{i} \right \rfloor - \left \lfloor \frac {2x}{i} \right \rfloor +1) (2 \left \lfloor \frac {i+2x-2}{2i} \right \rfloor - \left \lfloor \frac {i+2x-2}{i} \right \rfloor +1)=0$</p>

<p>The solutions are all the prime numbers, and only them.</p>

<p>The function</p>

<p>$y=-1+\sum\limits_{i=2}^{\infty} ( 2 \left \lfloor \frac {x}{i} \right \rfloor - \left \lfloor \frac {2x}{i} \right \rfloor +1) (2 \left \lfloor \frac {i+2x-2}{2i} \right \rfloor - \left \lfloor \frac {i+2x-2}{i} \right \rfloor +1)$</p>

<p>is also a divisor function, because its values represent the number of proper divisors for every integer $x&gt;1$.</p>

<p>Inside the sum, the first factor produces a square wave of period $i$ and amplitude of 1. The second factor reduces the duty cycle of the first wave to 1.</p>

<p>As for the Sieve of Eratosthenes, to calculate all the prime numbers not greater than a number n, the sum can be stopped at $i=\sqrt{n}$.</p>

<p>It seems to me an original function. Is it of some interest? Can it be simplified? Is there any way to evaluate the equation, that is to bring the x to the left, outside the floor functions and the sum?</p>

<p><strong>added images</strong></p>

<p>square waves: $i=4$</p>

<p><a href=""http://i.stack.imgur.com/ji1sb.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/ji1sb.jpg"" alt=""q4""></a></p>

<p>$i=9$</p>

<p><a href=""http://i.stack.imgur.com/U2zEb.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/U2zEb.jpg"" alt=""q9""></a></p>

<p>The complete value</p>

<p><a href=""http://i.stack.imgur.com/8U0LE.jpg"" rel=""nofollow""><img src=""http://i.stack.imgur.com/8U0LE.jpg"" alt=""ss""></a></p>

<p>See the primes: 2,3,5,7,11,13,17,19 where the value is zero.</p>
",<number-theory>
"<p>Let $d_1$, $d_2$, ..., $d_n$ be positive integers. Let $B$ be the $n \times n$ matrix
$$\begin{pmatrix}
d_1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1 \\
1 &amp; d_2 &amp; 1 &amp; \cdots &amp; 1 \\
1 &amp; 1 &amp; d_3 &amp; \cdots &amp; 1 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; 1 &amp; 1 &amp; \cdots &amp; d_n \end{pmatrix}.$$
When does $B$ have a square root in $\mathrm{Mat}_n(\mathbb{Z})$?</p>

<p>Motivation: The <a href=""http://en.wikipedia.org/wiki/Friendship_graph"" rel=""nofollow"">Friendship Theorem</a> states that the only graph in which every pair of vertices is joined by a path of length $2$ is the ""Friendship Graph"", which you can see at the linked article. If $A$ is the adjacency matrix of such a graph, with degree sequence $(d_1, d_2, \ldots, d_n)$, then $A^2=B$. So this contributes the solution $(d_1, d_2, \ldots, d_n) = (2,2,2,\ldots,2,2m)$, with $n=2m+1$.</p>

<p>I was preparing notes on the friendship theorem and got distracted by trying to figure out when this matrix has an integer square root at all. It seemed like it might make a nice challenge for here.</p>
",<number-theory>
"<p>I'm having a problem with a section of Niven's book the Theory Of Numbers.  I am trying to show:</p>

<p>If an integer $\alpha \in \mathbb{Q}(\sqrt{m})$ is neither zero nor a unit, prove that $|N(\alpha)|&gt;1$.</p>

<p>An element of $\mathbb{Q}(\sqrt{m})$ would look like $a+b\sqrt{m}$ where $a,b$ are rational integers $0, \pm1, \pm2,...$  The norm is defined as $N(\alpha)=\alpha\bar{\alpha}$, where $\bar{\alpha}$ is defined as the conjugate of $\alpha$ ($\alpha=a+b\sqrt{m}, \bar{\alpha}=a-b\sqrt{m}$).</p>

<p>So since $\alpha$ can not be zero or a unit, we have the two cases where 1).  $a\neq 0, b\neq 0$.  2).  $a\neq \pm 1, b\neq 0$.  Therefore $a\ge 2, b\ge 1$.  Now</p>

<p>$N(\alpha)=\alpha\bar{\alpha}=(a+b\sqrt{m})(a-b\sqrt{m})=a^2 -b^2m$.  By our given conditions, we see that $N(\alpha)\neq 0$ or $1$  Now I have to show that $|N(\alpha)|&gt;1.$  I was thinking contradiction, so suppose that there exist an $a,b$ such that $|a+b\sqrt{m}|\le 1$.  Then
$$-1\le a^2+b^2m\le 1$$
However, with $m\ge 2$ this is impossible unless either $a=b=0$ or $a=1, b=0$.  But these cannot be since we have restricted these conditions in the initial problem.  Therefore
$|N(\alpha)|&gt;1$. </p>

<p>Is this the correct way of solving this particuluar problem?  ( I think I worked it out while I was typing, but any verification would be helpful....)</p>
",<number-theory>
"<p>Which is the smallest integer $n&gt;1$, such that $$n^{5000}+n^{2013}+1$$ is prime ?
 Since $x^{5000}+x^{2013}+1$ is irreducible over $\mathbb{Q}$ and has value $1$ for $x=0$,
 there should be infinitely many such $n$, if Bunyakovsky's conjecture is
 true.</p>
",<number-theory>
"<p>is there a short proof of the fact that there is a finite amount of consecutive smooth numbers (meaning Given a finite set B, there is a finite amount of pairs $n,n+1$ so that both can be expressed as the product of elements from B only.</p>

<p>I was told there is a proof via the Thue–Siegel–Roth theorem but the proof of the theorem is pretty long and I'd be happy if there was another proof.</p>
",<number-theory>
"<p>Let's say I have the following equalities</p>

<p>$a_1x_1 + a_2x_2 + a_3x_3 + a_4x_4 = b_1x_1 + b_2x_2 + b_3x_3 + b_4x_4 = c_1x_1 + c_2x_2 + c_3x_3 + c_4x_4$</p>

<p>Where the $a$'s, $b$'s, and $c$'s are known, non-negative integers.</p>

<p>Is there an efficient way to check if a solution exists (the $x$'s) such that they are non-negative real numbers (except for the trivial case of all $x$'s being 0)? I don't need to actually calculate them, just need some way to see if a solution even exists.</p>
",<number-theory>
"<p>Does anyone know how to find the m-th coefficient of</p>

<p>$-t=(3+2\sum_{k=1}^{\infty} \frac{t^{2k}}{(2k)!})(\sum_{n=0}^{\infty} D_n \frac{t^n}{n!})$?</p>

<p>The answer is:</p>

<p>$0=3 \frac{D_m}{m!}+2\sum_{k=1}^{m/2} \frac{1}{(2k)!} D_{m-2k} \frac{1}{(m-2k)!}$</p>

<p>but i don't understand it why this is the m-th coefficient. </p>

<p>This is an exercise trying to find a recursive formula for $D_m$. In this exercise the numbers $D_m$ are defined by the generating function:</p>

<p>$G(t)=\frac{-t}{e^t +1 + e^{-t}} = \sum_{m\geq 0 } D_m \frac{t^m}{m!}$</p>
",<number-theory>
"<p>Question:</p>

<blockquote>
  <p>Let $n = pq$ be a product of two distinct odd primes and put $d = \gcd(p − 1, q − 1)$.<br>
  (a) Prove that $n$ is a pseudoprime to the base $b$ if and only if $b^d\equiv  1 \pmod n$. 
  (b) Conclude (using part (a)) that $|Pn| = d^2.$</p>
</blockquote>

<p>I got part (a) on my own. I just don't see how I can conclude part (b) with it. Could someone guide me in the right direction?</p>
",<number-theory>
"<p>Let $a&gt;0$ be a real number, such that for all integers $n\geq 1$: $n^a \in \mathbb N$<br>
Show that $a$ must be an integer.</p>

<hr>

<p>It's not difficult to show this when $a$ is a rational number: $2^\frac{p}{q}$ is irrational when the fraction is in lowest terms and $q \neq 1$.</p>

<p>When $a$ is irrational, then for all $n\geq 1$, there exists $m_n\in \mathbb N^*$, such that:
$$a = \frac{\log m_n}{\log n}; \quad \text{$m_n$ is not a power of $n$}$$</p>

<p>I think considering $n=2,3$ is enough to show a contradiction, but I can't seem to find it. This is what I get:
$$a = \frac{\log p}{\log 2} = \frac{\log q}{\log 3}$$
$$p = 2^{\log q/\log 3} $$</p>

<p>I think the RHS is irrational when $q$ is not a power of $3$, but I can't prove it. The closest thing I have to a solution is <a href=""http://math.stackexchange.com/a/974590/120267"">this answer on a similar question</a>. But it uses an unproven conjecture, and I was hoping for a more elementary proof.</p>
",<number-theory>
"<p>Do primes become more or less frequent as you go further out on the number line?  That is, are there more or fewer primes between $1$ and $1,000,000$ than between $1,000,000$ and $2,000,000$?</p>

<p>A proof or pointer to a proof would be appreciated.</p>
",<number-theory>
"<p>Some $k$ prime numbers $n_1, n_2, ..., n_k$ are given. Then some natural number $x$ is provided. </p>

<p>Then we want to figure natural numbers (including zero) $m_1, m_2, ..., m_k$ so that $n_1m_1 + n_2m_2 + ... + n_km_k = x$.</p>

<p>1) Suppose that for given $k$ and given sequence $n_k$, $x$ can be linearly decomposed as above. Then what would be the general algorithm for doing this? Note that I want cases for all possible $k$ from 2 to any number less than infinity.</p>

<p>2) As $k$ increases, would the number of possible cases of sequence $m_k$ decrease?</p>
",<number-theory>
"<p>Prime numbers are numbers with no factors other than one and itself.</p>

<p>Factors of a number are always lower or equal to than a given number; so, the larger the number is, the larger the pool of ""possible factors"" that number might have.</p>

<p>So the larger the number, it seems like the less likely the number is to be a prime.</p>

<p>Surely there must be a number where, simply, every number above it has some other factors.  A ""critical point"" where every number larger than it simply will always have some factors other than one and itself.</p>

<p>Has there been any research as to finding this critical point, or has it been proven not to exist?  That for any <code>n</code> there is always guaranteed to be a number higher than <code>n</code> that has no factors other than one and itself?</p>
",<number-theory>
"<p>Let $n$ be a positive integer. Suppose $a$ and $b$ are randomly (and independently) chosen two $n$-digit positive integers which consist of digits 1, 2, 3, ..., 9. (So in particular neither $a$ nor $b$ contains digit 0; I am adding this condition so that division by $b$ will be possible, and that we don't get numbers of the form $0002$ and so on). Here ""randomly"" means each digit of $a$ and $b$ is equally likely to be one of the 9 digits from $\{1,2,3,..., 9\}$. </p>

<p>My question concerns the divisibility of these integers:</p>

<blockquote>
  <p>1) What is the probability that $b$ divides $a$ ?</p>
</blockquote>

<p>The answer, of course, will depend on $n$. Denote this probability by $p(n)$. I would be happy with rough estimates for $p(n)$ as well :)</p>

<blockquote>
  <p>2) Is it true that $p(n)\to 0$ as $n\to\infty$?</p>
</blockquote>

<p>I think answer to question 2) is yes (just by intuition). </p>
",<number-theory>
"<p>I know that there are some solutions in the reel numbers, but how can I prove that there are none in the rational ones?</p>
",<number-theory>
"<p>I am looking to evaluate the sum $$\sum_{1\leq k\leq mn}\left\{ \frac{k}{m}\right\} \left\{ \frac{k}{n}\right\} .$$</p>

<p>Using matlab, and experimenting around, it seems to be $\frac{(m-1)(n-1)}{4}$ when $m,n$ are relatively prime.  How can we prove this, and what about the case where they are not relatively prime?</p>

<p><strong>Conjecture:</strong>  Numerically, it seems that for any $m,n$ we have $$\sum_{1\leq k\leq mn}\left\{ \frac{k}{m}\right\} \left\{ \frac{k}{n}\right\} =\frac{(m-1)(n-1)}{4}+C(\gcd(m,n))$$
where $C(\gcd(m,n))$ is some constant depending only on the $\gcd(m,n)$.</p>

<p><strong>Additionally:</strong>  Can we sum this even when it is not a complete interval? Suppose that $0&lt;a&lt;b&lt;mn,$ do we have an exact form for $$\sum_{a\leq k\leq b}\left\{ \frac{k}{m}\right\} \left\{ \frac{k}{n}\right\}.$$ </p>

<p><strong>Remark:</strong> In the one variable case we have $$\sum_{1\leq k\leq n}\left\{ \frac{k}{n}\right\} =\frac{n-1}{2}$$ the sum over an interval $a,b$ has an explicit form.</p>
",<number-theory>
"<p>Find all polynomials $P$ with integer coefficients such that $P(n)$ divides $2^n-1$ for all positive integers $n$.</p>
",<number-theory>
"<p>Given a prime $q$, and another prime $p$ = 20q + 1, I am able to find generators in $\mathbb{Z}_p$. Does $-1$ have a square root in $\mathbb{Z}_p{}^*$? 
Thanks!</p>
",<number-theory>
"<p>what's up folks?</p>

<p>I'm solving the red book of math problems, problem 16 which is to solve the following recurrence relation:</p>

<p>$\sum_{k=1}^n {n \choose k} a(k) = \frac{n}{n+1}$</p>

<p>PS: ${n \choose k}  = \frac{n!}{k!(n-k)!}$ (it is not the Legendre symbol).</p>

<p>for every $n \in \mathbb{N}$</p>

<p>I'm wondering if it's possible to use Moebius Inversion formula with</p>

<p>$G(x) = \frac{x}{x+1}$ (one can define it to be zero if $0&lt;x&lt;1$)</p>

<p>and some $F$ I know one can use differential equations theory to solve this one, but I want to try to solve it by this way</p>

<p>spoiler alert: $a(k) = \frac{(-1)^{k+1}}{k+1}$</p>
",<number-theory>
"<p>What's with the definition of Bezout's Identity? As I understand it, it states that if $d = \gcd(a, b)$, then there exist integers $x,\ y$ such that $ax+by=d$. </p>

<p>Why the requirement that $d=\gcd(a,b)$ though? It seems to work even when this isn't the case. For example, let $a = 17$ and $b = 4$. Then $d = 1$, however setting $d = 2$ still generates an infinite number of solutions:
$$
x = -4n-2,\quad\quad y=17n+9\\
n\in\Bbb{Z}
$$
and for $(a,\ b,\ d) = (19,\ 17,\ 5)$ we get $x=-17n-6$ and $y=19n+7$. However for $(a,\ b,\ d) = (44,\ 55,\ 12)$ we <em>do</em> have no solutions.</p>

<p>So what's the fuss? Why require $d=\gcd(a,b)$? Is it like, you can't <em>guarantee</em> the existence of solutions to $ax+by=d$ unless $d=\gcd(a,b)$, and I just stumbled across a case where it happens to work? In that case can we classify <em>all</em> the cases where there are solutions $x,\ y$, more specifically than just $d=\gcd(a,b)$?</p>
",<number-theory>
"<p>So we choose two large primes p and q and multiply them together to get n.
We also pick an encryption exponent e and so for any message m, we can compute m^e (mod n) which is our ciphertext c.</p>

<p>So anyways, I understand (or rather, I'm familiar with) Fermat's little theorem that x^(p-1) is congruent to 1 mod p, as well as Euler's theorem, which seems to just tie together FLT and the notion that for any prime p, since there are p-1 numbers less than p that are coprime to p, $\phi(p) = p-1$</p>

<p>My question: since c is just m^e, then if we can find d, by solving de = 1 (mod p-1 * q-1), and then compute m by taking c^d = m^ed = m (mod n). What I don't understand is where the equation de = 1 (mod $\phi(n)$) comes from. Like why are we modding by (p-1)(q-1) instead of n? </p>

<p>There is a little section in my book that says the following: ""when we are working mod 11 we are essentially working with the exponents mod 10, not mod 11"" so I can see how that would relate to my question but I don't see the exact</p>
",<number-theory>
"<p>Assume that $P_n$ denotes the $n$'th prime for this entire question.</p>

<p><strong>Inspriation:</strong> I was dumbfounded by the fact that: $$\hat\prod_\limits{n=1}^\infty P_{n}=4\pi^2$$ 
After further investigation, I learned of many other properties of zeta-regulation, as well as their proofs (to a reasonable extent). I realized nothing had been done of this question: $$2*5*11\cdots=\hat\prod_\limits{n=1}^\infty P_{2n-1}=\kappa$$
and solve for $\kappa$. </p>

<p><strong>Issues:</strong> I am fairly competent in the usage of zeta-regularization, but am lost here, because it seems that zeta regularization doesn't work with $2n-1$ used instead of $n$. Unfortunately, I couldn't really figure out how to apply that on to this here. I was driven, from Resource 1, that a <em>potential</em> to use bounds was created, but my inability to logically understand this problem made it impossible to determine if $4\pi^2$ or $\sqrt{2\pi}$ would be upper or lower.</p>

<p><strong>Questions:</strong></p>

<p>a) Can $\kappa$ be zeta-regularized?</p>

<p>b) If it can, could you please assist me in a calculation of $\kappa$? </p>

<p><strong>Side notes:</strong> I have had this question on my mind almost forever, and would really love an answer. Although I would most appreciate a proof, really anything will help me here. I am also somewhat uncertain with my tag choices, so please consider editing before immediately downvoting. The following were helpful in the construction of this problem. </p>

<p>1) <a href=""http://math.stackexchange.com/questions/177946/when-is-an-infinite-product-of-natural-numbers-regularizable"">When is an infinite product of natural numbers regularizable?</a></p>

<p>2) <a href=""http://mathworld.wolfram.com/Zeta-RegularizedProduct.html"">http://mathworld.wolfram.com/Zeta-RegularizedProduct.html</a></p>

<p>(I realize the former has been unanswered, but the problem had helpful comments as well as the idea to find a bound. )</p>
",<number-theory>
"<p>It is relatively easy to show that if $p_1$, $p_2$ and $p_3$ are distinct primes then $\sqrt{p_1}+\sqrt{p_2}$ and $\sqrt{p_1}+\sqrt{p_2}+\sqrt{p_3}$ are irrational, but the only proof I can find that $\sqrt{p_1}+\sqrt{p_2}+...+\sqrt{p_n}$ is irrational for distinct primes $p_1$, $p_2$, ... , $p_n$ requires we consider finite field extensions of $\mathbb{Q}$.</p>

<p>Is there an elementary proof that $\sqrt{p_1}+\sqrt{p_2}+...+\sqrt{p_n}$ is irrational exist?</p>

<p>(By elementary, I mean only using arithmetic and the fact that $\sqrt{m}$ is irrational if $m$ is not a square number.)</p>

<p>The cases $n=1$, $n=2$, $n=3$ can be found at in the MSE question <a href=""http://math.stackexchange.com/questions/964729/sum-of-square-root-of-primes-2"">sum of square root of primes 2</a> and I am hoping for a similar proof for larger $n$.</p>
",<number-theory>
"<p>Given 3 diophantine equations:</p>

<p>$$x_1y_1+x_2y_2=x_3y_3+x_4y_4$$</p>

<p>and </p>

<p>$$x_1+x_2 = x_3+x_4$$ and</p>

<p>$$y_1+y_2 = y_3+y_4$$</p>

<p>We're interested in solutions to this system of equations when all variables are positive. I conjecture that any solutions have $x_1=x_3$ or $x_1=x_4$ and $y_1=y_3$ or $y_1=y_4$. Any tips on how to go about proving this? Thanks.</p>
",<number-theory>
"<p>The classical <a href=""http://en.wikipedia.org/wiki/M%C3%B6bius_function"" rel=""nofollow"">Möbius function</a> $\mu(n)$ fulfills the multiplicative inversion formula, e.g. <a href=""http://math.stackexchange.com/questions/83805/an-identity-involving-the-mobius-function"">see this thread</a>. Now I see in the theory of <a href=""http://en.wikipedia.org/wiki/Poset"" rel=""nofollow"">posets</a>, they generalize the concept of that function, see <a href=""http://en.wikipedia.org/wiki/Incidence_algebra#Special_elements"" rel=""nofollow"">incidence algebra</a> and from this point of view, the previous definition is the case for the natural numbers and its order induced by divisibility. </p>

<p>In the last link, they also define a ""zeta-function"" $\zeta(x,y)$ for elements $x,y$ of a poset - I think that's just the boolean version of ""$x\le y$"" telling me if two numbers $x,y$ are appropriately ordered. There is <a href=""http://en.wikipedia.org/wiki/M%C3%B6bius_inversion_formula"" rel=""nofollow"">a formula</a> of how to obtain the inverse of such a $\zeta$ using $\mu$, and this appears to be the source of the equal names.</p>

<p>But I fail to see if this new function is otherwise the analog of the Riemann zeta function $\zeta(z)$ (and other members of the <a href=""http://en.wikipedia.org/wiki/List_of_zeta_functions"" rel=""nofollow"">family</a> of continuous zeta functions). Does that analytic function also somehow represent a characteristic function for ordered intervals of numbers? How is the inversion formula a generalization of it, when the number theoretic $\zeta(z)$ doesn't even have two arguments?</p>

<p>Or can I maybe connect $\zeta(x,y)$ to the idea behind the Arithmetic and Dedekind zeta function, which know about the spaces (ideals..) which can be constructed from their domain?</p>
",<number-theory>
"<p>Is there any simple algorithm which can tell if a string is a repeat of its substring?
For example, $1212121212$ is a repeat of $12$, $135746135746$ is a repeat of $135746$.</p>
",<number-theory>
"<p>Show that for any $a \in \mathbb{Z}, 42 \mid (a^{7} − a)$.</p>

<p>I saw this question on Rosen textbook and it doesn't have answer key so I am wondering can you guide me how to do it? </p>

<p>What I have tried is that since I don't know the number a so I substitute a number for a.</p>
",<number-theory>
"<p>$\frac17 = 0.(142857)$...</p>

<p>with the digits in the parentheses repeating.</p>

<p>I understand that the reason it's a repeating fraction is because $7$ and $10$ are coprime.  But this...cyclical nature is something that is not observed by any other reciprocal of any natural number that I know of (besides multiples of $7$). (if I am wrong, I hope that I may find others through this question)</p>

<p>By ""cyclical,"" I mean:</p>

<pre>
1/7 = 0.(142857)...
2/7 = 0.(285714)...
3/7 = 0.(428571)...
4/7 = 0.(571428)...
5/7 = 0.(714285)...
6/7 = 0.(857142)...
</pre>

<p>Where all of the repeating digits are the same string of digits, but shifted.  Not just a simple ""they are all the same digits re-arranged"", but the same digits <strong>in the same order</strong>, but shifted.</p>

<p>Or perhaps more strikingly, from the <a href=""http://en.wikipedia.org/wiki/142857_(number)"">wikipedia article</a>:</p>

<pre>
1 × 142,857 = 142,857
2 × 142,857 = 285,714
3 × 142,857 = 428,571
4 × 142,857 = 571,428
5 × 142,857 = 714,285
6 × 142,857 = 857,142
</pre>

<p>What is it about the number $7$ in relation to the base $10$ (and its prime factorization $2\cdot 5$?) that allows its reciprocal to behave this way?  Is it (and its multiples) unique in having this property?</p>

<p><a href=""http://en.wikipedia.org/wiki/Cyclic_number"">Wikipedia</a> has an article on this subject, and gives a form for deriving them and constructing arbitrary ones, but does little to show the ""why"", and finding what numbers have cyclic inverses.</p>
",<number-theory>
"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://math.stackexchange.com/questions/21351/upper-bound-exact-length-of-decimal-expansion-of-simple-fraction"">Upper bound/exact length of decimal expansion of simple fraction</a>  </p>
</blockquote>



<p>I noticed that <a href=""http://www.wolframalpha.com"" rel=""nofollow"">WolframAlpha</a> given an operation like $\frac{n}{m},\;n,m \in N$ that result in a periodic decimal number, computes really fast the length of the period.</p>

<p>E.g. $\frac{3923}{6173}$ has a period of 3086: <a href=""http://www.wolframalpha.com/input/?i=3923/6173"" rel=""nofollow"">here</a>.</p>

<p>I was wondering how this computation is done: is there some method to do this (except the trivial one of executing the division and looking for a sequence repetition) ?</p>
",<number-theory>
"<p>Let $\zeta_{p^n}$ be the primitive $p^n$-th root of unity where $p$ is a prime and $K_n=\mathbb Q(\zeta_{p^n})$ the $p^n$-th cyclotomic field. Let $K_\infty=\bigcup K_n$. </p>

<p>Could someone give a proof of the isomorphism $\text{Gal}(K_\infty/\mathbb Q)\cong \mathbb Z_p^{\times}$?</p>

<p>Many thanks in advance. </p>
",<number-theory>
"<p>Consider some binomial chain that looks like this:</p>

<p>$$\binom{N}{k_1}\binom{N-k_1}{k_2}\binom{N-k_1-k_2}{k_3}\binom{N-k_1-k_2-k_3}{k_4} \cdots \binom{N-k_1-k_2-\cdots -k_{t-1}}{k_t}$$</p>

<p>Where all variables $N$ and $k_1$ through $k_t$ are known positive integers.</p>

<p>Is there some simple way to compress this?</p>
",<number-theory>
"<p>Which is the single best book for Number Theory that everyone who loves Mathematics should read?</p>
",<number-theory>
"<p><strong>Question</strong>: When $p$ is an odd prime, show that the number of quadratic residues $a$ modulo $p$ with $1\leq a\leq p-1$ is $(p-1)/2$</p>

<p><strong>Answer</strong>: From Euler's criterion $\left(\frac{a}{p}\right)\equiv a^{(p-1)/2}\pmod{p}$</p>

<p>When we apply Lagrange's theorem, the congruence $a^{(p-1)/2}\equiv 1\pmod{p}$ has at most $(p-1)/2$ solutions</p>

<p>By Fermat's little theorem, we have $a^{p-1}\equiv 1\pmod{p}$, so</p>

<p>$a^{p-1}-1\equiv(a^{(p-1)/2}-1)(a^{(p-1)/2}+1)\equiv 0\pmod{p}$</p>

<p>The answer then says this ""has precisely $p-1$ solutions""</p>

<p>How do we deduce from this that this $a^{p-1}-1$ has precisely $p-1$ solutions?</p>
",<number-theory>
"<p>Greetings Mathematics Community.</p>

<p>I am having much difficulty in solving the following problem:</p>

<p>If $m\equiv 2$ (mod 4), show that $\mathbb{Q(\zeta_m)}=\mathbb{Q(\zeta_{\frac{m}{2}})}$ where $\zeta$ is a primitive root of unity. I know that $\zeta_m=e^{\frac{2\pi i}{m}}$ and by Euler's equation, we have $\cos(\frac{2 \pi }{m})+i\sin(\frac{2\pi }{m})$.</p>

<p>I have tried to compare $[\mathbb{Q(\zeta_{\frac{m}{2}})}:\mathbb{Q}]$ with $[\mathbb{Q(\zeta_m)}:\mathbb{Q}]$ by substituting $\frac{m}{2}$ in Euler's equation to get
$\cos(\frac{4 \pi }{m})+i\sin(\frac{4\pi }{m})$. My intuition is telling me that I should somehow use the given fact that $m\equiv 2$ (mod 4), but I am not sure how. </p>

<p>Furthermore, I do know that this extension degree can be found using Euler's $\phi-$function on $m$. But I am unsure about how to apply it in my situation.</p>

<p>As always, any help is greatly appreciated. Thanks in advance. </p>
",<number-theory>
"<p>Is there any general form to determine the number of non-congruent solutions to equations of the form $f(x) \equiv b \pmod m$?</p>

<p>I solved a few linear congruence equations ($ax \equiv b \pmod m$) and I know those have only one solution because we're basically finding $a^{-1}$ and all the inverses of $a$ are congruent.</p>

<p>What's the number of solutions for congruences of higher degree polynomials? (quadratic, qube, etc).</p>

<p>Thanks a lot.</p>
",<number-theory>
"<p>Let $\mathbb{F}_q$ be a finite field with $q$ elements, let $N$ be a positive divisor of $q-1$, and let $\xi_N$ be an element of $\mathbb{F}_q^*$ of order $N$. One can similarly define the Ramanujan sum for $\mathbb{F}_q$ as 
$$
c_N(k) = \sum_{\substack{i=1 \\ (i,N) = 1}}^N \xi_N^{ik}.
$$
That is, $c_N(k)$ is the sum of $k$-th powers of the primitive $N$-th roots of unity in $\mathbb{F}_q$. I'm wondering how many zeros does the sum have? What are its values? How about a lower bound for this? Thanks!</p>
",<number-theory>
"<p>My number theory is terrible so I don't know what ""class"" of problem this secretly is.  I'm looking for all positive integer solutions to the equation:</p>

<blockquote>
  <p>$M^2=5N^2+2N+1$</p>
</blockquote>

<p>That is, I want positive integer $M$ and $N$ to make the above true.  I've got the obvious solution ($N=0$, $M=1$) but I don't know how to go about getting more solutions.  It has been suggested to me that there should be infinitely many solutions, and I would like to find them all.</p>

<p>I could transform it to look like Pell's equation by completing the square on the right, but it won't have integer coefficients (or you could multiply it through by the denominators, but then it wouldn't look like Pell's equation), so I don't think that helps much.</p>

<p>I don't know enough number theory to guess at other things, but I'm happy to read something on this topic.</p>
",<number-theory>
"<p>For example:
say $n = 2$.
The numbers from $1$ to $2^2$ are $1, 2, 3, 4$.
i.e. $1, 10, 11, 100$ in binary.
So the result is $1$, because only one number i.e. $3$ is there such that it has <code>11</code> in it.</p>

<p>For $n = 3$,
$3, 6, 7$ have '11', so the result is $3$.</p>
",<number-theory>
"<p>Let $a_1,\ a_2,\ a_3,\ \ldots,\ a_n$ be distinct positive integers. 
Find $x_1,\ x_2,\ x_3,\ \ldots,\ x_n,\ y \in \mathbb{Z^+}$ 
such that: $$\left\{\begin{array}{rl}(x_1,x_2,\ldots,x_n)&amp;=1\\ a_2x_1+a_3x_2+\cdots+a_nx_{n-1}+a_1 x_n &amp; =y x_2\\ \cdots\cdots\cdots\cdots &amp; = \cdots\cdots \\ a_n x_1+a_1 x_2+ \cdots + a_{n-1} x_n&amp;=y x_n \end{array}\right.$$</p>

<p>How to solve this system equation ? </p>

<p>I am not good at diophantine equation and the problem above is so hard.</p>
",<number-theory>
"<p>Let $L/K$ be abelian.  There is a natural way to define the Artin reciprocity map on the ideles using the notion of an <strong>admissible cycle</strong>.  I don't want to go into the details of what that is right now, but essentially what I'm having trouble with is the following.  In order to show that the Artin map is well defined on the ideles $\mathbb I_K$, I must show:</p>

<blockquote>
  <p>Suppose $x \in K^{\ast}$ is a local norm at each ramified place.  Also suppose that for any real place $v$ of $K$ (and corresponding embedding $ \sigma: K \rightarrow \mathbb{R}$) which has a complex place $w$ lying over it, we have $\sigma(x) &gt; 0$.  Then $$\prod\limits_{v } (\mathfrak p_v, L/K)^{ord_v(x)}$$
  is the identity element of $Gal(L/K)$, where $(\mathfrak p, L/K)$ is the Frobenius element and $v$ runs through all the unramified places.  </p>
</blockquote>

<p>One way to do this would be to show that $x$ is a global norm, or at least a local norm at each place $v$ where $ord_v(x) \neq 0$.  But I don't know if this is true.  Any hints?</p>
",<number-theory>
"<p>In Intro Number Theory a key lemma is that if $a$ and $b$ are relatively prime integers, then there exist integers $x$ and $y$ such that $ax+by=1$.  In a more advanced course instead you would use the theorem that the integers are a PID, i.e. that all ideals are principal.  Then the old lemma can be used to prove that ""any ideal generated by two elements is actually principal.""  Induction then says that any finitely generated ideal is principal.  But, what if all finitely generated ideals are principal but there are some ideals that aren't finitely generated?  Can that happen?</p>
",<number-theory>
"<p>I have come to know of an elementary proof, by Chebyshev, that there exists a real $\alpha&lt;1$ such that $$ p_n &gt; \alpha n \log n, $$ where $p_n$ is the $n$-th prime. As I am failing to find it online, I hope someone can provide a link to it or reproduce it.</p>
",<number-theory>
"<p>This is a question from Frohlich's book 'Galois Module Structure of Algebraic Integers', Ch.1. </p>

<p>Let $K$ be a number field and $\Omega_K=\text{Gal}(K^c/K)$ where $K^c$ is the separable closure of $K$. Also let $\Gamma$be any finite group and $R_\Gamma$ ring of virtual characters of $\Gamma$. Next let $\mathfrak I(\mathbb Q^c)$ be the direct limit of the idele groups $\mathfrak I(E)$ as $E$ runs over the number fields in $\mathbb Q^c$. The values of the virtual characters of $\Gamma$ lie in some number field $F$ containing $K$.</p>

<p>The book states that $$\text{Hom}_{\Omega_K}(R_\Gamma,\mathfrak{I}(\mathbb Q^c))=\text{Hom}_{\Omega_K}(R_\Gamma,\mathfrak{I}(F))$$</p>

<p>How can we show this? For a start, how do we know that the LHS is a subgroup of the RHS? </p>

<p>Many thanks for your help.</p>
",<number-theory>
"<p>suppose I have a finite field $\mathbb{F}_q$, where $q = p^m$ and $p$ is 
prime. Let $0 \not = t \in \mathbb{F}_q$. I was wondering
if someone could tell me what the number of solution to
$$
x^2 + y^2 + z^2 = t
$$
would be (where $x,y,z \in \mathbb{F}_q$)? In particular is there always a solution?
I would appreciate any reference also.</p>

<p>Thanks!</p>
",<number-theory>
"<p>Let $f: N \to N$, $f(2) = 3$, and $f(ab) = f(a)f(b)$, that is, f is a multiplicative function. f is also strictly increasing. Show that no such function exists.</p>

<p>Progress: Apparently, this is proven by contradiction.  So I used $f(2) = f(2 * 1) = f(2) * f(1)$.  This gives me $3 = 3 * f(1)$, which means $f(1) = 1$.</p>

<p>I see no contradiction though.  f is strictly increasing, not decreasing, so it makes sense that $f(1) = 1$ but $f(2) = 3$.  Help please?</p>
",<number-theory>
"<p>Suppose that an infinite set $S= {0,1,2,4,8,...}$ of integers written in monotonically increasing order (that is, all other members are integers greater than 8) has the property that Euclidean division of any integer $a$ in $S$ by any integer $b\ne0$ in $S$ (regardless of whether $a&gt;b$) gives quotient $q$ and remainder $r$ also in $S$. That is, $S$ is closed under Euclidean division. Obviously, $S$ could then be ${0,1,2,4,8,16,...}$, but is it possible to prove that there are no other infinite sets $S=0,1,2,4,8,...$ satisfying the closure property?</p>
",<number-theory>
"<blockquote>
  <p>Prove that in every sequence of $79$ consecutive positive numbers written in decimal notation there is a number the sum of whose digits is divisible by $13$.</p>
</blockquote>

<p>I tried to take one by one sets of $79$ consecutive positive numbers. Then I tried to solve with sets,relation,function. But I am not getting any idea how to start solving the question.</p>
",<number-theory>
"<p>This result is used in the Erdos' Distance problem, in the Landau-Ramanujan constant, but I can't find a proof anywhere.</p>

<p><a href=""http://en.wikipedia.org/wiki/Erd%C5%91s_distinct_distances_problem"" rel=""nofollow"">http://en.wikipedia.org/wiki/Erd%C5%91s_distinct_distances_problem</a>
<a href=""http://en.wikipedia.org/wiki/Landau%E2%80%93Ramanujan_constant"" rel=""nofollow"">http://en.wikipedia.org/wiki/Landau%E2%80%93Ramanujan_constant</a></p>
",<number-theory>
"<p>[1] Number of ordered pair of unequal positive integer solution of $x+y+z = 10$</p>

<p>[2] Number of ordered pair of unequal positive integer solution of $x+y+z+w = 20$</p>

<p>$\bf{My\; Try}::$ For $(1)$ one:: Here $x,y,z&gt;0$ and $x,y,z\in \mathbb{Z^{+}}$</p>

<p>$\bullet $ If $x=1$, Then $y+z=9$, So $(y,z) = (2,7)\;,(3,6)\;,(4,5)\;(5,4)\;,(6,3)\;,(7,2)$</p>

<p>$\bullet $ If $x=2$ Then $y+z=8$, So $(y,z) = (1,7)\;,(3,5)\;,(5,3)\;(7,1)$</p>

<p>$\bullet $ If $x=3$ Then $y+z=7$, So $(y,z) = (1,6)\;,(2,5)\;,(5,2)\;(6,1)$</p>

<p>$\bullet $ If $x=4$ Then $y+z=6$, So $(y,z) = (1,5)\;,(5,1)$.</p>

<p>$\bullet $ If $x=5$ Then $y+z=5$, So $(y,z) = (1,4)\;,(2,3)\;,(3,2)\;(4,1)$</p>

<p>$\bullet $ If $x=6$ Then $y+z=4$, So $(y,z) = (1,3)\;,(3,1)$</p>

<p>$\bullet $ If $x=7$ Then $y+z=3$, So $(y,z) = (1,2)\;,(2,1)$</p>

<p>So Total unordered pair is $ = 24$</p>

<p>My Question is , is there is any other Method to calculate the ordered pair in less complex way</p>

<p>because above is very Lengthy method</p>

<p>Help Required</p>

<p>Thanks.</p>
",<number-theory>
"<blockquote>
  <p>Express $\mathbb{Q}(\sqrt{3},\sqrt[3]{5})$ in the form $\mathbb{Q}(\theta)$.</p>
  
  <p>Hint: Let $f,g$ be the minimal polynomials of $\sqrt{3}$ and $\sqrt[3]{5}$, respectively. Factor $f$ and $g$ in $\mathbb{C}$ to obtain expressions of the form $$f(x)=\prod\limits_{j=1}^n (x-\alpha_j),g(x)=\prod\limits_{k=1}^m (x-\beta_k)$$</p>
  
  <p>where $\alpha_1=\sqrt{3}$ and $\beta_1=\sqrt[3]{5}$. Choose $c \in \mathbb{Q}$\ $\lbrace 0 \rbrace$ so that $$\alpha+c\beta \notin \lbrace \alpha_j+c\beta_k|1\leq j \leq n, 2 \leq k \leq m \rbrace$$ It can then be shown that $\mathbb{Q}(\sqrt{3},\sqrt[3]{5})=\mathbb{Q}(\sqrt{3}+c\sqrt[3]{5})$</p>
</blockquote>

<p>Following the hint, I think $c=1$ works. Now I try to prove that $\mathbb{Q}(\sqrt{3},\sqrt[3]{5}) \subseteq \mathbb{Q}(\sqrt{3}+\sqrt[3]{5})$ (the other containment is easy), but I'm having trouble with this last step.</p>
",<number-theory>
"<p>Suppose: $ x_1 + x_2 + x_3 + x_4 + x_5 + x_6 = 1$ , and $x_1x_3x_5 + x_2x_4x_6 \ge \dfrac {1}{540} $ and $\dfrac{p}{q}$ is the maximum possible value of
$x_1x_2x_3 + x_2x_3x_4 + x_3x_4x_5 + x_4x_5x_6 + x_5x_6x_1 + x_6x_1x_2$</p>

<p>Find $p+q$</p>

<p><strong>Details and Assumptions</strong></p>

<p>$x_1, x_2, \dots, x_6$ are non-negative real numbers.</p>

<p>$p$ and $q$ are positive relatively prime integers.</p>
",<number-theory>
"<p>Prove that there are infinitely many prime numbers of the form $6n-1$. I proved that there are infinitely many prime numbers but I couldn't bring it in the form given in the question. While proving it, I was getting $6n+1$ not $6n-1$.</p>
",<number-theory>
"<p>It is a theorem in elementary number theory that if $p$ is a prime and congruent to 1 mod 4, then it is the sum of two squares. Apparently there is a trick involving arithmetic in the gaussian integers that lets you prove this quickly. Can anyone explain it?</p>
",<number-theory>
"<p>Find $a,b,c \in \mathbb {Q}$ such that:
$\left\{\begin{array}{rl} x^3&amp;\in \mathbb Q \\ x&amp;\notin \mathbb{Q}\\ ax^2+bx+c &amp;=0\end{array}\right.$</p>

<p>I tried Vieta's formulas, but seem like it didn't help.</p>

<p>I think $a=b=c=0$ is only solution.</p>
",<number-theory>
"<blockquote>
  <p>The least common multiple of $\{1,2,...,n\}$ is greater than $2^{n-1}$ for any $n \ge 3$.</p>
</blockquote>

<p>I found this in a MATHEMATICA book, but I don't know how to prove this. Can you help me?</p>
",<number-theory>
"<p>If i define $f(m,n)=$ $$\sum_{1\leq k\leq mn}\left\{ \frac{k}{m}\right\} \left\{ \frac{k}{n}\right\} .$$</p>

<p>Then prove $$f(m+n,n) - f(m,n) =\frac{n^2-n}{4}$$
for all $m$ and $n$.</p>

<p>This question came from part of answer from this question: <a href=""http://math.stackexchange.com/questions/140499/a-sum-of-fractional-parts/140517#140517"">A sum of fractional parts.</a></p>
",<number-theory>
"<p>Suppose we have a fixed (generally composite) $k$, and we want to find the largest power of $k$ that divides $n!$ for $n$ large.</p>

<p>If $k$ is square-free, we need only consider the behavior of the largest prime $p$ dividing $k$: if $p^i | n!$, then certainly $q^i|n!$ for any prime $q&lt;p$, and so $k^i|n!$.</p>

<p><em>Most of the time</em>, when $k$ is composite, a similar argument is possible and we need only consider a single prime factor of $k$. Legendre's formula for the prime factorization of $n!$ tells us that the highest power of $p$ dividing $n!$ is
$$
\sum_{j=1}^\infty \left\lfloor \frac{n}{p^j}\right\rfloor = \frac{n}{p-1} + O(\log n) \, .
$$
So, for a fixed prime power $p^a$, the largest power of $p^a$ that divides $n!$ is $\frac{n}{a(p-1)} + o(n)$.</p>

<p>It follows that, if $k=p_1^{a_1}\dots p_n^{a_n}$, then we need only consider those $p_i$ which minimize $a_i(p_i-1)$. Most of the time there will only be one such $p_i$, in which case our life is no harder than it was when $k$ was square-free.</p>

<p>For example, if $k=24$, then we are interested only in the divisibility of $n!$ by the prime powers $8$ and $3$. The above analysis tells us that the largest power of $8$ that divides $n!$ is roughly $\frac{n}{3(2-1)}=\frac{n}{3}$, while the largest power of $3$ that divides $n!$ is roughly $\frac{n}{3-1}=\frac{n}{2}$. So the largest power of $24$ that divides $n!$ is always the same as the largest power of $8$ that divides $n!$, for $n$ sufficiently large.</p>

<p>However, there are exceptions!</p>

<p>For example, if $k=12=2^2 \cdot 3$, then the largest power of both $4$ and $3$ that divides $n!$ will be roughly $\frac{n}{2}$. Numerical experimentation suggests that $n!$ <em>usually</em> has more than twice as many factors of $2$ as it has factors of $3$, but the number of exceptions is large (for $n&lt;10^7$, the factors of $2$ are the scarce ones about $17\%$ of the time, and that number seems to decrease only slowly as $n$ increases).</p>

<p><strong>Can anything be said about which prime factor of $k$ will be the most scarce, in cases where the basic asymptotic analysis given above isn't strong enough?</strong></p>
",<number-theory>
"<p>I have asked a <a href=""http://math.stackexchange.com/questions/1402613/find-the-smallest-number-which-leaves-remainder-1-2-and-3-when-divided-by-11-5"">similar question</a> before on Chinese Remainder Theorem. </p>

<p>Now  concepts are getting clear. Thinking of a possible case where there are no solutions. Suppose the question is</p>

<pre><code>x ≡ 2 (mod 88)      
x ≡ 3 (mod 99)   
</code></pre>

<p>Then there will not be any solutions, correct?</p>

<p>Since 88 and 99 can be written as co-prime products 8*11 and 9*11,</p>

<pre><code>x ≡ 2 (mod 8)      
x ≡ 2 (mod 11) 
x ≡ 3 (mod 9)   
x ≡ 3 (mod 11)  
</code></pre>

<p>x ≡ 2 (mod 11) and x ≡ 3 (mod 11) will not come together. hence there are no solutions.</p>

<p>Is this right? Your comments are really helpful for me</p>
",<number-theory>
"<p>Consider the equation below:</p>

<p>$$\cos\dfrac{\pi}{m}=2\cos\dfrac{\pi}{r}\cos\dfrac{\pi}{n},$$ where $m,n$ and $r$ are non-zero integers.</p>

<p>Equality holds when $m=2$ and $r=2$ (or $n=2$), and also when $m=n$  and $r=3$ (alternatively $m=r$ and  $n=3$).</p>

<p>I would like to know any general conditions (if there are) between $m,n$ and $r$ for equality to hold.</p>
",<number-theory>
"<p>Adopting the following notation:</p>

<p>$$ R(b/a) = \text{Remainder of b when divided by a} $$</p>

<p>So I was trying to prove the following:</p>

<p>There always exists an $n$ for two primes $a$ and $b$ such that:</p>

<p>$$ R(b/a) = R(2n/a) $$</p>

<p>and satisfies:</p>

<p>$$ 2n &gt; b &gt; n &gt; a &gt; 2$$ </p>
",<number-theory>
"<blockquote>
  <p>There is a row of 1000 integers. There is a second row below, which is constructed as follows. Under each number $a$ of the first row, there is a positive integer $f(a)$ such that $f (a)$ equals the number of occurrences of $a$ in the first row. In the same way, we get the 3rd row from the 2nd row, and so on. Prove that, finally, one of the rows is identical to the next row.</p>
</blockquote>

<p>Attempt:</p>

<p>I looked at some cases. Suppose all integers are the same then $f(a_k) = 1000, \forall k$. For the third row then, $f(f(a_k)) = 1000, \forall k$. Similarly, $f(f(....a_k)..) = 1000, \forall k$.</p>

<p>I need to find an invariant. Can someone give me hints?</p>
",<number-theory>
"<p>In my book it said ""For nonnegative integers $a,n,$ and $N$ we have that $$(1+Na^{n+1})^a = 1+a \cdot Na^{n+1}+\binom{a}{2}N^2a^{2n+2}+Ma^{3n+3}$$ for some integer $M$."" Shouldn't it be $$(1+Na^{n+1})^a = 1+\binom{a}{1} \cdot Na^{n+1}+\binom{a}{2}N^2a^{2n+2}+\cdots+\binom{n}{n}N^{a}a^{an+a}?$$</p>
",<number-theory>
"<blockquote>
  <p>Find the smallest positive integer $a$ such that $1971$ divides $50^{n}+a \cdot 23^n$ for all odd integers $n$.</p>
</blockquote>

<p>I would re-write under this form
$$1971m-50^n=a \cdot 23^n$$
But where to go next?</p>
",<number-theory>
"<p>This is a curiosity question:</p>

<blockquote>
  <p><strong>Question</strong> Given two positive integers $a$ and $b$ do we have the following equivalence:
  $$\sum_{n=0}^{\infty}\frac{1}{n^2+2an+b}\in \Bbb Q \iff \exists k\in \Bbb N^+\text{ such that } a^2-b=k^2\ ?$$</p>
</blockquote>

<p><strong>My attempt</strong></p>

<ul>
<li>$(\Leftarrow)$ Assume that $a^2-b=k^2$ ave $k&gt;0$ then :
$$\begin{align}\sum_{n=0}^{\infty}\frac{1}{n^2+2an+b}&amp;=\sum_{n=0}^{\infty}\frac{1}{(n+a)^2-k^2}\\ \\
&amp;=\frac{1}{2k}\sum_{n=0}^{\infty}\left(\frac{1}{n+a-k}-\frac{1}{n+a+k}\right)\\ \\
&amp;=\frac{1}{2k}\sum_{i=0}^{2k-1}\frac{1}{i+a-k} \end{align}$$</li>
<li>$(\Rightarrow)$ I don't know how to approach this implication, but I know for example that if $a^2-b=0$ then using the sum:
$$\sum_{n=0}^{\infty}\frac{1}{(n+a)^2}=\frac{\pi^2}{6}-\sum_{i=1}^{a-1}\frac{1}{i^2}\notin \Bbb Q $$</li>
</ul>

<p>How can I approach the second implication, I don't know even if it's true or not but it seems when $\sqrt{a^2-b}\in \Bbb N$ that the implication would be true, for instance I don't know what would be the value of:
$$\sum_{i=0}^\infty\frac{1}{(n-a)^2+3} \text{ or } \sum_{i=0}^\infty \frac{1}{(n-a)^2-3}.$$</p>
",<number-theory>
"<p>So I'm searching a certain $n\in \mathbb Z\,$ that satisfies $2^{n-1}\cdot n+1=y^2$.</p>

<p>How do I find the different solutions and how do I prove this? </p>

<p>I think that it would be possible to transform the equation to a sort of Pell equation: $$y^2-n \cdot 2^{n-1}= 1$$ </p>

<p>Only this is not a Pell's equation because $n-1$ would have to be $2$.</p>

<p>Does another form exist? </p>
",<number-theory>
"<p>I write the positive numbers starting at $1$ in a triangle:$$\mathbb{N}_\triangle = \begin{matrix}
    &amp;&amp;&amp;&amp;&amp;21&amp;\ldots         \\
    &amp;&amp;&amp;&amp;15&amp;20&amp;\ldots       \\
    &amp;&amp;&amp;10&amp;14&amp;19&amp;\ldots     \\
    &amp;&amp;6&amp;9&amp;13&amp;18&amp;\ldots   \\
    &amp;3&amp;5&amp;8&amp;12&amp;17&amp;\ldots \\
    1&amp;2&amp;4&amp;7&amp;11&amp;16&amp;\ldots 
    \end{matrix}$$</p>

<p>I write $[n]$ for the $n^{th}-odd-column$ of $\mathbb{N}_\triangle$ and label the elements of $[n]$ as $x_1,\ldots,x_n$ where $x_1 =1+ {n(n-1) \above 1.5pt 2}$ and $x_n ={n(n+1) \above 1.5pt 2}$. For example $$[1] =\{1\}$$ $$[2] =\{4,5,6\}$$ $$[3] =\{11,12,13,14,15\}$$ Denote the number of elements in $[n]$ by $|[n]|$. I construct the following finite alternating sum for $[n]$  $$\mathfrak{a(n)} =x_n+\sum_{i=1}^{n-1}(-1)^{|[n]|-i}x_{|[n]|-i}$$ where $x_i \in [n]$ for $1 \leq i \leq n$. I am asking if the following claim is true?</p>

<blockquote>
  <p>If $n&gt;1$ then $$\mathfrak{a(n)} =2n^2-1$$</p>
</blockquote>

<p>For example consider $[3]$. First note that $|[3]|=5$ then </p>

<p>$$15+(-1)^{5-1}14+(-1)^{5-2}13+(-1)^{5-3}12+(-1)^{5-4}11=15+14-13+12-11=17$$ and $2*3^2-1=17$. The partition of the Natural numbers into the sets $[1],[2],[3],\ldots$ is known as  Smarandache's Sequential Sieve - <a href=""https://oeis.org/A007606"" rel=""nofollow"">A007606</a>. </p>

<p><strong><em>Edit 1</em></strong>: I just noticed that $|[n]| =2n-1$ in which case we can actually write $$\mathfrak{a(n)} =x_n+\sum_{i=1}^{n-1}(-1)^{2n-1-i}x_{2n-1-i}$$</p>
",<number-theory>
"<p>Find all solutions to the Diophantine equation $n^p+3^p=k^2$, where $p\in \mathbb{P}$ and $n,k$ positive integers.</p>

<p>I have tried everything, from mods to bounding to LTE; nothing seems to work on this. I did find one solution: $(n,p,k)=(4,2,5)$, which was motivated by noticing the resemblance to Pythagorean triples.</p>

<p>I should note that I don't know any advanced number theory (I'm in high school), so I apologize if there is a very simple approach I'm not seeing.</p>
",<number-theory>
"<blockquote>
  <p>Please refer this <a href=""http://www.careerbless.com/maths/speedmaths/cuberoot1.php"" rel=""nofollow"">site</a>. A method is provided for finding cube
  roots of perfect cubes.</p>
  
  <p>As per the method explained, suppose we are finding cube root of
  $157464$</p>
  
  <p>First we write as  $157,\quad 464$</p>
  
  <p>Last digit of $464$ is $4.$ Hence RHS=$4$</p>
  
  <p>$157-5^3 \ge 0$ ($5$ is the maximum). So LHS$=5$</p>
  
  <p>Hence, $\sqrt[3]{157464}=54$</p>
</blockquote>

<p>How we can prove this mathematically? Please give directions on how to start towards writing a proof for this. Thanks.</p>
",<number-theory>
"<p>The question is: If $n$ is a square, can $n$ consist of only odd digits?</p>

<p>I have a feeling that the answer is no, with the only exceptions being $n=1,9$. I am not sure how to go about proving this though. Any help or hints would be appreciated.</p>
",<number-theory>
"<p>solve $x^4 + y^4 = x^3 + y^3 + 10$  for  $x,y \in \mathbb{Z}$.</p>

<p>I tried solving this by trying to find upper bounds for $|x|$ and $|y|$, therefor it is quite useful to write:</p>

<p>$x^4 + y^4 - x^3 - y^3 = .....$</p>

<p>where ... is in the form of a square.</p>

<p>I tried to write $x^4 - x^3 = x^2(x^2+x) = x^2((x+\frac{1}{2})^2 - \frac{1}{4})$, but the $'\frac{1}{4}'$ is kind of troublesome, any tips or hints on how to get a good square, so i can estimate my polynomial?</p>

<p>Kees</p>
",<number-theory>
"<p>Step 1, make x columns rows data A and data B:</p>

<pre><code>date A row 0 is 3*n^2 + 3*n - 1
date B row 0 is 3*n^2 + 3*n
date A columns extended by add 5 + 6 * i
date B columns extended by add 7 + 6 * i
(i is columns index starts from 0)

here's a 3 x 3 data for example:

        date A:     date B:   

         5 17 35     6 18 36
        10 28 52    13 31 55
        15 39 69    20 44 74
</code></pre>

<p>Step 2, get  all numbers related to date A and date B less than max(B) = 74</p>

<pre><code>        date A:               date B:   

         5 17 35  59           6 18 36  60
        10 28 52              13 31 55         
        15 39 69              20 44 74  

        20 50                 27 57
        25 61                 34 70
        30 72                 41
        35                    48
        40                    55
        45                    62             
        50                    69           
        55                     
        60                     
        65                     
        70
</code></pre>

<p>Step 3,  from 0 to max(B) get numbers that not in the full data:</p>

<pre><code> [0,  1,  2,  3,  4,  7,  8,  9, 11, 12, 14, 16, 19, 21,
 22, 23, 24, 26, 29, 32, 33, 37, 38, 42, 43, 46, 47, 49, 
 51, 53, 54, 56, 58, 63, 64, 66, 67, 68, 71, 73]
</code></pre>

<p>Step 4, multiply step 3 data  items by 12 then add 5 , get prime form 12 * i + 5.</p>

<p>It seems this sieve is correct,how to prove this?</p>
",<number-theory>
"<p>How to prove the following identity:</p>

<p>$$\sum_{n\ge0}\frac{2q^{n^{2}+n}}{(q)_{n}^{2}(1+q^{n})}=\sum_{n\ge0}\frac{q^{n^{2}+n}}{(q)_{n}^{2}(1-q^{2n+2})}$$</p>
",<number-theory>
"<p>Are  there any upper bound for $a_i$ in $\gamma =\{a_0,a_1,\dots,a_i,\dots\}$ the  simple continued fraction expansion of  real positive algebraic numbers? </p>
",<number-theory>
"<p>A quadratic form represents an integer $n$ if there exist $x,y\in \mathbb{Z}$ such that $f(x,y)=n$.  It is proper if $\gcd{(x,y)}=1$.  It is said that if $f(x,y)=n$ and $\gcd{(x,y)}=g$, then $g^2|n$.  My question in regards to the material is this; why is it necessary that $g^2|n$?</p>

<p>Knowing $\gcd{(x,y)}=g$, I know that there must exist $a,b\in\mathbb{Z}$ such that $ax+by=g$  Also $$a^2x^2+2abxy+b^2y^2=n=g^2  $$
This is clearly a quadratic form and it is such that $g^2=n$.  Is this the reason?</p>

<p>EDIT:  I made a mistake originally that stated $\gcd{(x,y)}=1$ when it was my intention to write $\gcd{(x,y)}=g$  This I hope clarifies what I was asking, but I believe that I understand why it is such that $g^2|n$.  (Since $\gcd{(x,y)}=g, x=gk_1, y=gk_2$.  If $ax^2+bxy+cy^2=n$, substituting for $x,y$ gives the desired result I believe..)</p>
",<number-theory>
"<p>Let, $\Gamma, \Gamma'$ be $lattices$ of $\mathbb C$, define $elliptic$ $curves$ by $\mathbb C/\Gamma , \mathbb C/\Gamma'$, then </p>

<blockquote>
  <p>$\mathbb C/\Gamma , \mathbb C/\Gamma'$ are isomorphic $\Leftrightarrow$ $\Gamma=\lambda\Gamma'.$</p>
</blockquote>

<p>The $\Leftarrow$ part is easy, but how to prove the $\Rightarrow$ part?</p>

<p>(Rmk:</p>

<p>I am reading Serre's $A$ $Course$ $In$ $Arithmetic$, it doesn't particular treat elliptic curves, and just writes:</p>

<p>""Let us associate to a lattice $\Gamma$ of $\mathbb C$ the elliptic curve $E_\Gamma =\mathbb C/ \Gamma$. It is easy to see that two lattices $\Gamma, \Gamma'$  define isomorphic elliptic curves if and only if they are homothety.""</p>

<p>That's all I know about elliptic curves now, so I am not sure what does ""isomorphic"" mean in original question. ... I thought it means group isomorphism, but I am not sure now. It is helpful that anyone clarifies what is the author talking about.)</p>
",<number-theory>
"<p>Find the remainder when ${45^{17^{17}}}$ is divided by 204</p>

<p>I tried using congruence modulo. But I am not able to express it in the form of $a\equiv b\pmod{204}$.</p>

<p>$204=2^2\cdot 3\cdot 17$</p>
",<number-theory>
"<p>Consider $2y^x-1$; for what $y$ does this function's range not contain a prime? Having $x&gt;0, y&gt;1$.</p>
",<number-theory>
"<p>$\forall l,m,n\in \Bbb{Z_+}$, let $A:=\{k: m+1\leq k\leq m+n\text{ and }l-k^2\text{ is a square number}\}$.</p>

<p>Please prove that the number of elements in $A$ is not more than $C\sqrt{n\log n}$, where $C$ is a positive constant that is independent of $l,m$ and $n$.</p>
",<number-theory>
"<p>Consider a regular n-gon with side length $A$.</p>

<p>Let $p$ be a point in the polygon.
Let the distances from $p$ to the corners of the n-gon be $x_1,x_2,...,x_n$</p>

<p>Are there solutions with $A,x_1,x_2,...x_n$ all positive integers and $gcd(A,x_1,x_2,...,x_n) = 1$.</p>

<p>For the triangle ( $n=3$) this question has been answered already here</p>

<p><a href=""http://math.stackexchange.com/questions/485755/do-there-exist-an-infinite-number-of-rational-points-in-the-equilateral-triang"">Do there exist an infinite number of &#39;rational&#39; points in the equilateral triangle $ABC$?</a></p>

<p><a href=""http://mathoverflow.net/questions/180191/rational-distance-from-vertices-of-an-equilateral-triangle"">http://mathoverflow.net/questions/180191/rational-distance-from-vertices-of-an-equilateral-triangle</a></p>

<p>--</p>

<p>I assume for sufficiently large $n$ there are no solutions ?</p>

<p>In particular im intrested in $n=4,5,6$.</p>
",<number-theory>
"<p>I need to get integer solutions for the next equation: $$x^{2}-y^{4}=336$$ I know equations that look like $x^{2}-y^{2}=n$ have solutions $x$ and $y$ where $x=\frac{a+b}{2}$ and $y=\frac{a-b}{2}$, $a$ and $b$ being both odd or even. But I can not figure out how to solve the equation.</p>
",<number-theory>
"<p>Given a positive integer n, we have to calculate the following sum in the most efficient way: 
$$ \sum_{i=1}^n\frac{n}{\gcd(i,n)}$$</p>

<p>I think it is equivalent to:</p>

<blockquote>
  <p>On the set {1,2,…,N}{1,2,…,N} iterate the NN-cycle (12⋯N)(12⋯N) MM
  times. In the resulting permutation, what is the length of the cycle
  that contains 11?</p>
</blockquote>

<p>which is  N/gcd(N,M)</p>
",<number-theory>
"<p>Notation: </p>

<ul>
<li>$p$ - a prime integer,  </li>
<li>$\Bbb{Z}_p$ - set of $p$-adic integers,</li>
<li>$\Bbb{Q}_p$ - set of $p$-adic rationals,</li>
<li>$\Bbb{Q}$ - set of rationals,</li>
<li>$\Bbb{R}$ - set of reals.</li>
</ul>

<p>While reading up on $p$-adic numbers I came to know that $\Bbb{Z}_p$ is both open and closed in $\Bbb{Q}_p$. Since $\Bbb{Z}_p$ is properly contained in $\Bbb{Q}_p$ and is also a clopen set in $\Bbb{Q}_p$ we observe that $\Bbb{Q}_p$ is not connected. This was a bit of a surprise to me as the completion of $\Bbb{Q}$ in the Archimedean metric is $\Bbb{R}$ which is connected. </p>

<p>This made me curious as to what are the connected components of $\Bbb{Q}_p$ and whether there is a characterization for the components in terms of the $p$-adic metric (or any characterization at all). But I couldn't find anything on doing a Google search. </p>

<p>Hence, I thought it best to ask this question here in the hope that others might also find it interesting and something might come up. </p>

<p>So finally my question is - What are the connected components of $\Bbb{Q}_p$ and what interesting information do they tell us? Further, how do we compare or contrast this situation with that of $\Bbb{R}$? </p>
",<number-theory>
"<p>We know that :
$( x.y )$ mod $m$ = ( ($x$ mod $m$) . ($y$ mod $m$) ) mod $m$</p>

<p>Is there any property for:
$\frac{x}{y}$ mod $m$ like  $\frac{x \mod m}{y \mod m}$ mod $m$ . I hope this fails.</p>

<p>I want to find an efficient way to solve:
$$\frac{x_1 .x_2.x_3 ... x_i }{y_1 . y_2 . y_3 . . .y_j} \mod \ m$$
where, $x_i, x_j, m \le 10 ^9 ; $
and $\frac{x_1 .x_2.x_3 ... x_i }{y_1 . y_2 . y_3 . . .y_j}$ results in an integer</p>

<p><strong>Edit: If $a \ mod \ m = \ x$ and $b \mod m =\ y$, then can we express $(\frac{a}{b} \ mod \ m)$ in terms of x, y and m ??</strong></p>

<p>Any help will be appreciated :) Thanks</p>
",<number-theory>
"<p>My question is whether (*) below can be shown using the Erdős-Kac theorem? I don't think the distinction between $\Omega$ and $\omega$ is important here.</p>

<p>For lack of better notation let $\lambda_{r,s}(i) := 1$ if $ \Omega(i)\equiv s~ \text{(mod r)}$ and zero otherwise.  For $ (0\leq s,\hat{s}&lt; r)$ and $s\neq \hat{s},$</p>

<blockquote>
  <p>$$(*)\hspace{15mm} \sum_{i\leq n} \lambda_{r,s}(i) \sim \sum_{i\leq n} \lambda_{r,\hat{s}}(i)  $$</p>
</blockquote>

<p>Briefly the E-K theorem says that </p>

<p>$$\frac{\omega(n)-\log\log n}{\sqrt{\log\log n}}$$</p>

<p>is normally distributed. So the idea would be that for large n the area under the gaussian curve can be divided into bands (mod $r$) and the sums of areas for each $s ~\text{(mod r)}$ are roughly equal, with the caveat that for $r &gt; 2$ the numbers have to be very large for this to work.    </p>

<p>For $r = 3,~ n = 2^{22}$ we already have for $s \equiv 0,2,1,$ </p>

<p>$\sum_i \lambda_{3,0}(i)/n = 0.3320,$ </p>

<p>$\sum_i \lambda_{3,2}(i)/n = 0.3505, $ </p>

<p>$\sum_i \lambda_{3,1}(i)/n = 0.3174,  $</p>

<p>Note also: $r$ is fixed as n grows.  </p>
",<number-theory>
"<p>For a quadratic character $ \chi \bmod m $, define $\displaystyle r_\chi(n)=\sum_{d\,\mid\, n}\chi(d) $. I would like to prove that</p>

<p>$$\sum_{n \leq m }\frac{r(n)}{n} \ll \frac{m}{\varphi(m)}\exp \Big( 2 \sum_{\substack{p \,\leq\, m \\ p \text{ prime} \\ \chi(p)\,=\,1}} \frac{1}{p} \Big) $$</p>

<p>I think I found a characterization of $ r_{\chi} $: </p>

<p>$$r_\chi(n)=\prod_{p^e\,\|\,n}(1+\chi(p)+\cdots+\chi(p)^e)$$</p>

<p>$$ r_\chi(n)=\begin{cases}0
&amp; \text{if } \exists \text{ prime } p\mid n \text{ such that }  e_p(n) \text{ is odd and } \chi(p)=-1 \\ 
\tau(n) &amp; \text{if  all the primes } p\mid n \text{ are residues} \\ 
\tau(\frac{n}{m}), &amp; \text{otherwise} 
\end{cases} $$</p>

<p>Here $\tau(n)$ represents as usual, the number of divisors of $n$ and $\displaystyle m= \!\!\! \prod_{\substack{p^e\,\|\,n \\  \ \chi(p)=1}}p^e $.</p>

<p>I apologize for my bad latex skills and would appreciate any comments, ideas and sugestions. Thank you very much!</p>
",<number-theory>
"<p>I have a conjecture which I cannot prove or disprove.</p>

<p>Denote the $i$'th digit of $x$ in binary expansion by $d_i(x)$, where for $i=1$ the MSB is taken. Example: $d_3(110.1)=0$ and $d_4(110.1)=1$ (so it just ignores the dot).</p>

<p>Denote by $p_i$ the $i$'th prime number, i.e. $p_1=2, p_2=3, p_3=5$ and so on.</p>

<p>Let $$h_i^j(x)=\frac {d_i(x)} {p_i^{p_{2j}}}$$ if the $i$'th digit of $x$ is left to the decimal point, and $$h_i^j(x)=\frac {d_i(x)} {p_i^{p_{2j+1}}}$$ otherwise.</p>

<p>Define $f(x):\mathbb R^n\rightarrow\mathbb R$ as: </p>

<p>$$f(x)=\sum_{j=1}^n \sum_{i=1}^\infty  h_i^{j+in}(x_j) $$</p>

<p>Conjecture: $f$ is injective and finitely integrable over any finite measure balls on $\mathbb R^n$.</p>

<p>Caution: some explanations might work well for rational numbers only.</p>

<p>Please, if you think if it's incorrect, try to see if you can slightly modify $f$ to make the conjecture correct.</p>

<p>Thanks!!!</p>
",<number-theory>
"<p>I learned quadratic congruence by myself and stuck in these problems:</p>

<ol>
<li><p>I know if quadratic congruence $X^2=a(\mod\mbox{ p} )$ with $p$ is an odd prime number and $\gcd(a,p)=1$, then it has no solution or has exactly two solutions. So,
what is Theorem/Lemma that guarantee a quadratic congruence has solutions?</p></li>
<li><p>For linear congruence, we can use the extended Euclidean algorithm to find solution of linear congruence. So my question, what is method to find solution of quadratic congruence?</p></li>
</ol>

<p>I would really appreciate if anyone could help me out here</p>
",<number-theory>
"<p>Prove that for any natural number a and prime number p, $a^{p^{p-1}}\equiv a$ mod p. </p>
",<number-theory>
"<p>The rule of Mersenne Prime says that $2^p - 1$ is prime if $p$ is prime.</p>

<p>$2^{11} - 1 = 2047$ satisfies the condition, but it's not a prime as it can be divided by two prime numbers $23$ and $89$. Then, why do we use Mersenne Prime thing at all?</p>

<p>Last I checked, the biggest known prime today is a Mersenne Prime. Can't it be wrong?</p>
",<number-theory>
"<p>Prove that if $\sigma(n)=2n+1$ then $n$ is an odd perfect square.</p>

<p>(Here, $\sigma(n)$ is the sum of the positive divisors of $n$
including 1 and $n$ itself.)</p>

<p>As I know, this $n$ is a quasiperfect number, and I just proved that $n$ is a perfect square or $\frac{n}{2}$ is a perfect square.</p>
",<number-theory>
"<p>I recall reading a proof that showed these two sets were equinumerous, but I'm having trouble finding it. Is there any intuitive method to show that they are in fact equinumerous? It seems like since $[0,1]$ is a subset of $\mathbb{R}$ that $\mathbb{R}$ should be larger.</p>
",<number-theory>
"<p>What I need to show is that</p>

<p>For $\gcd(ab,p)=1$ and p is a prime, </p>

<p>the number of solutions of the equation $ax^2+by^2\equiv 1\pmod{p}$ is exactly $p-(\frac{-ab}{p})$.</p>

<p>I got a hint that I have to use Legendre symbol from the answer.</p>

<p>I think that I may count a solution one by one.</p>

<p>What I did :</p>

<p>$$(ax)^2 \equiv a-aby^2 \pmod{p}$$
It suffices to count $y$ such that $(\frac{a-aby^2}{p})=1$.</p>

<p>I tried to use the complete residue system or a primitive root but it didn't work.</p>

<p>The factorization also didn't work.</p>

<p>I think that the pigeonhole principle may not work because it just says the existence.</p>

<p>Thanks in advance.</p>
",<number-theory>
"<p>Does a subset of $R$ contain equal number of rational and irrational numbers? How to prove?</p>
",<number-theory>
"<p>I have the following expression</p>

<p>$$\frac{gcd(a,b)}{\varphi(gcd(a,b))}$$</p>

<p>$a,b$ are known positive integers. Is there any way to rephrase this or simplify it?</p>
",<number-theory>
"<p>The question is what are the possible values of $x$ when we have</p>

<p>$$x^{x^3} = 3$$</p>

<p>(that is $x^3$ in the exponent itself and not $x*3$).</p>

<p>I solved one answer by guessing that $x = \sqrt[3]3$. My work is below however I was only able to solve it assuming $x = \sqrt[3]3$, can anyone solve it without the assumption. I suspect that logarithms are involved which I have to review. Also there are supposed to be more solutions to this than just the one mentioned. Thanks in advance and enjoy. It's a fun problem<img src=""http://i.stack.imgur.com/eU36P.jpg"" alt=""![enter image description here"">]<a href=""http://i.stack.imgur.com/eU36P.jpg"" rel=""nofollow"">1</a></p>
",<number-theory>
"<p>Given two integers $N$ and $M$ , How to find out number of arrays A of size N, such that : </p>

<ol>
<li>Each of the element in array, $1 ≤ A[i] ≤ M$</li>
<li><p>For each pair i, j ($1 ≤ i &lt; j ≤ N$) </p>

<p>$GCD(A[i], A[j]) = 1$</p></li>
</ol>

<p>$M$ can be at max $100$. But $N$ can be upto $100000$. Is there some direct mathematical formula for the same ?</p>

<p>Example to make question clear : Say $N=3$ and $M=3$ then answer is $13$.</p>
",<number-theory>
"<p>Find the sum of all even positive divisors of 1000.</p>

<p>I prime factorised 1000 but the answer was not coming.</p>
",<number-theory>
"<p>I learnt at this <a href=""http://googology.wikia.com/wiki/Fast-growing_hierarchy"" rel=""nofollow"">site</a> that</p>

<p>$$\large f_{\omega^\omega}(n)\approx \underbrace{[n,...,n]}_{n\ n's}$$</p>

<p>For a simular approximation</p>

<p>$$\large f_{\omega^2}(n)\approx \underbrace{n\rightarrow n\rightarrow...\rightarrow n\rightarrow n}_{n\ n's}\ =:\ g(n)$$</p>

<p>Wythagoras concreted this approximation to the inequality</p>

<p>$$\large g(n+1)&lt;f_{\omega^2}(n)&lt;g(n+2)\ \ for\ \ n\ge 2$$</p>

<p>Is there a similar inequality in this case ?</p>
",<number-theory>
"<p>If you know $\phi (n)$ how can you derive the prime factorization of $n$ from this? For example, $\phi(100) = 40$, but how does $40$ help us come to $2^25^2$?</p>
",<number-theory>
"<p>As we know $\sqrt{2},\sqrt{3}$ are irrational numbers. And I see some proofs on the net.</p>

<p>So I doubt that how $e,\pi$ or already known irrational numbers are proved to be irrational.</p>

<p>In fact, I got interested in Riemann zeta function
$$\zeta(s)=\sum_{n=0}^{\infty} \frac{1}{n^s},$$</p>

<p>we know $\zeta(2)=\pi^2/6$ from Euler, 1737.</p>

<p>One mathematician (sorry to forgot his name) proved $\zeta(3)$ to be also irrational 40 years before.</p>

<p>Can somebody explain how he could do with that? To understand Apéry's theorem, is it very hard?</p>

<p>An question raises that could one real number make up of two different irrationals (for example: $e,\pi$), $e\pi$, or others can be rational?</p>
",<number-theory>
"<p>Diophantine equation. $X^2+Y^2=qZ^3$</p>

<p>I wonder at what values ​​of the coefficient $q$ equation has a solution.</p>

<p>And of course I wonder how she looks like a formula describing their solutions.</p>

<p>For the special case when $X^2+Y^2=Z^3$ You can get a basic formula.</p>

<p>Has the solutions:</p>

<p>$X=2k^6+8tk^5+2(7t^2+8qt-9q^2)k^4+16(t^3+2qt^2-tq^2-2q^3)k^3+$</p>

<p>$+2(7t^4+12qt^3+6q^2t^2-28tq^3-9q^4)k^2+8(t^5+2qt^4-2q^3t^2-5tq^4)k+$</p>

<p>$+2(q^6-4tq^5-5q^4t^2-5q^2t^4+4qt^5+t^6)$</p>

<p>.................................................................................................................................................</p>

<p>$Y=2k^6+4(3q+t)k^5+2(9q^2+16qt+t^2)k^4+32qt(2q+t)k^3+$</p>

<p>$+2(-9q^4+20tq^3+30q^2t^2+12qt^3-t^4)k^2+4(-3q^5-tq^4+10q^3t^2+6q^2t^3+5qt^4-t^5)k-$</p>

<p>$-2(q^6+4tq^5-5q^4t^2-5q^2t^4-4qt^5+t^6)$</p>

<p>.................................................................................................................................................</p>

<p>$Z=2k^4+4(q+t)k^3+4(q+t)^2k^2+4(q^3+tq^2+qt^2+t^3)k+2(q^2+t^2)^2$</p>

<p>$q,t,k$ - What are some integers any sign.  After substituting the numbers and get a result it will be necessary to divide by the greatest common divisor. This is to obtain the primitive solutions.</p>
",<number-theory>
"<p>Let $p$ be an odd prime and $a, b \in \Bbb Z$ with $p$ doesn't divide $a$ and $a$ doesn't divide $b$. Prove that among the congruence's $x^2 \equiv a \mod p$, $\ x^2 \equiv b \mod p$, and $x^2 \equiv ab \mod p$, either all three are solvable or exactly one.</p>

<p>Please help I'm trying to study for final in number theory and I can't figure out this proof.</p>
",<number-theory>
"<p>If a positive integer $n$ is both a perfect square and a perfect cube , then is it true that $7$ divides $n(n-1)$ ?  </p>
",<number-theory>
"<p>I was reading some <a href=""http://www.math.uconn.edu/~kconrad/blurbs/gradnumthy/SL2classno.pdf"" rel=""nofollow"">notes</a> of Keith Conrad where he proves that the number of orbits of the $\text{SL}(2,\mathcal{O}_K)$-action on $\mathbb{P}^{1}(K)$ for a number field $K$ is precisely the class number of $K$.</p>

<p>I am wondering if there is any kind of ""higher-order"" arithmetic information found in looking at the number of orbits of the $\text{SL}(n,\mathcal{O}_K)$-action on $\mathbb{P}^{n-1}(K)$ for $n&gt;2$ (or even on higher Grassmannians $\text{Gr}(r,K^{n})$, but let's not get too crazy for now). As a first question, will these numbers even be finite for all $n$?</p>

<p>For $K=\mathbb{Q}$, I believe one can use a generalized Euclidean algorithm to show that the action above is transitive for all $n$, at least for the projective spaces $\mathbb{P}^{n-1}(\mathbb{Q})$, but I'm not sure if one can adapt this argument to work even for $\mathcal{O}_K$ that are UFDs but not Euclidean.</p>

<p>Does anyone know of any references on this question?</p>
",<number-theory>
"<p>How many odd primes $p$ are there such that both $ \dfrac {p+1}2$ and $\dfrac{p-1}4$ are primes ?</p>
",<number-theory>
"<p>Does an algebraic number field $\mathbb{Q}\bigl(\sqrt[3]{d}\bigr)$ with $d$ cube-free exist, which contains a non-integral element $\alpha$ with integer trace and integer norm?</p>
",<number-theory>
"<p>Let $x$ be a positive integer and $y = x^2 + 2$. Can $x$ and $y$ be both prime? The answer is yes,
since for $x = 3$ we get $y = 11$, and both numbers are prime. Prove that this is the only value of x for which both x and y are prime.</p>

<p>I have to somehow prove this is true, and the only hint we were given is that we should consider cases depending on the remainder of $x$ modulo $3$. Does anyone have any tips on how to start proving this is true?</p>
",<number-theory>
"<p>The <a href=""http://en.wikipedia.org/wiki/Van_der_Waerden%27s_theorem"">van der Waerden theorem</a> states that given any natural numbers $k$ and $r$, there exists a natural number $W=W(k,r)$ such that if the set $\{1,2\cdots W\}$ is divided into $r$ classes (also called colors) then there exists a $k$-term arithmetic progression in one class.</p>

<p>I am trying to prove the theorem according to the outline given <a href=""http://en.wikipedia.org/wiki/Van_der_Waerden%27s_theorem#Proof_of_Van_der_Waerden.27s_theorem_.28in_a_special_case.29"">here</a>. </p>

<p>I have understood the proofs for $W(3,2) \le 325$ and $W(3, 3) \le 7(2·3^7+1)(2·3^{7·(2·3^7+1)}+1)$ and on similar lines have worked out a proof for $W(3,4)\le 9(2.4^9+1)(2.4^{9(2.4^9+1)}+1)(2.4^{9(2.4^9+1)(2.4^{9(2.4^9+1)}+1)}+1)$. </p>

<p>What I want to do is to prove the theorem in general on similar lines. The main problem is for me as to what will be the upper bound to start off with and then how to deal with the cumbersome notation. </p>

<p>I understand asking someone to prove the full result is not appropriate here, but if you can give me any help I will greatly appreciate it.</p>

<p>Thanks.</p>
",<number-theory>
"<p>Let $c_1,c_2,\ldots,c_{\varphi(m)}$ be the reduced residue set modulo $m&gt;2$. Show that $$c_1+c_2+\cdots+c_{\varphi(m)} \equiv 0 \pmod{m}.$$</p>

<p>My solution looks something like this.</p>

<p>If $c_i \in {\mathbb{Z}_m}^*$, then $m-c_i \in {\mathbb{Z}_m}^*$. Hence, we may take the reduced residue system $$\{c_1,c_2,\ldots,c_{\varphi(m)/2},m-c_{\varphi(m)/2},\ldots,m-c_2,m-c_1\}.$$</p>

<p>Hence, 
\begin{align} 
c_1+c_2+\cdots+c_{\varphi(m)} &amp;= c_1+c_2+\cdots+c_{\varphi(m)/2}+m-c_{\varphi(m)/2}+\cdots+m-c_2+m-c_1 \\
&amp;=m \cdot \varphi(m)/2 \equiv 0 \pmod{m},
\end{align}</p>

<p>since $\varphi(m)$ is even for $m &gt;2$.</p>

<p>Is this on the right track?</p>
",<number-theory>
"<blockquote>
  <p>Find all positive integers $x,y,z$ that satisfy: </p>
  
  <p>$$3^x - 5^y = z^2.$$ </p>
</blockquote>

<p>I think that $(x,y,z)= (2,1,2)$ will be the only solution. But how to prove that?</p>
",<number-theory>
"<p>Let $\zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s}$. We have $\frac{\zeta'}{\zeta}(s) = \sum_{n=1}^\infty \frac{\Lambda(n)}{n^s}$ for $s&gt;1$, where $\Lambda$ stands for the von Mangoldt function (<a href=""http://en.wikipedia.org/wiki/Von_Mangoldt_function"" rel=""nofollow"">http://en.wikipedia.org/wiki/Von_Mangoldt_function</a>).
It is easy to prove that there exists a constant $A$ such that $-\frac{\zeta'}{\zeta}(s) \leq \frac{1}{s-1} + A$ for every $s&gt;1$. By plotting $-\frac{\zeta'}{\zeta}(s)-\frac{1}{s-1}$ or $-\frac{\zeta'}{\zeta}(s)\cdot(s-1)$ on any computation program, it seems fair to conjecture that we can choose $A=0$. Unfortunately, I am unable to prove this. Is it a well-known inequality? Does anyone know how to prove this, if true?</p>

<p>I tried to use the formula $-\frac{\zeta'}{\zeta}(s) = \frac{1}{s-1} + 1 + s \int_1^\infty \frac{\psi(t)-t}{t^{s+1}} \mathrm{d}t$, where $\psi(x) = \sum_{n \leq x} \Lambda(n)$, but the erractic behavior of $\psi$ makes it useless (and any Chebyshev-type inequality does give $A&gt;0$).</p>

<p>Thanks in advance.</p>
",<number-theory>
"<p>I am trying solve this form, but it appears not easy problem, and also I can't find references about it. </p>

<p>I suppose some constrains should be stated, like ""$b,y &gt; 1$"" and ""$\gcd(a,b) \gcd(x/y) = 1$"" .</p>

<p>Here some examples
$$
\left( \frac{17}{21} \right)^{3} + \left(\frac{37}{21} \right)^{3} = 6
$$
$$
\left(\frac{73}{38} \right)^{3} - \left( \frac{17}{38}\right)^{3} = 7
$$</p>

<p>Example with two solutions
$$
\left(\frac{36}{13}\right)^{3} - \left(\frac{17}{13}\right)^{3} = \left( \frac{109}{31}\right)^{3} - \left(\frac{90}{31}\right)^{3} = 19
$$
Solutions to this problem seems to be scarce, so can any integer be written in this form?</p>

<p>Thanks</p>
",<number-theory>
"<p>I need your support.</p>

<p>Suppose I am performing an NTT in a finite field $GF(p)$. I assume it contains the needed primitive root of unity.</p>

<p>I am using it to compute the convolution of two vectors of length $n=2^m, m\in \mathbb{N}$. As usual, I double the length of the vectors to $2n=2^{m+1}$ to make the convolution in fact acyclic (right? may be wrong here) and thus guarantee the exact interpolation of the product during INTT. </p>

<p>These vectors are in fact long integers, with all coefficients less than $BASE\in\mathbb{N}$. I wanted to come up with an inequality for the prime number $p$ so that the convolution is recoverable (i.e. equal to the real product of polynomials). </p>

<p>Here are my considerations:</p>

<p>The worst case is when all coefficients of the vectors are equal to $BASE-1$. Then the convolution coefficients are</p>

<p>$$c_0 = a_0 b_0 = (BASE-1)^2$$
$$c_1 = a_0 b_1 + a_1 b_0 = 2\cdot (BASE-1)^2$$
$$...$$
$$c_{n-1} = a_0 b_{n-1} + ... + a_{n-1} b_0 = n\cdot (BASE-1)^2$$
$$c_n = \underbrace{a_0 b_n}_{=0} + a_1 b_{n-1} + ... + a_{n-1} b_1 + \underbrace{a_n b_0 }_{=0} = (n-1)\cdot (BASE-1)^2$$
...
$$c_{2n-1}=0$$.</p>

<p>Thus I have come up with an inequality for the prime $p$:</p>

<p>$$n \cdot (BASE-1)^2 &lt; p.$$</p>

<p>Does it guarantee that the convolution is computed correctly or have I missed something?</p>
",<number-theory>
"<p>In Calculus, whenever we see a constant and want to take the derivative of it, it always is 0. However in Number Theory, we have something called the arithmetic derivative in which we can differentiate to get some nonzero term. 
So we can denote the arithmetic derivative the same way as in calculus, say for some $x$, we can say $x'$ to be the arithmetic derivative. Some properties of arithmetic derivatives are that:</p>

<ul>
<li>For all primes, the arithmetic derivative is $1$.</li>
<li>Product Rule: $(xy)'=x'y+xy'$</li>
<li>$0'=1'=0$</li>
</ul>

<p>Now, there is also some lesser-known sub-part to the arithmetic derivative called the Arithmetic Logarithmic Derivative, $L(n)$, which is equal to $\frac{n'}{n}$. My question is that less than $100$, how many pairs of distinct positive integers (call them $a$ and $b$) does $L(a)=L(b)$?</p>
",<number-theory>
"<p>This question arose through a response to [this post] <a href=""http://math.stackexchange.com/questions/1644522/is-there-any-partial-sums-of-harmonic-series-that-is-integer/1644552#1644552"">Is there any partial sums of harmonic series that is integer?</a></p>

<p>For which integers $N&gt;1$ does the fraction $\frac 1N$ appear in the Egyptian Fraction expansion of $\frac {N-1}{N}$?</p>

<p>To specify: As such expansions are not unique, I should say which one I refer to. Here we consider the expansion obtained through the greedy algorithm.  </p>

<p>Thus $$\frac 12=\frac 12\;\;\&amp;\;\;\frac 34=\frac 12+\frac 14\;\;\&amp;\;\;\frac {11}{12}=\frac 12+\frac 13+\frac 1{12}$$ are easy examples.</p>

<p>A quick search for $N&lt;100$ yields $N=\{2,4,12,84\}$ as examples.  Taking that (short) list to OEIS leads to $[A053631][1]$, the sequence $a_i$ starting with $a_1=2$ and having the property that, for $i&gt;1$, $\{a_{i-1}+1,a_i,a_i+1\}$ are a Pythagorean triple. That sequence continues from $84$ as $3612,\, 6526884,\, 21300113901612,\dots$ and it is easy to verify that those three, at least, are examples for the present question as well.</p>

<p>Are these all examples?  Are there others?</p>

<p>Edit:  as remarked in the comments, in each of the cases cited above, $\frac 1N$ appears as the final term in the expansion.  </p>
",<number-theory>
"<p>I would like to prove the following equality</p>

<p>$$\sum_{n=1}^N \mu^2(n) = \sum_{k=1}^{\sqrt{N}} \mu(k) \cdot \lfloor N / k^2 \rfloor$$</p>

<p>with N a square number.</p>

<p>Can anyone give me a hint?</p>

<p>p.s. I know already that</p>

<p>$$\frac{\zeta(s)}{\zeta(2s) } = \sum_{n=1}^{\infty}\frac{ \mu^2(n)}{n^{s}}$$</p>

<p>Perhaps this can help?</p>
",<number-theory>
"<blockquote>
  <p>Find 
  $$ \sum_{n=1}^{\infty}\sum_{m=1}^{\infty}\sum_{p=1}^{\infty}\frac{1}{mnp(m+n+p+1)}. $$</p>
</blockquote>

<p>Use $$ \frac{1}{m+n+p+1}=\int_{0}^{1}{{{t}^{m+n+p}}\,dt} $$ so the sum equals $ -\int_0^1\ln^3(1-t)\,dt. $</p>

<p>But I don't understand the way that sum is that integral (or how the integral is distributed or something like this).</p>

<p>Help $\ddot\smile$</p>
",<number-theory>
"<p>Assuming the Riemann hypothesis is true, is it really possible to find high prime numbers more easily?</p>
",<number-theory>
"<p>Suppose $x$ and $y$ are some integers satisfying $$x^2-16=y^3.$$ I'm trying to show that $x+4$ and $x-4$ are both perfect cubes.</p>

<p>I know that the greatest common divisor of $x+4$ and $x-4$ must divide $8$, but I don't know where to go from there. Would anyone be able to help?</p>
",<number-theory>
"<p>I came across with the infinite series
$$\sum_{n=1,3,5,\ldots}^{\infty} \frac{1}{n^4}= \frac{\pi^4}{96}$$
when calculating a problem about an infinite deep square well in quantum mechanics.</p>

<p>Mathematica gives the result in the title, which is enough for a physics problem. But I just want to find how to evaluate the series. I think this sum should be connected to $\zeta(4)=\pi^4/90$, but can't figure out their relation. </p>
",<number-theory>
"<blockquote>
  <p>What is the sum of the digits of the smallest positive integer $n^4 + 6n^3 + 11n + 6$ is divisible by $700$. </p>
</blockquote>

<p><strong>Hints please.</strong></p>

<p>I got that $P(n) = n(n+1)(n+2)(n+3) \equiv 0 \pmod{700}$</p>

<p>I cannot seem to do anything else, what now?</p>

<p><strong>Hints only.</strong></p>
",<number-theory>
"<p>As a background, Ramanujan also gave a continued fraction for $\zeta(3)$ as</p>

<p>$\zeta(3) = 1+\cfrac{1}{u_1+\cfrac{1^3}{1+\cfrac{1^3}{u_2+\cfrac{2^3}{1+\cfrac{2^3}{u_3 + \ddots}}}}}\tag{1}$</p>

<p>where the sequence of $u_n$, starting with $n = 1$, is given by the <em>linear</em> function</p>

<p>$u_n = 4(2n-1) = 4, 12, 20, 28, \dots$</p>

<p>This has rather slow convergence. Using an approach similar to Apéry's of finding a faster converging version, I found via <em>Mathematica</em> that,</p>

<p>$\zeta(3) = \cfrac{6}{v_1 + \cfrac{1^3}{1 + \cfrac{1^3}{v_2 + \cfrac{2^3}{1 + \cfrac{2^3}{v_3 +\ddots}}}}}\tag{2}$</p>

<p>where the $v_n$ are now given by the <em>cubic</em> function</p>

<p>$v_n = 4(2n-1)^3 = 4, 108, 500, 1372, \dots$</p>

<p><em>Question</em>: Can anyone prove that (2), with $v_n$ defined by the cubic function, is indeed true?</p>

<p><em>Postscript</em>: A short description of Apéry's accelerated continued fractions for $\zeta(2)$ and $\zeta(3)$ is given <a href=""http://tpiezas.wordpress.com/2012/05/04/continued-fractions-for-zeta2-and-zeta3/"">here</a>.</p>
",<number-theory>
"<p>So the question is in the title. And I mean dense in positive real numbers ofcourse. Somehow I cannot grasp if this is very trivial or not. The prime numbers aren't that dense, but are there enough of primes to make the fractions constructed from them dense?</p>
",<number-theory>
"<p>Let $a \in (\mathbb{Z/nZ})^\times$ then $ax+ny = 1$ for some $x,y\in \mathbb{Z}$</p>

<p>$$-a = n - a$$</p>

<p>Then</p>

<p>$$ny = 1 - ax = 1 + (-a)x$$ </p>

<p>Putting value of -a</p>

<p>$$ny = 1 + (n-a)x \qquad \implies (a-n)x + ny = 1 \qquad \implies ax + n(y-x) = 1$$ </p>

<p>Am I right?</p>
",<number-theory>
"<p>Given small $\epsilon&gt;0$ how small should $n\in\Bbb N$ be such that if $a,b,c,d,q,r,u,v,x,y,m,m'\in\Bbb N$ with $gcd(a,b)=gcd(a,x)=gcd(b,y)=1$ the following relations can hold with constraints $c,d=(n^3,2n^3)$, $a,b\in(n,2n)$, $x\in(1,a)$, $y\in(1,b)$, $m,m'=\theta(n)$ ($\theta$ is landau function), $u,v\in(n^{2+\epsilon},2n^{2+\epsilon})$ and $q,r\in(n^4,2n^4)$? 
$$c{b^2}=au + qm,\quad q=ca-x$$
$$d{a^2}=bv + rm',\quad r=db-y$$</p>

<p>In other words given small $\epsilon&gt;0$ and large $n\in\Bbb N$ how many $a,b,c,d,q,r,u,v,x,y,m,m'\in\Bbb N$ satisfy these constraints?</p>

<hr>

<p>Note these imply $$c^2b^2-cqm=qu+xu$$ $$d^2a^2-drm'=rv+yv$$</p>
",<number-theory>
"<p>I am curious about the frequency of occurance of prime numbers in natural numbers. For example starting with the first non-prime 4 = 2^2, then 6 = 2x3, 8 = $2^3$, 9 = $3^2$ etc. Now of course a prime can not be a factor of a number $n \in \mathbb{N}$. Thus taking prime factorization of larger numbers to see some more interesting sets of primes. </p>

<p>1674 = $2^1 \times 3^3 \times 31^1$</p>

<p>1675 = $5^2 \times 67^1$</p>

<p>1676 = $2^2 \times 419^1$</p>

<p>and so on. I am wondering if there has been research done on the frequency of primes. I have found much about how prime numbers themselves are distributed among the natural numbers - but nothing about the frequency of their occurrences as constituents of numbers.</p>

<p>Thanks for any insight, or references to this subject matter.</p>

<p>Thanks,</p>

<p>Brian</p>
",<number-theory>
"<p>Let $S$ be a set of size $n$.  There is an easy way to count the number of subsets with an even number of elements.  Algebraically, it comes from the fact that</p>

<p>$\displaystyle \sum_{k=0}^{n} {n \choose k} = (1 + 1)^n$</p>

<p>while</p>

<p>$\displaystyle \sum_{k=0}^{n} (-1)^k {n \choose k} = (1 - 1)^n$.</p>

<p>It follows that </p>

<p>$\displaystyle \sum_{k=0}^{n/2} {n \choose 2k} = 2^{n-1}$.  </p>

<p>A direct combinatorial proof is as follows: fix an element $s \in S$.  If a given subset has $s$ in it, add it in; otherwise, take it out.  This defines a bijection between the number of subsets with an even number of elements and the number of subsets with an odd number of elements.</p>

<p>The analogous formulas for the subsets with a number of elements divisible by $3$ or $4$ are more complicated, and divide into cases depending on the residue of $n \bmod 6$ and $n \bmod 8$, respectively.  The algebraic derivations of these formulas are as follows (with $\omega$ a primitive third root of unity):  observe that</p>

<p>$\displaystyle \sum_{k=0}^{n} \omega^k {n \choose k} = (1 + \omega)^n = (-\omega^2)^n$</p>

<p>while</p>

<p>$\displaystyle \sum_{k=0}^{n} \omega^{2k} {n \choose k} = (1 + \omega^2)^n = (-\omega)^n$</p>

<p>and that $1 + \omega^k + \omega^{2k} = 0$ if $k$ is not divisible by $3$ and equals $3$ otherwise.  (This is a special case of the discrete Fourier transform.)  It follows that</p>

<p>$\displaystyle \sum_{k=0}^{n/3} {n \choose 3k} = \frac{2^n + (-\omega)^n + (-\omega)^{2n}}{3}.$</p>

<p>$-\omega$ and $-\omega^2$ are sixth roots of unity, so this formula splits into six cases (or maybe three).  Similar observations about fourth roots of unity show that</p>

<p>$\displaystyle \sum_{k=0}^{n/4} {n \choose 4k} = \frac{2^n + (1+i)^n + (1-i)^n}{4}$</p>

<p>where $1+i = \sqrt{2} e^{ \frac{\pi i}{4} }$ is a scalar multiple of an eighth root of unity, so this formula splits into eight cases (or maybe four).  </p>

<p><strong>Question:</strong>  Does anyone know a direct combinatorial proof of these identities? </p>
",<number-theory>
"<p>Let $\pi(x)$ be the number of primes not greater than $x$.</p>

<p><a href=""http://en.wikipedia.org/w/index.php?title=Prime-counting_function"">Wikipedia article</a> says that $\pi(10^{23}) = 1,925,320,391,606,803,968,923$.</p>

<p>The question is how to calculate $\pi(x)$ for large $x$ in a reasonable time? What algorithms do exist for that?</p>
",<number-theory>
"<p>A Fi-binary number is a number that contains only 0 and 1. It does not contain any leading 0. And also it does not contain 2 consecutive 1. The first few such number are 1, 10, 100, 101, 1000, 1001, 1010, 10000, 10001, 10010, 10100, 10101 and so on. I have n. How to calculate the nth Fi-binary number ?</p>
",<number-theory>
"<blockquote>
  <p>Find all triples $(a,b,c)$ of positive integers such that if $n$ is not divisible by any prime less than $2014$, then $\color{red}n+\color{red}c$ divides $\color{red}a^n+\color{red}b^n+\color{red}n$</p>
</blockquote>

<p>This problem is from the New Zealand TST 2014 and there is only one solution:</p>

<p>$\textbf{answer}$:$\color{red}(\color{red}a,\color{red}b,\color{red}c)\color{red}=(1,1,2)$</p>

<p>Can you someone help explain why there is only one solution?</p>
",<number-theory>
"<p>Can we prove by induction that for every integer $k &gt;1 $ , $2+2^k \choose r$ is even for all $2 &lt; r \le n=1+2^{k-1}$ ? Or by some divisibility properties of Binomial co-efficients ?</p>

<p>I wanted to start as $2^k \choose r$ $+$ $2$$2^k \choose r-1$ $+$ $2^k \choose r-2$ $=$$ 2+2^k \choose r$ but I don't know whether $2|{2^k \choose m}$ or not ... Please help . Thanks in advance  </p>
",<number-theory>
"<p>The question is this.</p>

<blockquote>
  <p>Suppose that p is a prime, $p\ge7$. Show that $(\frac np)=(\frac {n+1}p)=1$ for at least one number n in the set {1,2,...,9}.</p>
</blockquote>

<p>I think seperating into two cases when n=1 and when n=4 will help me prove it. But I can't think further...</p>

<p>Plz, HELP ME!!!!</p>
",<number-theory>
"<p><strong>Question:</strong></p>

<blockquote>
  <p>For any  $a,b\in \mathbb{N}^{+}$, if $a+b$ is a square number, then $f(a)+f(b)$ is also a square number. Find all such functions.</p>
</blockquote>

<p><strong>My try:</strong> It is clear that the function
$$f(x)=x$$ satisfies the given conditions, since:
 $$f(a)+f(b)=a+b.$$</p>

<p>But is it the only function that fits our needs? </p>

<p>It's one of my friends that gave me this problem, maybe  this is a Mathematical olympiad problem.  Thank you for  you help.</p>
",<number-theory>
"<p>$$s^{2}=4m^{2}n^{2}+p^{2};
p^{2}=m^{2}+n^{2};
1&lt;m&lt;n&lt;p&lt;s$$
I think that this equation does not have positive Integer solution, but how to prove?</p>
",<number-theory>
"<p>Use Polya's enumeration to determine the number of six-sided dice that can be manufactured if each of three different labels must be placed on two of the faces. </p>

<p>Can you help me please to solve this exercise ? I have looked for Polya's enumeration formula on Wikipedia but it is very strange for me. </p>

<p>Thanks :)</p>
",<number-theory>
"<p>Using Burnside's Lemma find out: </p>

<p>How many different necklaces having $5$ beads can be formed using $3$ different kinds of beads, if we discount :</p>

<p>(a) Both flips and rotations?</p>

<p>(b) Rotations only?</p>

<p>(c) Flips only?</p>

<p>I don't know how to apply this lemma. Have you some solved exercises?</p>

<p>thanks :) </p>
",<number-theory>
"<p>Is it possible that there be something simple and essential in number theory that great mathematicians could have missed so that we still do not have any proof of Riemann Hypothesis?</p>

<p>Is it meaningfully probable to take the consideration that conjectures like Riemann Hypothesis, Twin Prime conjecture etc are elementary but that we have missed something in number theory to find that simple $7$ page proof that an average undergraduate could understand?</p>
",<number-theory>
"<p>I have been studying cuban primes and while the official definition of cuban primes contains only two variations, I have also seen a reference to generalized cuban primes, which has a much larger set. I have also seen on only one site a third variation of the equation where the difference between the first and second cubed number is four, instead of one or two, which are the ones included in the official definition, and is a subset of generalized cuban primes (from what I can tell). </p>

<p>What exactly is the difference between generalized cuban primes and cuban primes, other than the equation that defines them? Do generalized cuban primes essentially state that there are more than two variations of the classic definition of cuban primes?</p>

<p>Cuban primes are defined as $\frac{x^3-y^3}{x-y}$, and generalized cuban primes are defined as ""primes of the form $x^2 + xy + y^2$; or primes of the form $x^2 + 3y^2$; or primes $\equiv 0$ or $1 \pmod 3$."" </p>

<p>The two officially recognized forms of cuban primes are those where $y=x+1$ and $y=x+2$, and the ""unofficial"" third form that I have seen one mention of on oeis.org is where $y=x+4$. I have been unable to find any other variations (such as $y=x+5$) even in oeis.org, however if it is applied, $y=x+5$ seems to be a subset of the generalized cuban primes.</p>
",<number-theory>
"<p>A monotonic number is a number in which the digits are in non-decreasing order. I found by computer that most of these numbers are squares of these numbers 
$$3 \ldots 34,3 \ldots 35,3 \ldots 37,3 \ldots 367,3\ldots 36 \ldots 7,16\ldots 67$$ 
My question is if there are finite amount of exceptions. I found in <a href=""http://math.fau.edu/Yiu/RecreationalMathematics2003.pdf"" rel=""nofollow"">this</a> (page 133) that if you are squaring only monotonic numbers then there are a finite amount of exceptions.</p>
",<number-theory>
"<p>Given a prime $p$ and positive integer $t \ll \log p$ (say $t = \sqrt{\log p}$), is there an  algorithm that is polynomial time in $\log p$ to sample uniform $X, Y \in \mathbb{F}_p$ conditioned on the following constraints:</p>

<p>1) For some $a \in \mathbb{F}_p$, $$X Y = a \mod p.$$
2) Express $X$ as a bitstring $X_1 \ldots X_{\lceil \log p\rceil}$ using the binary representation. Then the $\lceil \log p\rceil$ binary random variables so obtained lie in a given subspace of $\mathbb{F}_2^{\lceil \log p \rceil}$ of dimension $\lceil\log p\rceil - t$. </p>

<p>3) Same condition as 2) for $Y_i$'s.</p>

<p>I will be happy to get any pointers or reference for this. Even some intuition on why this might or might not be true will be helpful.</p>
",<number-theory>
"<p>Let $m\geq 3$, I need to show that  $ d \equiv 0 \ mod \ m $ where $d=\sum_{i \in (\frac{\mathbb{Z}}{m\mathbb{Z}})^*} i$ .</p>

<p>That is if we sum all elements in the group of units in $\frac{\mathbb{Z}}{m\mathbb{Z}}$,then it divisible by $m$. </p>
",<number-theory>
"<p>Suppose we are given a continued fraction $$\frac{p}{q}=a_{1}+\frac{1}{a_{2}+\frac{1}{a_{3}+\frac{1}{a_{4}+\cdots}}}$$</p>

<p>I am trying to find an expression, possibly asymptotic, for the sum of the $a_i$'s for a given $\frac{p}{q}$.</p>

<p>I understand that this is related to the Stern-Brocot tree. In particular, our problem is equivalent to finding on which row does the fraction $\frac{p}{q}$ first appear in the Stern-Brocot tree.</p>

<p>Are there any results to this problem?</p>
",<number-theory>
"<p>Imagine that i have a 50 x 100 cm baking tray, and i have a load of cinnamonbuns, shaped like a circle with a diameter of 10cm.</p>

<p>How do i calculate the best place to place my cinnamonbuns, as the ammount of cinnamonbuns increases?</p>

<p>When i say ""best"" i mean the position where each cinnamonbun is the farthest away from other cinnamonbuns (and the side of teh tray) possible.</p>
",<number-theory>
"<p>Could someone give me a hint on the computation of the asymptotic bound for the following series
$$
\sum_{n\leq x}\frac{\log n }{ \varphi(n)}\,,
$$
where $\varphi(n)$ is the Euler totient function? Also a bibliography suggestion would be ok.</p>
",<number-theory>
"<blockquote>
  <p>Let $M=\begin{bmatrix} a &amp; b \\ c &amp; d \end{bmatrix} \in M_{2}(\mathbb Z)$, prove that there exist
  $A,B,C \in M_{2}(\mathbb Z)$ such that $M=A^2+B^2+C^2$.</p>
</blockquote>

<p>My idea: if we can find $a_{i},b_{i},c_{i},d_{i}\in \mathbb Z,i=1,2,3$ such</p>

<p>$$a=a^2_{1}+a^2_{2}+a^2_{3}$$
$$b=b^2_{1}+b^2_{2}+b^2_{3}$$
$$c=c^2_{1}+c^2_{2}+c^2_{3}$$
$$d=d^2_{1}+d^2_{2}+d^2_{3}$$</p>

<p>then we prove it. But I can't find such numbers. Thank you.</p>

<p>This problem is from this paper: (has a solution)  (45)
<a href=""http://mathproblems-ks.com/?wpfb_dl=8"" rel=""nofollow"">http://mathproblems-ks.com/?wpfb_dl=8</a></p>

<p>And in fact I want to see other methods? Thank you (because I think this problem has other methods.)</p>
",<number-theory>
"<p>Would it be possible to bound this function for primes in terms of the maximum difference between the images of the function and their closest primes (for instance, the fifth term is 33 and has difference of two in terms of its closest prime, 31):</p>

<p>$2 g(n-1) = g(n)+1$</p>

<p>I have found that the images are main,y bounded by a difference of four 4 or 2 for the closest primes, however certain images can be bound by 30 as I have heuristicly found. But, my question is, would it be possible, perhaps using the P.N.T. and other number theoretical or group theoretical tools,to find a bound for the primes for each image?</p>
",<number-theory>
"<p>Is $$1+\cfrac{1}{1+\cfrac{1}{2+\cdots}} $$ or$\{1,1,2,3,4,5,\cdots,i,\cdots \} , i\in \mathbb{N}$ the simple continued fraction algebraic or transcendental?</p>

<p>Any reference is appreciated</p>

<p><strong>EDIT</strong> and replacing $\{1,1,2,3,4,5,\cdots,i,\cdots \}$ with $\{1,2,2^2,2^3,2^4,2^5,\cdots,2^i,\cdots \} $ $i\in \mathbb{N}$ or any other patternful sequences that are upper unbounded ,can we get an algebraic number?</p>
",<number-theory>
"<p>Show that it is possible to arrange the numbers 1, 2, . . . , n
in a row so that the average of any two of these numbers
never appears between them. </p>

<p><em>Hint</em>: Show that it suffices
to prove this fact when n is a power of 2. Then use mathematical
induction to prove the result when n is not a power of 2.</p>

<p>This is a problem from Rosen's discrete mathematics book.</p>

<p>I tried to think of it this way:</p>

<hr>

<p>Take any n,find its nearest power of 2,(say m)less than or equal to n,find the arrangement for 1,2,...m,then the rest of numbers m+1,....,n just fill in the gaps within the sequence 1,...,m in a certain way.</p>

<hr>

<p>e.g.,Take n=12,nearest power of two less than or equal to n,is,8.So arrange 1,2,...,8 which gives 1,5,3,7,2,6,4,8.Now the numbers 9,10,11,12 just fills in the arranged sequence in a particular way.Its the last part I am struggling with. How exactly does the rest of the numbers fit in?
I may be wrong with the above approach,so please share your solution,in that case.</p>
",<number-theory>
"<p>The Möbius inversion formula says given two arithmetic functions $\hat{g}(k)$ and $g(k)$ related by $$\sum_{d\mid k}\hat{g}(d)=g(k)$$
Then $$\sum_{d\mid k}\mu(d)g\left(\frac{k}{d}\right)=\hat{g}(k)$$
Can someone give me a very elementary proof of this?</p>

<p>I don't know anything about analytic number theory, though I know the definition of the Möbius function, and have used it before with out ever reading to deeply into it for example: I know that,
$\frac{x}{1-x}=x+x^2+x^3+x^4+\cdots$
And that if I subtract the even powers I get,
$\frac{x}{1-x}-\frac{x^2}{1-x^2}=x+x^3+x^5+x^7+x^9+\cdots$
And then If I subtract the the powers that are multiples of 3 I get,
$\frac{x}{1-x}-\frac{x^2}{1-x^2}-(\frac{x^3}{1-x^3}-\frac{x^6}{1-x^6})=x+x^5+x^7+x^{11}+\cdots$
Continuing in this matter one sees we are essentially yielding combinations of the original sum where the argument is a combination of distinct primes, and the coefficients are determined by weather or not the number of primes is even or odd. So by the definition of the Möbius function I can easily see, $\sum_{k=1}^\infty\frac{\mu(k)x^k}{1-x^k}=x$, although the first theorem I mentioned doesn't seem so obvious to me, and so I would appreciate a simple proof.</p>
",<number-theory>
"<p>I try to understand something from number theory and the author gave this as an excersise: Prove that $z\longmapsto 2\sqrt{p}\cos z$ is a bijection of a set $\Theta_{p}=\left[i\log\sqrt{p},0\right]\cup\left[0,\pi\right]\cup\left[\pi,\pi+i\log\sqrt{p}\right]$ onto $\left[-\left(p+1\right),p+1\right]$. Here $p&gt;2$ is a prime number. I proved that the picture lies in $\left[-\left(p+1\right),p+1\right]$, but injection and sirjection I can't prove. Help.</p>
",<number-theory>
"<blockquote>
  <p>Let $a_1$ be an integer. Then we assume  </p>
  
  <p>$$
a_{n+1} =
\begin{cases}
3a_n+1,&amp;\text{$a_n$ is odd}\\
\frac{a_n}{2},&amp;\text{$a_n$ is even}
\end{cases}
$$</p>
</blockquote>

<p>Now we prove that</p>

<blockquote>
  <p>for any $a_1\in\mathbb N$, there exists $N$ which satisfy: $a_n=1,2$ or $4$,$n\geq{N}$.</p>
</blockquote>

<p>At first I want to give it a suitable category for the problem: analysis. And I want to use the basic method: evaluate the upper bound for $a_n$, however I find it's not easy because the iteration is rely on the odd or even property of $a_n$. So I attempt the method of number theory. But I failed to find any way to go over it. Can anyone have idea? Thank you. </p>
",<number-theory>
"<p>Prove there is an infinite amount of natural numbers for which $\displaystyle \frac{n(n+1)}{2}$ is a perfect square. </p>
",<number-theory>
"<p>Please could someone give me a hint on this sequences question? The question is to prove that every integer appears infinitely many times in the following sequence:</p>

<p>$$ \pm 1^{2} , \pm 1^{2} \pm 2^{2} , \pm 1^{2} \pm 2^{2} \pm 3^{2} , \pm 1^{2} \pm 2^{2} \pm 3^{2} \pm 4^{2} , ... $$</p>

<p>Any help would be much appreciated, thank you (:</p>
",<number-theory>
"<p>Let $\mu(d)$ be the Möbius function, and $\mu_r(d)$ be the modified Möbius function which satisfies $\mu_r(d)=0$ if $d$ has strictly more than $r$ distinct prime factors. Let $\psi_r(n)=\sum_{d\mid n}\mu_r(d)$. Finally, we let $P_z$ be the product of all primes less than or equal to $z$. Then, what I am humbly requesting help with proving, is 
$$
\sum_{d\mid P_z}\frac{1}{d}\sum_{\delta\mid d}\mu(d/\delta)\psi_r(\delta) = 
\sum_{\delta\mid P_z}\frac{\psi_r(\delta)}{\delta}\sum_{d\mid P_z/\delta}
\frac{\mu(d)}{d}.
$$
Thank you for the help!</p>
",<number-theory>
"<p>Is there a polynomial $p(x)$ with real coefficients and degree at least one such that $[p(n)]$ is prime for every natural number $n$? If yes, what is such a polynomial $p(x)$ and if no, how to prove?</p>
",<number-theory>
"<p>I am looking for a reference/hints of proof towards statements of the kind;</p>

<p>Given an irreducible system of $n$ polynomial equations over $\mathbb Q$ in $n$ variables
$$P_i(x_1,...,x_n)=0,\quad i=1,...,n\,,$$
with $k$ (generally) complex solutions $\sigma_1,...,\sigma_k$, the following is true. </p>

<p>For any $P$ and $Q$ polynomials (*edit : in $n$ variables) over $\mathbb Q$, the following sum is an element of $\mathbb Q$;
$$\sum_{a=1}^k \frac{P(\sigma_a)}{Q(\sigma_a)}\in \mathbb Q\,.$$</p>

<p>I suppose that it is possible to prove this by hand in the simpler case of purely polynomial sums, I have no clue for the rational-function case.</p>
",<number-theory>
"<p>One question, two parts...</p>

<p>(a) Consider the group $(Z_n, + \mod{n})$. If $n$ is an odd prime number, determine (with proofs) if all automorphisms of $(Z_n, + \mod{n})$ are even permutations of $Z_n$. </p>

<p>(b) Determine the automorphism group of the group $S_3$ (under functional composition).</p>

<p>Strategy?</p>
",<number-theory>
"<p>Is every integer (say $d$) a quadratic residue mod some prime number $p$?</p>
",<number-theory>
"<p>I'm reading the book <em>A First Course In Modular Forms</em> and it defines the term <strong>weakly modular of weight $k$</strong> as following:</p>

<p>Let $K$ be an integer. A meromorphic function $f:H\rightarrow\mathbb{C}$ is weakly modular of weight $k$ if
$$
f(\gamma(\tau))=(c\tau+d)^kf(\tau)
$$</p>

<p>for every $\gamma\in$ SL$_2(\mathbb{Z})$ and $\tau\in H$, where $H$ means the upper half complex plane.</p>

<p>My question is, why in definition we use meromorphic function?</p>

<p>I think meromorphic means such function has poles in a set of isolated points. What if let $\tau$ be such a point? Then the RHS of the equation will be $(c\tau+d)^k\infty$, which is undetermined.</p>

<p>Any suggestion?</p>
",<number-theory>
"<p>Imagine a binary string of increasing length, up to infinity.</p>

<p>What makes it so special? Well, just a simple ""rule"":</p>

<p>for any given length (odd or even), if one folds the string in half, there is at least one couple of ones that overlaps.</p>

<p>The question is now about the distribution of ones (zeros), as a function of string length. What are its properties? Are there any?</p>

<p>A trivial example that follow the rule is the string $111010101010....$.</p>
",<number-theory>
"<p>I have tried fermats little theorem with fail. I cannot see how to proceed with this problem, find all prime $x$ such that $x^2+2$ is a prime.</p>
",<number-theory>
"<p>What is the most motivating way to introduce LCM of two integers on a first elementary number theory course? I am looking for real life examples of LCM which have an impact. I want to be able to explain to students why they need to study this topic.</p>
",<number-theory>
"<p>Open problem in Geometry/Number Theory.  The real question here is:</p>

<p><em>Is there an infinite family of points on $y=x^2$, for $x \geq 0$, such that the distance between each pair is rational?</em></p>

<p>The question of ""if not infinite, then how many?"" follows if there exists no infinite family of points that satisfies the hypothesis.</p>

<p>We have that there exists a (in fact, infinitely many) three point families that satisfy the hypothesis by the following lemma and proof.</p>

<p><strong>Lemma 1:</strong>  <em>There are infinitely many rational distance sets of three points on $y=x^2$.</em></p>

<p>The following proof is by Nate Dean.</p>

<p><em>Proof.</em>  Let $S$ be the set of points on the parabola $y = x^2$ and let $d_1$ and $d_2$ be two fixed rational values.  For any point, $P_0(r)=(r, r^2) \in S$, let $C_1(r)$ be the circle of radius $d_1$ centered at $P_0(r)$ and let $C_2(r)$ be the circle of radius $d_2$ centered at $P_0(r)$.  Each of these circles must intersect $S$ in at least one point. Let $P_1(r)$ be any point in $C_1(r) \cap S$ and likewise, let $P_2(r)$ be any point in $C_2(r) \cap S$.  Now let $dist(r)$ equal the distance between $P_1(r)$ and $P_2(r)$. The function $dist(r)$ is a continuous function of $r$ and hence there are infinitely many values of $r$ such that $P_0(r)$, $P_1(r)$,
and $P_2(r)$ are at rational distance. $ \blacksquare $</p>

<p>This basically shows that the collection of families of three points that have pairwise rational distance on the parabola is dense in $S$.</p>

<p>Garikai Campbell has shown that there are infinitely many nonconcyclic rational distance sets of four points on $y = x^2$ in the following paper: <a href=""http://www.ams.org/journals/mcom/2004-73-248/S0025-5718-03-01606-5/S0025-5718-03-01606-5.pdf"">http://www.ams.org/journals/mcom/2004-73-248/S0025-5718-03-01606-5/S0025-5718-03-01606-5.pdf</a></p>

<p>However, to my knowledge, no one has come forward with 5 point solutions, nor has it been proven that 5 point solutions even exist.</p>

<p>But I know that many people have not seen this problem!  Does anyone have any ideas on how to approach a proof of either the infinite case or even just a 5 point solution case?</p>

<p><strong>Edit:</strong>  The above Lemma as well as the paper by Garikai Campbell do <strong>not</strong> include the half-parabola ($x \geq 0$) restriction.  However, I thought that the techniques that he employed could be analogous to techniques that we could use to make progress on the half-parabola version of the problem.</p>
",<number-theory>
"<p>I need to define a formula for a half unit of the smallest decimal place in unit price ($UP$), or understand if this can be defined with a formula?</p>

<p>What I have is this</p>

<p>$$ T_{min,max} = Amt +/- (Qty * NT)$$</p>

<p>Where</p>

<p>$T$ - Tolerated Amount<br>
$Amt$ - Original Amount<br>
$Qty$ - Original quantity<br>
$NT$ - Half unit of the smallest decimal place in Unit Price ($UP$)</p>

<p>Examples of $NT$</p>

<p>$$UP = 2.456 {\implies} NT = 0.0005$$
$$UP = 2.4 {\implies} NT = 0.05$$
$$UP = 2.44500230012 {\implies} NT = 0.000000000005$$</p>

<p><strong>How do I write formula to calculate $NT$?</strong></p>
",<number-theory>
"<p>I'm looking for literature on solving problems of the form
$$
n_1^\alpha+\cdots+n_k^\alpha=(n_1+\cdots+n_k)^\beta
$$
for positive integers $n_1,\ldots,n_k$ and fixed parameters $k$ and $\alpha\ne\beta.$ Any ideas?</p>

<p>This is perhaps similar to multigrade equations or ""equal sums of like powers"" but not quite similar enough to use those results (as far as I can see). But surely this is too simple not to have been studied?</p>
",<number-theory>
"<p>I was thinking about this and am wondering if it is true. Currently trying to look for a counter example, but haven't found anything yet.</p>

<p>Conjecture: $p^\alpha$ can be written as the sum of two primes, for any prime $p$, $\alpha \geq 2 \in \mathbb{N}$.</p>
",<number-theory>
"<p>There are 3 parts of the problem.</p>

<ol>
<li>Let d be a perfect square, possibly 0. Show that there is a quadratic form $ax^2+bxy+cy^2=0$ of discriminant d for which a=0.<br><br></li>
<li>Let a,b,c be integers with $a\ne0$. Show that if one root of the eqatuion $au^2+bu+c=0$ is rational then the other one is, and that $b^2-4ac$ is a perfect square, possibly 0.</li>
<li>Show also that if $b^2-4ac$ is a perfect square, possibly 0, then the roots of the equation $au^2+bu+c=0$ is rational.</li>
</ol>

<p>For question 1., I wrote $d=k^2$ for some integer k. Then, I got to $b^2-k^2=4ac$ and got stuck.</p>

<p>For question 2 and 3, all the claims of the problem sounds perfectly right (which should be always right...) and I couldn't start from both of those ones.</p>

<p>Thank you.</p>
",<number-theory>
"<p>I understand the mechanics of the proof of Ostrowski's Theorem, but I'm a little unclear on why one should expect valuations to be related to primes. Is this a special property of number fields and function fields, or do primes of K[x,y] correspond to valuations on K(x,y) in the same way?</p>

<p>I'm hoping for an answer that can explain what exactly are the algebraic analogs of archimedian valuations, and how to use them - for example, I've heard that the infinite place on K(x) corresponds to the ""prime (1/x)"" - how does one take a polynomial in K[x] ""mod (1/x)"" rigorously?</p>

<p>Thanks in advance.</p>
",<number-theory>
"<p>Is it true that there are infinitely many nonprime integers $n$ such that $3^{n-1} - 2^{n-1}$ is a multiple of $n$?</p>
",<number-theory>
"<p>Apparently Fermat stated but didn't provide proofs of various theorems named after him, including <a href=""http://en.wikipedia.org/wiki/Fermat&#39;s_little_theorem"" rel=""nofollow"">Fermat's little theorem</a>, <a href=""http://en.wikipedia.org/wiki/Fermat&#39;s_theorem_on_sums_of_two_squares"" rel=""nofollow"">Fermat's theorem on sums of two squares</a>, <a href=""http://en.wikipedia.org/wiki/Fermat_polygonal_number_theorem"" rel=""nofollow"">Fertmat's polygonal number theorem</a>, and most famously, of course, <a href=""http://en.wikipedia.org/wiki/Fermat&#39;s_Last_Theorem"" rel=""nofollow"">Fermat's Last Theorem</a>. Why is this?</p>
",<number-theory>
"<p>How to find all natural $x$ for that $x^2 + (x+1)^2$ is a perfect square?</p>
",<number-theory>
"<p>Can you give a definition of the Conway base-13 function better than the one actually present on wikipedia (<a href=""http://www.google.it/url?sa=t&amp;source=web&amp;cd=1&amp;ved=0CBgQFjAA&amp;url=http%3A%2F%2Fen.wikipedia.org%2Fwiki%2FConway_base_13_function&amp;ei=empRTNK-OsqWON2_uPUE&amp;usg=AFQjCNFqIS-UhBV9Miw1QnAZJaxnswI3Yg&amp;sig2=AlFPiLqMkh3gd4VPEHJI-Q"">here</a>), which isn't clear? Maybe with some examples?</p>
",<number-theory>
"<p>Let's call $A\subseteq\mathbb{N}$ <em>dense</em> if $$\text{lim inf}_{n\to\infty}\frac{|A\cap\{1,\ldots,n\}|}{n} = 1.$$</p>

<p>Is the intersection of two dense sets dense again? Or does the intersection of two dense sets at least satisfy the weaker statement $$\text{lim sup}_{n\to\infty}\frac{|A\cap\{1,\ldots,n\}|}{n} = 1?$$</p>
",<number-theory>
"<p>This September I am participating in a competition called the Australian Intermediate Mathematics olympiad, and you may not have heard of it but it's very similar to the AIME. Could you please tell me how I could prepare for it well because I really want to do well.</p>

<p>I bought some math books on the internet and am studying them... is this a good way or preparing? These math books cover topics such as proving, number theory, combinatorics and so on... Here is a practice paper for the competition and I have done it many times. It is easy but I hope that this year's will be similar, however it is always better to preparing for it. => <a href=""http://www.amt.edu.au/wuaimo.pdf"" rel=""nofollow"">http://www.amt.edu.au/wuaimo.pdf</a></p>

<p>Edit: How much time should I be spending everyday practicing?</p>
",<number-theory>
"<p>I have a question regarding the follow problem:</p>

<blockquote>
  <blockquote>
    <p>Show that the prime number 27644437 splits completely in $L = \mathbb{Q}(\sqrt{55})$.</p>
  </blockquote>
</blockquote>

<p>From what I understand. This deals with ramification. </p>

<p>\begin{eqnarray}
\sum_1^{r}e_if_i = n &amp;, &amp;\text{where }n = [L:K]
\end{eqnarray}</p>

<p>For our prime number, $p$, to split completely into $L$, then $e_i=f_i =1$ for all $i$. Now in order for it to be completely split, it can not be ramified. Being ramified will require at least one of the $e_i&gt;1$. However, we do not need to worry about this situation in this problem because $p\not| \text{disc}(L)$. </p>

<p>Minimal Polynomial of $L$ is $x^2-55$. Since it is not ramified, this will only leave two possibilities: inert or split. Since we are dealing with a second degree polynomial and we already ruled out the possibility of being ramified, then it must be completely split. Thus, inert or completely split. </p>

<p>This is where it gets complicated. If I want to show it is completely split I need to find at least one integer such that $x^2 -55 (\text{mod}p) = 0$. </p>

<p>I do not need to find the second integer because $p$ does not ramify $L$. Looking over so many numbers just to verify it meets this condition is ridiculous. There has to be another approach.</p>

<p>I asked my professor and he gave me a hint suggesting I use the Quadratic Reciprocity Law. However, I do not know how to apply it correctly in this case. I would assume looking at the Quadratic Residue symbol formed by the discriminate of $L$ and the prime number $p$. This will either result in an answer of $0, 1, \text{or, } -1$; however, I am not sure what the value would mean. </p>

<p>Thank You for your time, and thank you in advance for any feedback. </p>
",<number-theory>
"<p>It is an exercise on the lecture that i am unable to prove.</p>

<p>Given that $gcd(a,b)=1$, prove that $gcd(a+b,a^2-ab+b^2)=1$ or $3$, also when will it equal $1$?</p>
",<number-theory>
"<p>I know that there's a proof (of Landau from 1908) that the numbers of integers that can be represented as sum of two squares which are smaller than $n$ is</p>

<p>$$
\Theta\left(n\over\sqrt{log (n)}\right)
$$</p>

<p>I would be thankful if someone can direct me to the proof (I only found a book in German),
or better yet if someone can prove it to me :)</p>

<p>Thank You</p>
",<number-theory>
"<p>A magic square of order $N$ is an $ N \times N $ matrix with positive integral
entries such that the elements of every row, every column and the two diagonals all add up to the same number. If a magic square is filled with numbers
in arithmetic progression starting with $a \in N $ and common dierence $ d  \in N$,
what is the value of this common sum?</p>

<p>I am stuck on this problem. Can anyone help me please...</p>
",<number-theory>
"<p>The fundamental theorem of arithmetic says that every integer $n&gt;1$ is of the form</p>

<p>$$ n = \prod_i {p_i}^{a_i} $$</p>

<p>where $p_i$ is the $i$ th prime and $a_i$ is a nonnegative integer.</p>

<p>My question is how many $m$ satisfy $ 1&lt;m&lt;n $ and </p>

<p>$$ m = \prod_i {(p_i-1)}^{b_i} $$</p>

<p>where $p_i$ is the $i$ th ODD prime and $b_i$ is a nonnegative integer.</p>

<p>Lets call $f(n)$ the amount of $m$ that satisfy the above.</p>

<p>What is a good approximation to $f(n)$ ?</p>
",<number-theory>
"<p>Let $G$ be a finite group. I know that the set of irreducible representations of $G$ over the complex numbers (up to isomorphism) is finite.</p>

<p>Let us fix our attention on some irreducible representation of $G$ over $\Bbb{C}$ 
$$\rho: G \longrightarrow GL_n(\Bbb{C})$$
My intuition tells me that there exists some finite extension $K \supset \Bbb{Q}$, and some irreducible representation
$$\sigma:G \longrightarrow GL_n(K)$$
such that $\rho = i \circ \sigma$, where $i: GL_n(K) \longrightarrow GL_n(\Bbb{C})$ is the inclusion (every matrix with entries in $K$ has entries also in $\Bbb{C}$).</p>

<p>For example, if $G=C_2$ is the group with two elements, we can think the two irreducible representations of $C_2$ as $\sigma_1, \sigma_2 : C_2 \longrightarrow GL_1(\Bbb{Q})$, so $K=\Bbb{Q}$.</p>

<p>However I don't know if this is true for any finite group  (but I strongly believe that this is true, maybe you can give me some reference).</p>

<p>My question is: given $G$ a finite group, can we find some number field $K$, such that all irreducible representations of $G$ over the complex numbers can be thought as irreducible representations over $K$ (i.e. all involved matrices actually have entries in $K$)? Can we find a minimal such number field?</p>
",<number-theory>
"<p>I've solved for it making a computer program, but was wondering there was a mathematical equation that you could use to solve for the nth prime?</p>
",<number-theory>
"<p>In part of my research, the following problem has come up.</p>

<p>Consider the system of equations (in complex numbers)</p>

<p>$$z^b w^c = 1,\quad z^d w^e = 1.$$</p>

<p>I am interested in the solution set when we restrict both $z$ and $w$ to be $a^{\textrm{th}}$ roots of unity, for some positive integer $a$. Of course, one immediately sees that $(z, w) = (1, 1)$ is a solution.  </p>

<blockquote>
  <p>What are some nice necessary and sufficient conditions on $a, b, c, d,$ and $e$ which guarantee that $(z, w) = (1, 1)$ is the ONLY solution?</p>
</blockquote>

<p>To give an idea of the flavor of answer I'd be most happy with, one must have $\textrm{gcd}(a, b, d) = \textrm{gcd}(c, e) = 1$, because if $z$ is any $\textrm{gcd}(a, b, d)^{\textrm{th}}$ root of $1$ (which is neccesarily an $a^{\textrm{th}}$ root of $1$), then $(z, 1)$ is a solution to both equations.</p>

<p>It also turns out that $z$ and $w$ must both be $\textrm{gcd}(a, be - cd)^{\textrm{th}}$ roots of $1$.</p>

<p>I'd love to have an answer like ""$\textrm{gcd}(a, be - cd) = \textrm{gcd}(a, b, d) = \textrm{gcd}(c, e) = 1$ is necessary and sufficient"", with, perhaps, a few more estimates on gcd terms.</p>

<p>This problem can also been generalized (and I am interested in that case as well).  Suppose you are given $3$ equations</p>

<p>$$z^a w^b = 1,\quad z^c w^d = 1,\quad z^e w^f = 1,$$</p>

<p>with $z$ and $w$ complex numbers of modulus $1$.</p>

<blockquote>
  <p>What are necessary and sufficient conditions on $a, b, c, d, e$ and $f$ which guarantee that the only simultaneous solution is $(z, w) = (1, 1)$?</p>
</blockquote>

<p>The previous problem is a special case of this (which comes from setting $b = 0$.  Clearly then, $z$ must be an ath root of unity.  It turns that if $b$ is $0$, using the fact that $\textrm{gcd}(d, f) = 1$ one can show $w$ must also be an $a^{\textrm{th}}$ root of unity).</p>

<p>And please feel free to retag as appropriate!</p>

<p>Thank you in advance.</p>
",<number-theory>
"<blockquote>
  <p>If $\gcd(a,30)=1$ then $60 \mid (a^4+59)$.</p>
</blockquote>

<p>If $\gcd(a,30)=1$ then we would be trying to show $a^4\equiv 1 \mod{60}$ or $(a^2+1)(a+1)(a-1)\equiv 0 \mod{60}$. We know $a$ must be odd and so $(a+1)$ and $(a-1)$ are even so we at least have a factor of $4$ in $a^4-1$. Was thinking I could maybe try to show that there is also a factor of $3$ and $5$ necessarily giving that $a^4-1\equiv0 \mod{60}$.</p>

<p>Other things I was thinking was that as $Ord_n(a) \mid \phi(n)=\phi(60)=16$ that we just need to show that $Ord_n(a) \in \{1,2,4 \}$.</p>

<p>Any hints? I have the exam soon =/</p>
",<number-theory>
"<blockquote>
  <p>Prove that 
  $$(a+b+c)^{333}-a^{333}-b^{333}-c^{333}$$
  is divisible by
  $$(a+b+c)^{3}-a^{3}-b^{3}-c^{3},$$
  where $a,,b,c -$ integers, such that $(a+b+c)^{3}-a^{3}-b^{3}-c^{3}\not =0$</p>
</blockquote>

<p><strong>My work so far:</strong></p>

<p>$(a+b+c)^{3}-a^{3}-b^{3}-c^{3}=3(a+b)(b+c)(c+a)$</p>

<p>Let $333=3\cdot111=3t,$ where $t=111$.</p>

<p>$(a+b+c)^{333}-a^{333}-b^{333}-c^{333}=(a+b+c)^{3t}-a^{3t}-b^{3t}-c^{3t}$.</p>

<p><strong><em>I need help here.</em></strong></p>
",<number-theory>
"<p>Let $V$ be the set of prime together with the symbol $\infty$. For a prime $v=p$, denote the $p$-adic numbers by $\mathbb{Q}_p$ and the real numbers by $\mathbb{Q}_\infty$. For $v\in V$ the Hilbert symbol is defined for $a,b\in\mathbb{Q}^*_v$ as</p>

<p>\begin{align*}
(a,b)_v=\begin{cases}+1,&amp;\text{ if }ax^2+by^2=z^2\text{ has a non-zero solution }(x,y,z)\in \mathbb{Q}_v^3;\\-1,&amp;\text{ else.}\end{cases}
\end{align*}</p>

<p>Furthermore for $v\in V, a,b\in\mathbb{Q}^*$ we denote by $(a,b)_v$ the Hilbert symbol of $(\bar a,\bar b)_v$ where $\bar a,\bar b$ are the images of $a,b$ in $\mathbb{Q}_v$.</p>

<p>Now a theorem by Hilbert says that $(a,b)_v=1$ for almost all $v\in V$ (and that furthermore $\prod_{v\in V}(a,b)_v=1$, but I'm not interested in this at the moment). The theorem can be found in ""A course in Arithmetic"" by Jean-Pierre Serre for example.</p>

<p>It basically says that there is a finite set $E\subseteq V$ such that 
\begin{align*}
(a,b)_v=\begin{cases}+1,&amp;\text{ if }v\notin E\\-1,&amp;\text{ if }v\in E\end{cases}
\end{align*}</p>

<p>My question is if this set $E$ has a common name in the literature. Something like $E_{a,b}$ would make sense to me (since it depends on $a$ and $b$). If there is no widely used name, what are your suggestions?</p>
",<number-theory>
"<p>Has anybody read <a href=""http://www.wired.com/wiredscience/2013/05/twin-primes/"">Yitang Zhang</a>'s paper on prime gaps? Wired reports ""$70$ million"" at most, but I was wondering if the number was actually more specific.</p>

<p>*<em>EDIT</em>*$^1$:</p>

<p>Are there any experts here who can explain the proof? Is the outline in the annals the preprint or the full accepted paper?</p>
",<number-theory>
"<p>I am stuck on this, and cannot advance. Prove with the pigeonhole principle that there must be a power of seventeen that ends in 00001.</p>
",<number-theory>
"<p>Is $\ln\ln n &lt; \sigma(n)/n$, where $\sigma(n)$ is the sum of the divisors of $n$? Or is this grossly innacurate or too vague? </p>
",<number-theory>
"<p>I've recently been working on a practice midterm for my number theory class, and here is a problem I've come across. As there are no solutions posted, I'd like to verify that what I'm doing is actually on the right track.</p>

<blockquote>
  <ol start=""6"">
  <li>Find all $a, b\in\mathbb Z$ such that $\frac12(\sqrt a+\sqrt b)$ is an algebraic integer.</li>
  </ol>
</blockquote>

<p>First I set $x = \frac12(\sqrt a + \sqrt b)$, and after squaring both sides, moving over the $a,b$ term, and squaring again, I get:
$16x^4 -8x^2(a+b) + (a-b)^2 = 0$. Therefore, $\frac12(\sqrt a + \sqrt b)$ is algebraic iff $2\mid(a+b)$ and $4\mid(a-b)$</p>
",<number-theory>
"<p>Find the set of primes $p$ for which $-3$ is quadratic residue $\text{mod } p$.</p>

<p>I have started my solution like this:</p>

<p>$1= \left(\dfrac{-3}{p}\right) = \left(\dfrac{-1}{p}\right)\left(\dfrac{3}{p}\right) = (-1)^\frac{p-1}{2}\left(\dfrac{3}{p}\right)$ </p>

<p>Using quadratic reciprocity  $\left(\dfrac{-3}{p}\right)$ becomes $(-1)^\frac{p-1}{2}\left(\dfrac{3}{p}\right)$</p>

<p>So up to here I have $1 = (-1)^\frac{p-1}{2}\cdot (-1)^\frac{p-1}{2}\left(\dfrac{p}{3}\right) = (-1)^{p-1}\left(\dfrac{p}{3}\right)$</p>

<p>Where $\left(\dfrac{a}{b}\right)$ stands for the Legendre symbol. What is my next step? I can not seem to see how to break down $p/3$ further. My solution should be when $p\equiv 1 \pmod 3$ but I cant seem to get there.</p>
",<number-theory>
"<p>This is a challenge problem in the Pell Equations chapter of my number theory book, but I'm not seeing the connection to Pell Equations. The Pell Equation with the coefficient $5$ is $5b^2+1=a^2$, but it doesn't look like the one I have.</p>

<p>Thanks if you can help me.</p>
",<number-theory>
"<p>Let $V$ be the elliptic curve V: $x^3$+ $y^3$ = A$z^3$ where  A > 2 is cube free natural number. A conjugate quadratic point of $V$ is one of the form $(a + b\sqrt d, a - b\sqrt d, c)$ (note that all quadratic point $(x, y, z)$ of $V$ can be reduced to have $z$ rational integer multiplying by the conjugate of $z$). I have found the result which follows and I am interested in knowing the opinion and remarks or possible objections of readers and mainly I expect for another proof. </p>

<p><strong>Prove that $V(\mathbb Q)$ has no rational points distinct of $(1, -1, 0)$ if and only if all the quadratic points of $V$ are conjugates.</strong></p>

<p>EXAMPLE.- This is true even with $x^3$ + $y^3$ = 2$z^3$ in which the only rational points are $(1, 1, 1)$ and $(1, -1, 0)$. One has here, with $A = 2$, the point $(4 +2\sqrt{-11}, -1 + \sqrt{-11}, -6)$ which is not conjugate because of the rational point $(1, 1, 1)$. </p>
",<number-theory>
"<p>There is a circumference with 14 points $\{p_{1}, p_{2}, ... p_{14}\}$. These points are assigned numbers 1 to 14 randomly. It must be proven that if points are taken three-by-three, these triplets being formed by consecutive points, there will be at least one triplet which has a sum bigger than 29.</p>

<p>This is how I operated: </p>

<p>There are 14 sums $\{s_{1}, s_{2}, ... s_{14}\}$ where $$s_{1} = p_{1} + p_{2} + p_{3}$$$$s_{2} = p_{2} + p_{3} + p_{4}$$$$...$$$$s_{14} = p_{14} + p_{1} + p_{2}$$ So each number appears 3 times, this is $$s_{1}+s_{2}+...+s_{14} = 3*(1+2+3+4+5+6+7+8+9+10+11+12+13+14) = 315$$ And, $$14*29 = 406$$ This is where I get stuck and don't know how to prove the statement using the pigeonhole principle.</p>
",<number-theory>
"<p>For $p$ an odd prime, why does $$\sum_{x=1}^{p-1}\left(\frac{x}{p}\right)=\left(\frac{0}{p}\right)$$</p>

<p>where $\left(\frac{x}{p}\right)$ is the Legendre symbol.</p>

<p>I'm not sure if I have given enough context for this to necessarily be true, but I read it in lecture notes and can't understand why it is true.</p>
",<number-theory>
"<p>Does every soluble negative pell equation, $a^2-Db^2=-1$, have infinitely many integer solutions $(a,b)$ where $a,b$ are <em>both</em> positive integers?</p>
",<number-theory>
"<p>Let $0&lt;x \leq 1$, We define a function such that $f(x)=y=\frac{1}{x}$ which results $y \geq 1$ . We have infinitely many numbers between $0$ and $1$, so we can match any $x$ to a number $y$ greater than $1$ via $f(x)$. There are also infinitely many numbers greater than $1$. We can match the numbers between this two intervals one to one. Which intuitively results  that the number of elements of these two intervals are equal. I am confused. How can it be possible?</p>
",<number-theory>
"<p>Show that in any finite field,each of its elements can be written as the sum of two squares.</p>

<p>Well,I hate to admit-this being also my first post-that I have not proven it yet.I tried to work on the multiplicative group but to no avail.Any help/tip will be welcome.</p>
",<number-theory>
"<p>Is (the number of primes $&lt; n$)   less than (the number of positive integers less than $n$ and relatively prime to $n$)?</p>
",<number-theory>
"<p>At least in physicist's thinking, information, vaguely, is something that allows one to select a subset from a set. </p>

<p>Say, a system can be in states A and B, we have done a measurement on it (extracted information), then it is in either A or B. Now we are able to say, in which state among all the possible states is the system in.</p>

<p>Now, consider numbers, say number e. One can write many digits to specify the boundaries of an interval to which e belongs. In fact, arbitrarily good precision can take arbitrarily large amount of information to specify the decimal representation of the number.</p>

<p>Now, however, one can also write an expression for e, say $e = \sum_{n}\dfrac{1}{n!}$, and it seems like e is defined to an arbitrary precision straight away. That is to say, here we have something, for which we would have needed infinite amount of information.</p>

<p>The question then: does this expression contain infinity of information/any information at all? Or, can information be defined at all for expressions? </p>

<p>One might surely argue that given an expression, one still has to perform infinite number of evaluations to obtation a decimal expression of the number. But then a number of other questions would arrive: ""Is it evaluations of faculties and additions then that produce information?"", ""Is the information only about decimal representations of numbers, but not the numbers themselves?"", and perhaps many more.</p>
",<number-theory>
"<p>When I look up why  $\mathbb{R}$ and $\mathbb{Q}$ are not homeomorphic, almost all the answers just say something along the line of ""Because, Cardinality"" and then ends there.</p>

<p>Can someone provides the reason or the proof as to why cardinality would matter?</p>
",<number-theory>
"<p>Koblitz states in his book on p-adic numbers on page 84:</p>

<blockquote>
  <p>Suppose that $\alpha \in \mathbb Q$ is such that $1 + \alpha$ is the square of a nonzero rational number $a/b$.
  Let $S$ be the set of all primes $p$ for which the binomial series for $(1 + \alpha)^{1/2}$ converges in $|\cdot|_p$.
  There is no $\alpha$ other than $8$, $16/9$, $3$, $5/4$ for which $(1 + \alpha)^{1/2}$ converges to the same value in $|\cdot|_p$ for all $p \in S$.</p>
</blockquote>

<p>Why is this an example of a very general theory of E. Bombieri? What does the theory (or theorem) say and what are its possible connections with p-adic numbers? Is it the Bombieri–Vinogradov theorem from analytic number theory?</p>
",<number-theory>
"<p>I have come across two instances of ""Coleman map""</p>

<p>Let $E$ be an elliptic curve defined over $\mathbb{Q}_p$. Let $k_\infty$ be the unique $\mathbb{Z}_p$ extension of $\mathbb{Q}_p$ contained in $\mathbb{Q}_p(\mu_{p^\infty})$ with Galois group $\Gamma = 1+p\mathbb{Z}_p \cong \mathbb{Z}_p$. Let $k_n$ be the $n$-th layer in this tower. Let $T=T_p(E)$ be the $p$-adic Tate module  of the elliptic curve $E$. Then the $\textit{Coleman map for E}$ is a map $$Col: \varprojlim_{n}  H^1(k_n, T^*(1)) \rightarrow \Lambda=\mathbb{Z}_p[[\Gamma]] $$
I am referring to page 572 of <a href=""https://www.math.uni-bielefeld.de/documenta/vol-coates/kobayashi.pdf"" rel=""nofollow"">this paper</a> where I learn that the power series $Col_z(x)$ equals the $p$-adic L-function of the elliptic curve E when $z$ is a special element discovered by Prof. Kato. </p>

<p>There is another instance where I have come across a Coleman power series, as a power series that generates norm-coherent sequence of units in the tower $\mathbb{Q}_p(\mu_{p^ \infty})/\mathbb{Q}_p$. That is, if I have a sequence of units $\mathbb{u}=(u_n)_{n \geq 0} \in \varprojlim_{n \geq 0} \mathcal{O}^{\times}_{\mathbb{Q}_p(\mu_{p^n})}$ (the inverse limit on the right hand side is w.r.t. the norm map of fields $\mathbb{Q}_p(\mu_{p^m}) \rightarrow \mathbb{Q}_p(\mu_{p^n}),  m \geq n$), then $\exists!$ unique power series $Col_{\mathbb{u}}(x) \in \mathbb{Z}_p[[x]]$ such that $Col_{\mathbb{u}}(\zeta_{p^{1+n}}-1)=u_n, \forall n\geq 0$. How does the first Coleman map relate to the second? Or even more generally, are there other instances where Coleman maps arise, and what is  the general philosophy behind Coleman maps? Any thoughts and/or link to any articles/notes are welcome. Thank you!!</p>
",<number-theory>
"<p>Prove: If a prime number $p\in \mathbb N$ is from the form $p=4k+3,k\in \mathbb N$, then its also a prime number in $\mathbb Z[i]$,i.e. if $p|(z_1\cdot z_2)$ then $p|z_1$ or $p|z_2$.</p>

<p>I dont have any idea how to solve it, so I am looking for some <strong>hints</strong>.</p>

<p>Thanks in advance :)</p>
",<number-theory>
"<p>I have an equation: $ x+y+z+t = 7$. I want to know how many solutions does this equation have? $x,y,z$, and $t$ are positive integers. I have no idea how to solve this. Can you please help me to solve this question?</p>
",<number-theory>
"<p>Is it true that, if $a^2-Db^2=-1$ is solvable in integers, then so is $x^2-Dy^2=D$ (*)?</p>

<p>For $D=5$ this is true, you can take $x=5$ and $y=2$, and indeed $5^2-5(2^2)=5$, so (*) is solvable. Is this true in general?</p>
",<number-theory>
"<p>The identity</p>

<p>$\displaystyle (n+1) \text{lcm} \left( {n \choose 0}, {n \choose 1}, ... {n \choose n} \right) = \text{lcm}(1, 2, ... n+1)$</p>

<p>is probably not well-known.  The only way I know how to prove it is by using <a href=""http://planetmath.org/encyclopedia/KummersTheorem.html"">Kummer's theorem</a> that the power of $p$ dividing ${a+b \choose a}$ is the number of carries needed to add $a$ and $b$ in base $p$.  Is there a more direct proof, e.g. by showing that each side divides the other?</p>
",<number-theory>
"<p>I'm new to modular form, reading the book <em>A First Course in Modular Forms</em></p>

<p>We have the weight 2 Eisenstein series
$$
G_2(\tau)=\sum_{c\in\mathbb{Z}}\sum_{d\in\mathbb{Z}_c'}\frac{1}{(c\tau+d)^2}
$$</p>

<p>where $\mathbb{Z}_c'=\mathbb{Z}-\{0\}$ when $c=0$ otherwise $\mathbb{Z}$.</p>

<p>Now I am given that
$$
(G_2[\gamma]_2)(\tau)=G_2(\tau)-\frac{2\pi ic}{c\tau+d}\text{for }\gamma=\begin{bmatrix}a&amp;b\\c&amp;d\end{bmatrix}\in\text{SL}_2(\mathbb{Z})
$$</p>

<p>And I am asked to prove that if we know that the above formula is correct for two particular matrices $\gamma_1,\gamma_2$,then it is correct for $\gamma_1\gamma_2$.</p>

<p>I try to do as following:
$$
\begin{align*}
(G_2[\gamma_1\gamma_2]_2)(\tau)&amp;=(G_2[\gamma_1]_2[\gamma_2]_2)(\tau)\text{ by property of the operator}\\
&amp;=(G_2[\gamma_1]_2)(\gamma_2(\tau))\cdot j(\gamma_2,\tau)^{-1}\text{ by the definition}
\end{align*}
$$</p>

<p>But after substitute $\tau$ by $\gamma_2(\tau)$ in the above given formula, I can not get the desired equation.</p>

<p>Can anyone help?</p>
",<number-theory>
"<p>How many incongruent solutions are there to $x^2 \equiv 1 \space (mod \space m)$?</p>

<p>As a hint, my teacher said make use of the Chinese Remainder Theorem.</p>

<p>What I have done so far is a case by case approach - I looked at $a=0,1,2$ and $a \geq 3$.</p>

<p>For each $p_i^{b_i}$, I found that there are two possible solutions, $x= \pm 1$, but passed this, I'm not too sure where to go.</p>
",<number-theory>
"<p>Are $(1,2), (2,3), (3,4)$, and $(8,9)$ the only consecutive integers that are a power of two and a power of three? And if they are, how do I prove this?</p>
",<number-theory>
"<p>Let $$a_n=1+\frac{1}2+\frac{1}3+\cdots+\frac{1}n=\frac{p_n}{q_n},$$ 
where $gcd(p_n,q_n)=1.$</p>

<p>$$\{a_n\}=\left\{1,\frac{3}{2},\frac{11}{6},\frac{25}{12},\frac{137}{60},\frac{49}{20},\frac{363}{140},\cdots\right\}$$</p>

<p>Hence $p_1=1,p_4=5^2,p_6=7^2,$ are there any other $n$ such that $\sqrt{p_n}\in\mathbb N$?</p>
",<number-theory>
"<p>I'd like to solve the following Pell equation:
$$
x^2-7y^2=-3
$$
Where $x$ and $y$ are integers. I applied the usual procedure, which avoids continued fractions:</p>

<p>The two minimal positive integer solutions are $(x_0,y_0)=(2,1)$ and $(x_1,y_1)=(5,2)$, thus the minimal rational solution of $x^2-7y^2=1$ should be $(p,q)=\left(\frac{4}{3},\frac{1}{3}\right)$. My script (it is in german so I don't link it here) tells me, that in this case, every pair of solutions is given by:
$$
x_{n+1}=\frac{4}{3}x_{n}+7\cdot\frac{1}{3}y_n \\
y_{n+1}=\frac{1}{3}x_{n}+\frac{4}{3}y_n
$$
If we proceed further, we can find that this gives:
$$
x_n=\frac{a_n}{3^n} \space\text{where}\space a_0=2,\space a_1=15,\space a_{n+1}=8a_n-9a_{n-1} \\
y_n=\frac{b_n}{3^n} \space\text{where}\space b_0=1,\space b_1=6,\space b_{n+1}=8b_n-9b_{n-1}
$$
But if we take these equations modulo $9$, we see that $(2,1)$ and $(5,2)$ are the only integer solution, but there surely is also $(37,14)$. Where did I go wrong? Every answer will be appreciated, but I'm not used to the approach with continued fractions, so preferably I would like to see an answer avoiding this.</p>

<p>EDIT:</p>

<p>My main question is:</p>

<p>Where is my fault? Or is my script wrong?</p>
",<number-theory>
"<p>...and encoding it as a probability distribution.</p>

<p>Suppose we have a sequence of non-negative integers that is periodic with period $N$: </p>

<p>\begin{equation*}
A_{1},A_{2},...,A_{N},A_{1}...
\end{equation*}</p>

<p>Each $A_{k}$ takes on a value no greater than some constant $B$:</p>

<p>\begin{equation*}
0 \leq A_{k} \leq B
\end{equation*}</p>

<p>We then take this sequence and do a simple convolution, for some constant $L &gt; 0$ and $1 \leq n \leq N$:</p>

<p>\begin{equation*}
S_{L}(n) = A_{n} + A_{n+1} +...+ A_{n+L-1}.
\end{equation*}</p>

<p>From $S_{L}(n)$ we then form a probability distribution $P(n)$ which gives the frequency of each of its values. Let $e_{j}(k) = 1$ if $j = k$ and $0$ otherwise. Then:</p>

<p>\begin{equation*}
P(n) = (e_{n}(S_{L}(1)) + e_{n}(S_{L}(2)) +...+ e_{n}(S_{L}(N))) / N.
\end{equation*}</p>

<p>What I would like to find out is the extent to which this process can be reversed. I have two data points:</p>

<p>1) I know (pretty much) everything about the probability distribution $P(n)$: the distribution itself, its mean, range, variance, skewness, kurtosis, etc.</p>

<p>2) I can tell you the frequency of values of $A_{k}$ in one period, so that if the sequence is 1,0,2,3,1,0, I can tell you there are two 0's, two 1's, one 2, and one 3.</p>

<p>To what extent am I able to reconstruct the sequence $A_{k}$ from these two data points?</p>
",<number-theory>
"<p>Consider the prime counting function</p>

<p>$$ \pi(x) = \ the \ number \ of \ primes \ less \ than \ or  \ equal \ to \ x$$</p>

<p>It is well known due to the sieve eratosthenes that given an integer $n$ and the set of primes less than or equal to $\sqrt{n} = p_1, ... p_k$ that the total number of additional primes generated is:</p>

<p>$$ A(n)=  n - \sum_{i = 1}^{k}\left[ \frac{n}{p_i} \right] +\sum_{i = 1, j \ne i, j = 1}^{k,k}\left[ \frac{n}{p_i p_j} \right] ...  $$</p>

<p>Based on simple inclusion and exclusion:</p>

<p>Therefore naturally I would assume that </p>

<p>$$\pi(\sqrt{n}) + A(n) = \pi(n)$$</p>

<p>That is primes less than the the root of n plus primes bigger than the root of n but less than n gives all the primes less than n.</p>

<p>But instead the formula in this wiki page: <a href=""http://en.wikipedia.org/wiki/Prime-counting_function#Algorithms_for_evaluating_.CF.80.28x.29"" rel=""nofollow"">http://en.wikipedia.org/wiki/Prime-counting_function#Algorithms_for_evaluating_.CF.80.28x.29</a> asserts what I have is:</p>

<p>$$\pi(n) + 1 = \pi(\sqrt{n}) + A(n)$$</p>

<p>Where is this '1' coming from?</p>
",<number-theory>
"<p>I've read <a href=""http://math.stackexchange.com/questions/59680/how-to-tell-if-some-power-of-my-integer-matrix-is-the-identity/59709#59709"">this question</a> about identity power of an integer matrix. </p>

<p>But how about power of a matrix modulo $p^\alpha$.
$$A^m \equiv I \pmod{p^\alpha} $$</p>

<p>How can I find the minimal $m$ that the above equation hold?</p>

<p>Or how to prove that such $m$ does not exist? </p>
",<number-theory>
"<p>I've a very basic question on absolute values on fields. If $K$ is a valued field with absolute value $|- |:K\to \mathbb R_{\geq0}$ then is the map $|-|':K\to \mathbb R_{\geq0}$ defined by $|x|'=|x|^r$ for some $r&gt; 0$ also an absolute value? How do I show that the triangle inequality still holds for $|-|'$? Maybe this is not possible?</p>

<p>Many thanks. </p>
",<number-theory>
"<p>I have recently read ""The music of the primes"" by Marcus du Sautoy (see excerpt <a href=""https://plus.maths.org/content/music-primes"" rel=""nofollow"">here >>></a>). There he writes:</p>

<p>""So how fair are the prime number dice? Mathematicians call a dice ""fair"" if the difference between the theoretical behaviour of the dice and the actual behaviour after $N$ tosses is within the region of the square root of $N$. The heights of Riemann's harmonics are given by the east-west coordinate of the corresponding point at sea-level. If the east-west coordinate is $c$ then the height of the wave grows like $N^c$. This means the contribution from this harmonic to the error between Gauss's guess and the real number of primes will be $N^c$. So if the Riemann Hypothesis is correct and $c$ is always $1/2$, the error will always be $N^{1/2}$ (which is just another way of writing the square root of $N$). If true, the Riemann Hypothesis means that Nature's prime number dice are fair, never straying more than the square root of $N$ from Gauss's theoretical prime number dice.""</p>

<p>However, we know from Helge von Koch (1901)[2] that if the Rieman Hypothesis true, then:
$$\pi(x)=Li(x)+\mathcal O(\sqrt x \log x)$$
where $\pi(x)$ prime counting function and $Li(x)$ according Gauss.</p>

<p><strong>My question:</strong></p>

<p>Is what Sautoy says correct? Is the error indeed $(\sqrt x \log x)$ or as Sautoy states $\sqrt x$? Or did I misunderstand something?</p>

<p>[2]: Von Koch, Helge (1901). ""Sur la distribution des nombres premiers"" (On the distribution of prime numbers). Acta Mathematica (in French). 24 (1): 159–182.</p>
",<number-theory>
"<p>Let $\mathbb{K}$ a finite extension of $\mathbb{Q}$ and $\mathcal{O}_\mathbb{K}$ its ring of integers. Assume $\mathcal{O}_\mathbb{K}=\mathbb{Z}[\alpha]$, that is generated as a ring by a single element $\alpha\in\mathbb{K}$. I am asked to show that, for every prime $p\in\mathbb{Z}$, there are at most $p$ places on $\mathbb{K}$ over $p$, that is at most $p$ ways to extend the non archimedean value $|\cdot|_p$ to $\mathbb{K}$.</p>

<p>I don't know how to start attacking the problem, any hint?</p>

<p><strong>Edit</strong>: there was a typo in the original problem. The <em>correct</em> question was to prove that there are at most $p$ places on $\mathbb{K}$ over $p$ <strong>of residue degree 1</strong>, and the proof follows from Adam Hughes' answer.</p>
",<number-theory>
"<p>A few months ago, someone told me there existed a scheme theoretic proof of the irreducibility of cyclotomic polynomials. I've tried coming up with a proof, and when that didn't really yield anything $($just the scattered thoughts I have below$)$, I searched online for a reference to a proof, to no avail. Can anyone provide a proof or a reference to one? Does a proof even exist via schemes, or was this someone trolling me? Would this question be a better fit on MathOverflow?</p>

<p>Here are my thoughts so far on such a proof, if it were to exist.</p>

<p>The standard linear algebraic $($or in context of ""modern"" frameworks, the ""$p$-adic""$)$ formulation of the irreducibility of cyclotomic polynomials is as follows.</p>

<p><strong>Theorem.</strong> Let $n \in \mathbb{N}$. The degree of $\mathbb{Q}(\zeta_n)/\mathbb{Q}$ is $[\mathbb{Q}(\zeta_n):\mathbb{Q}] = \phi(n)$.</p>

<p>This is saying is that the vector space generated by the powers of $\zeta_n$ over $\mathbb{Q}$ is of dimension $\phi(n)$. The most important case in the proof is where $n = p$, where one shows that the $\mathbb{Q}$-vector space $\mathbb{Q}(\zeta_p)$ has dimension $p-1$. This is done by showing $$\dim_\mathbb{Q}(\mathbb{Q}(\zeta_p)) = \dim_\mathbb{Z}(\mathbb{Z}[\zeta_p]) = \dim_{\mathbb{F}_p}(\mathbb{Z}[\zeta_p]/p),$$and that $\dim_{\mathbb{F}_p}(\mathbb{Z}[\zeta_p]/p) = p-1$.</p>

<p>Now, the purpose of scheme theory is to make geometry out of rings. In the case of this elusive proof, I imagine the scheme theoretic perspective claims that the extension of rings $\mathbb{Z}[\zeta_n]/\mathbb{Z}$ is like a ramified cover of topological spaces. One imagines $\mathbb{Z}$ as space populated by its primes, and similarly for $\mathbb{Z}[\zeta_n]$. The scheme theoretic restatement of the above theorem is that this is a connected cover of degree $\phi(n)$. In the standard proof of the linear algebraic formulation above one sees this by looking above the prime $p$, where one sees ""nilpotency"" of order $p-1$, which implies that the cover must have degree $p-1$.</p>

<p>Is this viable? Should I use some sort of monodromy or something? $($But that's harder to make sense of in the arithmetic setting...$)$</p>
",<number-theory>
"<p>The number $711000000$ can be written as $79^1 \times 2^6 \times 3^2 \times 5^6$. How are these numbers found? </p>

<p>I guess the more general question is - given $n \in \mathbb Z $, how can you 'decompose' it into the product of primes raised to different powers?</p>
",<number-theory>
"<p>Let $n\in\mathbb{N}$.  Then, when is $p^n(p^n-1)$ divisible by $2n$ for all $p$ prime?  I know the following:</p>

<ul>
<li>$n$ must be $1$ or even. (in the odd case, $p=2$ gives a counterexample).</li>
<li>If $n=2^k$ for $k\geq 0$, then $p^n(p^n-1)$ divisible by $2n$ for all $p$ prime.</li>
</ul>

<p>There are more values for $n$ that work as well for $p$ prime ($e.g.$ $n=6$). However, for example, $n=10$ does not work ($i.e.$ $2^{10}(2^{10}-1)=1047552$ which is not divisible by $20$). </p>

<p>Is there something more general to be said?  I'm hoping for a statement along the lines of:</p>

<blockquote>
  <p>""$p^n(p^n-1)$ divisible by $2n$ for all $p$ prime <em>if and only if</em> $\ldots$"" </p>
</blockquote>
",<number-theory>
"<h1>Problem</h1>

<p>Let $A^{*}_{S}$ be the set of sentences consisting of S1, S2, and all sentences of the form</p>

<p>$\phi (0)\rightarrow\forall v_{1}(\phi (v_{1}\rightarrow\phi(Sv_{1}))\rightarrow\forall v_{1}\phi (v_{1})$</p>

<p>where $\phi$ is a formula ( in the language of $\mathfrak{N}_{S}$ in which no variable except $v_{1}$ occurs free. Show that $A_{S}\subseteq\mathrm{Cn}A^{*}_{S}$. Conclude that $\mathrm{Cn}A^{*}_{S}=\mathrm{Th}\mathfrak{N}_{S}$. (Here $\phi(t)$ is by definition $\phi^{v_{1}}_{t}$. The sentence displayed above is called the induction axiom for $\phi$.)</p>

<p>Where</p>

<p>S1. $\forall x Sx\neq0$</p>

<p>S2. $\forall x \forall y (Sx=Sy\rightarrow x=y)$</p>

<p>S3. $\forall y(y\neq 0 \rightarrow \exists x y=Sx)$</p>

<p>S4.n $\forall x S^nx\neq x$</p>

<h2>So Far</h2>

<p>So, I understand thus far that to show $A_{S}\subseteq\mathrm{Cn}A^{*}_{S}$ I need to show that the induction axiom above defines S3 and S4.n; I understand that the best way to do this is by induction, and that the base case for showing S3 is (below).  What I do not understand is where to take the induction step from there.</p>

<h2>Prove S3</h2>

<p>Base Case: Let y in $\forall y(y\neq 0 \rightarrow \exists x y=Sx)$ be 0.  It is the case that $A_{S}^{*}\vdash(0\neq0\rightarrow\exists x0=Sx)$, as the antecedent is logically false ($\vdash 0\ne 0$).</p>

<p>Induction Step: ...Not really sure.</p>
",<number-theory>
"<p><strong>Question:</strong></p>

<p>Suppose that $\mathbb Q[\sqrt{d}]$ is a UFD, and $α$ is an integer in $\mathbb Q[\sqrt{d}]$ so that $α$ and $\barα$ have no common factor, but $N(α)$ is a perfect square in $\mathbb Z$. How can I show that $α$ is a perfect square in the quadratic integers in $\mathbb Q[\sqrt{d}]$?</p>

<p><strong>What I have Done:</strong></p>

<p>I'm not sure if I'm approaching this correctly but if  $\alpha$ and  $\bar{\alpha}$ have no common factor, then $\alpha=\pi_{1}\pi_{2}\cdots\pi_{k}$ and  $\bar{\alpha}=\pi'_{1}\pi'_{2}\cdots\pi'_{j}$ where $ \pi_{i}$ and  $\pi'_{i} $ are prime in  $\mathbb{Q}(\sqrt{d})$. But $N(\alpha) = \alpha\bar{\alpha}=\pi_{1}\pi_{2}\cdots\pi_{k}\pi'  _{1}\pi'_{2}\cdots\pi'_{j}=n^{2}$ where  $n \in \mathbb{Z}$. Somehow I need to show that $\alpha=\beta^{2}$ where  $\beta$ is a quadratic integer in $ \mathbb{Q}(\sqrt{d})$ (i.e.,  $\alpha$ is a perfect square in the quadratic integers in  $\mathbb{Q}(\sqrt{d})$)</p>
",<number-theory>
"<p>Is it known exactly for which integers $a,b,c$ the equation $ax^2+bxy+cy^2=0$ has a nontrivial solution? If not, what is known about this problem in general? (I am asking about the specific problem of ""for which integers $a,b,c$ does $ax^2+bxy+cy^2=0$ have a nontrivial solution in integers $x$ and $y$?)</p>

<p>Thanks</p>
",<number-theory>
"<p>Can $\pi(x)$ be written in terms of $\psi(x)$? I can only seem to approximate it:</p>

<p>$$
\pi(x)\approx\sum_{n=1}^{\infty}\left[\dfrac{\mu(n)}{n}\left(\dfrac{1}{\log(x^{1/n})}\left(\psi(x^{1/n})-x^{1/n}+\sqrt{\pi}\right)+\operatorname{li}(x^{1/n})-1\right)\right]
$$</p>

<p>Is there a relationship of equivalence between $\psi(x)$ and $\pi(x)$ (ie, an inversion formula), or can it only approximate it?</p>

<p>Out of interest I include the difference up to $10^5$ between the RHS and the LHS</p>

<p><img src=""http://i.stack.imgur.com/WuXn9.gif"" alt=""enter image description here""></p>

<p>and both together for very small $x$</p>

<p><img src=""http://i.stack.imgur.com/y4hvX.gif"" alt=""enter image description here""></p>
",<number-theory>
"<p>In my algebraic structures textbook I have come across a tricky question that I am trying to solve which goes as follows:</p>

<p>suppose that $d|(a^n-1) $ and $d|(a^m-1)$ where  $m,n$ are natural numbers and $a,d$ are integers, then show that $ d|(a^{gcd(m,n)}-1) $ . What I know is that clearly $a^m$ and $a^n$ are congruent to 1 modulo d, and that by Bezout's theorem the $gcd(m,n)$ is of the form $k=xm+yn$ so that $a^{gcd(m,n)}$ can be written as $a^{xm}a^{yn}$ but I am not sure how to tie all these ideas together. Any hints would be appreciated.</p>
",<number-theory>
"<p>By congruence computation we get that $n= a^5+b^5+c^5$ implies $n \not \equiv  4,5,6,7  \pmod{11} $<br>
 (with $a,b,c \in \mathbb{Z}$)  </p>

<p>For $a,b,c \in \{-100,-99, \dots , 99, 100\}$, the set of integers $n \in \{0,1,\dots , 99,100 \}$ we get is<br>
$$\{ 0, 1, 2, 3, 12, 30, 31, 32, 33, 34, 63, 64, 65, 96 \}$$The point is that it is exactly the same for $a,b,c \in \{-10000,-9999, \dots , 9999, 10000\}$, so that we could expect that there is no other natural number $n \le 100$ representable like that. Nevertheless according to what happens for cubes (see <a href=""http://mathoverflow.net/a/223121/34538"">here</a>), we could also expect the existence of such representations with large integers. By the congruences above, the smallest natural numbers to look is $n=8$.  </p>

<p><em>Question:</em> Can a sum of three fifth power of integers be $8$?    </p>

<p>Next we should look to $n = 9,10,11,13,14,19,20,21,22,23,24,28,29, \dots$ </p>
",<number-theory>
"<p>I know this has been hinted at a previous page but I can't seem to find a complete answer.</p>

<p>we know that $\gcd(a,m) = ax_1+mx_2$ from the euclidean algorithm. In a similar way, we know that $\gcd(b,m)=bx_2+mx_3$ and $\gcd(ab,m)=abx_5+mx_6$, and so</p>

<p>$$\frac{\gcd(a,m)\gcd(b,m)}{\gcd(ab,m)}=\frac{(ax_1+mx_2)(bx_2+mx_3)}{abx_5+mx_6}=\frac{abx_1x_3+amx_1x_4+bmx_2x_3+m^2x_2x_4}{abx_5+mx_6}$$</p>

<p>I don't understand how we can say that it divides without a remainder.</p>

<p>this is not homework. I'm doing this for sports.</p>
",<number-theory>
"<p>The question is as follows.</p>

<blockquote>
  <blockquote>
    <p>Let $K = \mathbb{Q}(\sqrt[m]{a},\sqrt[n]{b}) $, where $m,n,a,b$ are positive integers such that they are pairwise coprime. Assume that $[K:\mathbb{Q}]=mn$/ Prove that no prime numbers can totally ramify in $K/\mathbb{Q}$.</p>
  </blockquote>
</blockquote>

<p>I assume we would need to find such prime numbers that are ramified in $\mathbb{Q}(\sqrt[m]{a})/\mathbb{Q}$, $\mathbb{Q}(\sqrt[n]{b})/\mathbb{Q}$ respectively. I know if $p|\text{disc}(L)$, then it is ramified. The discriminate can be found using the following:</p>

<p>$$
\text{disc}(\alpha) = (-1)^{\frac{k(k-1)}{2}}\left[(-1)^{1-k}(k-1)^{k-1}c^k + k^kd^{k-1}\right]
$$</p>

<p>Where $f(x) = x^k + cx + d$. And since $a,b,n,m$ are pairwise coprime, then disc($K$)=disc$(\mathbb{Q}(\sqrt[m]{a}))^n$disc$(\mathbb{Q}(\sqrt[n]{b}))^m$. </p>

<p>Now if there is a prime number that will totally ramify in $K/\mathbb{Q}$, then there is only one ramification index, $e_i$, such that $e = mn = [K:\mathbb{Q}]$. All I would need to do is show that there is more than one ramification index via transitivity. This is where I am stuck. How would I go about showing there is more than one ramification index, such that no prime number that totally ramifies $K/\mathbb{Q}$. </p>

<p>Thanks in advance for any feedback.  </p>
",<number-theory>
"<blockquote>
  <p>If $p$ is an odd prime and $k$ an integer with $0&lt;k&lt;p-1$ prove that $1^k + 2^k + \ldots + (p-1)^k$ is divisible by $p$. Given hint: use primitive root.</p>
</blockquote>

<p>This is a question on a practice final of mine. For $k$ being odd, it seems obvious (as the $\pm$ terms cancel out), but I cannot figure out how to do this for the general case.</p>
",<number-theory>
"<p>I have researched this question and need help.  In general is there only one solution to the equation $\sigma(nx) = (n+4)x$ for every $n$?  Specifically, is there only one solution to $\sigma(5x) = 9x$? ( I know x = 2 is a solution, but is there a way to prove that there are no more?)</p>
",<number-theory>
"<p>Throughout this question, an L-function is both an automorphic L-function and an element of the Selberg class such that whenever $F$ and $G$ are L-functions, then so are $F.G$ and $F\otimes G$, where $\otimes$ denotes the Rankin-Selberg convolution.</p>

<p>Let's now consider the 'kernel affine space' of an L-function $F$, defined as the affine space of minimal dimension containing all the non trivial zeroes of $F$. The analogue of RH for $F$ holds if and only if its kernel affine space is 1-dimensional.</p>

<p>My question is the following: as the tensor product of $R^{n}$ with $R^{m}$ is isomorphic to $R^{mn}$ can one establish that the kernel affine space of $F\otimes G$ is 1-dimensional if and only if the kernel affine spaces of both $F$ and $G$ are themselves 1-dimensional? Is a 'motivic' interpretation possible?</p>

<p>Thanks in advance.</p>
",<number-theory>
"<p>For example:</p>

<p>If n = 12</p>

<p>Then starting at 4! because 3! is less than 12</p>

<p>(4!) % 12 == 0</p>

<p>Therefore (4!) * 5 = (5!) % 12 == 0 </p>

<p>etc </p>

<p>and so on until (n-1!) because every factorial after that would obviously be divisible by n. </p>

<p>This is not easy for my (extremely out of date)computer to solve so here is a short list of numbers I have found this to be true for:</p>

<p>1,2,3,6,8,12,24,30,40,60,120,144,180,240,360,720,840,1008,1260,1680,2520, 5040,5760,6720</p>

<p>*arguably 1 2 and 3 should not be included in this list</p>
",<number-theory>
"<blockquote>
  <p>Prove that $(m, n) = (1, 1)$ is the only solution for the Diophantine Equation $$2 \cdot 5^n = 3^{2m} + 1$$ where $(m, n) \in (\mathbb{Z}^+)^2$.</p>
</blockquote>

<p>I've managed to prove that both $m$ and $n$ are odd seeing $\bmod 3\text{ and } 10$ respectively. Also, $\forall n \ge 1$, $10$ divides the LHS. I am not able to proceed from here. Any help would be appreciated.</p>
",<number-theory>
"<p>I know Dijkstra's algorithm to find the shortest way between 2 nodes, but is there a way to find the shortest path between 3 nodes among $n$ nodes? Here are the details:</p>

<p>I have $n$ nodes, some of which are connected directly and some of which are connected indirectly, and I need to find the shortest path between 3 of them.</p>

<p>For example, given $n = 6$ nodes labelled A through F, and the following graph:</p>

<pre><code>A--&gt;B--&gt;C
A--&gt;D--&gt;E
D--&gt;F
</code></pre>

<p>How can I find the shortest path between the three nodes (A,E,F)?</p>

<p>I am looking for a solution similar to Dijkstra's shortest path algorithm, but for 3 nodes instead of 2.
<br/>
Please Note : <br/>
1- The Starting Node is A  <br/>
2- The Sequential is not important just the path needs to cover all these Nodes   <br/>
3- Their is no return back to A   <br/>
Please find the diagram Image
<img src=""http://i.stack.imgur.com/M1wxF.png"" alt=""enter image description here"">
Regards &amp; Thanks<br />
Nahed</p>
",<number-theory>
"<p>I was recently thinking about prime numbers, and at the time I didn't know that they had to be greater than $1$. This got me thinking about negative prime numbers though, and I soon realized that, for example, $-3$ could not be prime because $3 \cdot (-1) = -3$. In some sense $-1$ could be though because its only factors that are integers are $-1$ and $1$, and this is allowed for primes. Is there some way, by this logic, that $-1$ can be considered a prime then?</p>
",<number-theory>
"<p>Prove that the equation $n^a + n^b = n^c$, with $a,b,c,n$  positive integers, has infinite solutions if $n=2$, and no solution if $n\ge3$.</p>
",<number-theory>
"<blockquote>
  <p>Find all the numbers $a$ such that the number $an(n+2)(n+4)$ is an integer for all $n \in \mathbb{N}$</p>
</blockquote>

<p>It's trivial to see that if $a$ is irrational, we get no solution. </p>

<p>Thus $a \in \mathbb{Q} \Rightarrow a = \dfrac{p}{q} ~ (*)$ where $p, q \in \mathbb{Z}$ and $\gcd(p,q)=1$. Then</p>

<p>$$\begin{split} an(n+2)(n+4)=k &amp;\stackrel{(*)}{\Rightarrow} \dfrac{p}{q}n(n+2)(n+4)=k \\ &amp; \Rightarrow pn^3+6pn^2+8pn-kq=0 \end{split}$$</p>

<p>By the integer root theorem, for $n$ to be an integer it should be $kq = zn ~ (1)$. Hence </p>

<p>$$\begin{split} \Rightarrow pn^3+6pn^2+8pn-kq=0 &amp;\stackrel{(1)}{\Rightarrow} pn^2+6pn+8p-z=0 \\ &amp;\Rightarrow n = \dfrac{-6p \pm 2\sqrt{p^2+zp}}{2p} \\ &amp;\stackrel{n&gt;0}{\Longrightarrow} n = -3 + \dfrac{\sqrt{p^2+zp}}{p} \in \mathbb{Z}\end{split}$$ </p>

<p>But again for $n$ to be an integer it should be $p^2+zp=p^2m^2 \Rightarrow p = \dfrac{z}{m^2-1} ~(2)$ </p>

<p>By (1) and (2) we get that $$kq=p(m^2-1)n \Rightarrow \dfrac{p}{q} = \dfrac{k}{(m^2-1)n} \stackrel{n=m-3}{\Longrightarrow} \boxed{a = \dfrac{k}{(m-3)(m-1)(m+1)}}$$</p>

<p>We see that if $m$ is a small number (like $2$ and $3$) the equation is satisfied, but for $m$ being a larger number (like $10$)then the initial number is not an integer. Please tell me if I have done anything wrong or if I should add anything to my solution.</p>
",<number-theory>
"<p>I have to divide $2860$ by $3186$. The question gives only $2$ minutes and that division is only half part of question. Now I can't possibly make that division in or less than $2$ minutes by applying traditional methods, which I can't apply on that division anyways.</p>

<p>So anyone can perform below division using faster technique?</p>

<p>$2860/3186$</p>

<p>Thanks for reading, hoping to get some answers. :)</p>

<p>This is a multiple choice question, with answers $6/7$, $7/8$, $8/9$, and $9/10$.</p>
",<number-theory>
"<p>Bernouli's Formula for sum of kth powers of first n natural numbers is given by: 
$$f_k(n)=\frac{1}{k+1}\sum_{j=0}^k{k+1\choose j}B_j(n+1)^{k+1-j}$$
where $Bj$ is the $j^{th}$ Bernoulli Number and is in a sense recursively given by:$$B_j=-\frac{1}{j+1}\sum_{i=0}^{j-1}{j+1 \choose i}B_i$$.</p>

<p>I did find a generalized proof of this for Generalized case where powers can be complex numbers. I am looking for simpler proofs. Do you have any idea if this can be proved by induction.</p>

<p>Thank you.</p>

<p>PS. I am not sure of tags and appreciate if they are corrected.</p>

<p><strong>Added</strong> By simpler I mean that do involve only integer powers.</p>
",<number-theory>
"<p>This is Problem 1.7 from <a href=""http://www.colby.edu/personal/fqgouvea/deform.dvi"" rel=""nofollow"">Gouvea's lecture notes on deformations of Galois representations</a>. In particular, he asks you to show that it has many subgroups of finite index which are not closed. So here's what I've got so far, which may be wrong.</p>

<p>I can write the compositum as F = <b>Q</b>[&radic;–1, &radic;2, &radic;3, &radic;5, &radic;7 ...] (can I?) and then the Galois group G = Gal(F/<b>Q</b>)  is isomorphic to a direct product &Pi;<sub>p</sub> (<b>Z</b>/2<b>Z</b>) where the product is taken over all primes p, as well as p=-1, and the pth component is generated by the conjugation &sigma;<sub>p</sub> defined by &radic;p -> –&radic;p.</p>

<p>An example of a subgroup which isn't closed would be the subgroup <strong>H</strong> consisting of <i>finite</i> products of conjugations, since for example, <strong>H</strong> contains the sequence &sigma;<sub>2</sub>, &sigma;<sub>2</sub>&sigma;<sub>3</sub>, &sigma;<sub>2</sub>&sigma;<sub>3</sub>&sigma;<sub>5</sub>, &sigma;<sub>2</sub>&sigma;<sub>3</sub>&sigma;<sub>5</sub>&sigma;<sub>7</sub>... which converges to the automorphism ""conjugate everything"", and this automorphism is not contained in <strong>H</strong>.</p>

<p>However this subgroup is nowhere near being finite index--it has the cardinality of the natural numbers, whereas G has the cardinality of the reals. The only finite index subgroups I can think of take are of the form Gal(F/K) where K is a finite extension of <b>Q</b>, but of course these are by definition all closed. So I guess I've stuffed up somewhere, and I'd be really grateful for any help?! In know this may seem a bit ""homework questiony"" but it's not, it's just something that's really bugging me!</p>
",<number-theory>
"<p>what is the best Schnirelmann Constant for Goldbach Conjecture  ?</p>

<p>On <a href=""http://mathworld.wolfram.com/SchnirelmannConstant.html"" rel=""nofollow"">http://mathworld.wolfram.com/SchnirelmannConstant.html</a> 
the best Schnirelmann Constant is 7 ( from Ramaré )</p>

<p>My understanding is that Ramaré results is only for sufficiently large number, not for <strong>all numbers</strong>.</p>

<p>Is my understanding right ?</p>

<p>If so, what is the current best Schnirelmann Constant for <strong>all number</strong> ?</p>
",<number-theory>
"<p>Let $W_{s,r,n}$ be the total number of ways that the sum $s$ can be displayed after throwing $r$ number of $n$-sided dice. Define</p>

<p>$$W_{s,0,n} =
\begin{cases}
1,  &amp; \text{if s = 0} \\
0, &amp; \text{if s $\neq$ 0} \\
\end{cases}
$$</p>

<p>for all $s \in \mathbb Z$ and $n \in \mathbb N^*$. Can you prove that</p>

<p>$$W_{s,r,n} =W_{s-1,r,n} + W_{s-1,r-1,n} - W_{s-(1+n),r-1,n}$$</p>

<p>for all $s \in \mathbb Z$, $r \in \mathbb N^*$, and $n \in \mathbb N^*$?</p>
",<number-theory>
"<p>Does a positive constant $\nu$ exist so that $\varphi(n)&gt;\nu\cdot n$ for all $n$? Clearly this problem is exactly the same as asking if $\prod\limits_{i=1}^\infty \frac{p_i-1}{p_i}=0$. This is because $\varphi(n)=n\prod\limits_{p|n}\frac{p-1}{p}$. And if $n$ has $k$ prime divisors this is clearly greater than $n\prod_{i=1}^k\frac{p_i-1}{p_i}$.</p>

<p>So the problem breaks down to figuring out to what the sequence $a_n=\prod\limits_{i=1}^n\frac{p_i-1}{p_i}$ converges. (It clearly converges since it is decreasing and bounded). In fact we only need to figure out if it converges to zero or something else.</p>
",<number-theory>
"<p>Not really sure how to approach this one at all.</p>

<p>I tried</p>

<p>$$\begin{align}
n^{k+1} &amp; \equiv n^k + 1 \pmod p \\
n^k \times n &amp; \equiv n^k + 1 \pmod p \\
n^k \times n - n^k &amp; \equiv 1 \pmod p \\
n^k \times (n - 1) &amp; \equiv 1 \pmod p
\end{align}$$</p>

<p>but since $n$ is a primitive root, this means that $n^{p-1} \equiv 1 \bmod p$ but I can't really figure out what to do from here.</p>

<p>Any suggestions?</p>
",<number-theory>
"<p>Prove that $(k^k)$ is periodic modulo 3 and find its period. Not sure how to approach this.</p>

<p>Edit: So I know the sequence is 1, 1, 0, 1, 2, 0, ... (plugging in k = 1, 2, 3, ...) so I think the period should be 6 since thats the length of each cycle. I also know a function f(x) is periodic if there exists some minimum n such that f(x + n) = f(x) for all x in the domain of f.</p>

<p>Plugging every 6 term is 0 so f(x + 6) = f(x) mod 3.. is this right?</p>
",<number-theory>
"<p>$$\int_0^\infty x^t\operatorname{csch}x\text{ d}x=\frac{a\zeta(t+1)}{b}$$
for $t\in\Bbb{N}$</p>

<p>How might one represent $a,b$ in terms of $t$?
(Note that $a,b\in \Bbb{N}$)</p>

<p>If possible, could one also provide a proof please? </p>
",<number-theory>
"<p>Is $\pi$ periodic in any base-k numeral system, where k is integer ? And what is the status of this problem?</p>
",<number-theory>
"<p>I'm having trouble figuring out how to show the general existence part of the following problem. </p>

<p>Suppose $n\in\{1,2,3...\}$ and $n\equiv 7\mod{10}$. Show that $\exists$ a prime divisor $p$ of $n$ s.t. $p\equiv 3\mod{10}$ or $p\equiv 7\mod{10}$.</p>
",<number-theory>
"<p>I feel like this wants to use the Hasse bound somehow since that's really the only tool we talked about with regard to counting points on a curve, but I'm not entirely sure how to get to that conclusion.</p>
",<number-theory>
"<p>I have $k = 37$ and $m = 101$<br>
How do I find $a$, given the value of $a^k \bmod m$?</p>

<p>I think this has to do with order of integers.</p>
",<number-theory>
"<p>How can I calculate $27^{41}\ \mathrm{mod}\ 77$ as simple as possible?</p>

<p>I already know that $27^{60}\ \mathrm{mod}\ 77 = 1$ because of <a href=""http://en.wikipedia.org/wiki/Euler%27s_theorem"">Euler’s theorem</a>:</p>

<p>$$ a^{\phi(n)}\ \mathrm{mod}\ n = 1 $$
and
$$ \phi(77) = \phi(7 \cdot 11) = (7-1) \cdot (11-1) = 60 $$</p>

<p>I also know from using modular exponentiation that $27^{10} \mathrm{mod}\ 77 = 1$ and thus</p>

<p>$$ 27^{41}\ \mathrm{mod}\ 77 = 27^{10} \cdot 27^{10} \cdot 27^{10} \cdot 27^{10} \cdot 27^{1}\ \mathrm{mod}\ 77 = 1 \cdot 1 \cdot 1 \cdot 1 \cdot 27 = 27 $$</p>

<p>But can I derive the result of $27^{41}\ \mathrm{mod}\ 77$ using $27^{60}\ \mathrm{mod}\ 77 = 1$ somehow?</p>
",<number-theory>
"<p>This is probably an easy question. Im Assuming whoever can answer this has access to S-boxes and P boxes etc. </p>

<p>Suppose the input to a round of DES is
$1010101010......10101010$. (64 bits)</p>

<p>Suppose the round key is
$11111111111111111111.....1111111111111111$ (48 1’s)</p>

<p>Compute the 53rd output bit of this round.</p>

<p>I am a little confused in how to approach this. Do I break up the 64 inputs into two 32 inputs and transform the second 32 into 48? Do I then add to the key mod 2?</p>

<p>If anyone can provide an answer or even the relevent steps involved (Ie like how to know which S-box to use) that would be great.</p>
",<number-theory>
"<p>I'm having some trouble with this proof. Here's the question: Use mathematical induction and Euclid's Lemma to prove that for all positive integers $s$, if $p$ and $q_1, q_2, \dotsc, q_s$ are prime numbers and $p$ divides $q_1q_2\dotsb q_s$, then $p=q_i$ for some $i$ with $1 ≤ i ≤ s$.</p>

<p>Here's what I know: Euclid's Lemma says that if $p$ is a prime and $p$ divides $ab$, then $p$ divides $a$ or $p$ divides $b$. More generally, if a prime $p$ divides a product $a_1 a_2 \dotsb a_n$, then it must divide at least one of the factors $a_i$. For the inductive step, I can assume $p$ divides $q_1q_2\dotsb q_{s+1}$ and let $a=q_1q_2\dotsb q_s$. Then, $p$ divides $aq_{s+1}$ and either $p$ divides $a$, $p$ divides $q_{s+1}$, or $p=q_{s+1}$. I know that since $q_i$ is prime, it cannot divide $q_i$ unless $p=q_i$ for some $1 ≤ i ≤ s$. I'm just not sure how to formulate the proof. Usually with Induction I can set some property $P(n)$ and test it is true for some base like $P(0)$ or $P(1)$ for the base step. I'm unsure how to go about it here. </p>
",<number-theory>
"<p>Let $p$ be a prime. If $\frac{p-1}{4}$ and $\frac{p+1}{2}$ are also primes then prove that $p=13$.</p>
",<number-theory>
"<p>I have an application that wants controllable random functions from $\mathbb{Z}^2$ and $\mathbb{Z}^3$ to $2^{32}$ , where by controllable I basically mean seedable by some parameters (say, on the order of 3 to 5 32-bit integers) such that the same seeds will always produce the same functions.  The most obvious way of doing this (for the two-dimensional case, say) would seem to be computing the value at some point $(x,y)$ by using $x$, $y$, and the seed parameters as seeds for something like an LFSR generator or a Mersenne Twister, then running the RNG for some fixed number of steps and taking the resultant value as the value of the function at that point.</p>

<p>My question is, how can I be certain that this procedure won't keep too much correlation between adjacent 'seed points', and is there either a straightforward analysis or even just some general guideline for how many iterations would be necessary to eliminate that correlation?  My first back-of-the-envelope guess would be that each iteration roughly doubles the decorrelation between given seed values, so that 32 iterations would be necessary to achieve the requisite decorrelation over a range of $2^{32}$ values (and in practice I'd probably double it to 64 iterations), but that's strictly a guess and any proper analysis would be welcome!</p>

<p><strong>Edited for clarification:</strong> To further outline the issue, I may be sampling this random function $f$ (for some given seed parameters) at arbitrary values, and need those samples to be identical between passes; so for instance, if a first application computes $f(0, 0)$, $f(437, 61)$, $f(-23, 129)$, and then $f(5,3)$, and a second (potentially concurrent) application computes $f(1,0)$ and then $f(5,3)$, both passes need to find the same value of $f$ at $(5,3)$.  I may also be sampling $f$ at arbitrary points, so I'd like the evaluation to take constant time (and in particular, evaluating $f(x,y)$ shouldn't take time linear in $x+y$).</p>
",<number-theory>
"<p>This question was motivated by pondering <a href=""http://math.stackexchange.com/questions/1442/is-there-a-direct-proof-of-this-lcm-identity"">this lcm identity</a>.</p>

<p>Consider that $\gcd(1,6,15) = 1$, but $\operatorname{lcm}(1,6,15)=30$, but $1\cdot 6\cdot 15 = 90$.  $(2,6,15)$ shows a similar phenomenon.</p>

<p>So what is the correct identity for $n$-ary $\gcd$/$\operatorname{lcm}$?</p>
",<number-theory>
"<p>I'm reading Ergodic Theory and Differential Dynamics by Ricardo Mane. 
There is a theorem in the book that states the following:
If x $\in$ $R^n$, the translation L $_{\pi(x)}$: $T^n \rightarrow T^n$ is ergodic if and only if (k,x) $\notin$ Z for every k $\in Z^n$.</p>

<p>I was hoping someone could either give me or direct me to, a comprehensible proof of this result, as Mane's does not give very much explanation.</p>
",<number-theory>
"<p>This is a problem from Elementary Number Theory by Burton (7th ed.)
I am finding the smallest odd number n such that $2^n-1$ is divided by twin primes $p$ and $q$, where $3 &lt; p &lt; q$.</p>

<p>I followed a hint from the book, and result are $p \equiv -1\pmod {24}$ and $q \equiv 1\pmod {24}$. Now, how can I solve this?</p>
",<number-theory>
"<blockquote>
  <p>Show that $x^3 + y^3 + z^3 + t^3 = 1999$ has infinitely many integer solutions.</p>
</blockquote>

<p>I have not been able to find a single solution to this equation. With some trial I think there does not exist a solution with all of them positive. Can you please help me proceed?</p>

<p>Thanks.</p>
",<number-theory>
"<p>I am reading a book in discrete mathematics and it assumes that a multiplication of two integers yields an integer. </p>

<p>Although that this book's saying is justifiable since the book is making an assumption, I found that that this is ""completely wrong but it is still a good estimation"". </p>

<p>let $n \ge 1$ be an integer. Then, by the above assumption, $(n^2-3) * 5$ yields an integer. But, this is not accurate. </p>

<p>Factorize $n^2-3 = (n+\sqrt{3}) (n-\sqrt{3})$.</p>

<p>Now, use a computer system to evaluate this statement substituting any integer ($1$ or larger). You will find that, extremely, the result is not an integer and therefore we kinda have an contradiction between $n^2-3$ and its factorization. </p>

<p>So, ""Is it accurate to say that multiplication of two integers yields an integer ?""</p>
",<number-theory>
"<p>I have been given $\phi(m)$ and $m = pq$.<br>
Because $p$ and $q$ are primes, $\phi(m) = (p - 1)(q - 1)$<br>
So I was able to find that $p+q$ = sum<br>
But how do I find $p$ and $q$ after this?
The sum is larger than $23000$, so I do not know how to find $p$ and $q$. </p>
",<number-theory>
"<blockquote>
  <p>for $N \ge 4$. Show for prime numbers,  $p \equiv 1$ mod $(N!)$ that none of the numbers $1,2,...,N$ are primitive roots modulo $p$</p>
</blockquote>

<p>I can't figure out where to start with this question, all I can think to use is the Legendre symbol and Euler's Criterion but I haven't been able to do it. Any help would be much appreciated.</p>
",<number-theory>
"<blockquote>
  <p>The generating function for Bernoulli polynomials is given by:<br>
  $$\frac{ue^{ux}}{e^u-1}=\sum_{n\geq 0}B_n(x)\frac{u^n}{n!}$$  </p>
</blockquote>

<p>Now, I have the following expression:<br>
$$\frac{1}{\alpha}\frac{u^2e^{u(\alpha x+y)}}{e^u-1}\sum_{k=1}^{\alpha-1}\frac{1}{e^u-e^{2\pi ik/\alpha}}$$<br>
and I want to rewrite it in terms of Bernoulli polynomials and then extract the coefficients of $\frac{u^n}{n!}$. </p>
",<number-theory>
"<p>Let $g_1(n)=\displaystyle{\sum_{p|n}} p$ and $g_2(n)=\displaystyle{\sum_{p^{\alpha}||n}} \alpha p,$ where $p^{\alpha}||n$ means that $p^{\alpha}|n$ and $p^{\alpha+1}$ does not divide $n.$ I am searching for estimates of these two functions or even upper bounds for them is sufficient for me.
Can someone help me?</p>

<p>Thanks in advance.</p>
",<number-theory>
"<p>I read a question on mathematics.I have not been able to figure out the answer.</p>

<p>the question is</p>

<p>given a set $Q=\{1,2,3,4,5,6,7\}$ such that I can have a subset $L$ from this set $Q$. I have been given a number string $Y$ in such a way that I know its starting digit and the end digit.</p>

<p>Now the rest of the digits from second digit to the last but one digit can be have any numbers of digits from the subset $L$.</p>

<p>Now the question how can i decide whether $Y$ can be made divisible by $3$ or $7$.</p>
",<number-theory>
"<p>When trying around with the <code>DivisorSigma</code> function of Mathematica, I found this Identity:</p>

<p>$\#\{a\mid\exists b\in\mathbb{Z}[i]: ab=n\}=\underbrace{\#\{a\mid\exists b\in\mathbb{N}: ab=n\}^2}_{\sigma(0,n)^2}\Leftrightarrow \forall p|n,\;p\text{ prime}: p\equiv 1\mod 4$</p>

<p>(In words: the number of gauss integer divisors of n is equal to the square of the number of integer divisors of n iff n is in <a href=""http://oeis.org/A004613"" rel=""nofollow"" title=""A004613"">A004613</a>)</p>

<p>I could verify it for values up to 10 million. However, I have been unable to find any Identities that could allow me to prove them. I could take several results from exactly that OEIS page, however none of those helped me out. I am missing some result that connects the gaussian integer divisors with the real ones. I would be very happy with any help on this problem.</p>
",<number-theory>
"<p>Let $p$ be a prime number. Then if $ f(x) = (1+x)^p$ and $g(x) = (1+x)$, then is $f \equiv g \mod p$?</p>

<p>I'm trying to prove that for integers $a &gt; b &gt; 0$ and a prime integer $p$, ${pa\choose b} \equiv {a \choose b}.$ To do this I use FLT to show that  $(1+x)^{pa} \equiv (1+x)^a \mod p$ and compare the coefficients of $x^b$ to complete the proof. Am I applying FLT correctly?</p>

<p>In general, do most theorems regarding integers/reals generalize to polynomials over the integers/reals? Are there some common pitfalls that I could make when trying to generalize such theorems?</p>
",<number-theory>
"<p>Find the smallest possible value of $n_1+n_2+\cdots+n_k$ such that $LCM(n_1,n_2,\ldots,n_k)=(2^2)(3^3)(5^5)$. Note that $k$ is not fixed.</p>

<p>I know the answer should be $k=3$, $n_1=2^2$, $n_2=3^3$, and $n_3=5^5$. How do I prove this rigorously?</p>

<p>Note: LCM is least common multiple.</p>
",<number-theory>
"<p>Let $f_n(x)$ be defined as the $n$th digit of the number $x$.</p>

<p>The result of $f_n(x)$  can  be only ${0,1,2,3,4,5,6,7,8,9}$ for base 10.</p>

<p>For example, if $x=12.46$, then</p>

<p>$f_2(x)=0$;$f_1(x)=1$;$f_0(x)=2$;$f_{-1}(x)=4$; $f_{-2}(x)=6$ ; $f_{-3}(x)=0$.</p>

<p>If we have such function , we can write any real number easily as shown below:</p>

<p>$x=\sum \limits_{n=-\infty}^\infty f_n(x) 10^n$</p>

<p>I tried to find  power series expression of the function.
$f_n(x)=a_0(n)+a_1(n)x+a_2(n)x^2+\cdots$</p>

<p>$$\begin{align*}
x&amp;=\sum \limits_{n=-\infty}^\infty f_n(x) 10^n\\
&amp;=\sum \limits_{n=-\infty}^\infty (a_0(n)+a_1(n)x+a_2(n)x^2+\cdots
) 10^n\\
\sum \limits_{n=-\infty}^\infty a_0(n) 10^n&amp;=0\\
\sum \limits_{n=-\infty}^\infty a_1(n) 10^n&amp;=1\\
\sum \limits_{n=-\infty}^\infty a_2(n) 10^n&amp;=0
\end{align*}$$</p>

<p>But this do not give me so many thing to define $a_k(n)$</p>

<p>Is it possible to find  $a_k(n)$ with some method that known?</p>

<p>I also wonder what the function properties of $f_n(x)$ are? (such as $f_n(x+y)$, $f_n(x.y)$  etc.) I wonder the literature about the function. </p>

<p>Could you please share your knowledge about the function?
Sorry for your time if It was asked before or very basic for number theory.</p>

<p>Thanks a lot for advices and answers</p>
",<number-theory>
"<p>Fermat's theorem on sum of two squares states that an odd prime $p = x^2 + y^2 \iff p \equiv 1 \pmod 4$</p>

<p>Applying the descent procedure I can get to $a^2 + b^2 = pc$ where $c \in \mathbb{Z} \gt 1$</p>

<p>I want $c = 1$, so how do I proceed from here? How do I apply the procedure iteratively?</p>

<p><strong>Example:</strong></p>

<p>$$
p = 97
$$</p>

<p>$$97 \equiv 1 \pmod 4 \implies \left(\frac{-1}{97}\right) = 1 \implies x^2 \equiv -1 \pmod {97}$$ has a solution</p>

<p>$$x^2 + 1 \equiv 0 \pmod {97}$$
$$x^2 + 1 = 97m$$
We find an $x,m$ that solves the equation.
$$x = 75, m = 58$$
Now, we pick an $a,b$ such that $\frac{-m}{2} \leq a,b \leq \frac{m}{2}$
$$a \equiv x \pmod m = 17$$
$$b \equiv y \pmod m = 1$$</p>

<p>Observations:</p>

<ol>
<li><p>$ a^2 + b^2 \equiv x^2 + 1 \equiv 0 (\mod m)$</p></li>
<li><p>$ (a^2 + b^2) = mc$</p></li>
<li><p>$ (x^2 + 1) = mp$</p></li>
</ol>

<p>Plugging in $a,b,m$ for 2, we get $c = 5$</p>

<p><br>
By <a href=""https://en.wikipedia.org/wiki/Brahmagupta%E2%80%93Fibonacci_identity"" rel=""nofollow"">this</a> identity, we know that</p>

<p>$(a^2 + b^2)(c^2 + d^2) = (ac + bd)^2 + (ad - bc)^2$</p>

<p>**$(a^2 + b^2)(x^2 + 1^2) = (ax + b)(a - bx) = m^2pc$</p>

<p>Dividing ** by $m^2$, $pc = (\frac{ax+b}{m})^2 + (\frac{a-bx}{m})^2$</p>

<p>Plugging in $a,b,m,p,c$ we get that $22^2 + (-1)^2 = 97*5$</p>

<p>So we have two squares that add up to 5 times our $p$. How do we turn the 5 into a 1? What is the next step in the descent?</p>
",<number-theory>
"<h2>Question</h2>

<p>If:</p>

<p>$$f(a) + f(b) = f(ab)$$
$$ f(1) = 0 $$
$$ a&lt;b \implies f(a) &lt; f(b)  \forall  a,b \in N  $$</p>

<p>where $N$ is the set of natural numbers.</p>

<p>Prove or disprove $f$ must be the $\log$ function.</p>

<h2>Background</h2>

<p>I was recently wondering about the uniqueness of a function given:</p>

<p>$$f(a) + f(b) = f(ab)$$
$$ f(1) = 0 $$
$$ a&lt;b \implies f(a) &lt; f(b)  \forall  a,b \in R^+  $$</p>

<p>where $R^+$ is the set of positive real numbers.</p>

<p>All of these imply it must be the $\log$ function. I was wondering however what would be the consequence of relaxing the third condition </p>

<p>$$a&lt;b \implies f(a)&lt;f(b)  \forall  a,b \in N $$</p>

<p>where $N$ is the set of natural numbers.</p>

<p>This would allow $f(x)$ to be a combination of the $\log$ and number theoretic functions such as $A(x)$. Where </p>

<p>$$A(x) = \text{number of prime factors of $x$}$$</p>

<p>We note,</p>

<p>$$ A(x) + A(y) = A(xy)$$
$$ A(1) =0 $$</p>
",<number-theory>
"<p>I needed (for my research) to solve a Diophantine equation, in particular,
$$ 2 a + 3 b + 4 c + 5 d = 12 .$$
And I could easily solve it
(for example, on solution is $a=2, b=1, c=0, d=1$).
But this made me wonder if such equations, with their coefficients increasing sequences of
natural numbers, are a special case of Diophantine equations that are <em>always</em> explicitly solvable, 
despite the negative solution to Hilbert's 10th problem.</p>
",<number-theory>
"<p>Let p be a prime with p > 7. Prove that there are at least two consecutive quadratic residues modulo p. [Hint: Think about what integers will always be quadratic residues modulo p when p ≥ 7.]</p>
",<number-theory>
"<p>How you prove this? I'm looking the Erdös proof from Bertrand Postulate and there are many things I don't get. 
Please don't hints, I'm newbie in combinatorics techniques.</p>

<p>In the  book I don't get how $$\displaystyle\prod_{m+1&lt; p\leq 2m+1}p  \leq \left( \begin{matrix} 2m+1 \\ m \end{matrix} \right).$$</p>
",<number-theory>
"<p>I am currently running into a problem related to coprime numbers.</p>

<p>Consider a set of $d$-dimensional integer vectors, $z \subset \mathbb{Z}^d$ such that each component $z_i$ is bounded by another integer $K$. Let us denote this set as:</p>

<p>$$Z_K^d = \Big \{ z \in \mathbb{Z}^d ~\big|~ z_i \in \{0,1,\ldots,K \} ~ i = 1,\ldots, d \Big\}$$</p>

<p>We can visualize $Z_K^d$ as the set of integer coordinates within a $d$-dimensional hypercube of size $K+1$. Note that $Z_K^d$ contains $\big|Z_K^d\big| = (K+1)^d$ distinct vectors, as each of its $d$ components can take on $K+1$ values. </p>

<p>I am interested in determining the number of vectors $z \in Z_d^K$ that are <a href=""http://en.wikipedia.org/wiki/Coprime_integers#Generalizations"" rel=""nofollow"">coprime</a>. Formally, a vector $z \in Z_d^K$ is said to be coprime if the greatest common divisor of all of it's components is 1. <a href=""http://en.wikipedia.org/wiki/Coprime_integers#Properties"" rel=""nofollow"">As explained in the Wiki article</a>, we can also think of these vectors as points with integer coordinates that are 'visible' from the origin (in the sense that there is no other point with integer coordinates between these points and the origin). </p>

<p>Let us denote this subset of coprime vectors $P_K^d \subseteq Z_K^d$ and define it as:</p>

<p>$$P_K^d = \Big \{ z \in Z_K^d ~\big| ~\text{gcd}(z_1,\ldots,z_d)=1 \Big\}$$</p>

<p>I am wondering if there is a <strong>closed-form expression</strong> or a <strong>closed-form upper bound</strong> for the <em>density</em> of these coprime vectors in my original set:</p>

<p>$$\gamma_K^d = \frac{\big|C_K^d\big|}{\big|Z_K^d\big|}$$</p>

<p>I have been actively reading up on the topic (which is outside of my area of expertise) and it seems that the value of $\gamma_K^d$ is asymptotically related to the Riemann zeta function as </p>

<p>$$ \lim_{K\rightarrow\infty} \gamma_K^d = \zeta(d)$$</p>

<p>While this is insightful, it does not take into account that the set that I am interested in is bounded. In addition, the value of $\zeta(d)$ can either be an upper bound or a lower bound on this ratio (so I cannot use it in another bound).</p>
",<number-theory>
"<p>I was looking at a 10x10 multiplication table, and I decided to count the unique products. There are 42 out of a possible 100 numbers represented. I had to wonder, why 42? I counted the 58 non-listed numbers -- most of them are either primes >10 or multiples of those primes. I couldn't figure out a satisfying reason like that for 75, 84, 96 and 98. I feel like the number 42 should be calculable from the natural log of 100 and 10, or something, but I can't really figure it out.</p>

<p>Also, I counted the frequency of all the numbers represented. They all have either 1, 2, 3, or 4 instances. The six numbers that have only 1 instance are all squares -- those of 1, 5, 7, 8, 9, and 10. This makes sense -- squares don't fall into that 4x5 = 5x4 redundancy. The 23 numbers with 2 instances are kind of the default I guess. The four with 3 are the remaining squares -- 4, 9, 16, and 36. 4 and 9 make sense to me, as they are squares within the basic range of the times table, so they are hit by themselves, their square root, and 1. I'm having trouble accounting for why 16 and 36 have three representatives. I'm also having trouble seeing the pattern (or patterns) in the numbers with four representatives.</p>

<p>I ultimately want to find a way to, given a times table of any size, calculate whether a given number will appear, and how many times.</p>

<p>Also, I'm pretty sure that for times tables 7x7 and below, more than half of the possible numbers are represented on the table, but for times tables above 7x7, less than half are represented. Why is 7 the turning point?</p>
",<number-theory>
"<p>I was told that multinomial expansion can be used to determine how many representations of four squares a number like 53 has?  I have a number theory textbook and have done some googleing neither has turned up anything much on this method. Can any of you shed any light?</p>
",<number-theory>
"<p>let $n$ be an integer $&gt;1$, and suppose that $p=2^n+1$ is a prime. 
Show that $3^{(p-1)/2} +1$ is divisible by $p$ (First show that $n$ must be even)</p>
",<number-theory>
"<p>I am trying to solve a very interesting problem about the ring $\mathbb{Z}/n\mathbb{Z}$ and Euler function $\phi (n)$, but i am not sure how to start, i have a few ideas, but none of them leads me to the end of the proof. So, here is the problem.</p>

<p>Let $n$ be a squarefree integer( integer is one divisible by no perfect square, except 1 ). Let $k\in \mathbb{Z}/n\mathbb{Z}$ and $e=1+j\phi (n)$, where $\phi (n)$ is the Euler function and $j\in \mathbb{N}$. Show that $k^{e}=k$.</p>

<p>My first thought, when i saw what i have to prove, was that i have to show the idempotence of the element $k$. I tried to show it, but i couldn't...
Then i recalled that $\phi (n)$ is the order of the unit group of $\mathbb{Z}/n\mathbb{Z}$, but i don't know how should i use it here...or i also know that $\phi(n)$ is always positive and even number, so $j\phi(n)$ must be also even, so the number $e$ is odd...</p>

<p>Can anybody help me with this problem? I have the feeling the things must be easy. I would be glad to read your hints, ideas or remarks. Thank you in advance!</p>
",<number-theory>
"<p>Riemann gave an explicit form for the counting function of the primes.</p>

<p>Is there an explicit form for the counting function $f(x) = \sum_p \ln(\ln(p))$ where the sum is over $p$ : the number of primes smaller than $x$ such that $\ln(\ln(p)) &gt;0$ ?</p>
",<number-theory>
"<p>I was testing out a few summation using my previous descriped methodes when i found an error in my reasoning. I'm really hoping someone could help me out.</p>

<p>The function which i was evaluating was 
$\sum_{n=1}^{\infty} n\ln(n)$ which turns out to be $-\zeta'(-1)$. This made me hope i could confirm my previous summation methode for divergent sums. </p>

<p>My divergent summtion methode (see previous questions) gives for every $d\geqslant2$:
$$\sum_{n=1}^{\infty}f(dn)-f(n)=\sum_{n=1}^{\infty}\sum_{p=1}^{d-1} f(n)e^{ip\pi2n/d}$$
$$\sum_{k=0}^{\infty} \sum_{n=1}^{\infty}\sum_{p=1}^{d-1} -(d)^{2k} n\ln(d^kn)e^{2i\pi*pn/d}=\sum_{n=1}^{\infty} n \ln(n) \tag 1$$</p>

<p>Fill in $d=2$ cause that's the most easy, gives:
$$\sum_{k=0}^{\infty} \sum_{n=1}^{\infty} -(2)^{2k}k n\ln(2)(-1)^n-(2)^{2k} n\ln(n)(-1)^n$$
$$\sum_{k=0}^{\infty} \sum_{n=1}^{\infty} -(4)^{k} n\ln(n)(-1)^n=\sum_{n=1}^{\infty}  n\ln(n)/3(-1)^n \tag 2$$
$$\sum_{k=0}^{\infty} \sum_{n=1}^{\infty} -(2)^{2k}k n\ln(2)(-1)^n=\sum_{n=1}^{\infty} -\frac{4}{9}n\ln(2)(-1)^n= \frac{1}{9}\ln(2) \tag 3$$</p>

<p>equation (1)=(2)+(3)
$$(\sum_{n=1}^{\infty} (-1)^n*n \ln(n)/3)+\ln(2)/9\approx 0.165421153 \tag 4$$</p>

<p>Now i work out $\lim_{m\to\infty}  \sum_{n=1}^{m} n \ln(n)$
$$\sum_{n=1}^{m} n \ln(n)=m(m+1)\ln(m+1)/2-(m)(m+2)/4+\ln(m+1)/12+error \tag 5$$
It turns out the error is most likely $-1/6+~0.165421153$ so the value above.
The fact i get an expression with the value found above is cool. Actualy i'm close but       </p>

<p>$\textbf{[Question]}$ why the $-1/6$. Did i failed isolating a constant part in my approximation? And if so were did i fail to get it out.     </p>

<p>Ps: since i guessed the formula, it would be nice if someone could confirm the fomula if correct (or false).</p>
",<number-theory>
"<p>Suppose that $x$ solves $x^4-x^2+1= 0 \pmod p$. Show that $p=1 \pmod {12}$. Following a hint I have rewritten the equation as $(x^2-1)^2=-3 \mod p$ and $(2x^2-1)^2=-x^2 \pmod p$. The first equation gives that $p=1 \pmod 3$ by using quadratic reciprocity and noting that $1$ is the only applicable quadratic residue. However, I am not able to show that $p=1 \pmod 4$. Since this is a condition in some formulations of quadratic reciprocity I guess that it should be used here as well. This is exercise 13 chapter 5 from the book by Ireland and Rosen if it is any help. Thankful for any hints!</p>
",<number-theory>
"<p>Today I set out to invent a two character numeral system designed to make factorization trivial. Indeed, it lets one factor non-trivial numbers with over thousand digits within 30 seconds per hand - the upshot is that the notation isn't particularly suited for arithmetic. In fact, when I try to add certain two digit numbers, my computer gives me an overflow warning. </p>

<p>The idea is to not use any base like binary or decimal, as in $28=2\cdot 10^1+8\cdot 10^0$, but to hardcode the prime factors $28=2^2\cdot 3^0 \cdot 5^0 \cdot 7^1$ into the notation. So denote the start and end of a number by ""$s$"" and ""$e$"". I set </p>

<p>$0:=se$</p>

<p>and declare a natural number to be any string of the form ""$sxe$"" where $x$ is a finite string of numbers. The $n$'th number in the sequence $x$ denotes the power oth the $n$'th prime number and ""$sxe$"" is the associated product. So from an ordinal point of view</p>

<p>$1=2^0=s0e=ssee$</p>

<p>$2=2^1=s1e=ssseee$</p>

<p>$3=2^0\cdot 3^1=s01e=ssesseee$</p>

<p>$4=2^2=s2e=sssseeee$</p>

<p>$5=2^0\cdot 3^0\cdot 5^1=s001e=ssesesseee$</p>

<p>...</p>

<p>$28=2^2\cdot 3^0 \cdot 5^0 \cdot 7^1=s2001e=sssseeesesesseee$</p>

<p>I came to the conclusion that the language consists of all the strings with equal number of $s$'s and $e$'s, where while scanning from the left there are always more $s$'s than $e$'s and the substring $esee$ isn't allowed. The first condition makes sure that all numbers close and the second one disallows superfluous factor of powers of 0.</p>

<p>I've written a script which brute force generates these strings (it forms all permutations of even numbers of $s$'s and $e$'s and then drops the disallowed ones) and also one to translate them to decimal expression (which relies on knowing what the $n$'th prime is):</p>

<p><a href=""http://pastebin.com/RwkGX6TV"">http://pastebin.com/RwkGX6TV</a></p>

<p>This e.g tells me that 2417851639229258349412352 is sssesssseeeeee and the nice thing is that all numbers factor easily:</p>

<p>$sssesssseeeeee$ </p>

<p>$= s(s(se)(s(s(s(se)e)e)e)e)e$</p>

<p>$=s(s0(s(s(s0e)e)e)e)e$</p>

<p>$=s(s0(s(s1e)e)e)e$</p>

<p>$=s(s0(s2e)e)e$</p>

<p>$=s(s0(2^2)e)e$</p>

<p>$=s(2^0\cdot 3^{2^2})e$</p>

<p>$=2^{3^{2^2}}.$</p>

<p>For now, I've generated most of the numbers with 20 characters and I've added an artificial bound of $R=10^{50}$ to the size of the exponents. This is necessary, as e.g. the number $13$, being the $6$th prime, reads $ssesesesesesseee$, i.e. $s000001e$ but it's neighbors in this enumeration can be numbers with thousands of digits. Generating the strings is simple down at $14$ but multiplication is hard. Later, also higher number of strings are needed and the way I produce the permutation is probably inefficient. </p>

<p>To generate a bigger library of numbers, I'd like to know if someone has an idea for a less arbitrary enumeration of the the strings which avoid the uncomputably big numbers. </p>

<p>Does someone maybe know an good way to enumerate the numbers in the form $1,2,3,2^2,5,2\cdot 3,7,\cdots,3^{2^5}\cdot 11^2\cdot 19^3,\cdots$?</p>
",<number-theory>
"<p>Denote $\pi(x)$ be the number of primes $\leq x,$ $p(n)$ be the $n$-th prime number. 
We have $\pi(p(n))=n.$</p>

<p>It's well known that 
$$\pi(x)\sim \frac{x}{\log x}
\\p(n)\sim n\log n.$$</p>

<p>Is it always true that if $f(x)$ is a function and</p>

<ul>
<li><p>$f(x)&gt;0, \forall x&gt;0$</p></li>
<li><p>$f(x)$ is a <a href=""http://en.wikipedia.org/wiki/Monotonic_function"">monotonic function</a> </p></li>
<li><p>$f(x)=O(x^r)$ for some $r&gt;0$</p></li>
<li><p>$\sum_{n=1}^{\infty}f(n)=\infty$</p></li>
</ul>

<p>then $$\sum_{p\leq x}f(p)\sim \sum_{t\leq \pi(x)}f(t\log t)$$?</p>
",<number-theory>
"<p>Let $O$ be the ring of integers of some number field and $I$ any nonzero ideal of $O$. Prove that there is some number $n \in \mathbb{Z}_+$  that is in ideal $I$.
I suppose I should use that $O$ is Dedekind domain, so every ideal can be written as product of prime ideals, but I don't know how to use that. Any help is appreciated. </p>
",<number-theory>
"<p>The question is :
If $n$ be a six digit number formed by the numbers $1,2,3,4,5,6$ such that $n$ is divisible by $5$, then what are the possible remainders if $n$ is divided by $45$.
Now,since $n$ is divisible by $5$ so it must end with $5$ and the remaining five numbers can be arranged in $5!$ ways in five different positions.Now,the important thing that I observe is that most of the numbers leave the remainder $30$ when they are divided by $45$. I use the word 'most' as I have not checked all such numbers.But I don't find suitable reason behind it.Do all these numbers belong to the same remainder class which is $30$. If the answer is 'Yes' then can anybody explain me the actual reason behind it</p>
",<number-theory>
"<p>What is the largest computable mathematical division in terms of the number of digits that can be handled by a typical desktop computer using the best available big number libraries, assuming input is a decimal string and output is an exact answer? By an exact answer I mean a value that shows the fixed and repeating values, e.g. 1/832 = 0.001201923(076923)</p>

<p>By ""computable"" I mean the value can be calculated by a desktop computer within a day.</p>

<p>Note that because any division can be performed by multiplying by the reciprocal, this is equivalent to asking, what is the largest computable value for the denominator of a reciprocal given current desktop technology?</p>

<p>For example, if I have input string ""1 / 3127362377"", then this decimal might have as many as several billion digits in the repeating or non-repeating portion of the value. Would these billion digits be computable within a day by a desktop computer? If not, what is the largest (smallest?) analogous reciprocal that could be so calculated?</p>
",<number-theory>
"<p>Algebraic class field theory tells us that $\text{Gal}(\mathbb{Q}^{ab}/\mathbb{Q})$ is isomorphic to the group of connected components of the quotient $\mathbb{Q}^{\times}\backslash \mathbb{A}_{\mathbb{Q}}^{\times}\cong \prod_p \mathbb{Z_p}^{\times}\times \mathbb{R}_{&gt;0}$, where $\mathbb{A}_{\mathbb{Q}}$ is the ring of adèles of $\mathbb{Q}$. </p>

<p>It's then said that the group of connected components is given by $\prod_p \mathbb{Z_p}^{\times}$, how can I see this?</p>

<p>Thank you very much in advance!</p>
",<number-theory>
"<p>I've been trying to see whether following assertion is true in order to give a quick proof of another problem I was doing: if $K$ is a finite dimensional extension of the $p$-adic numbers $\mathbb{Q}_p$, we have the multiplicative field norm $N_{K/\mathbb{Q}_p}: K \rightarrow \mathbb{Q}_p$.  While the norm is not guaranteed to be surjective, is it always possible to find some $k \in K$ such that $N_{K/\mathbb{Q}_p}(k)$ has ($p$-adic) absolute value $1/p$?</p>

<p>One way I would imagine to do this is to show that there exist $\alpha, \beta \in K$ such that $|N(\alpha)| = p^A$ and $|N(\beta)| = p^B$ with $A, B$ relatively prime.  The assertion then follows because there exist integers $x, y$ such that $xA + yB = 1$, whence $\alpha^{-x}\beta^{-y} \in K$ with $|N(\alpha^{-x}\beta^{-y})| = |N(\alpha^{-x})N(\beta^{-y})| = |a|^{-x}|b|^{-y} = p^{-Ax}p^{-By} = 1/p$.  </p>

<p>My claim seems to me altogether reasonable since otherwise there exists some prime $q$ such that the absolute value $p^s$ of every $N(k)$ as $k$ runs through all of $K$ will be such that $s$ is always divisible by $q$, which doesn't seem right.</p>
",<number-theory>
"<p>I just have a really quick question of an example that I was trying to come up with. </p>

<p>Are there any number rings which are UFDs but not PIDs?</p>
",<number-theory>
"<p>Let m be a positive integer greater than 1.</p>

<p>Prove that if r is a primitive root of m, then $r^{φ(m)/2} ≡ -1$ (mod m).</p>
",<number-theory>
"<p>The <a href=""http://eom.springer.de/p/p072630.htm"" rel=""nofollow"">Phragmen-Lindelöf theorem</a> gives a consequence of the Riemann hypothesis, viz, the <a href=""http://en.wikipedia.org/wiki/Lindel%C3%B6f_hypothesis"" rel=""nofollow"">Lindelöf hypothesis</a>. As such this is weaker than Riemann hypothesis; but it is still considered that even a proof of this weaker result will be a breakthrough.</p>

<p>Question:</p>

<blockquote>
  <p>What is the strongest known result yet on the Lindelöf hypothesis?</p>
</blockquote>
",<number-theory>
"<p>This is probably obvious, but I don't quite see it.</p>

<p>Archimedean completions of different number fields are always isomorphic to the same $\mathbb{R}$ or $\mathbb{C}$. Is the same true in the non-archimedean case?</p>

<p>More precisely, llet $K$, $L$ be non-isomorphic number fields, and $\mathfrak{p}$ a nonarchimedean prime  of both $K$ and $L$.</p>

<blockquote>
  <p>When is it true that $K_\mathfrak{p}\cong L_\mathfrak{p}$?</p>
</blockquote>
",<number-theory>
"<p>Let $\mathcal{V_K}$ be the set of valuations of a number field $K$.</p>

<blockquote>
  <p>Can it be that $\mathcal{V_L}=\mathcal{V_K}$, for the set of
  valuations of another number field $L$ non-isomorphic to $K$?</p>
</blockquote>
",<number-theory>
"<p>I recently read a very good inequality concerning the no of primes $\pi(x)$:</p>

<p>$$\pi(n)>\frac{1}{6}\frac{n}{\log n}\mathrm{\ for\ }n\ge 2$$</p>

<p>Are any other such elementary inequalities concerning the primes?</p>
",<number-theory>
"<p>The problem is from my friend who sent a message looking for help. I don't think for a long time to solve, so I hope someone can help me to solve it. Thank you </p>

<p><strong>Question</strong>:</p>

<blockquote>
  <p>Given a integer $n$ greater than $1$, suppose that $x_{1},x_{2},\cdots,x_{n}$ are integers such that none of them is divisible by $n$, and neither is their sum. Prove that there exists at least $n-1$ non-empty subsets $A\subseteq \{1,2,3,\cdots,n\}$, such that $\displaystyle\sum_{i\in A}x_{i}$ is divisible by $n$.</p>
</blockquote>
",<number-theory>
"<p>We know that if $ \displaystyle d(n)= \sum\limits_{d \mid n} 1$, then we have </p>

<p>$$ \sum\limits_{n \leq x} d(n)= x\log{x} + (2C-1)x + \mathcal{O}(\sqrt{x})$$</p>

<p>I have referred Apostol's ""Analytic Number theory"" and i understood the first half of the proof where the error term is $\mathcal{O}(x)$, but please tell me as to how to improve the error term to $\sqrt{x}$.</p>
",<number-theory>
"<p>Hello :) I want to compute alle reduced quadratic numbers with discriminat $65$. We call a number $\gamma$ reduced if $\gamma&gt;0$ and $-1&lt;\gamma'&lt;0$. We are working in quadratic field extension $\Bbb{Q}(\sqrt{m})$ and intergers in this extension. Thus we know $b^2-4ac=65$ and $\gamma=\frac{-b+\sqrt{65}}{2a}$, thus we must have: $0&lt;-b+\sqrt{65}&lt;2a&lt;b+\sqrt{65}$. But what then? Must we start with a $b$ and find the right $a$ and $c$? Can someone give an example? </p>

<p>Another question: Suppose $\mathfrak{p}$ is a prime ideal above 2 (thus $\mathfrak{p}\cap\Bbb{Z}=p\Bbb{Z}$) in $\Bbb{Q}(\sqrt{65})$. Show that $\mathfrak{p}^2$ a principal ideal is an compute the generator. ..... Here i have no idea how to start. Someone with hints, ideas or solutions?</p>

<p>Thank you :)</p>
",<number-theory>
"<p>How can we efficiently find square root of 5 in a mod prime field. By quadratic reciprocity we can argue that 5 is a square in modulo p(prime) is p is square modulo 5. But how exactly can we calculate it.</p>
",<number-theory>
"<p>I was wondering if the following is already a known result in mathematics. I have tested it and it seems to work every single time. </p>

<p>If I write the Fibonacci sequence in $\bmod (a)$ form and it repeats after $b$ terms, I will call this the period, and one of the two conditions is true then $a$ is a prime. </p>

<p>If $$b = \frac{a - 1}{n}$$ such that $\frac{a - 1}{n}$ yields a natural number.</p>

<p>Or:</p>

<p>If $b = n(a + 1)$</p>

<p>Then $a$ must be a prime number. </p>

<p>However if $a$ is a prime will not necessarily exhibit these properties. </p>

<p>I would really appreciate your help.</p>
",<number-theory>
"<p>Suppose $a_i$ is a sequence of positive integers. Define $a_1 = 1$, $a_2 = 2$ and $a_{n+1} = 2a_n + a_{n-1}$. Does it follow that </p>

<p>$$ \gcd(a_{2n+1} , 4 ) = 1 $$ ???</p>

<p>Im trying to see this by induction assuming above holds, we need to see that $\gcd(a_{2n+3} , 4 ) = 1$.</p>

<p>But, $\gcd(a_{2n+3} , 4 ) = n_0(2a_{2n+1} + a_{2n-1}) + 4n_1$ for integers $n_0, n_1$. But this quantity does not seem to give me $1$. Can someone help me with this problem? thanks</p>
",<number-theory>
"<p>Show that $\operatorname{Hom}_{\mathbb{Z}}\left ( \mathbb{Z}/n\mathbb{Z},\mathbb{Z}/m\mathbb{Z} \right )\cong \mathbb{Z}/\left ( n,m \right )\mathbb{Z}$</p>

<p>I think that the hom-set (of $\mathbb{Z}$ module homomorphisms ) is isomorphic to $\left \{ a\in \mathbb{Z}/m\mathbb{Z},na=m\mathbb{Z} \right \}=\left \{ k+m\mathbb{Z} \right \}$ where $m\mid (nk)$ but I can't show that it's isomorphic to $\mathbb{Z}/\left ( n,m \right )\mathbb{Z}$</p>
",<number-theory>
"<p>I know that there exists nine 2007 digit number where each two digit number is made up of two neighbor digits, those numbers are:</p>

<ol>
<li>$$12345678901234....90123456789012345678   $$         </li>
<li>$$23456789012345....01234567890123456789$$</li>
<li>$$34567890123456....12345678901234567890$$</li>
<li>$$45678901234567....23456789012345678801$$</li>
<li>$$56789012345678....34567890123456789012$$</li>
<li>$$67890123456789....45678901234567890123$$</li>
<li>$$78901234567890....56789012345678901234$$</li>
<li>$$89012345678901....67890123456789012345$$</li>
<li>$$90123456789012....78901234567890123456$$</li>
</ol>

<p>Why there is nine numbers? because number can not be started with '0' digit! Right?</p>

<p>Anyway I can not find out which numbers can be divided by 17 or 23 from this list?
I have no idea how to get there. Any hints or ideas on how I should tackle this one?</p>
",<number-theory>
"<p>Evaluate the following Legendre symbols using quadratic reciprocity:</p>

<ol>
<li>$\left(\frac{295}{401}\right)$</li>
<li>$\left(\frac{713}{1009}\right)$</li>
</ol>

<p>I know that can flip the numbers and reduce because both $401$ and $1009$ are equivalent to $1 \pmod{\!p}$ and so on, but I am starting to get weird numbers and I think I did something wrong in one of my steps.</p>
",<number-theory>
"<p>Let $E$ be an elliptic curve with a 3-torsion point $P$ and $G = \operatorname{Gal}(\overline{\mathbb{Q}}/\mathbb{Q})$. Let $X = \{O, P, -P\}$ where $O$ is the point at infinity and $X$ is a $G$-module. Why does $H^{1}(G, X)$ inject into $\operatorname{Sel}_{3}(E/\mathbb{Q})$?</p>
",<number-theory>
"<p>Show that, for any prime $p$, there are integers $x$, and $y$ such that $p$ is divisible by $(x^2+y^2+1)$    Can you show me what to start with? do I prove $p$ is divisible by $x^2$ and $y^2$ separately?</p>
",<number-theory>
"<p>Let $G$ be a finite group, although this may not be necessary for almost everything that follows. One of the ways of defining Galois homology groups is using the standard resolution for the $\mathbb{Z}$, explicitly we do the following. Let </p>

<p>$$ \cdots\to P_2\to P_1 \to P_0 \to \mathbb{Z} $$ </p>

<p>be the standard resolution. Let $I\subset \mathbb{Z}[G]$ be the ideal generated by elements of the type $(g-1)$. For two $G$-modules $M,N$ the $G$ module structure on $M\otimes N$ is given by $$g\cdot (m\otimes n):=gm\otimes gn.$$</p>

<p>Define 
$$H_i(G,M)=H_i\bigg(\cdots\to \frac{P_2\otimes M}{I(P_2\otimes M)}\to \frac{P_1\otimes M}{I(P_1\otimes M)} \to \frac{P_0\otimes M}{I(P_0\otimes M)}\to 0\bigg).$$</p>

<p>If $$0\to M'\to M\to M''\to 0$$ is a short exact sequence of $G$-modules then we have a short exact sequence of $G$-modules 
$$0\to \frac{P_i\otimes M'}{I(P_i\otimes M')}\to \frac{P_i\otimes M}{I(P_i\otimes M)} \to \frac{P_i\otimes M''}{I(P_i\otimes M'')}\to 0.$$
As a consequence we get a short exact sequence of complexes 
$$0\to \frac{P_\bullet\otimes M'}{I(P_\bullet\otimes M')}\to \frac{P_\bullet\otimes M}{I(P_\bullet\otimes M)} \to \frac{P_\bullet\otimes M''}{I(P_\bullet\otimes M'')}\to 0$$
and consequently a long exact sequence of homology groups. The connecting homomorphism is defined in the usual way; we start with an element in $\frac{P_i\otimes M''}{I(P_i\otimes M'')}$ which is 0 under the differential, lift it to an element in $\frac{P_i\otimes M}{I(P_i\otimes M)}$ and apply the differential to get an element in $\frac{P_{i-1}\otimes M'}{I(P_{i-1}\otimes M')}$, whose homology class is the required element. </p>

<p>Now consider the case when $M=\mathbb{Z}[G]$ and $M''=\mathbb{Z}$, and $\bullet=1$. In this case the connecting homomorphism should give an isomorphism $H_1(G,\mathbb{Z})\to H_0(G,I)$.
Let $\alpha$ be an element in $\frac{P_1}{IP_1}$ whose differential is 0. We can lift this to the element $\alpha\otimes 1$ in $\frac{P_1\otimes \mathbb{Z}[G]}{I(P_1\otimes \mathbb{Z}[G])}$, but now applying the differential, which in this case is $d\otimes 1$, we will always get 0 since $d\alpha=0$, which is clearly not correct. The same problem would appear if $M''$ is a summand of $M$ as abelian groups. Could someone please point out what is it that I am doing wrong.</p>
",<number-theory>
"<p>\begin{align}
    a^2 + ba + c &amp;= 0 &amp; \text{{No real roots.}}\\
    \lfloor a^2 \rfloor + ba + c &amp;= 0 &amp; \text{{At least one real roots.}}
\end{align}</p>

<p>Are there any values of b and c that will make the given number of roots correct?</p>

<p>The first thing I thought about doing was finding the discriminant .In order for the first equation to have no real roots, we must have $b^2 – 4c &lt; 0$. That means that $b^2 &lt; 4c$. 
I know that the discriminant of the second one must be 0 but I am not sure how to express it because a floor function is involved. What should I do? 
Will this approach get me anywhere or are there any better methods? </p>
",<number-theory>
"<p>Do there exist $10$ distinct integers such that the sum of any $9$ of them is a perfect square?</p>
",<number-theory>
"<p>We know that $\displaystyle\zeta(2)=\sum\limits_{n=1}^{\infty} \frac{1}{n^2} = \frac{\pi^2}{6}$ and it converges.</p>

<blockquote>
  <ul>
  <li>Does there exists a bijective map $f:\mathbb{N} \to \mathbb{N}$ such that the sum $$\sum\limits_{n=1}^{\infty} \frac{f(n)}{n^2}$$ converges.</li>
  </ul>
</blockquote>

<p>If our $s=2$ was not fixed, then can we have a function such that $\displaystyle \zeta(s)=\sum\limits_{n=1}^{\infty} \frac{f(n)}{n^s}$ converges</p>
",<number-theory>
"<p><strong>Problem</strong>: Let $n$ be an integer and $p$ a prime dividing $5(n^2-n+\frac{3}{2})^2-\frac{1}{4}$. Prove that $p \equiv 1 \pmod{10}$.</p>

<p>The polynomial can be re-written as $(\sqrt{5}(n^2-n+\frac{3}{2})-\frac{1}{2})(\sqrt{5}(n^2-n+\frac{3}{2})+\frac{1}{2})$. If this vanishes mod $p$ then $5$ is a quadratic residue mod $p$, which shows that $p \equiv \pm 1 \pmod{5}$ (the primes 2 and 5 are easily ruled out). It feels like the problem should be solvable by understanding the splitting of primes in the splitting field of this polynomial, but I can't find an appropriate ""reciprocity law"".</p>

<p>The things I'm not sure about are:</p>

<ol>
<li>How does one rule out the primes congruent to $-1$ mod $5$?</li>
<li>Under what circumstances is it the case that the set {rational primes that split in the ring of integers of some number field} is the union of arithmetic progressions? This a kind of generalized reciprocity law but I don't know in what generality they are known to hold.</li>
</ol>
",<number-theory>
"<p>This question is a variant of problem 4, pg. 21, from Birkhoff and Maclane, <em>A Survey of Modern Algebra</em>. </p>

<blockquote>
  <p>Given a function $w: \mathbb{N}^+ \rightarrow \mathbb{N}$  that behaves like a valuation function, i.e., </p>
  
  <p>(1) $ w(ab) = w(a) + w(b) $</p>
  
  <p>(2) $ w(a+b) \geq \min(w(a), w(b)).$</p>
  
  <p>Show that it is either</p>
  
  <ol>
  <li>constant $0$ function, $w(a) = 0$
  or </li>
  <li>a multiple of a $p$-adic valuation, in other words $\forall a \in \mathbb{N}^+, w(a) = k v_p(a)$ for some $p, k$ with $v_p(p^\alpha d) = \alpha $ when $(p, d) = 1 $.</li>
  </ol>
</blockquote>

<p>Note that this problem is relatively simple if $w$ is defined over $\mathbb{Z}\backslash \{0\}$ instead of $\mathbb{N}^+$ which is the problem listed in the book. My question is whether the stronger statement above is also true. </p>

<p>My current proof attempt is incomplete.</p>

<p>If $\forall a,\ w(a) = 0$ we are done. Otherwise let $n$ be the least number s.t. $w(n) \neq 0$. Easy to show from (1) that $n \neq 1$ since $w(1) = 0$ and that $n$ is prime.</p>

<p>By unique factorisation theorem and (1) to get the result I need only prove that for all primes $p$, $p\neq n \implies w(p) = 0$.</p>

<p>I tried to proceed using well founded induction. If $p \lt n$ done. Otherwise $p \gt n$. If $n \gt 2$ then $n, p$ are odd and $n+p$ is even hence $n+p=2q$. Now $n \nmid p$ so $n \nmid n + p$. If $n \gt 2$ we have $n \nmid q$. Since $q \lt p$ the induction hypothesis gives us $w(q) = 0$ and since $n \gt 2$, $w(2) = 0$. Thus $w(n+p) = w(2q) = w(2) + w(q) = 0 \ge \min(w(n), w(p))$. This gives us that $w(p) = 0$. </p>

<p>I can't see how to solve the $n=2$ case. Is the case where $n=2$ solvable or is there a counterexample? </p>

<p>Some perhaps useful facts. </p>

<p>If $w(2) \neq 0$ and any other $w(p')=0$ for $p'$ prime, $p'\neq 2$ then $w(p) =0$ for all primes $p\neq2$. Easy to prove since every prime $p$ is an even multiple less than some power $r$ of $p'$ and by (1) $w(p^r) = 0$ and then by (2) $w(p) = 0$.</p>

<p>It can't be the case that $w$ behaves identically on all primes, i.e. $w(p) = k$ for all primes $p$ since it is easy to compute counterexamples, e.g. $2^2*5^2 + 3^5 = 7^3$ that will contradict (2).</p>

<p>Thus it must be distinct at some two primes $p_1, p_2$. This gives two distinct relatively prime numbers where w is equal, $ p_1^{w(p_2)} $ and $p_2^{w(p_1)}$. I hoped that this would lead to a contradiction but haven't found a way forward yet.</p>
",<number-theory>
"<p>A generalization of the conjecture </p>

<p>$$\pi(x+x^{\theta}) - \pi(x) \sim \frac{x^\theta}{\log x} $$ (Ingham, 1937 or earlier) might be</p>

<p>$$\Delta \pi_k = \pi_k((x+1)^2) - \pi_k(x^2)\sim \frac{x}{\log x}\frac{(\log\log x^2)^{(k-1)}}{(k-1)!} $$</p>

<p>in which $\pi_k(x)$ is the number of numbers with k primes including repetitions not exceeding x. </p>

<p>For the case $k = 1$ we have $\Delta \pi_1 \sim \frac{x}{\log x},$ suggesting that the number of primes on a square interval approaches that of the interval $[1,x].$ All very speculative. My question is simply whether we can find a similar (speculative) statement for $\Delta \pi_2,$ etc., relating the number of primes on the square interval to that of some interval $[1,x]?$ In other words, can we solve for example:</p>

<p>$$\frac{x}{\log x}\log\log (x^2)= \frac{y}{\log y} \sim \pi(y) $$ for y in terms of x?   </p>

<p>It's easy enough to plot $f(x)=\frac{x}{\log x}\log\log x$ and find $\pi(y)\approx f(x).$ For example: </p>

<p>$$\frac{1000}{\log 1000}\log\log 1000^2 \approx \frac{3048}{\log 3048}.$$  </p>

<p>Thanks for any insight.  </p>

<p>I have failed the Turing test twice today, so editing appreciated.</p>
",<number-theory>
"<p>Find two positive integers $x$ and $y$ such </p>

<p>$$\sqrt{69+20\sqrt{11}}=\sqrt{x}+\sqrt{y}$$</p>

<p>I have worked intensively with this task but I really can't find a solution to this problem. I hope that I can get a hint here. </p>
",<number-theory>
"<p>I have thought of a conjecture similar to Goldbach Conjecture. I have shown the result to be true with a program in C++ up until $n=30000$.</p>

<blockquote>
  <p>$\forall n&gt;2$ with $n$ even,there exists two primes $p,q$ with $n&lt;p,q&lt;2n$ $\;$ and $\;$  $p+q=3n$</p>
</blockquote>

<p>We know Betrand's postulate:</p>

<blockquote>
  <p>For any integer $n &gt; 3$, there always exists at least one prime number $p$ with
  $n &lt; p &lt; 2n - 2$.</p>
</blockquote>

<p>But I don't know if this postulate can help me.</p>

<p>I wanted to place it somewhere public so people can think about it.
Are there any results similar to this?</p>
",<number-theory>
"<p>It is well known that a polynomial $$f(n)=a_0+a_1n+a_2n^2+\cdots+a_kn^k$$ is composite for some number $n$.</p>

<p>What about the function $f(n)=a^n+b$ ? </p>

<blockquote>
  <p>Do positive integers $a$ and $b$ exists such that $a^n+b$ is prime for every natural number $n\ge 1$ ?</p>
</blockquote>

<p>I searched for long chains in order to find out whether there is an obvious upper bound. For example $4^n+4503$ is prime for $n=1,\ldots,14$.</p>
",<number-theory>
"<p>Recently I am reading Stein's <em>Complex Analysis</em>, and he is going to prove the prime number theorem after estimating the value $1/\zeta(s)$. However, I don't understand the technical details of the proof in Proposition 1.6 in chapter 7. Here are what I have before going to my problem.</p>

<ol>
<li>If $\sigma \ge1$ and $t\in\mathbb R$, then $\log\left|\zeta^3(\sigma)\zeta^4(\sigma+it)\zeta(\sigma+2it)\right|\ge1$ (Chapter 7, Corollary 1.5)</li>
<li>If $\sigma,t\in \mathbb R$, $|t|\ge1$, $0\le\sigma_0\le1$ and $\sigma_0\le\sigma$, then for every $\epsilon&gt;0$, there exists a constant $c_\epsilon$ such that $|\zeta(\sigma+it)|\le c_\epsilon|t|^{1-\sigma_{0}+\epsilon}$ (Chapter 6, Proposition 2.7)</li>
</ol>

<p>Here is my question, in Chapter 7, Proposition 1.6: The book tells that the following inequality holds for $\sigma\ge1$ and $|t|\ge1$:</p>

<p>$$|\zeta^4(\sigma+it)|\ge c|\zeta^{-3}(\sigma)||t|^{-\epsilon}\ge c'(\sigma-1)^3|t|^{-\epsilon}$$</p>

<p>The first inequality is obviously concluded from (1) and (2) above. However I can't see why the second inequality holds. Can anybody give me some hints on it?</p>
",<number-theory>
"<p><strong>Statement</strong>  For every $n > 1$ there is always at least one prime $p$ such that $n &lt; p &lt; 2n$.</p>

<p>I am curious to know that if I replace that $2n$ by $2n-\epsilon$, ($\epsilon>0$) then what is the  $\inf (\epsilon)$ so that the inequality still holds, meaning there is always a prime between $n$ and $2n-\epsilon$</p>
",<number-theory>
"<p>Let $\Lambda$,$\Lambda'$ be two lattice in $\mathbb{C}$ and $m\neq 0\in\mathbb{C}$ satisfying 
$$
m\Lambda\subset\Lambda'
$$</p>

<p>The, the book I'm reading says that by the theory of finite Abelian groups there exists a basis $\{\omega_1,\omega_2\}$ of $\Lambda$' and positive integers $n_1,n_2$ such that $\{n_1\omega_1,n_2\omega_2\}$ is a basis of $m\Lambda$.</p>

<p>I wonder which theorem is it using to deduce such conclusion? There is even no finite Abelian groups there.</p>

<p>update:</p>

<p>I think the author means theory of finitely-generated Abelian groups, doesn't him?</p>
",<number-theory>
"<p>In 1893 Hadamard proved that:</p>

<p>$$\xi(s) = \xi(0) \prod_{\rho} \left(1- \frac{s}{\rho} \right) \left(1- \frac{s}{1-\rho} \right)$$</p>

<p>where $\xi(z) = \frac12 z(z-1) \pi^{-\frac{z}{2}} \Gamma(\frac{z}{2}) \zeta(z)$ and $\rho = \sigma + \gamma i$ is a non-trivial zero of $\zeta(s)$ (i.e. $\gamma_n$ is the imaginary part of the n-th $\rho)$).</p>

<p>Riemann had already conjectured this in 1859 and since he needed an 'always real' function for his next thought steps, he assumed $s=\frac12 + t i$ and defined the function $\Xi(t)=\xi(s)$ that has been proven to be equal to:</p>

<p>$$\Xi(t)= \Xi(0)\prod_\gamma\left(1-\frac{t^2}{\gamma^2}\right)$$</p>

<p>It is known that $\xi(s)=\xi(1-s)$ and $\xi(s)=\overline{\xi(\overline{s})}$ and this implies that $\xi(s)\xi(\overline{s})$ must be real. Also known is that when there is a non-trivial zero $\rho$ lying off the critical line, then also $1-\rho, \overline{\rho}, \overline{1-\rho}$ must be zeros and their product will always be real:</p>

<p>$$\displaystyle\prod_\gamma\left(1-\frac{s}{\sigma+i \gamma}\right)
\left(1-\frac{s}{\sigma-i \gamma}\right)\left(1-\frac{s}{1-(\sigma+i \gamma)}\right)
\left(1-\frac{s}{\overline{1-(\sigma+i \gamma)}}\right)$$</p>

<p>This brought me to the following conjecture. Suppose $s=a+t i$ and $\Xi_a(t)=\xi(s)$ so $\Xi_a(0)=\xi(a)$, then the following product holds and is always real:</p>

<p>$$\Xi_a(t)\Xi_a(-t) = \Xi_a(0)^2\prod_\gamma \left(1-\frac{t^2}{\gamma^2}\right)\left(1-\frac{(-t)^2}{\gamma^2}\right)$$</p>

<p>or simpler:</p>

<p>$$\Xi_a(t)\Xi_a(-t) = \left(\xi(a)\prod_\gamma \left(1-\frac{t^2}{\gamma^2}\right)\right)^2$$</p>

<p>Numerical tests indicate that the conjecture is correct, but I am obvioulsy keen to find a proof (I do realise this is a way too big of a question to ask here!). Does anybody have a link that explains how $\Xi(t)= \Xi(0)\prod_\gamma\left(1-\frac{t^2}{\gamma^2}\right)$ has been derived? Also appreciate any other thoughts/steers on how to further progress this conjecture.</p>

<p>Thanks!  </p>
",<number-theory>
"<p>Since it is particularly easy to write down a basis of a Kummer extension $K=k(\mu)/k$ (where $\mu^n=a \in k$) as a $k$-vector space, I suspect that it is should not be terribly hard to write an explicit formula for a norm of an element in terms of its coordinates in this basis.</p>

<p>Is there a standard text where this derivation is carried out?</p>
",<number-theory>
"<p>How would you go about solving a multivariable, non-linear Diophantine Equation?</p>
",<number-theory>
"<p>Supose we have a set $S$ of natural numbers. We define: 
$$ f(x) = \begin{cases} {x \over 2} &amp; x \text{ is even } \\ {x - 1 \over 2} &amp; x \text{ is odd }\end{cases} $$</p>

<p>Now let $a_0 = a_1 = 1$ and $a_n = a_{n-1} + a_{n-2}$ for $n \notin S$ and $a_n = f(a_{n-1} + a_{n-2})$ otherwise for $ n &gt; 1$.</p>

<p>I want to calculate the remainder of $a_n \bmod p $ for $p$ prime. The problem is that the numbers become quite large so I can not store then. I must work with the reminders.</p>

<p>My fist idea was to work with the modular inverse of $2$ and e kept the remainder mod p at each step and instead of dividing by $2$, I multiply by $1/2$. The problem is that is does not work if $S$ has more than one element.</p>

<p>I think the solution is to keep the values $\bmod 2^y\cdot p$ where $y$ is the number of elements in $S$ $(\#S)$. And in the end take do remainder of $a_n \bmod p$. It seems to work but I don't understand why...</p>

<p>I'm sorry if I was not clear.</p>

<p>Thank you.</p>
",<number-theory>
"<p>The book I'm reading doesn't provide the definition of degree of an isogeny and I failed to google it. Can anyone tell me?</p>
",<number-theory>
"<p>Ramanujan defined his function $\phi(x)$ as $$\phi(q)=\sum^{\infty}_{n=-\infty}q^{n^2}$$ and other one as $$\psi(q)=\sum^{\infty}_{n=0}q^{\frac{k(k+1)}{2}}$$ So I have a question that is there an integral representation for these functions? </p>
",<number-theory>
"<p>As we know that $f(x)=x^2+1\equiv0 \pmod p $ has no integer solutions if $p\equiv 3\pmod 4$, does there exist a cubic polynomial $f(x)=ax^3+bx^2+cx+d~(a,b,c,d \in\mathbb Z,a\neq 0) $ such that $f(x)\equiv0 \pmod p $ has no integer solutions if $p\equiv 3\pmod 4$?</p>

<p>I only know that $f(x)=0$ has no integer solutions.</p>
",<number-theory>
"<p>Explaining my work on Maass wave forms to friends and family (all non-mathematician) typically earns me blank faces. So I wonder whether there is some good example to explain their meaning to laymen. I am aware of the inner-mathematical importance of Maass wave forms, but what are real life applications of Maass wave forms?</p>
",<number-theory>
"<blockquote>
  <p>Prove that $$\displaystyle \sum_{k=1}^n \bigg(\dfrac{1}{k}+\dfrac{2}{k+n}\bigg ) \leq \ln(2n) + 2 -\ln(2).$$</p>
</blockquote>

<p>I was thinking of using mathematical induction for this. That is,</p>

<p>We prove by induction on $n$. The case $n=1$ holds trivially since $2 \leq 2$. Now assume the result holds for some $m$. Then by assumption we know that $$\displaystyle \sum_{k=1}^{m+1} \bigg(\dfrac{1}{k}+\dfrac{2}{k+m}\bigg ) \leq \ln(2m) + 2 -\ln(2)+\dfrac{1}{m+1}+\dfrac{2}{2m+1}. $$ </p>

<p>We must relate this somehow to $\ln(2(m+1)) + 2 -\ln(2)$.</p>
",<number-theory>
"<p>I have to show that $2x^4-20x+8$ cannot be divided by $16$ without remainder. The only thing comes to my mind is to write $16$ as $4^2$ which hasn't been of any help.</p>

<p>Could you give me some hints to prove this?</p>
",<number-theory>
"<p>Let $n=3^{1000}+1$. Is n prime?</p>

<p>My working so far:</p>

<p>$n=3^{1000}+1 \cong 1 \mod 3$</p>

<p>I notice that n is of form; $n=3^n+1$</p>

<p>Seeking advice tips, and methods on progressing this.</p>
",<number-theory>
"<p>Let $K=\mathbb{Q}(\zeta_m)$. Then if $p\nmid m$ is any odd prime, how i can show that Frobenius map is
$(p,K/\mathbb{Q})(\zeta_m)=\zeta_m^p$.</p>

<p>We know, if $P$ is a prime above $p$ </p>

<p>$$(p,K/\mathbb{Q})(x) \equiv x^p (\mbox{mod } P) \quad\forall x \in \mathbb{Z}_K$$</p>

<p>Thanks!</p>
",<number-theory>
"<p>In p-adic case, Schwart function is the function which has compact support and locally constant. But can we say its uniform continuity from this?</p>

<p>I think it would not be true, but I am not certain with great convince.</p>

<p>Would you let me know this?</p>
",<number-theory>
"<blockquote>
  <p>Solve in positive integers: $5x^2+6x^3=z^3$.   </p>
</blockquote>

<p>$x^2(6x+5)=z^3$ </p>

<ul>
<li>If $(x,5)=5$, let $x=5k$. So $k^2(6k+1)=\left(\frac{z}{5}\right)^3$, we're left with solving $6n^3+1=m^3$.    </li>
<li>If $(x,5)=1$, then we're left with solving $6n^3+5=m^3$.   </li>
</ul>

<p>Here we only used $(a,b)=1,\: ab=c^3$ gives $a=n^3, b=m^3$.   </p>

<p>How to solve these 'cubic Pell equations'?</p>
",<number-theory>
"<p>Few friends are going to a party. Each person has his own collection of T-Shirts. There are 100 different kind of T-Shirts. Each T-Shirt has a unique id between 1 and 100. No person has two T-Shirts of the same ID.</p>

<p>They want to know how many arrangements are there in which no two persons wear same T-Shirt. One arrangement is considered different from another arrangement if there is at least one person wearing a different kind of T-Shirt in another arrangement.</p>

<p><strong>Example 1 :</strong> If Their are 2(=N) friends and Each of the next N lines contains at least 1 and at most 100 space separated distinct integers, denoting the ID's of the T-Shirts ith person has.</p>

<pre><code>3 5
8 100
</code></pre>

<p>Answer for this case is 4 </p>

<p>Explanation : 4 possible ways are (3,8), (3,100), (5,8) and (5,100).</p>

<p><strong>Example 2 :</strong> If N=3 and collection with each of 3 friends is as follow :</p>

<pre><code>5 100 1
2
5 100
</code></pre>

<p>Then here also answer will be 4 as 4 possible ways are (5,2,100), (100,2,5), (1,2,100), and (1,2,5).</p>
",<number-theory>
"<p>In a book I'm reading the following modulus comes up,
$$ \operatorname{mod} \: \mathbb{Q}^{*2}$$ 
and I'm struggling to understand what it means. I understand $\mathbb{Q}^{*} = \mathbb{Q}\backslash \{0\}$ but not the square.</p>

<p>Context: $\delta := \frac{a.b}{2} \: (\operatorname{mod} \: \mathbb{Q}^{*2})$ with $a,b \in \mathbb{R}$.</p>
",<number-theory>
"<p>Here is the problem:</p>

<p>$ 445^{445} + 225^{225}  \pmod{9}$</p>

<p>I found out that for $ 445^{445}  \pmod{9} = 7$.</p>

<p>but for $ 225^{225}  \pmod{9}$ when I do this:</p>

<p>$ (225 \bmod 9)^{225  \bmod 8}$ for the first equation I have $0$ mod.</p>

<p>What should I do?</p>
",<number-theory>
"<p>Would someone be kind enough to correct my error here?</p>

<p>$S=1+2+3+4+\cdots$</p>

<p>Now starting with the 1st prime number, regroup the sum:</p>

<p>$S=(1+3+5+7+\cdots) + (2+4+6+\cdots)$</p>

<p>$S=(1+3+5+7+\cdots) + 2(1+2+3+\cdots)$</p>

<p>$S=(1+3+5+7+\cdots) + 2S$</p>

<p>Now repeat the same procedure for every prime to acquire:</p>

<p>$S=1+2S+3S+5S+\cdots$</p>

<p>$1=\frac{1}{S}+2+3+5+\cdots$</p>

<p>$S=1-\frac{1}{2}-\frac{1}{3}-\frac{1}{5}-\cdots$</p>

<p>Does this logic make sense at all?</p>
",<number-theory>
"<p>Let $q$ be a prime such that $q \equiv 2 (\mod 3)$ , then is it true that $a^2+ab+b^2=qc^2$ has no solution in non-zero integers $a,b,c$ ? </p>
",<number-theory>
"<p>I seek a (very) elementary proof that the zeta function of an elliptic curve $E$ over $\mathbb{F}_q$ has the form
$$Z(T)=\frac{1-aT+qT^2}{(1-T)(1-qT)}.$$
Something tedious and computational making use of Weierstrass Normal Form (I am happy to assume that char$(\mathbb{F}_q)\neq 2,3$) would be good! I am also happy if someone can suggest a reference which only deals with a certain class of elliptic curve. </p>

<p>Many thanks!</p>
",<number-theory>
"<p>Number of ordered triples $(a, b, c)$ with $gcd(a, b, c) = 1$ and $1 \leq a, b, c \leq n$ can be computed using the following formula:
$$
C(n) = \sum_{k=1}^n\mu(k) \left \lfloor \frac{n}{k} \right \rfloor^3
$$,
where $\mu(n)$ is Moebius function. </p>

<p>But how it can be derived?</p>

<p>Thanks!</p>
",<number-theory>
"<p><em>This is a ""fiddling"" in a small project of mine with which I'm concerned from time to time for <a href=""http://math.stackexchange.com/questions/39378/series-of-logarithms-sum-limits-k-1-infty-lnk-ramanujan-summation?rq=1"">three years now</a>. I try to focus on the core of the problem, please ask if more context is needed.</em><br>
<hr>
Consider the divergent series 
$$ \sum_{k=1}^\infty {(-1)^{k-1} \over k }\zeta(-k) \underset{\mathcal N}{=} s_1 = -0.081061466...   $$
Here the symbol ""$\underset{\mathcal N}{=} $"" means, that I did that sum by the Noerlund-summation-method using 64 terms. The value which I expect by some other derivation which I'll explain below is $ -\zeta(0)' = 0.91893853 $ which differs exactly by 1.        </p>

<p>More context: the coefficients at the zetas are taken just from the matrix of Stirling-numbers of the first kind, denote them simply as $s1_{r,c}$ , so the defition stems really from:
$$ \sum_{r=1}^\infty s1_{r,1} \cdot {1! \over r! }\zeta(-r) \underset{\mathcal N}{=} s_1  $$</p>

<p><hr>
Next consider the divergent series taken from the next column in the Stirling matrix:
$$ \sum_{k=1}^\infty s1_{k,2} \cdot { 2! \over k! }\zeta(-k) \underset{\mathcal N}{=} s_2 = -0.006356455...   $$
The value which I expect by the other derivation is $ \zeta(0)'' = -2.00635645591... $ which differs (relatively near) by $2!$. (The difference can be made smaller by taking more terms for the Noerlund-summation)
<hr>
To make things short, I'm doing the dotproduct
$$ Z \cdot S1 \underset{\mathcal N}{=} Y $$
where the infinite rowvector $Z$ contains the consecutive zetas $\zeta(0),\zeta(-1),\zeta(-2), ...$ and $S1$ is the matrix containing the Stirlingnumbers first kind, scaled by factorials such that
$$ S1_{r,c} = s1_{r,c} \cdot { c!\over r!} $$
getting the result-vector $Y$ which deviates from my expected result of derivatives  $ \zeta(0)^{(c)}$ by factorials such that
$$ Y[c]= (-1)^c \cdot (\zeta(0)^{(c)} + c!) $$
<hr>
The problem is connected with that of the Ramanujan-summation of the series of like powers of logarithms:
$$ \sum_{k=0}^{\infty} \log(1+k)^c \underset{\mathcal Z}{=}  (-1)^c \cdot \zeta(0)^{(c)} $$
where I get (by ""$\mathcal  Z $"" eta-regularization) the ""magic constants"" having the same values as I described above and which deviate by the expected values for that sums (by the signed $\zeta(0)$-derivatives) exactly the factorials. (See <a href=""http://math.stackexchange.com/questions/39378/series-of-logarithms-sum-limits-k-1-infty-lnk-ramanujan-summation?rq=1"">my earlier question in MSE</a> but in which I had not yet that more general view with the columns of the Stirlingmatrix)           </p>

<p><em>Additional remarks:</em> the complete background can be found <a href=""http://go.helms-net.de/math/divers/BernoulliForLogSums.pdf"" rel=""nofollow"">in this article</a> (I'm just editing the concerning paragraphs) and was remotivated by the <a href=""http://math.stackexchange.com/questions/437690/computing-the-value-of-logarithmic-series-qs-n-ln1s-ln2s-ln3"">recent question here in MSE</a>      </p>
",<number-theory>
"<p>I was wondering whether it has been proven/disproven yet or at least conjectured that the bernoulli denominator of $B_{2n}$ is divisible by $2n+1$ if and only if $2n+1$ is prime?</p>

<p>If not, must the denominator always share a common factor with the value $2n+1$?</p>

<p>Or the question could be written as is denom($B_{2n}) = \prod\limits_{p:(p-1)|(2n)} p    = (2n+1)m$ where m is an integer always true if and only if $2n+1$ is prime?</p>
",<number-theory>
"<p>I search a composite number near $4^{4^4}$ with a very large smallest prime factor. A candidate is $$4^{4^4}+253=4^{256}+253$$</p>

<p>The number is composite and has $155$ digits, so it is in the range , the quadratic sieve can handle. But the calculation will take several days.</p>

<p>I ran $500$ elliptic curves with $B1=250K$ and $150$ curves with $B1=1M$ and will continue the search with ECM. Everyone reading this is invited to run MPQS.</p>
",<number-theory>
"<p>Let $r_k(n)$ be the number of ways to write $n$ as the sum of $k$ squares of integers. </p>

<blockquote>
  <p><strong>Theorem:</strong> If $k \ge 5$ then there are constants $C,c&gt;0$ such that for any $n$, $$cn^{k/2 - 1} \le r_k(n) \le Cn^{k/2 - 1}$$</p>
</blockquote>

<p>What is the simplest way to prove it as stated? I am especially interested in the lower bound. Is there a recommended book on the subject?</p>

<p>I have read about the Hardy-Littlewood circle method in E. Grosswald's <em>Representations of Integers as Sums of Squares</em>, but it seems to give a much more precise formula than what I need, and I feel there's a chance it might be simpler than that.</p>
",<number-theory>
"<p>Let $E=\mathbb{C}/\Lambda$ be a complex elliptic curve where $\Lambda=\omega_1\mathbb{Z}\oplus\omega_2\mathbb{Z}$</p>

<p>Let $f$ be a nonconstant elliptic function with respect to $\Lambda$.</p>

<p>Let $P=\{x_1\omega_1+x_2\omega_2:x_1,x_2\in[0,1]\}$ be the parallelogram with opposing boundary identified appropriately.</p>

<p>There exists some constant $t$ such that $f$ has no zeros and poles on $t+\partial P$.</p>

<p>Now I want to compute the integral
$$
\frac{1}{2\pi i}\int_{t+\partial P}\frac{zf'(z)}{f(z)}d{z}
$$</p>

<p>I need to show that it is 
$$
\sum_{x\in E}\nu_x(f)x
$$
where $\nu_x(f)$ is the order of $f$ at $x$, which means $f(z)=(z-x)^{\nu_x(f)}g(z)$ where $g(x)\neq 0$.</p>

<p>I can find that it is 
$$
\omega_1\int_{t}^{t+\omega_2}\frac{f'(z)}{f(z)}dz-\omega_2\int_{t}^{t+\omega_1}\frac{f'(z)}{f(z)}dz
$$</p>

<p>and each integral is an integer because $f$ is the same at the two endpoints. But I cannot get the desired formula.</p>

<p>Any help?</p>
",<number-theory>
"<p>Let $f$ be a meromorphic function on the region $Im(z)&gt;0$, $v_p(f)$ be the order of $p$. (The number $n$ such that $\frac{f(z)}{(z-p)^n}$ is holomorphic and non-zero at $p$.)</p>

<p>Moreover, assume $f$ is a $modular$ $function$ for $SL(\mathbb Z)$ of weight $k$.</p>

<p>Then we have the following application of $residue$ $theorem$.</p>

<p><img src=""http://i.stack.imgur.com/Lfqzo.png"" alt=""enter image description here""></p>

<p>Why is the last limitation true? The argument principle only tell us the integral over a circle.</p>
",<number-theory>
"<p>Similar to the cfracs in <a href=""http://math.stackexchange.com/questions/1778344/two-complementary-continued-fractions-that-are-algebraic-numbers"">this post</a>, define the two complementary continued fractions,</p>

<p>$$x=\cfrac{-(m+1)}{km\color{blue}+\cfrac{(-1)(2m+1)} {3km\color{blue}+\cfrac{(m-1)(3m+1)}{5km\color{blue} +\cfrac{(2m-1)(4m+1)}{7km\color{blue}+\cfrac{(3m-1)(5m+1)}{9km\color{blue}+\ddots}}}}}\tag1$$</p>

<p>$$y=\cfrac{-(m+1)}{km\color{red}-\cfrac{(-1)(2m+1)} {3km\color{red}-\cfrac{(m-1)(3m+1)}{5km\color{red}-\cfrac{(2m-1)(4m+1)}{7km\color{red}-\cfrac{(3m-1)(5m+1)}{9km\color{red}-\ddots}}}}}\tag2$$</p>

<p>The first one is the superfamily which contains Nicco's cfracs in <a href=""http://math.stackexchange.com/questions/1775471/a-continued-fraction-related-to-pythagoras-theorem-a2b2-c2"">another post</a>. Let $i$ be the <em>imaginary unit</em>. For $k&gt;1$ and $m&gt;1$, it can be empirically observed that $x$ obeys,</p>

<p>$$\left(\frac{(x+i)^m-(x-i)^m}{(x+i)^m+(x-i)^m}\right) \color{blue}{\left(\frac{(k+i)^{m+1}+(k-i)^{m+1}}{(k+i)^{m+1}-(k-i)^{m+1}}\right)^{(-1)^m}}=1\tag3$$</p>

<p>while $y$ obeys,</p>

<p>$$\left(\frac{(y+1)^m+(y-1)^m}{(y+1)^m-(y-1)^m}\right) \color{blue}{\left(\frac{(k+1)^{m+1}+(k-1)^{m+1}}{(k+1)^{m+1}-(k-1)^{m+1}}\right)^{(-1)^{m+1}}}=-1\tag4$$</p>

<p>where the colored part is a constant that depends on the choice of $k,m$. Hence, as shown in <a href=""http://math.stackexchange.com/questions/1581041/how-to-solve-in-radicals-this-family-of-equations-for-any-degree-k"">this post</a>, $x,y$ are <em>radicals</em> and algebraic numbers of degree $m$.</p>

<blockquote>
  <p><strong>Question:</strong> <em>How do we prove that $(3)$ and $(4)$ are indeed true?</em></p>
</blockquote>

<p><strong>P.S.</strong> Since,</p>

<p>$$\left(\frac{(z+i)^m+(z-i)^m}{2}\right)^2+i^2\left(\frac{(z+i)^m-(z-i)^m}{2}\right)^2 = (z^2+1)^m$$</p>

<p>then the structure of $(3)$ explains the observations about $a^2+b^2=c^m$ in Nicco's post.</p>
",<number-theory>
"<p>Is there a way to find 2 sqare numbers with a certain distance without trying every square number?</p>

<p>Example:</p>

<p>$$
a^2 + 204 = b^2
$$</p>
",<number-theory>
"<p>Let $\Lambda$,$\Lambda'$ be two complex lattices and $m\neq 0\in\mathbb{C}$ satisfying $m\Lambda\subset\Lambda'$. Suppose $\omega_1,\omega_2$ are the basis of $\Lambda$, $\omega'_1,\omega'_2$ are the basis of $\Lambda'$. So we have</p>

<p>$$
\begin{bmatrix}m\omega_1\\m\omega_2\end{bmatrix}=\alpha\begin{bmatrix}\omega_1'\\\omega'_2\end{bmatrix}\text{ for some }\alpha\in M_2(\mathbb{Z})
$$</p>

<p>Then it is said that $[\Lambda',m\Lambda]=\det\alpha$.</p>

<p>Why?</p>
",<number-theory>
"<p>As in, why does the iteration of the function until $g_{64}$ guarantee this property that defines Graham's number? Why was this number chosen?</p>

<p>If I had to guess (emphasis on guess), I'd say that the Ramsey theoretical problem involving  Graham's number involves ${4 \choose 2} = 6$ line segments between four points and two ways to color each, and $2^6 = 64$. But I don't know at all.</p>
",<number-theory>
"<p>Is it guaranteed that there will be some $p$ such that  $p\mid2^n-1$ but $p\nmid 2^m-1$ for any $m&lt;n$?</p>

<p>In other words, does each $2^x-1$ introduce a <em>new</em> prime factor?</p>
",<number-theory>
"<p>The following example is drawn from Milne's Galois Theory notes, p.42 (<a href=""http://www.jmilne.org/math/CourseNotes/FT.pdf"" rel=""nofollow"">http://www.jmilne.org/math/CourseNotes/FT.pdf</a>)</p>

<p>We study the extension $\mathbb{Q}[\zeta]/\mathbb{Q}$ where $\zeta=e^{2\pi i/7}.$</p>

<p>We find that $\mathbb{Q}[\zeta]$ is the splitting field for the minimal polynomial $x^7-1,$ and that it is a degree 6 Galois extension over $Q.$</p>

<p>We let $\sigma$ be the element of $\text{Gal}(\mathbb{Q}[\zeta]/\mathbb{Q}) = (\mathbb{Z}/7\mathbb{Z})^{\times}$ such that $\sigma \zeta= \zeta^3.$ Note that $\sigma$ is a generator for $\text{Gal}(\mathbb{Q}[\zeta]/\mathbb{Q}).$</p>

<p>We now ask: what is the subfield $S$ of $\mathbb{Q}[\zeta]$ which corresponds to the order 2 subgroup $&lt;\sigma^3&gt;.$ </p>

<p>We note that $\sigma^3 \zeta=\zeta^6= \overline{\zeta}.$ So in particular, $\zeta+ \overline{\zeta}$ is fixed by $\sigma^3$ and so $\mathbb{Q}[\zeta+\overline{\zeta}] \subset S.$ </p>

<p>Milne claims that we also have $S \subset \mathbb{Q}[\zeta+\overline{\zeta}].$ While this seems reasonable, is there a systematic way to see this? </p>
",<number-theory>
"<p>I would like to pose the following conjecture.Given</p>

<p>$$\phi(q) =\cfrac{1}{1-q+\cfrac{q(1-q)^2}{1-q^3+\cfrac{q^3(1-q^2)^2}{1-q^5+\cfrac{q^5(1-q^3)^2}{1-q^7+\ddots}}}}$$ </p>

<p>and</p>

<p>$$\psi(q)=\cfrac{-q}{1-q+\cfrac{q(1-q)^2}{1-q^3+\cfrac{q(1-q^2)^2}{1-q^5+\cfrac{q(1-q^3)^2}{1-q^7+\ddots}}}}$$ </p>

<p>Then prove/disprove that the two continued fractions are equivalent</p>

<p>$$\phi(q)=\psi\left(\frac{1}{q}\right)$$</p>
",<number-theory>
"<p>Let $a,b$ be positive integers such that $a\mid b^2 , b^2\mid a^3 , a^3\mid b^4 \ldots$ that is $a^{2n-1}\mid b^{2n} ; b^{2n}\mid a^{2n+1} , \forall n \in \mathbb Z^+$ , then is it true that $a=b$ ?</p>
",<number-theory>
"<p>Prove that a number in the sequence $2,3,4,...,n \ (n>2$, is relatively prime to all other numbers if and only if it is a prime that exceeds $\displaystyle\frac{n}{2}$. Does such a prime always exist?</p>
",<number-theory>
"<p>Everyone knows that $\pi$ is an irrational number, and one can refer to this <a href=""http://planetmath.org/encyclopedia/PiAndPi2AreIrrational.html"" rel=""nofollow"">page</a> for the proof that $\pi^{2}$ is also irrational.</p>

<p>What about the highers powers of $\pi$, meaning is $\pi^{n}$ irrational for all $n \in \mathbb{N}$ or does there exists a $m \in \mathbb{N}$ when $\pi^{m}$ is rational.</p>
",<number-theory>
"<blockquote>
  <p>Given integers $m$, $c$ and $n$. Find $m$ such that $m^2 \equiv c \ (\mod n) $</p>
</blockquote>

<p>I used <a href=""https://en.wikipedia.org/wiki/Tonelli%E2%80%93Shanks_algorithm"" rel=""nofollow"">Tonelli-Shanks algorithm</a> to caculate the square root, but in my case $n$ is not a prime number, $n = p^2,\ p$ is a prime number. </p>

<p>I read <a href=""http://www.mersennewiki.org/index.php/Modular_Square_Root"" rel=""nofollow"">this page</a>. It is said that:</p>

<blockquote>
  <p>In this article we will consider the case when the modulus is prime.
  Otherwise we can compute the square roots modulo the prime factors of
  $p$ and then generate a solution using the <a href=""https://en.wikipedia.org/wiki/Chinese_remainder_theorem"" rel=""nofollow"">Chinese Remainder Theorem</a>.</p>
</blockquote>

<p>Using Tonelli-Shanks algorithm again, I found $m_p$ such that $m_p^2 \equiv {c} \ (\mod p)$.</p>

<blockquote>
  <p>I'm stuck with finding $m$ from $m_p$. That page above does not tell me clearly about how to solve that case. Please help me !</p>
</blockquote>
",<number-theory>
"<p>Let $V$ a finite dimensional vector space over $\mathbb{Q}$ and let $F$ be a number field. Assume that there is an injective morphism of rings $F \hookrightarrow End(V)$. I would like to understand why $V_\mathbb{C}:=V \otimes_{\mathbb{Q}} \mathbb{C}$ decomposes into a direct sum </p>

<p>$$
V_\mathbb{C}=\bigoplus_{\sigma \in Hom(F, \mathbb{C})} V_\sigma 
$$ </p>

<p>according to complex embeddings of $F$. My idea is the following: if $F=\mathbb{Q}(\alpha)$ then the endomorphism $\alpha \in End(V)$ has the same minimal polynomial as $\alpha$ viewed as an element of $F$. Its roots are $\{\sigma(\alpha)\}_{\sigma \in Hom(F, \mathbb{C})}$ so one can define </p>

<p>$$
V_\sigma=\{ x \in V_\mathbb{C}| \alpha \cdot x=\sigma(\alpha)x\} 
$$</p>

<p>1) Why this is independent of $\alpha$?</p>

<p>2) Is there a more conceptual way to think of it? </p>
",<number-theory>
"<p>I used similar technique as Fourier's proof of irrationality of $e$ <a href=""https://en.wikipedia.org/wiki/Proof_that_e_is_irrational"" rel=""nofollow"">https://en.wikipedia.org/wiki/Proof_that_e_is_irrational</a> to show that this series is indeed an irrational number but I was wondering if there are other elementary proofs, specially techniques related to polynomials with rational coefficients. Any hint, complete proof, reference, etc. is much appreciated. </p>

<p>EDIT : presenting my proof as @AndréNicolas requested : I just followed the same steps; let's assume the series is $s$. Now choose $k_0$ such that $m^{k_0^2} &gt; q$. Define $x = q m^{k_0^2} \left(s - \sum_{k=1}^{k_0}{\frac{1}{m^{k^2}}} \right)$. Now if $s$ is not irrational then $s=\frac{p}{q}$ for some non-negative $p$ and $q$. Then it is easy to check that $x$ is an integer. Now we need to reach to a contradiction by proving that $x &lt; 1$. 
After simple manipulations, $x = q m^{k_0^2} \sum_{k=k_0+1}^{k_0}{\frac{1}{m^{k^2}}}$
Now, for $k=k_0+1,k_0+2, \ldots$, we have $k^2 &gt; k+k_0^2$ so
$$ q m^{k_0^2} \sum_{k=k_0+1}^{k_0}{\frac{1}{m^{k^2}}} &lt; q m^{k_0^2} \sum_{k=k_0+1}^{k_0}{\frac{1}{m^{k+k_0^2}}} = \frac{q}{m^{k_0^2+1}}\frac{1}{1-\frac{1}{m}}=\frac{1}{m-1}$$ which follows from our assumption about $q$ and $k_0$.</p>
",<number-theory>
"<p>When I test for small values of k, it seems that the order of $5+2^k\mathbf{Z}$ in $(\mathbf{Z}/2^k\mathbf{Z})^*$ is $2^{k-2}$. How do I prove this?</p>
",<number-theory>
"<p>Is there any recent research into the <a href=""http://en.wikipedia.org/wiki/Sprague%E2%80%93Grundy_theorem"" rel=""nofollow"">Sprague-Grundy values</a> of <a href=""http://en.wikipedia.org/wiki/Grundy%27s_game"" rel=""nofollow"">Grundy's game</a>?</p>

<p>It was calculated to $2^{35}$ integers but with no sight of recurrence.</p>

<p>Has anyone come up with anything new to compute a SG formula for the game?</p>
",<number-theory>
"<p><code>Legendre</code>, <code>Jacobi</code> and <code>Kronecker</code> Symbols are powerful multiplicative functions in computational number theory. They are useful mathematical tools, essentially for <a href=""https://en.wikipedia.org/wiki/Primality_test"" rel=""nofollow"">primality testing</a> and <a href=""https://en.wikipedia.org/wiki/Integer_factorization"" rel=""nofollow"">integer factorization</a>; these in turn are important in cryptography.</p>

<p>Yet, it would be nice to have a discussion here on their use in classical number theory and math problems. In other words, what kind of problems can they be effectively applied to? Or simply, when/where to use them?</p>

<p><strong>Definitions</strong></p>

<p>For any integer $a$ and any positive odd integer $n$ the <code>Jacobi</code> symbol is defined as the product of the <code>Legendre</code> symbols corresponding to the prime factors of $n$:</p>

<p>$\left(\tfrac{a}{p}\right)$ represents the <code>Legendre</code> symbol, defined for all integers $a$ and all odd primes $p$ by
$$\Bigg(\frac{a}{n}\Bigg) = \left(\frac{a}{p_1}\right)^{\alpha_1}\left(\frac{a}{p_2}\right)^{\alpha_2}\cdots \left(\frac{a}{p_k}\right)^{\alpha_k}\mbox{ where } n=p_1^{\alpha_1}p_2^{\alpha_2}\cdots p_k^{\alpha_k}.$$
$$
\left(\frac{a}{p}\right) = \left\{
\begin{array}{rl}
0 &amp; \text{if } a \equiv 0 \pmod{p},\\
1 &amp; \text{if } a \not\equiv 0\pmod{p} \text{ and for some integer } x:\;a\equiv x^2\pmod{p},\\
-1 &amp; \text{if } a \not\equiv 0\pmod{p} \text{ and there is no such } x.
\end{array}
\right.
$$ 
Following the normal convention for the empty product, $\left(\tfrac{a}{1}\right) = 1$. The <code>Legendre</code> and <code>Jacobi</code> symbols are indistinguishable exactly when the lower argument is an odd prime, in which case they have the same value.</p>

<p>The <code>Kronecker</code> symbol, written as $\left(\frac an\right)$ or $(a|n)$, is an extension of the <code>Jacobi</code> symbol to all integers.</p>

<p>Let $n$ be a non-zero integer, with prime factorization </p>

<p>$$n=u \cdot p_1^{e_1} \cdots p_k^{e_k},$$</p>

<p>where $u$ is a unit (i.e., $u=\pm1$), and the $p_i$ are primes. Let $a$ be an integer. The <code>Kronecker</code> symbol $(a|n)$ is defined by </p>

<p>$$ \left(\frac{a}{n}\right) = \left(\frac{a}{u}\right) \prod_{i=1}^k \left(\frac{a}{p_i}\right)^{e_i}. $$</p>

<p>For odd number $p_i$, the number $(a|p_i)$ is simply the usual <code>Legendre</code> symbol. This leaves the case when $p_i=2$. We define $(a|2)$ by </p>

<p>$$ \left(\frac{a}{2}\right) = 
\begin{cases}
 0 &amp; \mbox{if }a\mbox{ is even,} \\
 1 &amp; \mbox{if } a \equiv \pm1 \pmod{8},  \\
-1 &amp; \mbox{if } a \equiv \pm3 \pmod{8}.
\end{cases}$$</p>

<p>Since it extends the <code>Jacobi</code> symbol, the quantity $(a|u)$ is simply $1$ when $u=1$. When $u=-1$, we define it by</p>

<p>$$ \left(\frac{a}{-1}\right) = \begin{cases} -1 &amp; \mbox{if }a &lt; 0, \\ 1 &amp; \mbox{if } a \ge 0. \end{cases} $$</p>

<p>Finally, we put</p>

<p>$$\left(\frac a0\right)=\begin{cases}1&amp;\text{if }a=\pm1,\\0&amp;\text{otherwise.}\end{cases}$$</p>

<p>These extensions suffice to define the <code>Kronecker</code> symbol for all integer values $a,n$.</p>
",<number-theory>
"<p>Let $\psi(x) := \sum_{n\leq x} \Lambda(n)$ where $\Lambda(n)$ is the Von-Mangoldt function.
I want to show that if $$ \lim_{x \rightarrow \infty} \frac{\psi(x)}{x} =1 $$ then also $$\lim_{x\rightarrow \infty} \frac{\pi(x) \log x }{x}=1.$$</p>

<p>I tried to play a little bit with $\psi$, what I want to show is that:</p>

<p>$$\left| \frac{\pi(x) \log x}{x} -1 \right| \leq \left| \frac{\psi(x)}{x} -1 \right| \rightarrow 0$$</p>

<p>So I tried to develop $\psi$ a little bit, but I got astray.</p>

<p>So I have 
$$ \frac{\psi(x)}{x} -1 = \sum_{p^k \leq x , k \geq 1} \frac{\log p}{x} -1 = \frac{1}{x}\left(\sum_{p\leq x} \log p + \sum_{p^2\leq x} \log p + ...+ \sum_{p^k \leq x, p^{k+1} &gt;x} \log p \right) -1 $$
and I want to estimate its aboslute value from below, but I don't have any idea?</p>

<p>Any hints?</p>

<p>Thanks.</p>
",<number-theory>
"<p>I am working with the solutions in integers of another equation, and I have arrived to the problem of find all solutions $(x,t)$ of $x^2 +45x= t^2 $, but I don´t know how to solve this kind of equations.
What part of number theory does apply here?</p>
",<number-theory>
"<p>Show that </p>

<p>$$\sum_{n=1}^{\infty}\frac{\sinh\big(\pi n\sqrt2\big)-\sin\big(\pi n\sqrt2\big)}{n^3\Big({\cosh\big(\pi n\sqrt2}\big)-\cos\big(\pi n\sqrt2\big)\Big)}=\frac{\pi^3}{18\sqrt2}$$</p>

<p>I have no hint as to how to even start.</p>
",<number-theory>
"<p>Let $p_N/q_N$ be the $N^\text{th}$ convergent of the continued fraction for some irrational number $\alpha$. It turns out that for any other approximation $p/q$ (with $q \le q_N$) which isn't a convergent $|\alpha q - p| > |\alpha q_{N-1} - p_{N-1}|$. I'm wondering if there are any nice proofs for this result?</p>

<hr>

<p>In my book this is proved by picking $x,y$ that solves</p>

<p>$$
\begin{pmatrix} p_N &amp; p_{N-1} \\ q_N &amp; q_{N-1} \end{pmatrix}
\begin{pmatrix} x \\ y \end{pmatrix} =
\begin{pmatrix} p \\ q \end{pmatrix}
$$</p>

<p>since $x$ and $y$ have opposite sign, as well as $\alpha q_N - p_N$ and $\alpha q_{N-1} - p_{N-1}$ have opposite sign we can conclude that $|\alpha q - p| = |x (\alpha q_N - p_N) + y (\alpha q_{N-1} - p_{N-1})| = |x| |\alpha q_N - p_N| + |y| |\alpha q_{N-1} - p_{N-1}|$ which proves the theorem.</p>

<p>I am looking for different proofs than this one.</p>
",<number-theory>
"<p>The Riemann zeta function, is the function of the complex variable $s$, defined in the half plane $\Re(s)&gt;1$ by the absolutely convergent series $\zeta(s) = \sum_{n} n^{-s}$</p>

<p>and extends to the whole of $\mathbb{C}$ by analytic continuation. It is known that all the complex zeros of $\zeta(s)$ satisfy $0&lt;\Re(s) &lt;1$.</p>

<p>But am not sure how on they are calculated ?</p>
",<number-theory>
"<p>Given,</p>

<blockquote>
  <p>$$aX + bY = c$$</p>
</blockquote>

<p>where,</p>

<blockquote>
  <p>$$c &gt; b &gt; a &gt; 0;\quad X, Y &gt; 0;\quad b\nmid c, a\nmid c$$</p>
</blockquote>

<p>I want to find out if a solution exists as efficiently as possible (I'm not interested in the solutions). Are there any calculations I can make before (or without the need for) finding $\operatorname{gcd}(a, b)$ that can possibly save some time (even if for only few special cases)? $c, b, a$  can be very large numbers.</p>

<p>""Probably not"" still counts as answer for me. You don't have to be 100% certain. I just want to make sure I'm not missing something that's very obvious. </p>

<p>P.S.,
English is not my first language.</p>
",<number-theory>
"<p>How many positive factorials are also perfect Squares. So for example $1!=1=1^2$. How many others exist other than 1?</p>

<p>Is there any way to prove this?</p>
",<number-theory>
"<p>Is it true that $\sin(n^k) ≠ (\sin n)^k$ for any positive integers $n$ and integers $k ≠ 1$?</p>

<p>What if $n &gt; 0, k ≠ 1$ are rational?</p>
",<number-theory>
"<p>Proth number is a number of the form :</p>

<p>$z⋅2^k+1$
where z is an odd positive integer and k is a positive integer such that : $2^k&gt;z$</p>

<p>Is there a form for divisors of Proth Numbers? (Like Mersenne and Fermat Numbers have specific forms of divisors) 
My search did not turn any results.</p>

<p>Thank you...</p>
",<number-theory>
"<p>Let $N={abc}$ be Three Digit Number such that $$abc+bca+bac+cab+cba=3194$$ Find the Number</p>

<p>My Try: I added both sides the left over number $acb$ both sides Then we get</p>

<p>$$222(a+b+c)-3194=b+10c+100a$$</p>

<p>Help needed from here</p>
",<number-theory>
"<p>I'm stuck on a line in the proof of Bombieri implies Linnik, where</p>

<blockquote>
  <p><strong>Bombieri</strong>: 
  For primitive $\chi$ mod $q$ with $q \leq T$ we define $$N(\alpha, T; \chi)=\#\{\rho=\beta+i\gamma \;:\; \Lambda(\rho,\chi)=0, \beta&gt;\alpha, |\gamma|&lt;T\}.$$</p>
  
  <p>There exists $c&gt;0$ such that, for all $T \geq 2$ and all $1/2 \leq \alpha &lt;1$, $$\sum_{q \leq T}\sum_{\chi \text{ mod }q}^*N(\alpha,T;\chi)=O(T^{c(1-\alpha)})$$</p>
  
  <p>where the asterisk on the sum over characters indicates that only primitive characters are to be inlcuded. </p>
  
  <p>If there exists a real primitive character $\chi_1$ mod $q_1$ with $q_1&lt;T$, such that $\Lambda(s,\chi_1)$ has zero $\beta_1&gt;1-\frac{\delta}{\log T}$, then we have the improved estimate</p>
  
  <p>$$\sum_{q \leq T}\sum_{\chi \text{ mod }q}^*N'(\alpha,T;\chi)=O(T^{c(1-\alpha)}(1-\beta_1)\log T)$$</p>
  
  <p>where the ' on the $N(\alpha,T;\chi)$ indicates that the single exceptional zero is to be excluded.</p>
  
  <p><strong>Linnik</strong>: If $a$ and $q$ are coprime then there exists a prime $p \equiv a$ mod $q$ with $p&lt;O(q^{O(1)})$.</p>
</blockquote>

<p>In the exceptional zero case, assuming Bombieri, I've got to the point where</p>

<p>$$\sum_{p \leq x, p \equiv a \text{ mod }q }\log p \geq \frac{1}{2 \phi(q)}\left(x-\frac{x^{\beta_1}}{\beta_1}\right)+O\left(\frac{x(\log x)^2}{T}\right)$$</p>

<p>for $T=q^A$ with $A&gt;3$ and $x=T^B$ with $B&gt;12$ a large constant. I also know that $$\beta_1\leq 1-\frac{c}{\sqrt{T}(\log T)^2}.$$</p>

<p>I think what I want to show is that $x^{\beta_1}\leq x^{1-\epsilon}$ for some $\epsilon&gt;0$, because since $(O(x(\log x)^2/T)=O(x^{1-\epsilon})$ for small enough $\epsilon$, Linnik's theorem will follow. But just throwing in the bound for $\beta_1$ doesn't seem to give me this.</p>
",<number-theory>
"<p>I looking to find a measure of the following set:
$A=\{ (a_1,a_2,...a_k) \in \mathbb{R}^k : |1+a_1z_1+a_2z_2+...+a_kz_k|&lt;\delta \}$
and where $z_i \in \mathbb{Z}$.
I believe the measure of this set should be $\mu(A) \le c\delta$  where $c$ is some constant.</p>

<p>Can any one suggest any thing? I also, would like to know what branch mathematics deals with a problems like this.
Thank you in advance.  </p>
",<number-theory>
"<p>I want to show that, given an ideal $I \subseteq \mathcal O_K$ (where $K/\mathbb Q$ is an algebraic number field), there is a finite extension $K'/K$ such that,
$I\mathcal O_{K'}$ becomes a principal ideal in $\mathcal O_{K'}$.</p>

<p>The example solution for this problem uses $K' = K[X]/(X^h - x)$ where $(x) = I^h$
and $h$ is the class number for the field $K$.</p>

<p>I see why $I\mathcal O_{K'}$ is a principal ideal, however what I fail to see is why $K'$ is a field. I know that $K'$ being a field is equivalent to $(X^h - x)$ being a maximal ideal, and this in turn is equivalent to $X^h - x$ being irreducible. Why is this the case?</p>
",<number-theory>
"<p>I am trying to solve an equation in integers to give a square number.</p>

<p>$$2 x^2 + 3 x +1 = y^2$$</p>

<p>while also satisfying $x=k^2 * n$ where $n$ is a very large integer given to us and $k$ can be any integer chosen to form a solution. The $y$ can be any integer needed to form a solution.</p>

<p>I am looking for a method to find solutions in integers only.</p>

<p>I am new to this kind of equation and can't tell easy from hard from impossible.
This is not school work.  </p>

<p>Thanks for your help.</p>
",<number-theory>
"<p>What function when given the inputs $x, y$ returns the given $z$? </p>

<ul>
<li>When $x = 2, y = 10$, $z = 1$</li>
<li>When $x = 6, y = 10$, $z = 2$</li>
<li>When $x = 50, y = 70$, $z = 5$</li>
<li>When $x = 16, y = 17$, $z = 1$</li>
<li>When $x = 1, y = 3$, $z = 0$</li>
<li>When $x = 20, y = 30$, $z = 3$</li>
<li>When $x = 100, y = 140$, $z = 9$</li>
</ul>

<p>I'm trying to find if there is a relation between these examples.</p>
",<number-theory>
"<p>Assume $a$ and $b$ are natural numbers, $A=\{a,a^2,a^3,\cdots\}$ and $B=\{b,b^2,b^3,\cdots\}$, find $\min\left|a_i-b_j\right|$ where $a_i\in A$ and $b_j\in B$. </p>

<p>For example, if $a=3$, $b=10$, then $\min\left|a_i-b_j\right|=\left|3^2-10^1\right|=1$. </p>

<p>Variation: what if $a$ and $b$ are both prime numbers? </p>
",<number-theory>
"<p>I am trying to prove the local Kronecker-Weber theorem for tamely ramified abelian extensions $L|\mathbb{Q}_p$. At some point in the proof I need to show that $\mathbb{Q}_p(u^{1/e})$ is unramified under the assumptions that $u$ is a unit (possibly in some extension field) and $(e,p)=1$. Supposedly the following argument works: Since $e$ and $p$ are coprime and $u$ is a unit, the discriminant of $x^e-u$ is not divisible by $p$ and hence the splitting field is unramified.</p>

<p>I can't see why the discriminant is not divisible by $p$. I have expanded the discriminant but keep running into an issue where the number of terms may be divisible by $p$,.</p>
",<number-theory>
"<blockquote>
  <p>Show that the norm of a prime ideal in a number field $K$ is a power of some prime number, i.e., if $P$ is a prime ideal in $O_K$ for some number field $K$, then $N_\mathbb{Q}^K(P)=p^n$ for some prime number $p$ and some positive integer $n$. </p>
</blockquote>

<p>Here is my approach:</p>

<p>Any prime ideal lies over some prime number $p$. If we consider the ideal decomposition of $pO_K$, and apply the norm operator, we get the following: </p>

<p>$pO_K=p_1^{e_1} \cdots p_r^{e_r}$ for some $r$ since $O_K$ is a Dedekind domain. Applying the norm operator to this, we get</p>

<p>$N(pO_K)=N(p_1^{e_1} \cdots p_r^{e_r}) = N(p_1^{e_1})\cdots N(p_r^{e_r})$ since the norm has the multiplicative property. </p>

<p>This is where I am unsure if I have completely answered the question because I found a list of primes as opposed to the suggested $p^n$ in the problem statement.</p>

<p>Thanks in advance, any help is greatly appreciated. </p>
",<number-theory>
"<p>John has been newly hired to clean tables at his restaurant. So whenever a customer wants a table, he must clean it.</p>

<p>But John happens to be a lazy boy. So in the morning, when the restaurant is opened, all the tables are dirty from night before.</p>

<p>The customer don't leave the table unless they are politely requested to do so. And customers can order meal later again. So if they were already having a table, they can be served on the same table [John doesn't have to clean]. But if they don't have a table then they must be given some table [John must clean]</p>

<p><strong>Problem :</strong> </p>

<p>The restaurant has N tables. When a customer requires a table, he/she can occupy any unoccupied table. However if all tables are occupied, then John is free to ask politely any customer to leave his/her table. And the freed table can be given to the waiting customer.</p>

<p><strong>Now Given the  list telling the order of customer meal requests for the entire day, 
we need to find find the minimum number of times he has to clean the tables.</strong></p>

<p><strong>Example 1 :</strong> Let their are 2 tables that mean N=2 and their are 4 order that means M=4 and list showing the  order in which customers ask for meal is [1 2 3 4]</p>

<p>Then here answer is 4 </p>

<p><strong>Explanation :</strong> In the starting all tables i.e. 2 tables are unoccupied. 
When customer 1 asks for table, he can be given any of the 2 tables. 
John has to clean either of the table. Next customer 2 can occupy the other free table. John must clean second time. When customer 3 comes John can ask either customer 1 or 2 to leave. Suppose he asks customer 1 to leave. Then he has to clean table for the third time. When customer 4 comes, he can ask customer 2 or 3 to leave. In either case He will have to clean the table for the fourth time.</p>

<p><strong>Example 2:</strong> Let N=3 and M=5 and list be [1 2 1 3 4]</p>

<p>Then here answer will be 4.</p>

<p><strong>Explantion :</strong> Suppose the tables are listed as [-, -, -]. A possible optimal order of allocating tables can be [1, -, -] -> [1, 2, -] -> [1, 2, -] -> [1, 2, 3] -> [4, 2, 3]. So John will have to clean table for order no. 1, 2, 4 and 5. That is 4 times.</p>
",<number-theory>
"<p>Can sieve method prove ternary (three) prime Goldbach conjecture (Vinogradov Theorem) ?</p>

<p>I had done some research, I could not find any articles on this.
Can anyone provide some help on this ?</p>

<p>I assume that sieve method will be prove ternary (three) prime Goldbach conjecture (Vinogradov Theorem), because it can prove (1, 2) Chen's Theorem.</p>
",<number-theory>
"<p>I have given expression $(X+X^a+X^b+X^c+X^d+....)^k$ where $a$, $b$, $c$ ... and $k$ are whole numbers.</p>

<p>How we can find the $Z$ coefficient, i.e, $X^Z$ of the above term $1\le Z \le k$?</p>

<p>For Example $(X+X^3+X^5+X^8)^4$. How can I find the $X^{13}$ coefficient.</p>
",<number-theory>
"<p>The problem concerns covering the unit square with translates of a specific figure, which I will refer to as a ""cross"", using as few translates as possible. The difficulty seems to result from the fact that the figure is very concave.</p>

<p>To begin with, let $T^2$ be the unit square $[-1/2,1/2]^2$ with the opposite sides identified. Also, fix a parameter $\varepsilon &gt; 0$. Let us consider a ""cross"" $K$ defined as:
$$ K = \{(x,y) \in T^2 \ : \ |xy| &lt; \varepsilon \} $$
For reasonably small $\varepsilon$ this looks like the coordinate axes ""thickened"" somewhat. We are interested in the translates of $K$, with the convention that if a part of the translate $K$ ""sticks out"" of $T^2$ it gets ""wrapped around"" on the opposite side; thus $(x,y) \in K + (a,b)$ iff $(x,y) \equiv (x'+a,y'+b) \pmod{1}$ for some $(x',y') \in K$. The task is now to cover $T^2$ using as few crosses as possible. It is fairly obvious that the exact number cannot be found, so I am only asking for the asymptotics of this number for sufficiently small $\varepsilon$. </p>

<p>The obvious attempt is to find a suitable rectangle contained in a cross, and use these rectangles to cover the square. One can easily find rectangles of area $\Theta(\varepsilon)$ (i.e. $C \varepsilon$ for a universal constant), so one can cover the square with $\Theta(1/\varepsilon)$ crosses. The area of the largest convex figure that fits into a cross is $\Theta(\varepsilon)$, so this cannot be significantly improved without new insight.</p>

<p>On the other hand, the area of a cross is $\Theta(\varepsilon \log \frac{1}{\varepsilon})$. Thus, one needs at least $\Theta(1/\varepsilon \log \frac{1}{\varepsilon})$ crosses for a cover.</p>

<p>Unfortunately, the two bounds do not agree. Ideally, I would like to know (asymptotically) what's the least cardinality of a cover. More realistically, I would appreciate any argument showing that either of the bounds can be improved.</p>

<p><em>Motivation: The problem came up when I was considering recurrence rates of generalised polynomials. It is related to asking for an upper bound on the least positive integer $n$ so that $\left&lt; n \alpha \right&gt; \left&lt;n \beta \right&gt; \in (-\varepsilon,\varepsilon)$, where the brackets indicate the fractional part.</em></p>
",<number-theory>
"<p>Let $a, m$ be positive integers and $m &gt; 1$. I'm interested in the sequence $(a^k)_{k 
\in \mathbb{N_0}} \mod m$. Since there are only $m$ different values that can occur in the sequence and since $a^k = a \cdot a^{k-1}\ \ \forall k &gt; 0$ is only dependent on the previous element, I conclude that there exist $i, C \leq m$, such that $a^{k + C} = a^k\ \ \forall k \geq i$.</p>

<p>Example: $a = 2, m = 12$, we get the sequence $1, 2, 4, 8, 4, 8, 4, 8... \mod 12$ with $x = 2, C = 2$.</p>

<p>Given $a$ and $m$, I want to algorithmically find some $i$ and $C$ with the above property. I'm especially interested in the case where $gcd(a,m) \neq 1$, since otherwise a trivial solution is $i = 0, C = \phi(m)$ according to <a href=""http://en.wikipedia.org/wiki/Euler%27s_theorem"" rel=""nofollow"">Euler's theorem</a>, if I'm not mistaken. The obvious algorithm is naive in that it just evaluates the sequence element by element and stops as soon as it hits a duplicate. I'm sure we can do better than $O(m)$ exponentiations by using the Chinese Remainder Theorem and factorizing $m$ or something, but since I don't have a math background it's a bit hard for me to put my finger on it.</p>

<p>The ultimate goal is to evaluate ""exponential-tower""-type expressions of the form ${a_1}^{{a_2}^{a_3^{a_4^{\ldots}}}}$ modulo some prime $p$, as asked in <a href=""http://stackoverflow.com/questions/21367824/how-to-compute-an-exponential-tower-modulo-a-prime/21368784#21368784"">an algorithm question on Stack Overflow</a> I tried to contribute my two cents there by applying some observations, but I'm personally interested in this problem and pretty sure the algorithm can be improved if we can solve the particular problem of finding the cycle length faster.</p>

<p>If somebody has another general idea of solving the exponential-tower problem, that's very interesting as well and I'd love to hear it, but it's not the primary point of this question :)</p>
",<number-theory>
"<p>I am not sure if it's possible to get infinite prime numbers from this sum:
$$p=k^j+j^k$$
with $j\in\mathbb{N}, k\in\mathbb{N}$
I tried for $j=1,2,...9,k=1,2,...9$ and I get only eleven prime numbers.
If I consider the matrix:
$$A(k,j)=k^j+j^k$$
in which the components $A(j,k)=1$ iff $p$ is prime
this is a sparse matrix in which the prime numbers are mostly in the first four rows. Can someone give me some hint to prove $A$ contains infinite prime numbers in the limit $j\to \infty$, $k\to\infty$</p>
",<number-theory>
"<p><strong>Question</strong>- A lattice point $(x,y)\in\mathbb{Z}^2$ is called <em>visible</em> if $gcd(x,y)=1$. Prove that given a positive integer $n$, there exists a lattice point $(a,b)$ whose distance from every <em>visible</em> point is greater than $n$.</p>

<p>I am totally nowhere near progress on this. I was thinking of trying pigeon hole principle, but cant find any appropriate candidate pigeons. Please give any hints to start.</p>
",<number-theory>
"<p>I am reading the article <a href=""http://ac.els-cdn.com/S0022314X06002381/1-s2.0-S0022314X06002381-main.pdf?_tid=58e1dbf6-6492-11e6-964f-00000aacb35d&amp;acdnat=1471449220_0d78b1e8734f32b6dfb2fcea25dc7313"" rel=""nofollow"">On $L^{\infty}$ norms of holomorphic cusp forms</a>. I am particularly interested to the following :
<img src=""http://i.stack.imgur.com/ra62s.png"" alt=""Image""> 
I don't see how we can get from
$$ |y^k f(z)|     \ll \frac{y^k k^\epsilon(4\pi)^{k-1/2}}{\sqrt{\Gamma(2k)}(2\pi y)^{k-1/2}}\sum_{n\ge 1}(2\pi ny)^{k-1/2+\epsilon}e^{-2\pi ny}$$
that  : $|y^kf(z)|\ll k^{{1/4}+\epsilon}$ for $y\gg k.$</p>

<p>Can someone clarify to me it ?</p>
",<number-theory>
"<p>I'm trying solve: $~a^3 + b^3 = c^3~$ has no nonzero integer solutions.<br>
If $~(c−b)=1~$ then $~c^3-b^3=3c^2-3c+1=a^3,~$</p>

<p>from  <a href=""http://www.wolframalpha.com/"" rel=""nofollow"">Wolframalpha</a> get:</p>

<p>$$
c = \dfrac{3- \sqrt{3}\sqrt{4a^{3}-1}}{6} \\
c = \dfrac{\sqrt{3}\sqrt{4a^{3}-1}+3}{6}
$$
How to prove $~\sqrt{3}\sqrt{4a^{3}-1}~$ isn't an integer? (eidt: while $~a,\ b,\ c ~$ are nonzero integers)</p>
",<number-theory>
"<p>Is it true that for $n \in \mathbb{N}$ we can have $4n = x^{2} + y^{2}$ or $4n = x^{2} - y^{2}$ for $x,y \in \mathbb{N} \cup (0)$.</p>

<p>I was just working out a proof and this turns out to be true from $n=1$ to $n=20$. After that I didn't try, but I would like to see if a counter example exists for a greater value of $n$.</p>
",<number-theory>
"<p>solve $x^3 + 2y^3 + 3z^3 = 4xyz$ for $x,y,z \in \mathbb{Q}$</p>

<p>My main question is how to solve this in $\mathbb{Q}$, i know how to solve these kind of problems in $\mathbb{Z}$, where i usually look at modulo small primes, such as 3,5. But with $\mathbb{Q}$ i get totally confused, any smart methods i can use solving this?</p>

<p>Kees</p>
",<number-theory>
"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://math.stackexchange.com/questions/169797/write-down-the-sum-of-sum-of-sum-of-digits-of-44444444"">Write down the sum of sum of  sum of digits of $4444^{4444}$</a>  </p>
</blockquote>



<p>If we write $ 4444^{4444}$ in decimal form .Then the sum of digits is equal to :$A$ .If we assume that: $B$ is the sum of digits :$A$ .How to find the sum of digits :$B$</p>
",<number-theory>
"<p>This is a pet idea of mine which I thought I'd share.  Fix a prime $q$ congruent to $1 \bmod 4$ and define a sequence $F_n$ by $F_0 = 0, F_1 = 1$, and</p>

<p>$\displaystyle F_{n+2} = F_{n+1} + \frac{q-1}{4} F_n.$</p>

<p>Then $F_n = \frac{\alpha^n - \beta^n}{\alpha - \beta}$ where $\alpha, \beta$ are the two roots of $f(x) = x^2 - x - \frac{q-1}{4}$.  When $q = 5$ we recover the ordinary Fibonacci numbers.  The discriminant of $f(x)$ is $q$, so it splits $\bmod p$ if and only if $q$ is a quadratic residue $\bmod p$.  </p>

<p>If $\left( \frac{q}{p} \right) = -1$, then the Frobenius morphism $x \mapsto x^p$ swaps $\alpha$ and $\beta$ (working over $\mathbb{F}_p$), hence $F_p \equiv -1 \bmod p$.  And if $\left( \frac{q}{p} \right) = 1$, then the Frobenius morphism fixes $\alpha$ and $\beta$, hence $F_p \equiv 1 \bmod p$.  In other words,</p>

<p>$\displaystyle F_p \equiv \left( \frac{q}{p} \right) \bmod p.$</p>

<p>Quadratic reciprocity in this case is equivalent to the statement that</p>

<p>$\displaystyle F_p \equiv \left( \frac{p}{q} \right) \bmod p.$</p>

<p><strong>Question:</strong>  Does anyone have any ideas about how to prove this directly, thereby proving quadratic reciprocity in the case that $q \equiv 1 \bmod 4$?</p>

<p>My pet approach is to think of $F_p$ as counting the number of ways to tile a row of length $p-1$ by tiles of size $1$ and $2$, where there is one type of tile of size $1$ and $\frac{q-1}{4}$ types of tiles of size $2$.  The problem is that I don't see, say, an obvious action of the cyclic group $\mathbb{Z}/p\mathbb{Z}$ on this set.  Any ideas?</p>
",<number-theory>
"<p>One version of Modularity Theorem says that </p>

<blockquote>
  <p>The elliptic curves with rational $j$-values arise from modular forms.</p>
</blockquote>

<p>Where </p>

<p>$$j(\tau)=1728\frac{g_2^3(\tau)}{\Delta(\tau)}$$</p>

<p>I know every terminology in the statement, but I don't know what ""arise from"" mean. Can anyone tell me?</p>
",<number-theory>
"<p>Polygonal numbers are of the form $\cfrac{n^2(s-2)-n(s-4)}{2}$, where $s$ is the number of sides of the polygon and $n$ is to say which one it is (the $n^{th}$ $s$-gonal number)</p>

<p>So my question is, how can you tell if a number is polygonal, and if it is, which $n,p$ plugged into the formula get you the number. Basically is there some kind of check if a number is polygonal?</p>
",<number-theory>
"<p>I need some help in proving </p>

<p>$$\left( \displaystyle\int_{0}^{\infty} t^{50} e^{-t} \,\mathrm dt \right)^{1/2}$$</p>

<p>isn't a perfect square. The only way I can think is repeated integration by parts which is obviously impractical and I still likely wouldn't be able to deduce if the result was a perfect square.</p>

<p>Thanks very much guys! :D </p>
",<number-theory>
"<p>For any natural number $n > 1$, define $E(n)$,to be the highest exponent to
which a prime divides it. For instance, $E(12)=E(36)=2$. Show that $$\lim_{N \to \infty} \frac{1}{N} \sum\limits_{n=2}^{N} E(n)$$ exists and find its value </p>
",<number-theory>
"<p>By mathematical induction, how would you show $\gcd(n,n+1)= 1$ for every integer $n$?</p>

<p>I'm thinking you would start by knowing that some integer, d, would divide both n and n+1. That's all I have so far. </p>
",<number-theory>
"<p>I am out of hints here. Its trivial to show $1$ is a solution. How to show it's the only solution?</p>

<p>Can someone please give me some hints?</p>

<p><em>Please do not use congruence, limits or derivatives because they are out of scope of the chapter which has this question.</em></p>
",<number-theory>
"<p>I understand that $\mathbb{Q}(x) \cong \mathbb{Q}(u)$ for all transcendental $u$, where $\mathbb{Q}(x)$ is the field of rational forms over $\mathbb{Q}$ and thus that all simple extensions of the rational numbers by transcendental ones are isomorphic to one another, but is it true, proven, that these extensions are not all $equal$ to each other? </p>

<p>I understand that that would be an incredible thing to prove, and could simplify many problems in transcendental number theory if true, so is it possible for it to be? $Could$ these extensions all equal each other? Or is that, easily or otherwise, proven to be an impossible task? I recall that transcendental numbers are uncountable, whereas I would assume that $\mathbb{Q}(x)$ is, though I have not enough knowledge of set theory to settle that in my mind.</p>
",<number-theory>
"<p>Give the postive integer $n&gt;1$, there exist infinite positive integer $k$
such that $\lfloor \dfrac{n^k}{k}\rfloor$ is odd </p>

<p>Maybe we can Use Euler's theorem,$$n^{\phi{(k)}}\equiv 1\pmod k$$
let
$$n^{\phi{(k)}}=rk+1\Longrightarrow n^k=\left(kr+1\right)^{\frac{k}{\phi{(k)}}}\Longrightarrow \dfrac{n^k}{k}=\dfrac{\left(kr+1\right)^{\frac{k}{\phi{(k)}}}}{k}$$I don't think this last part is relevant to understanding the exercise as stated, but I may be wrong. </p>
",<number-theory>
"<blockquote>
  <p>$(m_1,m_2)=1, a \equiv b \pmod {m_1} , a \equiv b \pmod {m_2} \Leftrightarrow a \equiv b \pmod {m_1 \cdot m_2}$</p>
</blockquote>

<p>The proof of the direction $"" \Rightarrow ""$ is the following:</p>

<p>$a \equiv b \pmod {m_1} , a \equiv b \pmod {m_2}$</p>

<p>Then $m_1 \mid a-b, m_2 \mid a-b$</p>

<p>So $[m_1, m_2] \mid a-b$</p>

<p>But $[m_1, m_2]=\frac{m_1 \cdot m_2}{(m_1, m_2)}=m_1 \cdot m_2$</p>

<p>So $m_1 \cdot m_2 \mid a-b$, so $a \equiv b \pmod {m_1 \cdot m_2}$</p>

<p>$$$$
Could you explain me how we conclude from $""m_1 \mid a-b, m_2 \mid a-b ""$ that $""[m_1, m_2] \mid a-b""$??</p>
",<number-theory>
"<p>I was just wondering why can't we use prime number's theorem to prove <a href=""http://en.wikipedia.org/wiki/Bertrand%27s_postulate"" rel=""nofollow"">Bertrand's postulate</a>.<br>We know that if we show that for all natural numbers $n&gt;2, \pi(2n)-\pi(n)&gt;0$ we are done. <br>Why can't it be proven by just showing (By using the prime number's theorem) that for every natural numbers $n&gt;2, \frac{2n}{ln(2n)}-\frac{n}{ln(n)}&gt;0$?</p>
",<number-theory>
"<p>Find a function $h_3(c)$ that will make
$$g_3 : \Bbb N × \Bbb N × \Bbb N → \Bbb N, \qquad (a, b, c) \mapsto 2^{a−1}3^{b−1}h_3(c)$$
a bijection. Your $h_3(c)$ should be an increasing function, i.e. $h_3(c_2) &gt; h_3(c_1)$ if $c_2 &gt; c_1$, and may have a multi-part definition, i.e. of the form
$$
h_3(c) =\cases{h_3'(c) &amp; if a certain condition is satisfied\\ h_3''(c) &amp; otherwise}
$$</p>
",<number-theory>
"<p>Is there any solution of the equation other than $x=2$?
Please help me. Thank you in advance.</p>
",<number-theory>
"<p>Solve for positive integers $x,y,z$</p>

<p>$$\frac{4}{13}=\frac{1}{x}+\frac{1}{y}+\frac{1}{z}$$</p>

<p>I tried to solve it by some generel work but it didn't help.</p>
",<number-theory>
"<p>Let $\sigma$ be the divisor sum function, $\gamma$ the Euler-Mascheroni constant and $n&gt;5040$. Robin showed that if the inequality$$\displaystyle \sigma(n)&lt;e^{\gamma}n\log\log n$$ ever fails, it does infinitely often. It is quite intuitive that infinitely many of the counterexamples (if not all) would be <a href=""http://en.wikipedia.org/wiki/Superabundant_number"" rel=""nofollow"">superabundant</a>, i.e. natural numbers $a$ such that $\displaystyle \frac{\sigma(a)}{a}&gt;\frac{\sigma(b)}{b}$ for all $b&lt;a$. 
My proof, which I'd like to have verified:</p>

<hr>

<p>Let $SA_k$ be the $k$-th superabundant number. Then assume, without loss of generality, </p>

<p>$$
\left\{ 
\begin{array}{c}
\sigma(SA_k)&lt;e^{\gamma}SA_k\log\log SA_k \\ 
\sigma(m)\ge e^{\gamma}m \log \log m \\ 
SA_l &lt; SA_k&lt;m&lt;SA_{k+1}
 \ , \end{array}
\right. 
$$
where $SA_l$ is the largest superabundant counterexample.
So we must have $$\displaystyle \frac{\sigma(m)}{\sigma(SA_k)}&gt;\frac{m \log\log m}{SA_k\log\log SA_k} \\ \ \ \ \ \ \ \frac{\sigma(m)}{m \log\log m} &gt;\frac{\sigma(SA_k)}{SA_k\log\log SA_k}. \ \ \ \ \ \  (1)$$
But since $m$ is between two consecutive superabundant numbers, it is not superabundant itself, hence it is by definition $\displaystyle \frac{\sigma(m)}{m}\le\frac{\sigma(SA_k)}{SA_k}$. Given that $ \log\log m &gt;\log\log SA_k$, we easily find $$\displaystyle \frac{\sigma(m)}{m \log\log m} &lt;\frac{\sigma(SA_k)}{SA_k\log\log SA_k},$$ contradicting $(1)$. As a result, we have no counterexamples $&gt; SA_l$, which is an absurdity. $ \ \ \ \ \ \ \ \ \ \  \square$</p>

<hr>

<p>Any comment, suggestion for improvement or alternative proof would be highly appreciated.</p>
",<number-theory>
"<p>Solve for $x,y,z\in\mathbb{N}$</p>

<p>$\frac{4}{13}=\frac{1}{x}+\frac{1}{y}+\frac{1}{z}$</p>

<p>I tried by some general methods but they didn't help me.</p>
",<number-theory>
"<p>Let $g(n)=\displaystyle{\sum_{p^{\alpha}||n}} \alpha p,$ where $p^{\alpha}||n$ means that $p^{\alpha}|n$ and $p^{\alpha+1}$ does not divide $n.$ Can someone provide me with a non trivial estimate of this function or even an upper bound is sufficient for me?</p>

<p>Thanks in advance.</p>
",<number-theory>
"<p>Is the following theorem well known?</p>

<p>Theorem
Odd positive integer N is a prime number if and only if there is no non-trivial solution for Diophantine equation</p>

<p>$x^2−y^2=N$</p>

<p>(trivial solution: $x=(N+1)/2; y=(N−1)/2)$</p>
",<number-theory>
"<p>If i have a large number (&lt;=10^5 Digits) how can i tell that if i can shuffle the number so that it become a multiple of 30 . if it is possible then i have to find the maximum multiple . Suppose if i have this number 
  97980 
if i make any combination of this number , all the combinations will be divisible by 30 , but the maximum one is 99870 . How can i shuffle this very large number and know whether it is possible or not ?</p>
",<number-theory>
"<p>I have the following assignment:</p>

<p>consider the map </p>

<p>$$|\cdot|:\mathbb{Z}[i]\longrightarrow \mathbb{N},\qquad |a+ib|:=a^2+b^2$$</p>

<p>1) Prove that $|\alpha|&lt;|\beta|$ iff $|\alpha|\leq |\beta|-1$ and $|\alpha|&lt;1$ iff $\alpha=0$</p>

<p>2) Let $\alpha,\beta\in\mathbb{Z}[i],\beta\neq 0$. Prove that the map $f:\mathbb{Z}[i]\longrightarrow\mathbb{Z}[i], f(\gamma):=\alpha-\gamma\beta$ is the composition of a dilatation by the factor $\sqrt{|\beta|}$, a rotation (angle?) and a translation.</p>

<p>3) Deduce that there exists $\gamma\in\mathbb{Z}[i]$ such that $|f(\gamma)|$ is strictly smaller than $|\beta|$.</p>

<p>$\textbf{Hint:}$ compare the size of a cell of the lattice $f(\mathbb{Z}[i])$ and the size of the set of points whose distance to $0$ is $\leq\sqrt{|\beta|}$.</p>

<p>What i did: point 1) is a trivial consequence of the fact that the norm takes integer non negative values. For point 2), I use complex multiplication of numbers which is: multiply absolute values and add angles. For point 3), i'm actually waiting for a miracle... I suppose i should prove that there exists a cell in $f(\mathbb{Z}[i])$ intersecting the open ball centered at the origin with radius $\sqrt{|\beta|}$, but i have no idea how to write down this. Only thing i noticed is that $f$ acts with a rotation, which does not affect distance from the origin, so that the only changes in $|\gamma|$ come from dilatation and by adding $\alpha$.</p>

<p>Could someone put me on the right direction?</p>
",<number-theory>
"<p>Given coprime $a,b$ and $n\in\Bbb N$, is there a simple criterion to find if there is <strong>no</strong> positive integer solution to
$$ax+by=n?$$</p>
",<number-theory>
"<p>I'm wondering, if it is true that the torsion subgroup of $y^2=x^3+p$ (for $p$ some prime, greater than 2), is always trivial?.</p>

<p>I was trying to prove this using Lutz-Nagell, but I can't quite get it.</p>

<p>Thank you</p>
",<number-theory>
"<p>In the <strong>Wikipedia</strong> article for the <a href=""https://en.wikipedia.org/wiki/Digamma_function"" rel=""nofollow"">Digamma function</a> one finds some identities due to <strong>Gauss.</strong> I've used the fourth of those from the section <strong>Some finite sums involving the digamma function</strong> to show if there were no mistakes that for $m&gt;1$ </p>

<p>$$\sum_{\substack{1\leq k\leq m-1 \\ (k,m)=1}}\sum_{r=1}^{m-1}\psi\left(\frac{r}{m}\right)\sin\left(\frac{2\pi r k}{m}\right)=\pi\frac{m\phi(m)}{2}-\pi\frac{m\phi(m)}{2}=0,$$</p>

<p>where $\phi(m)$ is the <strong>Euler's totient</strong> function and as you see in the reference $\psi(s)$ is the <strong>digamma function</strong>.</p>

<p>My approach was use Apostol's Exercise 14 with $f(x)=x$, from Chapter 2 of <strong>Apostol</strong>, Introduction to Analytic Number Theory, <strong>Springer</strong> (1976), also I need that $\sum_{\substack{1\leq k\leq m-1 \\ (k,n)=1}}k=\frac{n}{2}\phi(n)+\frac{n}{2}\sum_{d\mid n}\mu(d)=\frac{n}{2}\phi(n)+\frac{n}{2}$, on assumption that $n&gt;1$. Also the <strong>Gauss</strong> identity for the sum of the first $n$ positive integers.</p>

<blockquote>
  <p><strong>Question 1.</strong> Can you say if my statement (the first identity of current post) was right? <strong>Thanks.</strong></p>
</blockquote>

<p>After I tried the same with the other identity that is feasible do this calculations, that is the third, also due to Gauss. My calculations were $$\sum_{\substack{1\leq k\leq m-1 \\ (k,m)=1}}\sum_{r=1}^{m-1}\psi\left(\frac{r}{m}\right)\cos\left(\frac{2\pi r k}{m}\right)=m\phi(m)\log 2+m\cdot\log\left(\prod_{\substack{1\leq k\leq m-1 \\ (k,m)=1}}\sin \frac{k\pi}{m}\right)+\gamma\phi(m).$$</p>

<p><strong>There was a typo, fixed.</strong></p>

<blockquote>
  <p><strong>Question.</strong> Is there a <em>nice</em> closed-form for the factor $$\prod_{\substack{1\leq k\leq m-1 \\ (k,m)=1}}\sin \frac{k\pi}{m}$$ in the context of this post (I say nice/good in the context of the first question)? <strong>Thanks in advance.</strong></p>
</blockquote>
",<number-theory>
"<p>Given, $p$ is a prime number and $p&gt;3$. How do we prove that the remainder $r$ is always $1$ if $p^2$ is divided by $12$?</p>
",<number-theory>
"<p>Dirichlet's simultaneous approximation theorem says:</p>

<p>Given any $n$ real numbers $\alpha_1,\ldots,\alpha_n$ and for every natural number $N \in \mathbb{N}$, there exist integers $q \leq N$, and  $p_1,\ldots,p_n \in \mathbb{Z}$, such that:</p>

<p>$$ \Bigg|\alpha_i - \frac{p_i}{q}\Bigg| &lt; \frac{1}{qN^{1/n}} \text{    for } i=1,\ldots,n $$</p>

<p>I would like to prove a similar theorem, but want to insist that the $p_i$ all be odd.  It's easy to prove a version which has all the $p_i$ even, because the set of points in $\mathbb{R}^n$ with even coordinates is a lattice.  </p>

<p>The general version of Minkowski's Theorem say that a symmetric region containing the origin, if it's volume is sufficiently large, must contain a nonzero lattice point.  But the set of points with odd coefficients is only a coset of the lattice of points with even coefficients.</p>

<p>This seems obviously true that one should be able to replace ""lattice"" with ""coset of the lattice"" in Minkowski's Theorem, but I sure don't see how to do it.  I can get the result I want in one dimension by modifying the continued fraction construction, but that doesn't export to $n$ dimensions.  References and insights welcome.</p>
",<number-theory>
"<p>Although I am very much new to ""Analytic Number Theory"", there are some non mathematical questions which puzzle me. First of all, why was <strong>G.H.Hardy</strong> so much keen to have an elementary proof of the <strong>Prime Number Theorem</strong>. He also stated that producing such a proof, will change the complexion of Mathematics, but nothing like that has happened. What was on Hardy's mind? </p>

<p>The elementary proof although has some intricate tricks involved, I am curious to know whether the methodology can be applied for attacking more complex problems. I have seen that the analytic proof has a continuation and is not over and discuss some more interesting properties regarding the $\zeta$ function. </p>

<p>I also saw <a href=""http://mathoverflow.net/questions/16735/is-a-non-analytic-proof-of-dirichlets-theorem-on-primes-known-or-possible"">this thread</a>. Are there any such theorem's which piques peoples curiosity in getting an elementary proof. (While writing this FLT comes to my mind!!).</p>
",<number-theory>
"<p>Is there an extant published expository account, comprehensible to all mathematicians, of the conceptual differences between ancient Greek mathematical concepts and modern ones?</p>

<p>I have in mind things like this:</p>

<ul>
<li>Euclid (I'll need to look between the covers of a book to be sure whether this is right&nbsp;.&nbsp;.&nbsp;.&nbsp;.) didn't know how to multiply more than three numbers because no more than three lines can be mutually orthogonal; but he did know how to find the smallest number measured by more than three numbers (what today we would call the LCM);</li>
<li>Consequently (?) he didn't know about factoring numbers into primes (for example, $90= 2\cdot3\cdot3\cdot5$ is not the LCM of its prime factors) (and that's why he stopped short of stating, let alone proving, the uniqueness of prime factorizations), but of course they did know that every number is ""measured by"" at least one prime number;</li>
<li>(Maybe?) Euclid did not consider $1$ to be a number;</li>
<li>The ancient Greeks had no concept of real number.  They had a concept of congruence of line segments, so that they could say that one line segment goes into another between $6$ and $7$ times, and the remainder goes into the shorter segment between $2$ and $3$ times, etc. etc., so they knew what it meant to say the ratio of the length of segment A to that of segment B is the same as the ratio of the length of segment C to segment D.  They even knew what it means to say the ratio of lengths A to B is the same as the ratio of areas E to F, and similarly volumes.  But they did not make the mistake of knowing whether a particular area is less than a particular length.  Modern mathematicians seem to make that mistake by saying those are real numbers; I think modern physicists may avoid that error.</li>
<li>They did not have a concept of irrational number (since they didn't have a concept of real number), but they knew what it meant to say that two line segments <b>have no common measure</b>, and how to prove it in some cases (e.g. no segment can be laid end-to-end some number (=&nbsp;cardinality) of times to make the length of the side of a square and some other number of times to make the diagonal).</li>
</ul>
",<number-theory>
"<p>Suppose $H$ is an automorphic L-function obtained by applying the Rankin-Selberg convolution to two automorphic L-functions $F$ and $G$ each of them being different from the Riemann Zeta function. Is the set $\{F,G\}$ entirely determined by $H$?</p>
",<number-theory>
"<p>Question as in title, where $L(s,\chi)$ is the Dirichlet $L$-function associated with the nontrivial character modulo $3$. Please provide complete SAGE code. Thank you in advance.</p>
",<number-theory>
"<p>Square of an irrational number can be a rational number e.g. $\sqrt{2}$ is irrational but its square is 2 which is rational.</p>

<p>But is there a irrational number square root of which is a rational number?</p>

<p>Is it safe to assume, in general, that $n^{th}$-root of irrational will always give irrational numbers?</p>
",<number-theory>
"<p>A bijection
$g_4 : \mathbb N\times \mathbb N \times\mathbb N \times\mathbb N \to\mathbb N$, with $$(a, b, c, d) \mapsto 2^{a−1}3^{b−1}5^{c−1}h_4(d)$$
is constructed, with $h_4(d)$ as an increasing function. Find the values of $a, b, c, d$ such that</p>

<p>(i) $g_4(a, b, c, d) = 7236$</p>

<p>ii) $g_4(a, b, c, d) = 833$</p>
",<number-theory>
"<p>In a related <a href=""http://math.stackexchange.com/questions/2571/about-powers-of-irrational-numbers"">question</a> we discussed raising numbers to powers.  </p>

<p>I am interested if anybody knows any results for raising numbers to irrational powers.  </p>

<p>For instance, we can easily show that there exists an irrational number raised to an irrational power such that the result is a rational number. Observe ${\sqrt 2 ^ {\sqrt 2}}$. Since we do not know if ${\sqrt 2 ^ {\sqrt 2}}$ is rational or not, there are two cases.  </p>

<ol>
<li><p>${\sqrt 2 ^ {\sqrt 2}}$ is rational, and we are finished.  </p></li>
<li><p>${\sqrt 2 ^ {\sqrt 2}}$ is irrational, but if we raise it by ${\sqrt 2}$ again, we can see that
$$\left ( \sqrt 2 ^ \sqrt 2 \right ) ^ \sqrt 2 = \sqrt 2 ^ {\sqrt 2 \cdot \sqrt 2} = \sqrt 2 ^ 2 = 2.$$</p></li>
</ol>

<p>Either way, we have shown that there exists an irrational number raised to an irrational power such that the result is rational.</p>

<p>Can more be said about raising irrational numbers to irrational powers?  </p>
",<number-theory>
"<p>Let $\mathbb{P}$ denote the set of prime numbers. How would one evaluate
$$\prod_{p\in \mathbb{P}}\frac{p-1}{p}$$ I do not think that the fact that $$\prod_{n=2}^{\infty}\frac{n-1}{n}=\lim_{n\to\infty}\frac{1}{n}=0$$ can be applied, but it is worth noting. Additionally, if we take the log, we obtain
$$\log\left(\prod_{p\in \mathbb{P}}\frac{p-1}{p}\right)=\sum_{p\in\mathbb{P}}\log\left(\frac{p-1}{p}\right)=\sum_{p\in\mathbb{P}}\log(p-1)-\sum_{p\in\mathbb{P}}\log(p)$$</p>
",<number-theory>
"<p>Find $x \in \mathbb{Z}[i]$ such that:</p>

<p>$(1+2i)x \equiv 1 \mod 3+3i$</p>

<p>How would you go about doing this? Best I can think of is keep guessing....</p>
",<number-theory>
"<p>I have trouble in understanding Weil paring on $N$-torsion points on an elliptic curve. Please see <a href=""http://en.wikipedia.org/wiki/Weil_pairing"" rel=""nofollow"">Wikipedia</a> for the definition of Weil paring. I would like to know what Weil paring is  computing intuitively. I would also want to see some basic computation. </p>

<p>I will really appreciate your help. </p>
",<number-theory>
"<p>(This is different than <a href=""http://math.stackexchange.com/questions/50791/if-xm-e-has-at-most-m-solutions-for-any-m-in-mathbbn-then-g-is-cycl"">If $x^m=e$ has at most $m$ solutions for any $m\in \mathbb{N}$, then $G$ is cyclic</a>)
<br/>I was trying to solve this:</p>

<blockquote>
  <p>Let $G$ be a finite abelian group of order $n$ for which the number of solutions of $x^m=e$ is at most $m$ for any $m$ dividing $n$. Prove that $G$ must be cyclic. [Hint:Let $\psi (m)$ be the number of elements in $G$ of order $m$. Show that $\psi (m)\leq \phi (m)$, and use $n=\Sigma_{m\mid n}\phi (m) $] </p>
</blockquote>

<p>What I have proven is that $\Sigma_{d|n}(ψ(d)−ϕ(d))=0$, then if the statement in the hint holds, ψ(d)=ϕ(d) for every d|n. n divides itself, therefore, $\psi (n)=ϕ(n)≠0$. Therefore G admits a generator, and we'll be done.The problem is to prove the statement in the hint. Yet I have conluded a different seemingly contradictory result, which doesn't help solve the problem at all, since it could be that $ψ(n)=0,ψ(n)−ψ(d)&lt;0$ and $ψ(d)−ϕ(d)&gt;0$ for some $d≠n$. Of course I don't know how to show that $\psi (m)\leq \phi (m)$, and here is how I got this contradiction:
<br/>If $\psi (m)\neq 0$, then let $m\neq 1,m\mid n$. Consider an element $a\in G,|a|=m$. Within its cyclic group, there will be $\phi (m)$ elements with order $m$ since any element $a^i$ in it is a generator if and only if $\gcd(m,i)=1$. Therefore $\psi(m) \geq \phi (m)$ since some element out side the cyclic group may have order $m$ too.
<br/>If this reasoning is correct, I think it will contradict with the fact in the hint which is $\phi(m) \geq \phi(m)$. The number of solutions is $\Sigma_{d|m}\psi (d)$, which is always greater than $\psi (m)$. So no matter how great $\psi (m)$ is, or how $\psi (m) &gt;\phi (m)$, the number of solution may still be less than $m$. What's going wrong? How to prove the statment in the hint? Help me please. Also I have read through <a href=""http://math.stackexchange.com/questions/50791/if-xm-e-has-at-most-m-solutions-for-any-m-in-mathbbn-then-g-is-cycl"">If $x^m=e$ has at most $m$ solutions for any $m\in \mathbb{N}$, then $G$ is cyclic</a>, but for the accepted answer, it doesn't show how to prove the fact in the hint, and other answers do not use the fact in the hint.</p>
",<number-theory>
"<p>Let a, n be positive integers. Prove that n divides $\phi(a^n-1)$, where $\phi$ is Euler's $\phi$-function.</p>

<p>I know this problem can be done using number theory approaches, however I am rusty on those concepts can someone help me? </p>
",<number-theory>
"<p>I am <a href=""https://github.com/gazman-sdk/quadratic-sieve"" rel=""nofollow"">implementing</a> the <a href=""https://en.wikipedia.org/wiki/Quadratic_sieve"" rel=""nofollow"">quadratic sieve</a> algorithm. And I got run in unexpected problem.</p>

<p>Take a look at those two final steps of the algorithm as described in wiki.</p>

<blockquote>
  <ol start=""4"">
  <li>Use linear algebra to find a subset of these vectors which add to the zero vector. Multiply the corresponding $a_i$ together naming the
  result mod $n$: a and the $b_i$ together which yields a B-smooth square $b^2$.</li>
  <li>We are now left with the equality $a^2=b^2$ mod $n$ from which we get two square roots of ($a^2$ mod $n$), one by taking the square root in the
  integers of $b^2$ namely $b$, and the other the $a$ computed in step 4.</li>
  </ol>
</blockquote>

<p>In my case I found more than $100$ vectors with the size of $30+$ digits each, when multiplying them I get a number that got too many digits, so squaring it becomes a problem, it takes more time than solving the matrix itself(I use the <a href=""https://en.wikipedia.org/wiki/Methods_of_computing_square_roots#Babylonian_method"" rel=""nofollow"">babylon method</a> for squaring).</p>

<p>Is there a way to avoid creating such big numbers?</p>
",<number-theory>
"<p>So I think I understand how to calculate something like $(208\cdot 2^{-1})\mod 421$ using extended euclidean algorithm. But how would you calculate something like $(208\cdot2^{-21})\mod 421$? </p>

<p>Thanks, this is basically for my cryptography class; I'm just trying to understand the ""big step, baby step"" algorithm.</p>
",<number-theory>
"<blockquote>
  <p>Is there a (number theoretic or algebraic) trick to find a large
  nunber modulo some number?</p>
</blockquote>

<p>Say I have the number $123456789123$ and I want to find its value modulo some other number, say, $17$.</p>

<p>It's not fast for me to find the prime factorisation first. It's also not fast to check how many multiples of $17$ I can ""fit"" into the large number. </p>

<p>So I was wondering if there is any method out there to do this efficiently. </p>

<blockquote>
  <p>I am looking for something like the other ""magic trick"" where you sum
  all the digits and take the result $\mod 9$. </p>
</blockquote>
",<number-theory>
"<p>I'll try to format my question in a manner such that you can skip (irrelevant) parts.</p>

<p><strong>Exercise:</strong></p>

<blockquote>
  <p>Find all natural $n$ such that $3^{2n+1}-4^{n+1}+6^n$ is prime.</p>
</blockquote>

<p><strong>Motivation:</strong></p>

<p>I'm trying to prepare talented elementary school children for mathematics competition and the question from the title popped up somewhere as suggested for the purpose (explicitly stating this should be number theory exercise for elementary school). Now, this immediately rang some alarms, as powers are learned in grade 8 (final grade before high school), when they certainly don't know any modular arithmetic or induction, let alone more advanced number theory. I tried to solve it anyway to see if this could actually be used, but to my surprise, I couldn't. Obviously, when $n=1$ we get $17$, which is prime, but for $n&gt;1$ it seems that it will never be prime again, although I fail to show it (computer says that the expression is never prime in range $n\in[2, 100000000]$). There is, of course, possibility of a typo in question, but computer check up makes me believe that this is ""legit"" question.</p>

<p><strong>My work:</strong></p>

<p>First of all, for $n=1$ we have $3^{2n+1}-4^{n+1}+6^n = 17$ which is prime.</p>

<p>Secondly, it is easy enough to check that for $n = 2k,\ k\in\mathbb N$ we have $5\mid 3^{2n+1}-4^{n+1}+6^n$.</p>

<p>This looked promising, but for $n = 5$, we get $3^{2n+1}-4^{n+1}+6^n = 211\cdot 857$ as prime decomposition which is definitely worrisome.</p>

<p>I tried to find some appropriate $d\in\mathbb N$ such that $n = qd + r$ to build some casework with respect to $r$, but had no luck.</p>

<p>What I did notice is that if you pick $n$ in some non-trivial ideal $(d)$, there is at least one prime $p_d$ such that $p_d\mid 3^{2n+1}-4^{n+1}+6^n$, for all $n\in (d)$. </p>

<p><em>(though, I proved this only for $d=2$, for $d&gt;2$ this is but a computer inspired conjecture)</em></p>

<p>Here is a table:</p>

<p>\begin{array}{c | c}
d &amp; p_d \\ \hline
2 &amp; 5 \\
3 &amp; 19 \\
4 &amp; 5, 13 \\
5 &amp; 211 \\
6 &amp; 5, 7, 19 \\
7 &amp; 29, 71 \\
8 &amp; 5, 13, 97 \\
9 &amp; 19, 1009 \\
10 &amp; 5, 11, 211\\
\end{array}</p>

<p>Another attempt was obviously to try to factor $3^{2n+1}-4^{n+1}+6^n$ directly. Some obvious attempts led me to believe this approach might fail too. Indeed, polynomials $$x^{2n+1} - y^{2n+2} + x^ny^n,$$ $$x^{2n+1} - (x-1)^{2n+2} + x^n(x-1)^n$$ and $$x^{2n+1} - (x+1)^{n+1} + x^n(x-1)^n$$ are irreducible over $\mathbb Q$.</p>

<p>This leaves me without ideas. Any help would be much appreciated.</p>
",<number-theory>
"<p>Following a previous <a href=""http://math.stackexchange.com/questions/680122/cram%C3%A9rs-model-the-prime-numbers-and-their-distribution-part-1"">question</a> (here you'll find an introduction):</p>

<p>The book states that using the convergence of the binomial distribution towards the Poisson distribution, it's easy to show that $$|\{x\le\xi:\pi_S(x+\lambda \log x)-\pi_S(x)=k\}|\sim\xi\mathrm e^{-\lambda} \frac{\lambda^k}{k!}\quad(\xi\to\infty)$$ holds almost surely.</p>

<p>I couldn't prove this.</p>
",<number-theory>
"<blockquote>
  <p>Let $x,y$ be integers and $y$ be a (nonzero) quadratic residue modulo $p$ ($p$ is a prime). Prove that $xy$ is a quadratic residue modulo $p$ if and only if $x$ is a quadratic residue modulo $p$. </p>
</blockquote>

<p>If $x$ is a quadratic residue modulo $p$, then the result is trivial. How do we prove the other direction?</p>
",<number-theory>
"<p>Following a previous <a href=""http://math.stackexchange.com/questions/884932/cram%C3%A9rs-model-the-prime-numbers-and-their-distribution-part-3"">question</a> (<a href=""http://math.stackexchange.com/questions/680122/cram%C3%A9rs-model-the-prime-numbers-and-their-distribution-part-1"">here</a> you'll find an introduction):</p>

<p><a href=""http://projecteuclid.org/download/pdf_1/euclid.mmj/1029003189"" rel=""nofollow"">A paper by Maier</a> which refutes Cramer's Model suggests we should replace the heuristic ""$\Bbb P(n\in\mathcal P)=1/\log n$"" with $$\Bbb P(n\in\mathcal P|P^-(n)\gt z)=\frac{1}{\log n}\prod_{p\le z}_{p\in\mathcal P}(1-1/p)^{-1}\quad (z\approx \log n)$$ where $P^{-}(n)$ denotes the least prime number that divides n.
The book states that the new heuristic leads us to expects that the strong Cramér's conjecture $$\limsup_{n\to\infty}\frac{p_{n+1}-p_n}{(\log p_n)^2}=1$$ (which is derived in <a href=""http://matwbn.icm.edu.pl/ksiazki/aa/aa2/aa212.pdf"" rel=""nofollow"">this paper by Cramer</a>) is false and should be replaced by $$\limsup_{n\to\infty}\frac{p_{n+1}-p_n}{(\log p_n)^2}=2\mathrm e^{-\gamma}$$ where $p_n$ denotes the $n^{th}$ prime number, and $\gamma$ denotes Euler's constant.
For proving this last implication, I should mention Mertens' formula: $$\prod_{p\le z}_{p\in\mathcal P}(1-1/p)^{-1}=\mathrm e^\gamma\log z+O(1)\quad(z\ge 1)$$</p>

<p>I couldn't prove this.</p>
",<number-theory>
"<p>Greetings to one an all!</p>

<p>How can we prove the curve ""$x^2 -x = y^5-y$"" is a hyperelliptic curve?</p>

<p>Is a hyperelliptic curve the same as a hyperbolic elliptic curve or are there any differences?</p>
",<number-theory>
"<p>Following a previous <a href=""http://math.stackexchange.com/questions/884902/cram%C3%A9rs-model-the-prime-numbers-and-their-distribution-part-2"">question</a> (<a href=""http://math.stackexchange.com/questions/680122/cram%C3%A9rs-model-the-prime-numbers-and-their-distribution-part-1"">here</a> you'll find an introduction):</p>

<p>The book states that almost surely $$\pi_S(x+y)-\pi_S(x)=\mathrm{li}(x+y)-\mathrm{li}(x)+O(\sqrt y)$$ as soon as $y/(\log x)^2\to\infty$, with $y\le x$.</p>

<p>I couldn't prove this.</p>
",<number-theory>
"<p>Let $a,b,c$ and $d$ be rational.Find a rational parametric solutions for $a,b,c$ and $d$ so that 
$$(4/3)b^2c^2+(4/3)a^2d^2-(1/3)a^2c^2-(4/3)b^2d^2=\square.$$</p>
",<number-theory>
"<p>Does any one know how to prove the following identity?
$$
\mathop{\mathrm{Tr}}\left(\prod_{j=0}^{n-1}\begin{pmatrix}  
2\cos\frac{2j\pi}{n} &amp; a \\
b &amp; 0
\end{pmatrix}\right)=2
$$
when $n$ is odd. The product sign means usual matrix multiplication, and $a$ and $b$ are arbitrary real numbers.</p>

<p>Since the product of $2\cos\frac{2j\pi}{n}$ is $2$, so we only need to prove that the trace is a constant polynomial in $a$ and $b$.</p>

<p>Because of the cosine term, the approach of polynomial analysis used in <a href=""http://mathoverflow.net/questions/213246/product-of-a-finite-number-of-matrices-related-to-roots-of-unity"">my previous post</a> does not seem to work here.</p>
",<number-theory>
"<p>Let's say you have a 20 sided dice and every side is considered bad, but each time you roll a bad side it will no longer be bad for future rolls. 
I am looking for the mathmatical term for the scale of increase on the expected amount of times you will be able to roll success as bad sides are removed </p>
",<probability>
"<blockquote>
  <p>Let $X\sim\mathcal N(μ_1,σ_1^2)$ and $Y\sim\mathcal N(μ_2,σ_2^2)$ and $\mathsf{Cov}(X,Y)=c$ How can we compute $\mathsf {Cov}(X^2,Y^2)$?</p>
</blockquote>

<p>I think the answer should be something like $c^2$ and I think the joint PDF is really not necessary here. I think the approach is using some independent standard normal random variables by variable changing. But I can't make it out. </p>

<p>Thanks.</p>
",<probability>
"<p>How can one determine the probability function $f(x)$ given a situation and no equations? I am unsure if there is a special method or if you should just plug in values and look for a pattern.</p>

<p>For example, if you draw 2 numbers from 0 through 9 without replacement, and $X=$ {total of the 2 numbers}, then how could you determine $f(x)$? I came up with the sample space $|S|= {10 \choose 2}$, and found probabilities for X = 1,2,3, ... etc. </p>

<p>I got $\frac{1}{10 \choose 2}$ for $X=1$ and $X=2$ and then the numerator increases by 1 for $X=3,4$, increases again by one for 4 and 5, and then values are repeated in pairs like that if I have done this correctly. My confusion is how to determine a formula now. Can it even be done?</p>
",<probability>
"<p>Problem statement: Suppose we have a random sample $\{X_i:1\le i\le10\}$ from a normal distribution with mean $\mu=20$ and variance $\sigma^2=100$. Find the probability,
$$P\left(\sum_{i=1}^6X_i&gt;X_7+2X_8+2X_9+X_{10}+16\right)$$
My initial thoughts: For any $X_i$, we know the MGF is $M_{X_i}(t)=20t+50t^2$. I defined a new random variable $Y$ denoted by the linear combination in which every term in the inequality is on one side, i.e.
$$Y=X_1+\cdots+X_6-X_7\cdots-X_{10}-16$$
so that now the problem is finding $P(Y&gt;0)$. I find that the MGF of $Y$ is
$$M_Y(t)=e^{-16t}\left(20t+50t^2\right)^6\left(-20t+50t^2\right)^2\left(-40t+200t^2\right)^2$$
Is this making the problem more difficult than it needs to be? I don't think this looks like the MGF of a distribution I might be familiar with.</p>
",<probability>
"<p><img src=""http://i.stack.imgur.com/HqML7.png"" alt=""enter image description here""></p>

<p>im on the last part and my attempt was to:</p>

<p>find $P(|\bar{X_n} - \mu| &lt; 0.01) &gt; 0.99$ but in the solutions showed the equation is $P(|\bar{X_n} - \mu|/\mu &lt; 0.01 ) &gt; 0.99$ why do we have to divide through my $\mu$?</p>
",<probability>
"<p>Motivation: A friend asked me this question.</p>

<p>The Problem: Suppose you start off with a dollar. You flip a fair coin, if it lands on heads you win $50$ cents otherwise you lose $50$ cents. If after $n$ flips you have a nonzero amount of money, you win. What's the probability you win? What about the limiting case as $n$ tends to infinity?</p>

<p>edit: In this game you are not allowed to have negative money. Thanks, Jonathan Fischoff, the linked helped greatly.</p>
",<probability>
"<p>Imagine a process with two variables <em>min</em> and <em>max</em>, and two counters <em>hi</em> and <em>lo</em>.</p>

<p>We initialize <em>min</em> and <em>max</em> by selecting two random numbers (assume a uniform (0,1) distribution for convenience), and sorting them.  We also set the <em>hi</em> and <em>lo</em> counters to 0.</p>

<p>At each step we select a new random number <em>r</em> from our distribution and do the following</p>

<pre><code>if (r &lt; min) {
  lo++;
} else if (r &gt; max) {
  hi++;
} else if (lo &gt; hi) {
  max = r;
  hi++;
} else if (hi &gt; lo) {
  min = r;
  lo++;
} else {
  if (rand(0,1) &gt; 0.5 ){
    max = r;
    hi++;
  } else {
    min = r;
    lo++;
  }
}
</code></pre>

<p>If the random number is outside our range, we increment the appropriate counter, and if it lies inside our range we increment the lower counter and adjust our range appropriately.  The question is, after <em>n</em> iterations, where do we expect min and max to end up.</p>

<p>It can be seen that the difference between min and max (assuming a uniform(0, 1) distribution) is distributed as the minimum of <em>n + 2</em> uniform (0, 1) random variables.  However, it should be possible to say considerably more about the location of <em>min</em> and <em>max</em> over time.</p>

<p><strong>Edit</strong>  This algorithm is only vaguely related to median finding.  The process given here just takes a sequence of unifomly distributed random numbers and attempts to guess the median by remembering only two values.  This, of course is not an effective method, but similar algorithms are used (remembering more than two variables).</p>

<p>The process that I want to analyze has two values and two counters, and takes a sequence of random numbers, if the random number is larger than both, then increase the <em>hi</em> counter, if the number is smaller than both then increase the <em>lo</em> counter, and if the number lies between the two, either replace <em>max</em> with the current number and increase <em>hi</em> or else replace <em>min</em> with the current number and increase <em>lo</em>, whichever makes the <em>hi</em> or <em>lo</em> counters closer (in cases where either choice could be made, flip a coin).  <em>hi</em> and <em>lo</em> are counting the numbers that we have seen that are larger than <em>max</em> or smaller that <em>min</em> respectively.</p>

<p>What I want to know, is how to figure out the distribution of <em>min</em> and <em>max</em> as the number of iterations becomes large, also of interest is the distribution of <em>hi - lo</em>.  I can find the distribution of <em>max - min</em> using elementary order statistics.</p>

<p>If <em>min = max</em>, then <em>hi - lo</em> is an ordinary one dimensional random walk, and is easy to analyze.  When they are different the walk is subtly biased towards 0.  Similarly the values of <em>min</em> and <em>max</em> are biased towards 0.5, I want to know how to find out by how much they are biased.</p>
",<probability>
"<p>Let $X_n $ be uniformly distributed on $[0,1]$. We say $X_k$ is a local maximum if $X_k&gt; X_{k\pm 1}$. Let $A_n$ count the number of local maxima of the sequence unto and including $n$. Find $a_n, b_n$ such that </p>

<p>$$\frac{A_n-a_n}{b_n} \longrightarrow N(0,1)$$</p>

<p>If someone could give a hint on how to approach this problem that would be great. I understand that I will need to use the Lindberg central limit theorem at some point to show the convergence. </p>
",<probability>
"<p>Given two random integers $a$, $b$ calculate probability of $a^2$ + $b^2$ is divisible by $10$.</p>

<p>I've tried to simulate this process and got a result about $0.18$ (maybe incorrect), but have no idea why.</p>
",<probability>
"<p>If the stars are distributed randomly within the universe, what is the probability for a star to be the nearest neighbor of a star that is its nearest neighbor? What if the number of spatial dimensions is higher than 3 or even grows without limit? </p>

<p>PS. I am interested in the asymptotic behavior with the number of stars growing without bound.</p>

<p>PPS. It's a well-known problem (the ""birds on a wire problem"" in higher dimensions) and I know the answers. However, no idea how to solve this analytically.</p>
",<probability>
"<p>Vince buys a box of candy that consists of six chocolate pieces, four fruit pieces and two mint pieces.  He selects three pieces of candy at random without replacement.</p>

<blockquote>
  <ol>
  <li><p>Calculate the probability that the first piece selected will be fruit flavored and the other two will be mint.</p></li>
  <li><p>Calculate the probability that all three pieces selected will be the same type of candy.</p></li>
  </ol>
</blockquote>
",<probability>
"<p>I have a probability density function over $SO\left(3\right)$, which I am trying to sample from. The $pdf$ is given as a generalized fourier series:</p>

<p>$$ f\left(\omega,\theta,\phi\right)=\sum s_{\lambda}^{n}Z_{\lambda}^{n}\left(\omega,\theta,\phi\right)$$</p>

<p>where $s_{\lambda}^{n}$ are the coefficients and $Z_{\lambda}^{n}\left(\omega,\theta,\phi\right)$ are basis functions (symmetrized hyperspherical harmonics), and $SO\left(3\right)$ is parameterized by the variables $\left(\omega,\theta,\phi\right)$, which are the rotation angle, and spherical coordinates of the rotation axis, respectively. I want to sample values of $\left(\omega,\theta,\phi\right)$ (i.e. rotations) from this distribution, but I'm having trouble doing so.</p>

<p>So far I have essentially tried two methods: rejection sampling, a discrete method.</p>

<p>Both methods have given me something that is qualitatively similar to what I would expect, but they seem to have an erroneous uniform distribution superimposed. So I have two questions:</p>

<p><strong>(1) Any ideas why I might be getting this uniform noise?</strong></p>

<p><strong>(2) Any suggestions of how to fix it or a better way to sample from this kind of distribution?</strong></p>

<p><strong>Further Details:</strong>
To check my sampling method I used some software to generate a known distribution and sample rotations. I then computed $s_{\lambda}^{n}$ from these ""correct samples"". Then I tried to generate samples myself from the spectral form of $f\left(\omega,\theta,\phi\right)$ given above. Next I calculated $s_{\lambda}^{n}$ from my samples and compared the two. My samples led to the low order terms being generally too small in magnitude and the higher order terms being too large in magnitude.</p>

<p>The discrete method that I used consisted of generating a ""grid"" of points over $SO\left(3\right)$, and using these as the centers of bins. The bins were sized proportional to the probability density at the bin center. Then uniform samples were generated and the number that fell in each bin was then proportional to the associated probability of the respective bins.</p>

<p>Again, both methods produced a sort of background noise which looks like a uniform distribution on top of the correct distribution.</p>

<p>As, a side note, it seems like I ought to be able to exploit the form of this expression for efficient sampling, but I haven't made use of the spectral decomposition at all in my attempts.</p>
",<probability>
"<p>I am reading a book that defines the Malliavin derivative $D_tF$ as follows:
If  </p>

<ol>
<li><p>$F = \sum_{n=0}^{\infty} I_n(f_n)$ is the Wiener Chaos expansion.</p></li>
<li><p>$F$ is in the brownian filtration and $F \in L^2(P)$.</p></li>
<li><p>$\sum_{n=0}^{\infty} n n! ||f_n||_{L^2([0,T]^n)}^2 &lt; \infty$</p></li>
</ol>

<p>Then
 $D_tF = \sum_{n=1}^{\infty} nI_{n-1}(f_n(.,t))$.</p>

<p>My question is why do we need 3 to be so strong. It seems that in the theory the $n!$ is never used. Is it defined this way in order to have it match the malliavin derivative constructed using other methods?</p>

<p>If it is used in the theory can you please tell me the point where it becomes important.</p>

<p>Thank you</p>
",<probability>
"<p>This is for the GRE:</p>

<p><code>A fair coin is tossed once and a fair die with sides numbered 1, 2, 3, 4, 5, and 6 is rolled once. Let A be the event that the coin toss results in a head. Let B be the event that the roll of the die results in a number less than 5. What is the probability that at least one of the events A and B occurs?</code></p>

<p>How do you solve this to come up with $\frac{5}{6}$ (correct answer)? Probabilities are not easy. Any tip you can give is appreciated. Thanks.</p>
",<probability>
"<p>I have a sequence of iid Bernoulli random variables $X_1, X_2, \dots, X_n$ with $Pr(X_j) = p$, and I'd like to know (a lower bound on) the probability that there exists a consecutive subsequence of length $k$ of them (e.g. $X_i, X_{i+1},...X_{i+k-1}$) such that every random variable in the subsequence takes on the value 1.</p>

<p>Even though the individual variables are independent from each other, the subsequences aren't, and that's where I'm stuck.</p>

<p>Letting $E_{n,k}$ be the event that there is a subsequence of $k$ variables all set to 1: here are the first few terms.</p>

<p>For $k = 2, n=2, Pr(E_{n,k}) = p^2$</p>

<p>For $k = 2, n = 3, Pr(E_{n,k}) = p^3 + 2p^2(1-p)$</p>

<p>For $k = 2, n = 4, Pr(E_{n,k}) = p^4 + 4p^3(1-p) + 3p^2(1-p)^2$</p>

<p>For $k = 2, n = 5, Pr(E_{n,k}) = p^5 + 5p^4(1-p) + 9p^3(1-p)^2 + 4p^2(1-p)^3$</p>

<p>There's simple recurrences for the first, second, and last terms of this sequence, but I can't see how to generate the middle terms, much less sum everything up in a neat way.</p>
",<probability>
"<p>An extra sum of squares $SSR(X_p|X_1,...X_{p-1})$, assuming that no pair of predictor variables are perfectly correlated, measures the marginal reduction in the error sum of squares. Eventually one can view an extra sum of squares as measuring the measuring the marginal increase in the regression sum of squares when one or several predictor variables are added to the regression model.</p>

<p>Assuming the number of observational data $n$ is lager than the number of predictor variables $p$.</p>

<p>Can I have a rigourous proof that $SSTO=\sum_{i=1}^n (Y_i-\bar{Y})^2\geq SSR(X_p|X_1,...X_{p-1})\geq0$ ?</p>

<hr>

<p>Applying LSE</p>

<p>$SSE(X_1,...,X_{p-1})=\sum(Y_i-b_0-b_1X_{1,i}-...-b_{p-1}X_{p-1,i})^2=\sum(Y_i-b_0-b_1X_{1,i}-...-b_{p-1}X_{p-1,i}-0\times X_{p,i})^2\geq\sum(Y_i-b_0'-b_1'X_{1,i}-...-b_{p-1}'X_{p-1,i}-b_p'X_{p,i})^2=SSE(X_1,...,X_{p-1},X_p)$</p>

<p>$SSR(X_p|X_1,...X_{p-1})=SSE(X_1,...,X_{p-1})-SSE(X_1,...,X_{p-1},X_p)\geq 0$</p>

<p>Proved.</p>
",<probability>
"<p>Given an urn with a number of two objects, $A$'s and $B$'s, if I am to find the probability of the $i$-th object drawn without replacement to be an $A$ would I need to compute all the different ways that $i-1$ objects can first be drawn?</p>

<p>For example if $i = 3$ would I need to first compute the probability that the first two objects drawn are $A$ then $A$, $A$ then $B$, $B$ then $A$, and $B$ then $B$. Then find the probability of the 3rd object drawn being $A$ in each of these instances and sum all 4 ways that the 3rd objects is $A$? My particular $i$ is 6 so I want to make sure this is correct before actually doing it.</p>
",<probability>
"<p>Question: An urn contains n red and m blue balls. They are withdrawn one at a time until a total  of r; r · n, red balls have been withdrawn. Find the probability that a total of k balls are  withdrawn.</p>

<p>My attemp:</p>

<pre><code>let A=event that first k-1 draws will get r-1 red balls
let B=event that last (kth) draw will get rth red ball
P(kth draw is the rth red ball)= P(A AND B) = P(A)xP(B)

P(A)=  # of  ways to draw r-1 red balls in k-1 trail
     -----------------------------------------------------
               # of ways to draw k-1 balls
</code></pre>

<p>So I got stuck; I was thinkg that </p>

<pre><code> RBBB *R is one way to achieve P(A AND B)
 BBBR *R is another way. 
</code></pre>

<p>but the solution  for P(A) is:</p>

<pre><code> solution: P(A)= nC(r-1) x mC(n-r) x 1/(n+m)C(k-1)
</code></pre>

<p>This solution is like assuming</p>

<pre><code> R1B1B2B3 * R2       is one way
 R2B4B5B6 * R3       is another way   
</code></pre>

<p>Because they use nC(r-1)x mC(n-r).  Any ideas?</p>
",<probability>
"<p>Set of non negative weights $w_j$, set of non negative i.i.d. random variables $X_j$ and $f(y)$ is a decreasing nonnegative function in $y$. </p>

<p>I want to claim that: </p>

<p>if $\sum w_i&lt;\sum w^{\prime}_i$,then $\mathbb E f(\sum w_iX_i)&gt;\mathbb E f(\sum w^{\prime}_iX_i)$.</p>

<p>This seems intuitive but I would like a formal proof.</p>

<p>My attempt: The next line holds if all r.vs are coupled to a common r.v. $X_i\sim U$. But not sure if we can do that.
$$\sum w_iX_i&lt; \sum w^{\prime}_iX_i,$$
We have  $$f(\sum w_iX_i)&gt; f(\sum w^{\prime}_iX_i),$$ almost surely.</p>

<p>And we take expectation $$\mathbb E f(\sum w_iX_i)&gt;\mathbb E f(\sum w^{\prime}_iX_i). \;\;\;\;\;\;\; (a)$$  Therefore the result holds a.s.</p>

<p>Is this proof correct? If wrong where is the mistake?</p>

<p>The function $f$ I have is $\log(1+\frac{1}{y})$ and $X_i$ are exponential r.vs.</p>
",<probability>
"<p>I know this isn't that hard, but I have been looking and I don't know how to solve it.</p>

<p>The number of students whose grade is higher than 1149 is 44, and the total of students is 135. If the question where only for 1 random student, it would be 32%, but I don't know how it is for more than that.</p>

<p>Please help me.</p>
",<probability>
"<p>Let $X_1,X_2,\dots$ be I.i.d. And $S_n = X_1+X_2+\dots +X_n. $Prove if $S_n/n \to 0$ in probability then $(\max_{1\leq m \leq n}S_m)/n \to 0$ in probability.</p>

<p>I know the idea and there is a detail I don't know how to prove. 
If |Sn-Sk|$\leq$|Sn|+|Sk| and Sn/n limits to 0 in probability, how can I prove that minP(|Sn-Sk|$\leq n\delta$)$\to $ 1 as $ n \to \infty$ ($0\leq k \leq n$)</p>
",<probability>
"<p>The spectrum of a discrete random variable X consists of the points 1, 2, 3,..., n  and its probability mass function (pmf) fi = P(X = i) is proportional to 1/i(i+1). Determine the distribution function of X. Further, compute P(3 &lt; X &lt;= n).</p>
",<probability>
"<p>If four dice are tossed, find the probability that exactly 3 fives will show ( answer to the nearest thousandth in the for 0.xxx)?</p>
",<probability>
"<p>Given $P(A|B)= 0.5, P(B|A)=0.4,$ and $P(A) + P(B) = 0.9$ what is $P(A)=$ ?.</p>
",<probability>
"<p>The number of total outcomes of an experiment are $25$. If $A$ and $B$ are two non-empty independent events of the experiment such that outcomes in favour of event $A$ are $15$, then the minimum number of outcomes in favour of event $B$ can be? </p>
",<probability>
"<p>Assume we have a lottery with payouts $(2,3,5)$. So if you buy a ticket you can win a pot which will payout your ticket price multiplied by one of those numbers.<br>
The organizer expects a margin profit of $4\%$ from all tickets. So if the player plays with $1\$$ the mathematical expectation of outcomes will be $0.96$.</p>

<p>In my <a href=""http://math.stackexchange.com/questions/1470264/lottery-payout-with-organizer-margin"">previous question</a> I asked almost the same question thinking that my approach calculating the probability of each payout was right. I wanted the probability of each payout to be proportional to the payout $P_i \propto \frac1{x_i}$ (where $P_i$ is the probability of winning the $i$ payout and $x_i$ is the value of that payout). So I got an answer with these probabilities $(0.192,0.096,0.0576)$ and this explanation $$
\begin{align}
\sum P_i \cdot x_i &amp;= 0.192 \cdot 2 + 0.096 \cdot 3 + 0.0576 \cdot 5 \\
&amp;= 0.96
\end{align}
$$
Can someone please explain or help me to understand how these probabilities can be counted and the idea behind that. </p>

<p>My previous method which I used and assume was wrong looked like this.<br>
If $P_i$ is the probability of payoff $i$ and there are $N$ total positive payoffs that $P_ii = 0.96/N$. Then you get the formula that $P_i = \frac{0.96}{iN}$, which would look like this in my question <br>
$P_2 = \frac{0.96}{2*3}$ where $3$ is the number of positive payouts. <br>
Thank you</p>
",<probability>
"<p>The first $12$ natural numbers are given. Two distinct numbers are selected. What's the probability that their sum is divisible by $3$? </p>

<p>This looks very easy. I know answer is $1/3$ but in spite of knowing permutations and combinations I had to find favourable cases. The numbers were few. What if we were given  question like probability that two numbers natural less than $100$ are selected and their sum is divisible by $3$? Any smart guy wouldn't go on counting. There has to be some way out which I am missing. Can you guys tell what's the best way? Thanks!</p>
",<probability>
"<p>I have 3 (not independent) events $A, B, C$ and I know everything about how any two of them correlate. For example, I know:</p>

<p>$$ P[A], P[B], P[C], P[A,B], P[A,C], P[B,C], P[A|B], P[A|C], P[B|C], P[B|A], ...$$</p>

<p>Is there any way to use this information to calculate a correlation for the three of them, i.e.</p>

<p>$$    P[A,B,C] \text{ or } P[A,B|C]  \text{ or } P[A|B,C] $$</p>

<p>A numerical algorithm would also be fine if there isn't an exact formula.
If it is not possible, is there a way to get a confidence interval for these values?</p>
",<probability>
"<p>I have a $64$-card deck, with $4$ colours - red, green, blue, yellow, $4$ numbers - $1,2,3,4$ and $4$ letters A,B,C,D. So an example card could be yellow2D.</p>

<p>I was attempting to calculate the probability of $4$ of a kind, i.e. $4$ green cards, $4$ $3$'s, etc... having been dealt $k$ cards.</p>

<p>But I got all stuck!</p>
",<probability>
"<p>Say I roll a $6$ sided dice and I want to roll a $6$. what is the probability that I will have rolled the number I want after $6$ rolls?</p>

<p>I have been using this: $\displaystyle1-\left(1-\frac{1}{x}\right)^y$</p>

<p>where $x$ is the number of sides and $y$ is the amount of rolls, so it would be $\displaystyle1-\left(1-\frac{1}{6}\right)^6$ for rolling a specific number in $6$ rolls, which is $\approx66.5\%$ is this the correct way of calculating the probability of something like this, if not what is the proper way?</p>

<p>i'm not really sure why that formula works(if it does) so some elaboration on that would be nice.</p>

<p>sorry for lack of technical language</p>

<p>thanks in advance</p>
",<probability>
"<p>I have a population C of candidates C1..Cn<br>
An event will occur to Ci with unknown probability Pi (Pi are independent)</p>

<p>The population is divided into disjoint sets S1..Sm
For each sub set Si, P(Si) is known.</p>

<p>(right now, it the probability of an event happening to ANY member of Si, but it can
also be probability of an event happening to all members if it makes analysis easier..)</p>

<p>I am given a set of population subsets R1.. (independent, possibly overlapping subsets of C)</p>

<p>For each Ri, what can i say about P(Ri) ??<br>
Even better, are there any unions of subsets Ri U Rj U Rk for which the computation of P(union) is more accurate ?</p>

<p>Any ideas and pointers are helpful<br>
(I have a Bsc/Msc in CS, Math but i i am really rusty)</p>
",<probability>
"<p>I have a fairly simply question which I am not sure about. A 3 digits number is being chosen by random (100-999). What is the probability of getting a number with two identical digits ? (like 101). Thank you !</p>
",<probability>
"<p>Standard 52 cards deck, calculate the probability of drawing any 4 AND after that any of clubs. </p>

<p>My first intuition is this, there are two possibilities: a) drawing a non clubs 4; or b) drawing a clubs 4, after that you just deduct the card from the deck, so you get:</p>

<p>a) 3/52 * 13/51
b) 1/52 * 12/51</p>

<p>But in the classroom this was resolved as:</p>

<p>a) 4/52 * 13/51
b) 4/52 * 12/51</p>

<p>I don't understand the rationale behind this, is it right/wrong? </p>
",<probability>
"<p>Let $X$ be a random variable. let 
\begin{align*}
Y=\alpha_1+\alpha_2 X
\end{align*}
where $\alpha_1$ and $\alpha_2$ are parameters.</p>

<p>Now let 
\begin{align*}
Z=\hat{\alpha}_1+\hat{\alpha}_2 X
\end{align*}
where $\hat{\alpha}_1$ and $\hat{\alpha}_2$ are estimates of parameters.</p>

<p>As $n \rightarrow \infty$ , $\hat{\alpha}_1 \overset{p}{\to} \alpha_1$ and $\hat{\alpha}_2 \overset{p}{\to} \alpha_2$. </p>

<p>Now can I say $Y\overset{p}{\to}Z$ ($Y$is becoming $Z$ asymptotically, someone critize me by saying that it is strange to say a variable converge in probability to a variable) or $Y\overset{d}{\to} Z$ (in distribution)?</p>
",<probability>
"<p>A random number between $1$ and $100$ is generated every second.
What would be the average waiting time for a specific number ($1$ for instance) to be generated?
Probability distribution is uniform.
Each number is generated independently of the others.</p>
",<probability>
"<p>If $E(X^2)=1$ and $E(|X|)\ge a &gt;0$, then $P(|X|\ge\lambda a)\ge (1-\lambda)^2a^2$ for $0\le \lambda \le 1$.</p>

<p>I can see from the well known inequality $E(|X|) \le E(|X|^2)^{1/2}$ that it must be the case that $a\le 1$. But what to do next I'm not sure.</p>
",<probability>
"<p>If we have 3 players, playing extreme RPS game like image below:</p>

<p><img src=""http://i.stack.imgur.com/OZtXR.jpg"" alt=""enter image description here""></p>

<p>How much tie probabilities?</p>

<p>Thanks</p>
",<probability>
"<p><strong>This is my problem</strong></p>

<p>My problem is modeled by a basic Bayesian Network with only two layers. So I have parent and child nodes but the children has no children. Essentially a bipartite graph. The children depend on one or more of the parents and these edges/relations have a probability. The model is also extended by completing the bipartite graph with low probability edges to account for noisy data. So in the finished model each child $(C_x)$ has a dependency on each parent $(P_x)$.</p>

<p>What I want to be able to do is to infer the most probable solution by maximizing,</p>

<p>$$ max_{P_1,P_2,\dots,P_n}(P(P_1,P_2,\dots,P_n|C_1,C_2,\dots,C_m)) $$ 
where $ C_i \in \{0,1\} $, $ P_j \in \{0,1\} $. For a given observation on the state of the children.</p>

<p>So for a observation $O=\{C_1=0, C_2=1, C_3=1\}$</p>

<p>I want to calculate
$$P(P_1=1,P_2=0,P_3=0|C_1=0, C_2=1, C_3=1)$$
$$P(P_1=0,P_2=1,P_3=0|C_1=0, C_2=1, C_3=1)$$
$$P(P_1=1,P_2=1,P_3=0|C_1=0, C_2=1, C_3=1)$$
and so on.</p>

<p><strong>Here is how I try to solve it</strong></p>

<p>I want to be thorough so I will explain how I try to solve this. I'm not sure that I'm doing it right. Please point out to med if I'm doing this wrong.</p>

<p>What I do first is I complete the conditional distribution tables for the child nodes. From the model I have the CPT as for a child node $C_x$ something like this, the probabilites are chosen to make the example easy.</p>

<p>\begin{array}{ l l l|l l }
P_1 &amp; P_2 &amp; P_3 &amp; P &amp; \lnot P \\
  \hline
	0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 \\
	0 &amp; 0 &amp; 1 &amp; 0.1 &amp; 0.9 \\
	0 &amp; 1 &amp; 0 &amp; 0.4 &amp; 0.6 \\
	1 &amp; 0 &amp; 0 &amp; 0.2 &amp; 0.8 \\
\end{array}</p>

<p>And I complete this table by calculating</p>

<p>\begin{array}{ l l l|l l }
P_1 &amp; P_2 &amp; P_3 &amp; P &amp; \lnot P \\
  \hline
	0 &amp; 1 &amp; 1 &amp; (1- \lnot P) &amp; 0.9 \times 0.6 \\
	1 &amp; 0 &amp; 1 &amp; (1- \lnot P) &amp; 0.9 \times 0.8 \\
	1 &amp; 1 &amp; 0 &amp; (1- \lnot P) &amp; 0.6 \times 0.8 \\
	1 &amp; 1 &amp; 1 &amp; (1- \lnot P) &amp; 0.9 \times 0.6 \times 0.8 \\
\end{array}</p>

<p>And then I calculate the joint conditional probability by doing something like</p>

<p>$$P(P_1=1,P_2=0,P_3=0|C_1=0, C_2=1, C_3=1) = $$
$$P(P_1=1,P_2=0,P_3=0|C_1=0) \times$$ 
$$P(P_1=1,P_2=0,P_3=0|C_2=1) \times$$
$$P(P_1=1,P_2=0,P_3=0|C_3=1)$$</p>

<p>Basically I'm not convinced that the last step is correct. <strong>Any help or comments is greately appreciated!</strong></p>

<p>Yes, this is school related but it is not regular homework. It is a part of a thesis project I'm doing at a company.</p>

<p><strong>Graph update on request</strong></p>

<p><img src=""http://i.stack.imgur.com/eX6iS.png"" alt=""Graph""></p>

<p>Here is a graph example of my model. The regular edges are direct dependencies and the dotted edges are noisy or guess edges added to allow for noisy data. Guess edges has a low probability of $p = 0.0001$ and the regular edges all have the probability $(1-p)$. In my problem I do not care for the probability of individual events like $P(L_1)$ or $P(C_2)$, so they are assumed to be $1$. I make an observation on the state of the variables $C_1...C_n$ and given the graph model above I want to infer to most plausible cause. The possible causes  are $P_1...P_n$ or a combination of them that is most likely. Like I stated earlier on the example observation above.</p>

<p>Here is an example of the basic truth tables for this graph.
\begin{array}{ l|l l }
C_1 &amp; T &amp; F \\
  \hline
	P_1 &amp; 0.9999 &amp; 0.0001 \\
	P_2 &amp; 0.9999 &amp; 0.0001 \\
	P_3 &amp; 0.9999 &amp; 0.0001 \\
\end{array}</p>

<p>\begin{array}{ l|l l }
C_2 &amp; T &amp; F \\
  \hline
	P_1 &amp; 0.0001 &amp; 0.9999 \\
	P_2 &amp; 0.9999 &amp; 0.0001 \\
	P_3 &amp; 0.9999 &amp; 0.0001 \\
\end{array}</p>

<p>\begin{array}{ l|l l }
C_3 &amp; T &amp; F \\
  \hline
	P_1 &amp; 0.0001 &amp; 0.9999 \\
	P_2 &amp; 0.0001 &amp; 0.9999 \\
	P_3 &amp; 0.9999 &amp; 0.0001 \\
\end{array}</p>

<p>Does that make it any clearer?</p>
",<probability>
"<p>We have a fair dice that can produce <code>n</code> different numbers. How many times should we roll the dice to see every number at least once with probability <code>p</code>?</p>

<p>Not a homework, just interesting. Tried to solve myself but with no luck. </p>

<p>I think it could be sort of coupon collector problem, but I can't get exact formula.</p>
",<probability>
"<p>If one word can be at most 63 characters long. It can be combination of :</p>

<ul>
<li>letters from a to z</li>
<li>numbers from 0 to 9</li>
<li>hyphen - but only if not in the first or the last character of the word</li>
</ul>

<p>I'm trying to calculate possible number of combinations for a given domain name. I took stats facts here :</p>

<p><a href=""http://webmasters.stackexchange.com/a/16997"">http://webmasters.stackexchange.com/a/16997</a></p>

<p>I have a very poor, elementary level of math so I've got this address from a friend to ask this. If someone could write me a formula how to calculate this or give me exact number or any useful information that would be great.</p>
",<probability>
"<p>Can someone please give me a reference  to an (simple, realworld, i.e. not constructed) example of a discrete probability space such that there are three events in it that are pairwise independent but all three together are not independent (although I wouldn't mind, if someone would give me the example as an answer).</p>
",<probability>
"<p>Among $t = 60$ lottery tickets there are $w = 20$ prizes. We buy $b = 6$.
What is the probability that $g$ tickets will win, with $g=2$? Generalize this
to arbitrary numbers $t,w, b, g$.</p>
",<probability>
"<p>I want to create a model that tries to predict a user's behavior based on the random walks of similar users. The problem is similar to Netflix's recommendation challenge. One of the popular solutions was to use singular value decomposition to find movies that user would most likely like to see. </p>

<p>My question is more like this: what genre of a movie would a user like to see on a particular day? You move to each state with a probability, and one state could be ""watching no movie"". Does it make sense or even is it even feasible to approach the problem as a Markov process? How has this been done before?</p>

<p>I took only one class on Markov chains in college. 
Thanks.</p>
",<probability>
"<p>Given a poll, where $N$ people were polled, and $n_i$ people voted for party $i$, so that: $$\sum{n_i} = N$$
If there are M parliament seats in total we can expect: 
$$m_i = M\cdot\lim_{N\rightarrow\infty} (n_i/N)$$
To be the number of parliament seats party $i$ will have. </p>

<p>My question regards the error involved in this prediction: </p>

<p><strong>What is it's error distribution and what is it's variance?</strong> </p>

<p>If the number of seats were not finite, I'd say that the distribution would be Poisson and each poll value should be $n_i \pm \sqrt{n_i}$, but since the sum is given, It would seem the errors must be correlated in some way.</p>

<p>Any ideas?</p>
",<probability>
"<p>Five people check identical suitcases before boarding an airplane. At the baggage claim, each person takes one of the five suitcases at random. What is the probability that every person ends up with the wrong suitcase?</p>

<p>I think I need to use the principle of inclusion exclusion to solve this but I'm not quite sure how.</p>
",<probability>
"<p>I am examining Bayes' Theorem, and wondering about the alternative interpretations of ~A, as being:</p>

<ul>
<li>not A, &not; A</li>
<li>everything but A, &forall;-A</li>
</ul>

<p>And how this will affect the use of probabilities.</p>

<p>So, this is not so much a question about Bayes' Theorem, and more to do with how this split interpretation effects the numbers.</p>
",<probability>
"<p>A company that sells annuities must base the annual payout on the distribution of the length of life of the participants in the plan. Suppose the distribution of male participants' lifetimes is normally distributed with a mean of 68 years and a standard deviation of 5 years. Let the random variable X represent the lifetime of a male participant. What is the probability that a male participant would die between 64 and 65 years old?</p>
",<probability>
"<p>X, Y are both random variables of uniform distribution, $0\le\ X \le\ 3$, $0\le\ Y \le\ 4$, then what is the probability of $X \lt Y$? </p>
",<probability>
"<p>I'm working on a problem from Chow and Teicher's book on Probability Theory, page 123, #6(ii):</p>

<p>If $X_n, n\geq 1$ are i.i.d., $\mathcal{L_1}$ r.v.s, then $\sum (X_n / n)$ converges a.c. if $E|X_1|log^+ |X_1| &lt;\infty$ and $EX_1 = 0$.</p>

<p>The most relevant theorem that I've been thinking of using is one that says if $X_n$ are i.i.d. and $\mathcal{L_p}$ for $0&lt;p&lt;2$, then $\sum \left(X_n / n^{1/p} - E\left(\dfrac{X_nI_{\{|X_n|\leq n^{1/p}\}}}{n^{1/p}}\right)\right)$ converges a.c.; since, in this case, $p =1$, it would suffice to show that the series $\sum E\left(\dfrac{X_nI_{\{|X_n|\leq n\}}}{n}\right)$ converges a.c. to complete the exercise. However, I'm having trouble incorporating the $E|X_1|log^+ |X_1| &lt;\infty$ condition. I see how $EX_1 = 0$ implies that the summands of the series satisfy the following: $E\left(\dfrac{X_nI_{\{|X_n|\leq n\}}}{n}\right) = -E\left(\dfrac{X_nI_{\{|X_n|&gt; n\}}}{n}\right)$, but unfortunately I've been thus far unable get anything resembling a logarithmic series by manipulating summands here.</p>

<p>Perhaps I'm missing something obvious? Any help is greatly appreciated. </p>
",<probability>
"<p>I've been trying to understand the following:</p>

<blockquote>
  <p>The distribution of two continuous random variables is given by
  $$f_{X,Y}(x,y)=\frac{3}{7}x\space\space 1\le x\le 2,0\le y\le x$$and $0$ otherwise. What is the
  marginal distribution of $Y$ when $0\le y\le 1$?</p>
</blockquote>

<p>My question is, how do I choose the limits for the integral
$$\int_{-\infty}^{\infty}f_{X,Y}(x,y)dx$$</p>

<p>In the school solution they  did
$$f_Y(y)=\int_{1}^{2}\frac{3}{7}xdx=\frac{9}{14}$$</p>

<p>Why are those the limits? What if I wanted to calculate the marginal distribution of $Y$ when $1\le y\le$ 2? Any thumbs rules?</p>

<p>Thanks!</p>
",<probability>
"<p>So there are 480 squares and 99 mines on the advanced level of minesweeper. It got me thinking, what would be the chances of winning the game randomly clicking each square? So not being influenced by numbers and without it doing any multi openings, so you would need to click 381 boxes (I think) :)</p>
",<probability>
"<p>A bag contains 5 red and 7 black balls. Second bag contains 4 blue and 3 green balls. 1 ball is drawn from each bag. Find the probabilty for
1 red and 1 blue ball.</p>

<p>The answer is 5/21
But don't know the way to get it. 
Plzz help.</p>
",<probability>
"<p>Your (honest) opponent choose a random number from 1 to 13 inclusive. You have to guess the number, and you win if the guess is correct. If not, your opponent either reduces the number chosen by one or increases it by 1, and you guess again.</p>

<p>The question is, what is the minimum # of attempts necessary to guarantee a win for you.</p>

<p>I am not able to get a handle on the problem.</p>

<p>Also, (a new variant just thought of), how many guesses should be allowed for a fair or ""nearest to fair"" game ?</p>
",<probability>
"<p>Suppose I toss a fair coin 10 times. What is the probability that there is a run of at least 4 consecutive heads?</p>

<p>An approach would be to use the Principle of Inclusion-Exclusion on the events $E_i$ where 4 heads occur in positions $i,i+1,i+2,i+3$, where $1\leq i\leq 7$. But this results in a big calculation. </p>

<p>On the other hand, I found the generalization <a href=""http://math.stackexchange.com/questions/59738/probability-for-the-length-of-the-longest-run-in-n-bernoulli-trials"">here</a>, but I think for this problem (with small values $10$ and $4$) maybe there is an easier way to compute the desired value. What would be an easier way?</p>
",<probability>
"<p>I came across the following probability problem:</p>

<p>Start with $1$ black ball and $1$ white ball in a box. At each step, we will put in a new ball. If there are $a$ black balls and $b$ white balls, we put in a black ball with probability $\dfrac{a}{a+b}$ and a white ball with probability $\dfrac{b}{a+b}$. We do this until there are $n$ balls ($n\geq 2$). Prove that the probabilities that there are $1,2,\ldots,n-1$ black balls are all equal.</p>

<p>This problem is trivial by induction on $n$, the total number of balls. I wonder, however, if there is an intuitive way to interpret the result, without the use of induction?</p>
",<probability>
"<p>I would like to ask <strong>how can we derive PGF of any multivariate distribution</strong>?
and can anyone give <strong>an example</strong> of deriving the PGF of a multivariate distribution?
That will be great.
Thanks advance.</p>
",<probability>
"<p>I have stumbled upon many questions, and one of the weaknesses is the ability to test if the concept is distinguishable or not. For example this: </p>

<blockquote>
  <p>Nine delegates, three each from three different countries, randomly select chairs at a round table that seats nine people. Let the probability that each delegate sits next to at least one delegate from another country be $\frac{m}{n}$, where $m$ and $n$ are relatively prime positive integers. Find $m+n$.</p>
</blockquote>

<p>Take the race: $AAA$ Then are the people distinguishable or not? Is it: $A_1, A_2, A_3$ or not? I ask this because of cyclic shifts. I asked this question <a href=""http://math.stackexchange.com/questions/1399651/the-probability-that-each-delegate-sits-next-to-at-least-one-delegate-from-anoth"">Here.</a>. But drhab said they are not, but he uses the concept of $AaA$, or $AAa$ or $aAA$? Is there something with the chair being distinguishable or what is the whole issue exactly?</p>
",<probability>
"<p>Sorry for the topic being weird, but as a person not too good at math I would like to know whether the following argument is mathematically valid:</p>

<blockquote>
  <p>If $50$% of pregnancies are aborted, is it valid to say that a fetus has $50$% chance to be aborted? Considering that criteria for abortion is random. </p>
  
  <p>Is the following sentence that I heard, mathematically valid?
  ""$50$% of pregnancies are aborted, therefore if you are a fetus you have $50$% chance of not coming out alive""</p>
</blockquote>

<p>Thanks in advance.</p>
",<probability>
"<p>Let $N_t$ be a Poisson process and $S_{N_t}=X_1+...+X_{N_t}$. </p>

<p>Let $A_t=t-S_{N_t}$ and $B_t=S_{N_t}-t$</p>

<p>1) Show $P(B_t \geq x \ \text{and}\ A_t \geq y)=\frac{1}{E(X_1)} \int_{x+y}^{\infty} P(X_1 \geq u)du\:\:$ with $x,y,t \geq 0$</p>

<p>2) Deduce $A_t$ is independent of $B_t$</p>

<p>For the question 2) I don't know but for the question 1) I tried:</p>

<p>$P(B_t \geq x \ \text{and}\ A_t \geq y)=P(X_{N_t}-A_t \geq x | A_t \geq y)P(A_t \geq y)=P(X_{N_t} \geq x+y)P(A_t \geq y)=P(X_1 \geq x+y)P(A_t \geq y)$</p>

<p>However after I don't know:
$P(X_1 \geq x+y)=\int_{x+y}^{\infty}F'(u)du$ and $P(A_t \leq y)=P(S_{N_t-1} \leq t-y)$</p>

<p>Thank you</p>
",<probability>
"<p>Let's say I have $3$ events with probabilities $P(A) = 0.5, P(B) = 0.5$ and $P(C)= 0.5,$ and I need to find if </p>

<p>$$P(A \cap B \mid C) = P(A \mid C)P(B \mid C)$$</p>

<p>I am tying to prove this by expanding the formula above to:</p>

<p>$$P(A \cap B \mid C) = \frac{P(A \cap C)}{P(C)}\frac{P(B \cap C)}{P(C)}$$</p>

<p>Is this a correct assumption?</p>
",<probability>
"<p>Is the following statement true or not?</p>

<blockquote>
  <p>Let $X$, $Y$, $Z$ be $3$ events in the same sample space such that $p(X)$, $P(Y)$, $p(Z) &gt; 0$ and every pair of these events is independent. Then $p(X \cap Y \cap Z) &gt; 0$.</p>
</blockquote>
",<probability>
"<p>Abe and Bernard are dealt five cards each from the same $52$ card deck. Let $A$ be the event that Abe gets a flush (five cards of the same suit) and $B$ be the event that Bernard’s five cards are of pairwise different kinds (i.e. pairwise independent). Are $A$ and $B$ independent?</p>

<p><strong>Thoughts.</strong> Is $P(A) = P(A|B)$? In other words, is the probability of $A$ the same as the probability of $A$ given that $B$ occurred?</p>
",<probability>
"<p><strong>Three people have been exposed to a certain illness. Once exposed, a person has a 50-50 chance of actually becoming ill.</strong></p>

<p>a) What is the probability that exactly one of the people becomes ill?</p>

<p>I am a bit unsure, how to solve this question.</p>
",<probability>
"<p>I am looking at the proof of convergence in probability implying convergence in distribution. The proof begins by stating that if $X_n \leq x$ then either $ X \leq x + \epsilon $ or $ |X_n - X| &gt; \epsilon $. I can't quite see this implication;</p>

<p>$ X_n \leq x \Rightarrow X \leq x + \epsilon$ or $ X &gt; x + \epsilon$</p>

<p>$\Rightarrow X \leq x + \epsilon$ or $ X - X_n \geq X - x &gt; \epsilon $</p>

<p>but how do I obtain $X_n - X &gt; \epsilon$ ? </p>
",<probability>
"<p>Do you have an idea how I could model the following process somehow as a sum of independent indicator random variables?</p>

<p>I have given a grid of size $n \times n$ for $n \rightarrow \infty$. </p>

<p>Now I color each point of this grid uniformly at random with one out of $k$ colors, where $k=\mathcal{O}(1)$. </p>

<p>I am interested in the probability that none of the $3 \times 3$-subgrids in this grid are monochromatic (monochromatic=all 9 points have the same color) and should show this probability is at most $e^{- \Omega(n^2)}$.  </p>

<p>My task is explicitly to model this such that I can use Chernoff bounds. 
Chernoff bounds, in the context we had it, can be applied if we have a sum $X:=\sum_{i=1}^m X_i$ of independent indicator random variables $X_i \sim Be(p_i)$.</p>

<p>My problem is that clearly the different $3 \times 3$ grids are <em>not</em> independent, so the easy approach to state $X=\sum_{s \in S} X_s$ where $S$ is the set of all $3\times 3$ subgrids and $X_s$ is $1$ if $s$ is a monochromatic grid, and calculate $P[X=0]$ does not work. </p>

<p>Would be very happy about any hint. 
Thank you very much!</p>
",<probability>
"<blockquote>
  <p>Let $k\leqslant n$ denote two positive integers, $A$ an $n \times k$ matrix with $A'A = I_k$, and $X$ and $Y$ two independent random variables on $\mathbb R^n$, each rotationally invariant (that is, their distributions do not change under the orthogonal transformations). 
  Write $X' = (U',W')$ and $Y' = (V',Z')$ with $U$ and $V$ on $\mathbb R^k$ and $W$ and $Z$ on $\mathbb R^{n-k}$.</p>
  
  <p>Prove that $(A'X, A'Y)$ has the same distribution as $(U,V)$. </p>
</blockquote>

<p>I know that by the Cramer-Wold theorem, for rotationally invariant random variables if we have a linear functional $t$ then $t'X$ has the same distribution as $|t|U$. Also $U$ and $W$ are rotationally invariant each in its own right in this case. I think these two assumptions should be used somewhere in the proof I just don't know how to start, because to me this is basically saying that the joint distribution of $(X, Y)$ is completely determined by the distribution of their first coordinate which is similar to Cramer-Wold except that the factor $|t|$ here is one so somehow ""length"" of $A'$ should be $1$ or it should appear as $A'A$ which is the identity. 
Any help would be appreciated. </p>
",<probability>
"<p>A factory has produced n robots, each of which is faulty with probability $\phi$. To each robot a test is applied which detects the faulty (if present) with probability $\delta$. Let X be the number of faulty robots, and Y the number detected as faulty.</p>

<p>Assuming the usual indenpendence, determine the value of $\mathbb E(X|Y)$.</p>

<p>Please explain me the result in detail or give me a good hint pls, since I am very new to this concept (of conditional expectation).</p>

<p>Thanks!</p>
",<probability>
"<p>I'm trying to solve a question from Pathria's statistical mechanics textbook (10.21) but it is more math oriented.</p>

<p>Show that, for a general Gaussian distribution of variables $u_j$ , the average of the exponential of a linear combination of the variables obeys the relation:</p>

<p>$\left\langle \exp\left(\sum_j a_j u_j\right) \right\rangle=\exp\left(\dfrac{1}{2}\left\langle\left(\sum_j a_j u_j\right)^2 \right\rangle \right)$</p>

<p>I'm not entirely sure how to write this; I was doing Taylor series expansions of exponentials and I could solve it easily if it were a standard normal (mean=0).  I believe the linear combination of normal variables is normal, so this seems to have something to do with the log-normal distribution?  </p>

<p>I don't need a full solution; just a hint to go forward.</p>
",<probability>
"<p>A decision making problem will be resolved by tossing $2n + 1$ coins. If Head comes in majority one option will be taken, for majority of tails it’ll be the other one. Initially all the coins were fair. A witty mathematician replaced $n$ pairs of fair coins with $n$ pairs of biased coins, but in each pair the probability of obtaining head in one is the same the probability of obtaining tail in the other. Will this cause any favor for any of the options available? Justify with logic</p>
",<probability>
"<p>Tv company receives 90 job application. Information about applicants:</p>

<pre><code>--------------------------------------------
     | have bachelor | dont have bachelor |
     |     (B)       |     (B) negation   |
---------------------------------------------
exp  |     18        |          9         |
(D)  |               |                    |
---------------------------------------------
no exp |      36     |         27         |
(D)     |            |                    |
negation|------------|--------------------|
</code></pre>

<p>Calculate probability for: <code>P(D/B) and P(B neg/ D neg )</code></p>

<p>answers:  <code>P(D/B) =&gt; 1/3</code>,  <code>P(B neg/ D neg ) =&gt; 3/7</code></p>

<p>I tried to do this-></p>

<p>D/B = D - DnB = 18+9 - 18 = 9</p>

<p>P(D/B) = 9/90 = 1/10 (wrong)</p>

<p>and</p>

<p>D/B = B - BnD = 54 - 18 = 36</p>

<p>P(D/B) = 36/90 = 4/10 (wrong)</p>

<p>I have no idea where I am wrong. How to figure out the answer?</p>
",<probability>
"<p>A fair die is rolled nine times. What is the probability that 1 appears three times, 2 and 3 each appear twice, 4 and 5 once and 6 not at all?</p>

<hr>

<p>My approach is fairly simple. The dice is  fair, so we have a total of $6^9$ possible strings. Consider the set $\{1,1,1,2,2,3,3,4,5\}$. From this set is a total possible number of combinations of</p>

<p>$$\frac{9!}{3!2!2!}=30240$$</p>

<p>Thus, the probability of the above occuring is $30240/6^9$</p>

<p>My question is whether or not I have properly accounted for the probability of $4$ and $5$, and whether or not I should account for their probabilty of being rolled by considering the probability of <strong>never</strong> rolling a $6$. </p>

<p>Could someone explain why I should or should not have these concerns?</p>
",<probability>
"<p>I think I have some problem understanding markov chains, because we defined them as abstract objects but our professor does proofs with them as if they where just elementary conditional probabilities.</p>

<p>This is our <strong>definition</strong> of a <strong>markov chain</strong>: Given prob. space $(\Omega, \mathcal{A}, \mathbb{P})$, standard borel space $(S, \mathcal{S})$ and a sequence of random variables $X_n: \Omega \to \mathcal{S}, n \in \mathbb{N}$. $(X_n)_{n\in \mathbb{N}}$ is called markov chain if 
$$\forall B \in \mathcal{S}: \mathbb{E}[\mathbb{1}_B(X_n)|\sigma(X_0, ..., X_{n-1})] = \mathbb{E}[\mathbb{1}_B(X_n) |\sigma(X_{n-1})]$$</p>

<hr>

<p>So far, so good. But now we've got the following <strong>preposition</strong>: </p>

<p>Given $(\xi_i)_{i\in \mathbb{N}}$ iid rv on $\mathbb{R}^d$ and random variable $X_0$ independent of $(\xi_i)_{i \in \mathbb{N}}$ (also on $\mathbb{R}^d$) we define $X_n := X_0 + \sum_{i=1}^n \xi_i$. Then $(X_n)_{n \in \mathbb{N}_0}$ is a markov chain.</p>

<p><strong>Proof</strong>: $$P(X_n \in B | X_0 = x_0, ..., X_{n-1} = x_{n-1}) = $$</p>

<p>$$= P(\xi_n + x_{n-1} \in B | X_0 = x_0, X_0 + \xi_1 = x_1, ..., X_0 + \sum_{i=1}^{n-1}\xi_i = x_{n-1}) = $$</p>

<p>$$=_{independce} P(\xi_n + x_{n-1} \in B) = ... = P(X_n \in B | X_{n-1} = x_{n-1})$$
Why do we start treating these conditional expectations just like elementary conditional probability for events?</p>

<p>Sorry for the awful formatting of the proof </p>
",<probability>
"<p>Let $[n+1]$ be the set defined by $[n+1]=\{1,2,\ldots,n+1\}$.</p>

<p>Call a subset of $[n+1]$ with $r+1$ distinct elements an $(r+1)$-subset. How many $(r+1)$-subsets of $[n+1]$ have $(k+1)$ as their largest element?</p>

<hr>

<p>This seem fairly facile, so I am concerned that I am doing it wrong. By definition, $[n+1]$ is a bounded set. If $(r+1)$ is a subset of $[n+1]$, then $(r+1)$ is bounded as well. Denote that bound as $k+1$ s.t. $k \geq r_0 \forall r_0 \in (r+1)$.</p>

<p>Is anything wrong here?</p>

<p>Additionally, I am supposed to deduce that </p>

<p>$$\sum^n_{k=r}{k\choose r}={n+1\choose r+1}.$$</p>

<p>I have no idea where to start for this one. </p>

<p><strong>EDIT:</strong> Using the hints given by Hagen von Eitzen, I claim that the total number of subsets of $[n+1]$ that have $(k+1)$ as their largest element is $k$. Note that an $(r+1)$-element subset of $[n+1]$ is an $(r+1)$-element subset of $[n+1]$ that has $k+1$ as maximal elelment for exactly one k∈{r,…,n}.</p>

<p>I am fairly sure that I can prove this by induction. Can someone suggest anything for the second half and whether I did the first half correctly?</p>
",<probability>
"<p>I have been playing a game and came up with this question:</p>

<p>There are $n$ different object, and each time you randomly choose one of them. </p>

<p>One success is defined as one of the objects being selected $m$ times. What is the expected time to get one success (accumulating $m$ of any one object)?</p>

<p>In addition, what is the expected rate of success? </p>

<p>Please help...</p>
",<probability>
"<p>I came to the following problem: 
Let $A_1, A_2, ...$ be events in a probability space $(\Omega, F, \mathbb{P})$ and $\mathbb{P}[A_j]=1$ for all $j&gt;1$. I need to show that the probability of the intersection of all those events $A_j$, where j goes from 1 to infinity, is also $1$. </p>

<p>From what I understand, the events we have are not dependent so we can use the formula for a joint probability, so it will be the product of the probabilities of the events. However, I am not sure whether that formula holds in the general case. </p>

<p>Any suggestions?</p>
",<probability>
"<p>I want to calculate $P(X=Y)$, where $X,Y$ are independent and geometrically distributed, which means: $P(X=k) = P(Y=k) = p(1-p)^k$, $k \in \mathbb N_0$ and $p \in (0,1)$.</p>

<p>Can anybody tell me how to do this? I'm afraid I don't have an idea..</p>

<p>Thanks in advance!</p>
",<probability>
"<p>If we have two coins of radius of $R_1=8$ and $R_2=12$. Assume that 99% of people will be able to tell the size within $\pm5\%$ by touch it only, and assume it is a normal distribution.</p>

<p>Question A: How many people will mistakenly think a given coin is the other one.</p>

<p>Question B: If insert a third coin in between in a average radius, how many people make the mistakes for each coin?</p>

<p>Question C: If insert n coins in between with same step of change, what will be the worst case? (It must be a function of n, right?)</p>

<p>If the question does not make sense, please let me know.</p>
",<probability>
"<p>Suppose we have an urn with $N$ white balls and $M$ black balls. Suppose we draw $n$ balls and each time a ball is drawn, then we put it back. Let $X =$ number of white ball we get. Let $U = \{1,2,...,N+M\}$. For our probability space, we take $\Omega = U^n $. We want to find </p>

<p>$$ P(X = x) \; \; \; \text{where} \; 0 \leq x \leq n  $$</p>

<p>I am stuck trying to solve this problem. I know the cardinality of $\Omega$ is $(N+M)^n$.</p>
",<probability>
"<p>Let $Y_1,\,Y_2\ldots$ be a sequence of real random variables and $Y(\omega)=\liminf_{n\to\infty}Y_n(\omega)\in R$. I define using ricursion the sequenze $\tau_k(\omega)=\inf \{n&gt;\tau_{k-1} : |Y_n(\omega)-Y(\omega)|\leq 1/k\}$ and I want to prove that $\tau_k$ is measurable. Can anyone give me any idea? Thank you in advance.</p>
",<probability>
"<p>Let $\left\{ X_t \right\} $ be a stochastic process $MA(2)$ such that $X_t = Z_t + 0.8Z_{t-2}$. Where $\left\{ Z_t \right\} $ is White Noise $WN(0,1)$.</p>

<p>Compute variance of $$\frac{X_1+X_2+X_3+X_4}{4}$$</p>

<p>I don't have any idea how can I compute it. Could you give me a tip?</p>
",<probability>
"<p>Consider two real random variables $X,Y$ and the conditional expectation $\mathbb E[Y|X]$, also a random variable. What is the conditional expectation $\mathbb E[\mathbb E[Y|X]|Y]$? Is it $=Y$? Is it $=\mathbb E[Y|X]$? Is it even well-defined?</p>
",<probability>
"<p>Consider two Normally distributed random variables, $X_1 \sim N(0,T_1)$ and $X_2 \sim N(0,T_2)$, such that $X_2-X_1 \sim N(0,T_2-T_1)$. How to calculate $E[X_2^2\mid X_1]$?</p>
",<probability>
"<p>In the group of $100$ microprocessors $10$ microprocessors are faulty. $5$ microprocessors are selected randomly. Find the probability that</p>

<p>a) All selected microprocessors are correctly</p>

<p>b) Exactly one microprocessor is faulty</p>

<p>c) At least one microprocessor is faulty</p>

<p>d) Maximum $3$ microprocessors are correctly </p>

<p>I tried</p>

<p>a) $\frac{\binom{90}{5}} {\binom{100}{5}}$ =0.5837</p>

<p>b) $\frac{\binom{10}{1}*\binom{90}{4}} {\binom{100}{5}}$ = 0.3393</p>

<p>c) ${\binom{90}{5}}+{\binom{90}{3}}*{\binom{10}{1}}+{\binom{90}{3}}*{\binom{10}{2}}+{\binom{90}{2}}*{\binom{10}{3}}+{\binom{90}{1}}*{\binom{10}{4}} {\binom{100}{5}}$</p>

<p>and all that divided with ${\binom{100}{5}}$</p>

<p>d) ${\binom{90}{1}})*{\binom{10}{4}}+ {\binom{90}{2}}*{\binom{10}{3}}+{\binom{90}{3}}*{\binom{100}{2}}$</p>

<p>and all that divided with ${\binom{100}{5}}$</p>

<p>Is this good way or I made mistake in reasoning? </p>
",<probability>
"<p><strong>Given:</strong> $X = Y^2 + Z^2$ (hence $E[X] = E[Y^2] + E[Z^2]$)</p>

<p>$p(X = 1) = .52$, $p(X = 4) = .24$, $p(X = 16) = .24$<br>
$p(Y = -1) = .5$, $p(Y = 3) = .5$</p>

<p><strong>Question:</strong> Despite not being handed any information about $Z$, prove that this could never hold in classical probability theory.</p>
",<probability>
"<p>We've got a random sample of iid $X_1,\dots,X_n$. We're testing the mean of $X \sim \mathcal{N}(\mu,\sigma^2)$, where $\sigma^2$ is known. The size of the test $\alpha=0.05$.</p>

<p>$H_0: \mu=0$</p>

<p>$H_1: \mu=v$</p>

<p>By the Neyman-Pearson lemma the Most Powerful test is $\phi(X) = \mathbf{1}_A$ where the set $A =\{ x: \prod_{i=1}^n \frac{f(\mu_1,\sigma^2)}{f(\mu_0,\sigma^2)} &gt; k \} $</p>

<p>simplyfying we can reduce the test to:
\begin{equation}
\frac{1}{n}\sum_{i=1}^n X_i &gt; \frac{2\sigma^2(\log k+\mu_1^2 n)}{n \mu_1}
\end{equation}</p>

<p>where $\mu_1 = v$.
Calculating the critical value $k$ we evaluate:
$$
\begin{align}
\alpha=\mathbb{E}[\phi(X) |H_0] &amp;= \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^{\infty}\phi(x) e^{-\frac{x^2}{2\sigma^2}} dx\\
&amp;=\frac{1}{\sqrt{2 \pi \sigma^2}} \int_{-\infty}^{\infty}\mathbf{1}_A e^{-\frac{x^2}{2\sigma^2}} dx\\
&amp;= \frac{1}{\sqrt{2 \pi \sigma^2}} \int_{k}^{\infty}e^{-\frac{(x-\mu_1)^2}{2\sigma^2}} dx\\
&amp;= \frac{1}{\sqrt{2 \pi }} \int_{\frac{k-\mu_1}{\sigma}}^{\frac{\infty-\mu_1}{\sigma}}e^{-\frac{x^2}{2}} dx\\
&amp;=1-\Phi\Bigg(\frac{k-\mu_1}{\sigma} \Bigg)
\end{align}
$$</p>

<p>therefore we can derive $k=\mu_1 + \sigma \Phi^{-1}(1-\alpha)$</p>

<p>Is my approach correct, or did I mess up the calculation of the critical value?</p>
",<probability>
"<p>Given an infinitely differentiable function $ g: \mathbb{R} \rightarrow \mathbb{R}$, can we always find a distribution function $f_X$ of some random variable $X$ so that 
$g(t) = \int_{-\infty}^\infty e^{tx}f_X(x) dx$?</p>

<p>If my question is too vague or ill-posed, can anyone recommend any literature on the characterization of moment-generating functions?</p>
",<probability>
"<p>I'm attemtping to solve this problem:</p>

<blockquote>
  <p>Suppose a shot is fired at a circular target.  The vertical and the
  horizontal coordinates  of the point of impact (taking the center of
  the target as origin) are independent random variables, each
  distributed according to the standardized normal distribution. </p>
  
  <p>a. Write down the PDFs of the two coordinates.<br>
  b. Write down the joint
  PDF of the coordinates of the point of impact.<br>
  c. What is the PDF of
  the radius of the point of impact $r = \sqrt{x^2 + y^2}$ ?  Show all  steps you used to
  derive this expression.<br>
  <em>Hint: you can use the CDF method for finding the PDF of a transformed RV.</em><br>
  d. What is the probability that the
  point of impact will land in the ring of radii 2-3?</p>
</blockquote>

<p>MY attempt at a solution:</p>

<p>a)
P.D.F. of x is: $$\frac{1}{2\sqrt{\pi}}e^{-\frac{x^2}{2}}$$
P.D.F. of y is: $$\frac{1}{2\sqrt{\pi}}e^{-\frac{y^2}{2}}$$</p>

<p>b)
joint PDF is:
PDF(x) * PDF(y) = $$\frac{1}{2\sqrt{\pi}}e^{-\frac{y^2}{2}} * \frac{1}{2\sqrt{\pi}}e^{-\frac{x^2}{2}} = 2\pi^{-1}e^{\frac{-(x^2 + y^2)}{2}} $$</p>

<p>c) this is where I'm getting hung up on...</p>

<p>So the C.D.F. of a std. normal R.V. $x$ is:</p>

<p>$$\phi (x) = \frac{1}{\sqrt{2 \pi}}\int e^{\frac{-t^{2}}{2}}dt$$ </p>

<p>correct?</p>

<p>since $r = \sqrt{x^2 + y^2}$  is given, then is the CDF of $r$ just
$$\phi (r) = \frac{1}{\sqrt{2 \pi}}\int e^{\frac{-r^{2}}{2}}dr$$
And then would the PDF of the radius just be the first derivative of this?</p>
",<probability>
"<p>I am trying to solve this:</p>

<blockquote>
  <p>Consider a stick of length 1.  You break the stick in two
  random places, X and Y.<br>
  a. Define the individual
  probability distribution functions of the breaking points X and Y.<br>
  b. Write the joint PDF of the breaking points, if X and Y are
  independent.  Sketch its  support (domain) and indicate the density
  values of this domain.<br>
  c. Assume that Y is such that $Y &gt; X$. What is the joint PDF of $(X,Y)$ if it needs to be  uniform on this domain. Again, sketch its support and indicate the density value.<br>
  d. The two breaking points divide the stick in three segments. 
  What is the probability of the left-most segment is the shortest
  segment, when $Y &gt; X$?  Sketch the area in the $X-Y$  plane that
  corresponds to this event.  (Hint: to be the shortest segment the
  leftmost segment  needs to satisfy two constraints simultaneously.)</p>
</blockquote>

<p>Solutions:</p>

<p>a/b)</p>

<p>$$PDF(X,Y) = 1 | (x,y) \in [0,1]\times [0,1]$$$$0|otherwise$$</p>

<p>c) we know that $X$~U$(0,1)$. The clause that $Y\gt X$ means that $Y$~U$(X, 1)$.  </p>

<p>Now, Y has become dependent on X, correct? Therefore, the joint PDF isn't as simple as multiplying the PDF's together. How do I get the joint PDF in this case? </p>

<p>The answer I was given is that $$PDF(X,Y) = 2|(X,Y)\in [0,1]\times [0,1] \cap (Y &gt; X)$$ but I'm having trouble understanding why this is... </p>

<p>I think I understand the concept that, because $Y&gt;X$, the area in which Y can live is halved, because, when X is chosen (since it is chosen uniformly), Y's value depends on the value of X, and the uniform distribution means that the area Y can occupy is halved. but, why does the P.D.F. = 2 in this case? Wouldn't it still be equal to 1, as in: $$PDF(X,Y) = 1|(X,Y)\in [0,1]\times [0,1] \cap (Y &gt; X)$$ I thought the only change would be in the are for which the PDF was valid (hence the distribution needing to intersect the area where $Y&gt;X$</p>

<p>d) For this one, I came up with 3 constraints: $\frac{1}{2} &lt;Y&lt;\frac{3}{4}$ and $X&lt;\frac{1}{4}$</p>

<p>thus, $X$~U$(0,\frac{1}{4} )$ and $Y$~U$(\frac{1}{2} , \frac{3}{4})$</p>

<p>making $$PDF(X) = \frac{1}{\frac{1}{4}} = 4$$and $$PDF(Y)= \frac{1}{\frac{3}{4} -\frac{1}{2}} = 4$$</p>

<p>so joint $PDF(X,Y) = 16|\frac{1}{2} &lt;Y&lt;\frac{3}{4}$ and $X&lt;\frac{1}{4}$</p>

<p>However, this is incorrect, and I can't rightly figure out why.</p>

<p>Any advice as to where I'm going wrong? Thank you very much!</p>
",<probability>
"<p>I was wondering if someone could critique my argument here. The problem is to find the probability where exactly 2 people in a room full of 23 people share the same birthday.</p>

<p>My argument is that there are 23 choose 2 ways times $\displaystyle \frac{1}{365^{2}}$ for 2 people to share the same birthday. But, we also have to consider the case involving 21 people who don't share the same birthday. This is just 365 permute 21 times $\displaystyle \frac{1}{365^{21}}$. To summarize:</p>

<p>$$\binom{23}{2} \frac{1}{365^2} \frac{1}{365^{21}} P\binom{365}{21}$$ </p>
",<probability>
"<p>I deal a standard deck of 52 cards to you face up, one card at a time. Before
any deal of any card you can shout out ""NOW"". If you shout out ""NOW"" and the
next card I deal is a queen, then the game finishes and I give you $100. If the next
card isn't a queen, then the game finishes and I give you nothing. What is a fair
price for you to pay me in order to play this game with me?</p>

<p>What I think of is to use the optional sampling theorem and show it is a martingale. But I am a bit lost on the setup.</p>
",<probability>
"<p>Given some real number $a$ can anyone prove that if
$$
P(X &gt; a) &gt; P(Y &gt; a)
$$
is true then
$$
P(X &gt; Y) &gt; \frac12
$$
is also true.</p>
",<probability>
"<p>Suppose there are thirteen slips in a bag, labelled 1-13. If I draw a 10, 11, 12, or 13 then I stop adding to the sum and return the sum. If not, I add the number I draw to the current sum and place the slip back in the bag. </p>

<p>What is the expected value of the sum?</p>

<p><strong>My Attempt:</strong></p>

<p>S: total sum</p>

<p>Xi: the ith drawn slip</p>

<p>Probability of drawing a slip from 1-9: 9/13</p>

<p>Have a 9/13 chance of getting # from 1-9 and expect to add average of 5 and trying again...</p>

<p>Ending with a 10, 11, 12, or 13 each has 1/4 chance
$$
E[S] = \frac{1}{4}(\frac{9}{13}(5+E[S]) +10) + \frac{1}{4}(\frac{9}{13}(5+E[S]) + 11) +... +\frac{1}{4}(\frac{9}{13}(5+E[S])+13)
$$</p>

<p>which gives E[S] = 48.625</p>

<p>I wasn't sure how to factor in the four different ways that we could end the drawing (i.e. ending with a 10, 11, 12, 13). The answer doesn't seem right to me. </p>

<p>Additionally, let Y be the number of times a number of a particular number is drawn. What is E[Y]? I have no idea how to start with this with this part.  </p>
",<probability>
"<p>A non-closed path is chosen at random on the complete graph K9. All
paths are equally likely. What is the probability that the path contains
the edges {23} and {34} given that it is length 6? Given that it has
the edge {89}?</p>
",<probability>
"<p>I have a conditional probability problem I'm unsure can be answered given the information I have - as such I'm unsure if Bayesian Theorem is the way to answer it, or if the answer is staring at me in the face and I can't spot it.</p>

<p>It's easiest to think of this problem in terms of a game of baseball, with two teams - A and B.  I want to work out:</p>

<p><strong>Pr(A scores first and A wins)</strong> (A1) - and similarly<br>
<strong>Pr(B scores first and B wins)</strong> (A2)</p>

<p>Obviously these two scenarios alone won't sum to 100% as there are the possibilities that a team could score first and go on to lose.  Also worth keeping in mind that in a baseball setting Team A will bat first and have first chance to score.</p>

<p>I have the following known information:</p>

<p>Pr(A Wins) = 0.58  (B1)<br>
Pr(B Wins) = 0.42  (B2)</p>

<p>Pr(A Scores First) = 0.51 (C1)<br>
Pr(B Scores First) = 0.49 (C2)</p>

<p>Pr(The team who scores first wins) = 0.75  (D1)<br>
Pr(The team who scores first loses) = 0.25  (D2)</p>

<p>Is it as simple as saying that A1 = C1 * D1 (ie, the prob of team A scoring first by the prob that the team who scores first wins is the prob that team A will score first and win).  </p>

<p>In the back of my mind it seems to me this isn't valid as there is some dependency here, hence I'm unsure whether I need to use Bayes Theorem - or is it the case there isn't enough information here?</p>

<p>Thanks in advance.</p>
",<probability>
"<p><img src=""http://i.stack.imgur.com/5VGgn.jpg"" alt=""enter image description here""></p>

<p>Find $E[X]$ and $Var[X]$</p>

<p>So for the expectation so far I got that:</p>

<p>$$E[X] = E[X|N=n]P(N=n) = \large\frac{n+1}{\lambda} \frac{\lambda^{n}}{n!}e^{-\lambda}$$
but for conditioning on both a discrete and continuous random variable I am not sure whether to use the summation or integration. For integration it comes out to be just $\frac{n+1}{\lambda}$ which does not seem right since that would indicate that the expectation is independent of each other.</p>
",<probability>
"<p>A box contains $3$ yellow socks, $4$ blue socks, $1$ orange sock, and $2$ green socks. What is the probability of picking $2$ blue socks at the same time? What is the probability of picking $1$ green and $1$ blue sock at the same time?</p>

<p>I was thinking it is:<br>
$\large\frac{4}{10} \cdot \frac39$ for the $1^{st}$ question,  </p>

<p>$\large\frac{2}{10} \cdot \frac49$ for the $2^{nd}$ question.</p>
",<probability>
"<p>Question: </p>

<p>A store opens at $t =0$ and potential customers arrive in a Poisson manner at an average arrival rate of $λ$ <em>potential</em> customers per hour. As long as the store is open, and independently of all other events, each particular potential customer becomes an <em>actual</em> customer with probability $p$. The store closes as soon as ten actual customers have arrived.</p>

<p>Considering only customers arriving between $t =0$ and the closing of the store, what is the probability that no two <em>actual</em> customers arrive within $τ$ time units of each other?</p>

<p>Thanks! Could you give me idea or answer? 
Why downvote? The given answer is a little werid that I could not understand...</p>
",<probability>
"<p>I have a question.</p>

<p>I'm looking to calculate the probability of getting $4$ different suits cards in a $5$ card poker game using a standard $52$ card deck.</p>

<p>I think this is: $$\frac{\dbinom{4}{1}\dbinom{13}{2}\dbinom{13}{1}^{3}}{\dbinom{52}{5}}$$</p>

<p>What do you think?
Thanks.</p>
",<probability>
"<p>Consider a homogenenous Markov chain $\{X_n\,:\, n\in \mathbb N\}$ ($0\in\mathbb N$). The state space is $S$ with $|S|\le |\mathbb N|$ and $i\in S$. Consider moreover the function $1_{\{i\}}:S\longrightarrow\{0,1\}$ such that $1_{\{i\}}(i)=1$ and $1_{\{i\}}(s)=0$ if $s\neq i$ and then define the following random variable:</p>

<p>$$N_i:=\sum_{n=1}^\infty1_{\{i\}}(X_n)$$</p>

<p>Clearly $N_i$ counts (starting from $X_1$) ""the passages"" of the Markov chain from the state $i$.</p>

<p>I don't understand the following equality that can be found in the book  ""A course in stochastic - D. Bosq, H.T.Nguyen"" (page 57 on my edition):</p>

<p>$$E(N_i|X_0=i)=\sum_{n=1}^\infty P(X_n=i|X_0=i)$$</p>

<p>where $E(\cdot|\cdot)$ is the conditioned expected value. Please give me an explaination.</p>

<p>Thanks in advance.</p>
",<probability>
"<p>I'm trying to find the probability that $X$ is greater than, given a mean of $50$ and a std deviation of $5$.</p>

<p>I enter <code>normalcdf(66, 1E99, 50, 5)</code> in my calculator and receive and error.</p>

<p>Please help.</p>
",<probability>
"<p>A am a bit confused, when we are using choosing the critical value for a wilcoxon rank sum test (2-samples unpaired) when do we use the upper bound and when do we use the lower bound. So far i have only used the lower bound with the lowest of the two W values but keep seeing questions (A-level exam questions) where they specifically take the upper bound. For what reasons and when do we use the upper critical value and when the lower??</p>
",<probability>
"<p>Let E and F be independent with E = AUB and F=AB. Prove that either P(AB)=0 or else P(not A and not B)=0.
I dont know how to solve it. Please help.
Thank you very much.</p>
",<probability>
"<p>This is another question like <a href=""http://math.stackexchange.com/questions/520844"">this one</a>. And by the same reason, the book only has the final answer, I'd like to check if my reasoning is right.</p>

<p>A couple has 2 children. What is the probability that both are girls if the eldest is a girl?</p>
",<probability>
"<p>A population consists of F females and M males;the population includes f female smokers and m male smokers. If A is the event that the individual is female and B is the event he or she is a smoker , find  the  condition on f, m, F and M so that A and B are independent events?
The answer is f/F=m/M
I dont know how to get that answer. Please help.
Thank you.</p>
",<probability>
"<p>I'm trying to find if my approach to this kind of problems is correct. </p>

<p>For example: You have 3 boxes, and you have a 33% chance of finding an item in a box. What is the probability of finding items in: 0, 1, 2, 3 (all) boxes?</p>

<p>My answer:</p>

<p>$P=0.33$ $(!P =1 - 0.33=0.67)$</p>

<p>for 0 boxes: $(!P * !P * !P) * 3 = (0.67 ^ 3) * 3= 0.9$</p>

<p>for 1 box: $P * !P * !P + !P * P * !P + !P * !P * P  = (0.33 * 0.67 ^ 2) * 3 = 0.44$</p>

<p>for 2 boxes: $P * P * !P + P * !P * P + !P * P * P = (0.33 ^ 2 * 0. 67) * 3 = 0.21$</p>

<p>for 3 boxes: $(P * P * P) * 3 = (0.33 ^ 3) * 3 = 0.10$</p>

<p>The reasoning (for example for the 1 box case) is that we need to take the probability that the first chest contains an item AND $(*)$ the other don't, OR $(+)$ the second chest contains an item and the other don't, OR the third contains an item and the other don't.</p>

<p>Is this the correct way to calculate the probability for this type of problem?</p>
",<probability>
"<p>Let $X$ be a non-negative random variable. Prove that $$\Pr(X\geq a) \leq \frac{E[e^X]}{e^a}$$ </p>

<p>where e is Napier's base.</p>

<p>Can I know what the question means and how to prove it.</p>
",<probability>
"<p>If Government increases payment then they increase it by 9% . now if whether government will increase payment follows binomial distribution with parameters n=2 and p=(2/3) , then what percentage of payment increase is expected ? </p>

<p>my logic is let 
X denotes  government will increase payment.
then  x follows Bin(2, 2/3)</p>

<p>then E(X)=4/3.
so expected payment increase is 9*(4/3)=12%</p>
",<probability>
"<blockquote>
  <p>Let $X$ denote the number of tosses required to get the 5th head and $Y$ the number between the 6th and 7th heads. Are $X$ and $Y$ independent?</p>
</blockquote>

<p>Y will always depend on X . NO ?
i know geometric distribution has lack of memory property . but in this the underlying distribution is i think negative binomial. </p>

<p>i think 'no, they are not independent ' . help please. </p>
",<probability>
"<p>Let $\{X_n\}$ be a sequence of independent random variables on some probability space. </p>

<p>Then, by definition(according to the book that I am reading), I know that $\{\sigma(X_1),\sigma(X_2),\dots, \}$ is independent. </p>

<p>Then, I am wondering whether $\sigma(X_{n+1})$ is independent of $\sigma(\sigma(X_1),\sigma(X_2),\dots,\sigma(X_n))$. </p>

<p>Or in general, is $\sigma(\sigma(X_{n_1}),\sigma(X_{n_2}),\dots,\sigma(X_{n_k}))$ is independent of 
$\sigma(\sigma(X_{j_1}),\sigma(X_{j_2}),\dots,\sigma(X_{j_l}))$, where $\{n_1,n_2,\dots,n_k\} \cap \{j_1,j_2,\dots,j_l\} = \emptyset$?</p>
",<probability>
"<p>I know that there exists a particular measure, called <a href=""https://en.wikipedia.org/wiki/Haar_measure"" rel=""nofollow"">Haar measure</a>, defined on random matrices, i.e. $n \times n$ orthogonal complex matrices. 
My question is the following: can we define a probability measure on the space of $n \times n$ symmetric matrices with non negative integer coefficients? If yes, can you suggest me some case?</p>
",<probability>
"<p>Firstly apologies for lack of maths skill, I am a biologist and sadly fit the stereotype of being terrible at maths. </p>

<p>I am designing a set of unique identifiers for sample identification, which are generated from the genetic data associated with the sample. </p>

<p>I am trying to calculate the probability that within a given population of samples, two or more samples will be assigned the same identifier, which clearly is not desirable.</p>

<p>The identifiers are an ordered concatenation of $22$ pieces of data, each with three states i.e. RR, RA &amp; AA. These three states each have a different frequency in the population, as well as being different between the 22 pieces of data. I think that for these 22 pieces of data the number of possible identifiers will be $3^{22}$ =~ $3.14x10^{10}$. Presumably the average probability for each identifier would be $\large \frac{1}{3.14x10^{10}}$, but it would be the upper outliers that would be the limiting factor on the use of the method.</p>

<p>I have calculated the probability of the most probable identifier occurring in a single sample, which is ~$2x10^{-7}$. As such, I think that:</p>

<p>$p$ = $(2x10^-7)^{2}$ $\cdot n$</p>

<p>where p is the probability of the most likely identifier being assigned twice and n is the number of samples in the population.</p>

<p>This is as far as my maths can take me, though this clearly fails say where the identifier can be assigned more than twice, and as there are $3.14x10^{10}$ identifiers which could potentially also be repeatedly assigned. </p>

<p>I am struggling to see a way to deal with the multiple identifiers issue other than presuming that all identifiers have the same probability as this most likely identifier, though this would surely result in an enormously conservative estimate. </p>

<p>Once I have a way to do this I will then try to calculate the maximum size of a population sampled while maintaining a probability of replication of less than say $0.05$.</p>

<p>Any help with this would be greatly appreciated, sorry for the rather cumbersome explanation. I and others in my group have some skill in programming, so computational approaches that are more accurate are viable, and indeed preferable to conservative estimations.</p>

<p>Many thanks</p>
",<probability>
"<p>Say we have a compact space $X$ and a probability measure $P$ on $X$.</p>

<p>Assume that we know that for some event $A$ the sequence $f_n(x)\rightarrow f_\infty(x)$ converges a.s. in $Q$ which is the regular conditional probability space generated by conditioning $P$ on the event  $A$.</p>

<p>Under which conditions we can conclude that $f_n(x)\rightarrow f_\infty(x)$ converges a.s. in $P$ also?</p>
",<probability>
"<p>Given the probability density function $(x+y)$ if $0 \leq x \leq y \leq c$, $0$ otherwise.</p>

<p>I want to calculate $c$. I clearly do not know at all where to start. I tried to take some integrals but I just do not know how to find the ""limit"" of the function.
 However, my math skills are not too advanced that is why I am not quite sure if there actually is some calculation involved.</p>

<p>I would appreciate some help, thanks. </p>
",<probability>
"<p>I'll just give you the four rules that matter in this problem and explain my thought process and the other opinion.</p>

<p>Rule #1: 15 players are playing a game.</p>

<p>Rule #2: One player (J) can secretly select a player (not themselves).</p>

<p>Rule #3: A different player (K) can then secretly select a player (not themselves) and kill them.</p>

<p>Rule #4: If J selects K, then K will be given no choice and will kill J.</p>

<p>Clarification: J and K can select the same person.</p>

<p>It is then revealed that K killed J. What is the probability that J had selected K?</p>

<p>Let Q be the required probability.</p>

<p>I will attempt to calculate the probability that J did NOT select K given that K killed J.</p>

<p>Let's call A the event {J does NOT select K} and B the event {K kills J}. </p>

<p>We're trying to calculate P(A/B), which is equal to P(AB)/P(B). Clarification: AB means A∩B.</p>

<p>P(Β) first. B can happen if J selects K (1/14), or K selects J (1/14), minus the intersection (1/196). Thus, P(B)=2/14-1/196=27/196.</p>

<p>AB now. The probability that K selects J (1/14), minus P{K selects J AND J selects K} (1/196). So, P(AB)=13/196.</p>

<p>Thus, P(A/B)=0.481481... or about 48%. And Q=0.52. Almost.</p>

<p>Some people, however, argued that since we know J died to K, the sample space only includes the events {K killed J}, of which there are 14, and only 1 of those also includes J selecting K, thus Q=1/14. Part of the argument is that P(B)=1, since we know that K killed J.</p>

<p>I am not sure wether we are looking for dependent probability or intersection. The problem, however, is perfectly defined. If it is revealed that K killed J, what are the odds of J having selected K?</p>

<p>EDIT: Another clarification: While, yes, both J and K do not have to select another player, we assume that both did (although K will kill J if he is selected, even if he doesn't want to kill anyone).</p>

<p>I was also a bit vague about what J does when he selects another player because it is irrelevant. If you must know, during the day he selects the player he wants to interrogate that night, and in all nights except the first one (which this problem examines), kill them if he so wants. There are exceptions, but this is the general rule.</p>
",<probability>
"<p>We choose a random number from 1 to 10. We ask someone to find what number it is by asking a yes or no question. Calculate the expected value if the person ask if it is $x$ number till you got it right?</p>

<p>I know the answer is around 5 but i can't find how to get there.</p>

<p>I tried $\frac{1}{10}(1)+\frac{1}{9}(2)+\frac{1}{8}(3)+\frac{1}{7}(4)+\frac{1}{6}(5)+\frac{1}{5}(6)+\frac{1}{4}(7)+\frac{1}{3}(8)+\frac{1}{2}(9)$</p>

<p>but it doesn't work. Any help to point me in the right direction would be greatly appreciated.</p>

<p>Thank you</p>
",<probability>
"<p>You are dealt $20$ cards. What is the probability you have all kings given that you hold at least one king?</p>

<p>So I set it up like 
$$
P(4\textrm{ Kings | at least one king}) =\frac{{4 \choose 4}{48 \choose 9}}{{52 \choose 20}-{48 \choose 20}}
$$
Does this setup look correct? </p>
",<probability>
"<p>Suppose that I have a finite population of A's and B's, with properties:</p>

<ul>
<li><p>Population size: $n$</p></li>
<li><p>There are $n_1$ A's and $n - n_1$ B's  (so that $p = \frac{n_1}{n}$, $q = \frac{n - n_1}{n}$)</p></li>
</ul>

<p>I'm taking a sample of size r without replacement. What I need to demonstrate (<strong>if it can be demonstrated</strong>, I'm not sure) is that: if p > 0,5 then the probability that sample proportion $\bar{p}$ > p is greater than the probability that $\bar{p}$ &lt; p.</p>

<p>Put another way: <strong>If p > 0,5, then Prob($\bar{p} \ge p$) > Prob($\bar{p} \le p$)</strong></p>

<p>Plainly, that if deviations from the expected value occur, then they are more likely to occur in the direction of the more frequent type of member.</p>

<p>As an additional premise I have that $pr \in \mathbb{Z}$.</p>

<p>So, instead of using a cumulative hypergeometric distribution, I thought it would be simpler (avoiding the sums) to reformulate the problem in this manner:</p>

<p>$$\frac{\binom{n_1}{pr} \binom{n - pr}{r - pr}}{n \choose r} &gt; \frac{\binom{n - n_1}{qr} \binom{n - qr}{r - qr}}{n \choose r}$$</p>

<p>Is this ok? What it says -on the left for example- is basically ""from the population, take a subset of pr A's (so you know now that the sample will contain at least the same proportion of A's than the population), and then take the rest of the sample from the remainder of the population as you please"".</p>

<p>If this is ok, then how do I go on from there?
I managed to reduce what I need to prove to:</p>

<p>$$\frac{(pn)!}{(pn -pr)!} (n - pr)! &gt; \frac{(n - pn)!}{(n - r - (pn -pr))!} (n - (r - pr))!$$</p>

<p>(since the denominators cancel and, after applying the binomial coeeficient formula, (r - pr)! = (qr)! and (r - qr)! = pr! can also be cancelled).</p>

<p>Is what i've got so far all right? where to go from here? Thank you</p>

<p>EDIT: I've been trying the conjecture for various values here: <a href=""http://stattrek.com/online-calculator/hypergeometric.aspx"" rel=""nofollow"">http://stattrek.com/online-calculator/hypergeometric.aspx</a>  and the result seems to hold. Furthermore, there seem to be some interesting properties, such as, the greater p is, the greater the difference between $Prob(\hat{p} \ge p)$ and $Prob(\hat{p} \le p)$.</p>

<p>It seems to hold for sampling with replacement as well, which can be tried here: <a href=""http://onlinestatbook.com/simulations/CLT/clt.html"" rel=""nofollow"">http://onlinestatbook.com/simulations/CLT/clt.html</a></p>
",<probability>
"<p>Here's a nice probability puzzle I have thought about for a class I'm TAing, I'm curious to see different solutions :) It goes like this:</p>

<p>We have a classroom with $n$ seats available and $m \leq n$ incoming students. Each student has an (ordered) list of $k \leq n$ preferences for the seat he is going to take, where $k$ is some fixed positive integer. If at the moment of his arrival, a person's $k$ favorite seats are already taken, then he randomly chooses a seat from the remaining $n-k$. What is the probability that everyone occupies one of his favorite $k$ seats?</p>
",<probability>
"<p>Given two points $X,Y$ on two sides of square $[0,1]\times [0,1]$ ($X:(0,1/2),Y:(1,1/2)$ (PS: My original question is $X,Y$ on opposite of a square, but I think that's not the real case) )and $n$ points distributed uniformly(i.i.d) in the square (where $n$ is large, and $A$ denotes the set of $n$ points), can I caluculate the asymptotic behavior of the value $M(n)$, where $M$ is defined as
$$M(n)=E\left[\min_{B\subset A} \sum_{k=1}^{|B|+1} d(B_k,B_{k-1})^2\right]$$
where $B_k$ is the $k$th element of $B$,$B_0=X,B_{m+1}=Y$(We let $m=|B|$), and the expected value is taken over all the possible $A$ . That is to say, I would like to compute the expected value of the minimal weight defined as sum of the square of distance.</p>

<p>I know that when $n\to\infty,M(n)\to 0$. And in the $1$-dim case, this is easy, since it is only a Poisson process, and the distance between two consecutive points are surely exponential distribution.(Calculation suggests it's about $(n+3)/((n+1)(n+2))$,where $n$ is number of points added) But in the two dimensional case, I got stuck and don't know how to tackle it. This is a problem arouse from the calculation of the cost of a network. Any hint or reference are welcomed, Thanks!</p>

<p>(Some computer experiment suggests that the weight is about $\approx 1.1/\sqrt{n}=O(1/\sqrt{n})$. I also wonder if there are some similar results?)</p>
",<probability>
"<p>N uniform spheres are to be segregated into 4 boxes labelled A,B,C,D. What is the probability of not finding a sphere in box A if :</p>

<ol>
<li>N=4</li>
<li>N=10</li>
</ol>

<p>?</p>

<p>According to me , if they are the spheres and boxes are represented by X and * respectively and  listed as a string eg.XX*X*X* (Actually the * represents the separation between regions that represent a box ie. two spheres in each box would be shown as- XX*XX*XX*XX), the no. of ways in which they could be segregated is $\binom{10 +4-1}{10}$ where 10 is the no. of spheres and in this case the 10 spaces in the resultant 13 character string.</p>

<p>Subsequently, if one box is ignored as per the requirement of the question , the required probability should be :  $\frac{\binom{10+3-1}{10}}{\binom{10+4-1}{10}}$</p>

<p>Please provide an answer to the probability question and any mistakes that I may have made in my approach.</p>
",<probability>
"<p>Then you toss a fair coin as many times as the number of pips. For each heads, you win \$20; for each tails, you lose \$1. Let X = total amount you win (or lose if $X&lt;0$). </p>

<p>What is $E(X)$?</p>

<p><strong>My thoughts:</strong></p>

<p>If you roll $n$ you lose $n$ and then gain ($20 \cdot$ expected head) $-$ ($1 \cdot$ expected tails) </p>

<p>Expected heads = expected tails = $\frac{n}{2}$. </p>

<p>Probability of rolling $n$ is $\frac{1}{6}$. Therefore:</p>

<p>$E(X) = \sum_{n=1}^6 \frac{-n + 20\frac{n}{2} - 1\frac{n}{2}}{6}$ </p>

<p>$= \sum_{n=1}^6 \frac{-n + 10n - \frac{n}{2}}{6}$ </p>

<p>$= \sum_{n=1}^6 \frac{8.5n}{6}$ </p>

<p>$= \frac{8.5}{6}\cdot \frac{6\cdot7}{2}= \frac{(8.5)\cdot7}{2} = 29.75$</p>

<p>Does this work seem correct? I feel like I did something wrong. Thanks!</p>
",<probability>
"<p>I have a rectangular section of constant height and length and I choose a random starting point anywhere along its length. From the randomly chose starting point, I first add <code>x</code> units of red material immediately to its right and then fill the remaining section to the right of the red material with blue material.</p>

<p>Here's a graphic I drew up as an example:</p>

<p><img src=""http://i.stack.imgur.com/V1gQt.jpg"" alt=""enter image description here""></p>

<p>My questions is, how do I go about finding the probability that the variable blue material section will be greater than the static red material section?</p>

<p>And also is there a way to determine an average size of the blue section?</p>

<p>My idea was that that the average starting point would be in the middle of the rectangle, therefore the average blue material section would be:</p>

<p>$$
\frac{length\_rectangle}{2} - x
$$</p>

<p>Is that correct?</p>

<p>For the probability component, I figure that for the red material section to be greater than the blue section, the starting point would have to start at such a point that:
$$
(length\_rectagle - start\_point) &lt; 2x 
$$
Therefore, the probability that the starting point satisfies the inequality, would be just less than:
$$
\frac{2x}{length\_rectangle}
$$</p>
",<probability>
"<p>This is an interview question.</p>

<p>Given n red balls and m blue balls and some containers, how would you distribute those balls among the containers such that the probability of picking a red ball is maximized, assuming that the user randomly chooses a container and then randomly picks a ball from that. </p>

<p>My solution: </p>

<p>suppose we have c containers, distribute n/c read balls to each c. 
     If c == 1, put all of them together, it is
                n/(m+n)    </p>

<pre><code> If c == 2,  put 1 red in c1 and all left red and all blue ones in c2 in this way , we have:
              1/2 + 1/2 *(n-1) /(m+n-1) &gt; 1/2

 If c == 3,   put a red in c1, put a red in c2, put left red and all blue in c3, we have:
    1/3 + 1/3 + 1/3 * (n-2)/(m+n-2) &gt; 2/3

 If c == n,  put a red in each of p -1, and all left red and blue in pth container, we have:
            ( Sum of (1/n) from 1 to n-1 ) +  ( 1/n * 1/(m+1) )
            (n-1)/n + 1/n * 1/(m+1) == 1 (almost)
</code></pre>

<p>As n is large, the (n-1)/n is very close to 1 so that we maximize the probability to get a red balls.  </p>

<p>Any better ideas ? </p>
",<probability>
"<p>Suppose you ﬂip a fair coin repeatedly until you see a Heads followed by a
Tails. What is the expected number of coin ﬂips you have to ﬂip?</p>

<p>By manipulating an equation based on the result of the first flip, shown at this link:</p>

<p><a href=""http://www.codechef.com/wiki/tutorial-expectation"">http://www.codechef.com/wiki/tutorial-expectation</a></p>

<p>the answer is 6. This also makes sense intuitively since the expected value of the number flips until HH or TT is 3. But is there a way to tackle this problem by summing a series of probabilities multiplied by the values?</p>

<p>Thank you!</p>
",<probability>
"<p>If there is an 82% chance that within your average lifetime, lets assume 70 years (25567 days), that ""E"" event will happen: what is the percent chance that it will happen on any given day?
<br><br>I'm assuming that it'll be something like .02347% or something small. My problem is I've only taken up through Advanced Algebra and that was six years ago. I would have searched for this on google to solve it myself, but I'm not even sure what to call this beyond ""math I don't know"". 
<br><br>So if you could help me identify the solution, how to solve it, and what this is (i.e. I imagine terms like 'graphing polonomials' 'deductive statistics' or some such name that would give me an idea of what exactly it is that I should be learning to handle similar questions on my own). Thanks for the help in advance.</p>
",<probability>
"<p>I am trying to show that the stationary distribution for a Markov Chain on a continuous state space can be obtained by building a transition density kernel, which obeys the detailed balance rule where $P_{t+1|t}(X_{t+1}=i|X_{t}=j)P_{t}(X_{t}=j) = P_{t+1|t}(X_{t+1}=j|X_{t}=i)P_{t}(X_{t}=i)$. The Markov Chain is time-homogeneous so $P_{t+1|t}(X_{t+1}=i|X_{t}=j)=T(X_{t+1}=i|X_{t}=j)$ is independent of time step $t$ and $T$ is the continuous transition kernel (density kernel). I suppose that the distribution at the time $t$ is the stationary distribution $\pi$, which is $P_{t}(X_{t}=i) = \pi(i)$ and I aim to show that $P_{t+1}(X_{t+1}=i) = \pi(i)$ given the detailed balance.</p>

<p>I am currently proving this already but I am not sure whether my way of proof is mathematically valid.</p>

<p>Here is what I am doing:</p>

<p>1) I calculate $P_{t+1}(X_{t+1}=i)$ as $P_{t+1}(X_{t+1}=i) = \int_{-\infty}^{\infty} P_{t+1|t}(X_{t+1}=i|X_{t})P_{t}(X_{t})dX_{t} = \int_{-\infty}^{\infty} P_{t+1,t}(X_{t+1}=i,X_{t})dX_{t}$</p>

<p>2) According to the detailed balance, for each $i,j \in \mathbb{R}$, it is $P_{t+1|t}(X_{t+1}=i|X_{t}=j)P_{t}(X_{t}=j) = P_{t+1|t}(X_{t+1}=j|X_{t}=i)P_{t}(X_{t}=i)$. This means that the joint distribution function is symmetrical: $P_{t+1,t}(X_{t+1}=i,X_{t}=j) = P_{t+1,t}(X_{t+1}=j,X_{t}=i)$. Here, I think about the joint distribution as an uncountable sized, symmetric matrix. Therefore the integration over the row ""belonging"" to $X_{t+1}=i$ is equal to the integration over the column ""belonging"" to $X_{t}=i$. According to this logic it must be $P_{t+1}(X_{t+1}=i) = \int_{-\infty}^{\infty} P_{t+1,t}(X_{t+1}=i,X_{t})dX_{t} = \int_{-\infty}^{\infty} P_{t+1,t}(X_{t+1},X_{t}=i)dX_{t+1} = P_{t}(X_{t}=i)$.</p>

<p>3)Since $P_{t}(X_{t}=i) = \pi(i)$ we have that $P_{t+1}(X_{t+1}=i) = \pi(i)$ and this ends the proof.</p>

<p>The part 2 is where I am feeling uneasy about my way of proof. I know that the symmetry induced on the joint distribution of consecutive random variables must yield the change of integration variables in the step 2 as valid. This is indeed the case, if we had a finite state space Markov Chain. The joint distribution would be a finite sized matrix and the symmetry would immediately tell that the $i$th row and $i$th column are equal. But I am not comfortable with this way of thinking on a continuous state space, mainly because of my shallow mathematical background.</p>

<p>What would be a more mathematically rigorous way to show that the step 2 is correct? How should we show that integrals in the step 2 are equal to each other?</p>

<p>Thanks in advance.</p>
",<probability>
"<p>How to evaluate $$\frac{\Gamma\left(\frac{n}{2}\right)}{\Gamma\left(\frac{n-1}{2}\right)}$$, where n is integer > 0?</p>

<p>I know the gamma function formula will give</p>

<p>$$ \frac{(\frac{n-2}{2})!}{(\frac{n-3}{2})!}$$ How to simplify it?</p>
",<probability>
"<p>We say that a graph $G$ is distributed with $\mathcal{G}_{n,p}$ if it is a graph on $n$ vertices, and for which each of the ${n\choose 2}$ possible edges is chosen independently of the other edges and with probability $p$.</p>

<p>A <em>monotone property</em> $P$ of a graph is a set of graphs (on $n$ vertices) that is closed from above (that is, if $G\in P$ and $G\subseteq H$ then $H\in P$.</p>

<p>A function $f(n)$ is said to be a <em>threshold</em> for a property $P$ if for any $p(n)=\omega(f(n))$, $G\sim\mathcal{G}_{n,p}$ has $P$ asymptotically almost surely (a.a.s.), and for any $p(n)=o(f(n))$, $G\sim\mathcal{G}_{n,p}$ does not have $P$ a.a.s.</p>

<p>For example, if $P$ is ""has a triangle as a subgraph"", then $P$ is clearly monotone, and $f(n)=n^{-1}$ is a threshold for $P$. $f(n)=\frac{\ln{n}+\ln\ln{n}}{n}$ is a threshold for the Hamiltonicity property (in a stronger sense).</p>

<p><strong>My question is this:</strong> what are the thresholds for the properties of having ""quite short"" paths or cycles? By ""quite short"" I mean of length $\Theta(n^\varepsilon)$ for some $0&lt;\varepsilon&lt;1$, or of length $\Theta(\ln{n})$.</p>
",<probability>
"<p>We have $11$ bins with $10$ objects each. Every object is either black or white, and the $i$th bin ($1 \le i \le 11$) has precisely $(i -1)$ black objects in it. Someone selects, uniformly at random, one of those bins and then selects, also uniformly at random, two objects from it. What is the probability that these two objects are of the same color?</p>
",<probability>
"<p>The chances of being born with a certain disease are estimated as $1$ in $1200$. What is a good estimate of the chance that an island with $10000$ inhabitants has precisely $8$ people born with that particular disease? We assume that all $10000$ events of being born with that particular disease are mutually independent.</p>
",<probability>
"<p>Is it true that for continuous distribution $$E(X^a) =  \int_{-\infty}^{+\infty} x^a\cdot g(x)dx $$ where $g(x)$ is probability density function?</p>
",<probability>
"<p>If I have two events that occur on a specific interval (one every 8 seconds, the other every 200 milliseconds) but were not started synchronously, how can I calculate the frequency with which these two events will occur at the same time?</p>

<p>Looking at the numbers, it seems that unless they start synchronously, they will <em>never</em> coincide. If they started synchronously, in a perfect world, it would be every 8 seconds.</p>

<p>Obviously there is some variation/imperfection because they <em>do</em> coincide occasionally despite not starting at the same time.</p>

<p>I suppose I am looking for a harmonic, or additive function. Forgive me, my math knowledge is lacking.</p>
",<probability>
"<p>Is it</p>

<ul>
<li>The number of failures BEFORE the first success OR</li>
<li>The number of trials required to get a first success?</li>
</ul>

<p>Also, if I was to work out the expected value of a geometric random variable, say $p = 0.25$ (Expected value = $3$), does that mean that I will have $3$ failures AND THEN a success, or $2$ failures and then a success??</p>

<p>I would immensely appreciate some help here.
Thank you so much x</p>
",<probability>
"<p>The urn consists 4 balls (black and white) with at least 1 white. Two randomly drawed balls were both white. What it the probability of getting white ball again?</p>

<p>My solution:</p>

<pre><code>balls(w/b)    || 4:0  | 3:1  | 2:2  | 1:3  | 0:4
P(prior)      || 1/15 | 4/15 | 6/15 | 4/15 | 0/15
P(2w|prior)   || 1    | 1/2  | 1/6  | 0    | 0
P(posterior)  || 1/4  | 1/2  | 1/4  | 0    | 0
P(w|posterior)|| 1    | 1/2  | 0    | 0    | 0

P(w) = ∫P(w|u) P(u) du = 1 * 1/4 + 1/2 * 1/2 + 0 * 1/4 = 1/2
</code></pre>

<p>But the answer is $7/12$. Where have I made a mistake?</p>

<p>The prior was assumed:</p>

<pre><code>balls(w/b)    || 4:0 | 3:1 | 2:2 | 1:3 | 0:4
P(prior)      || 1/8 | 3/8 | 3/8 | 1/8 | 0/8
</code></pre>

<p>Meaning the first ball was white and all others were decided by coin toss. However, I think the original formulation does not correspond to this problem and should be replaced with something like: </p>

<pre><code>In an urn with a white ball one puts another 3 balls ...
</code></pre>
",<probability>
"<p>Let there be $2$ urns containing $a$ white, $b$ black balls and $c$ white, $d$ black balls respectively.Everytime a ball is drawn at random from the first urn  and is transferred to the second and similarly a ball from the second urn is transferred to the first urn , both events taking place  simultaneously at a particular moment.After n such operations, a ball is randomly selected from the first urn .Find the probability that it is white.</p>

<p>{Inspired by a problem of J.V. Uspensky}</p>
",<probability>
"<p>Let N denote the number of automobile accidents on a given stretch of interstate
highway over a specific period. It is known that N has the geometric distribution with pmf </p>

<p>$f_N(n)=x(1-x)^{n-1}$, $n=1,2,3,...$ </p>

<p>where x is bounded between 0 and 1, $0&lt; x&lt; 1$ </p>

<p>for the ith accident $Y_i=1$ if accident contains a fatality, </p>

<p>$Y_i=0$ if no fatality,</p>

<p>for each i, $P(Y_i=1)=p$, each $Y_i$ is independent</p>

<p>Let T be the total number of accidents with at least one fatality. </p>

<p>Then, $T=Y_1 + Y_2 +...+Y_N$</p>

<p>Find $E(T)$ and $Var(T)$,
I was thinking that since each $Y_i$ is bernoulli that is dependent on N then sum of each $E(Y_i)$ is $Nxp$ and the variance would be summing $p(1-p)$ which is $Npx(1-px)$</p>

<p>Maybe this is $E(T)=E(E(T|N))=\frac{p}{x}$ then $Var(T)=...$</p>

<p>Find $Corr(N,T)$=
$\frac{E(NT)-E(N)E(T)}{sd(T)sd(N)}$</p>

<p>Find $P(T=0)$</p>

<p>I seem to be missing something in my analysis that would lead me to get the rest of the problem</p>
",<probability>
"<p>Every night, different meteorologist gives the probability of rain for the next day. To judge their predictions, we use the following scoring system: if a meteorologist predicts rain with the probability $\color{blue}{p^*}$ and is right, that meteorologist receives a score of $1-(1-p)^2$; if wrong, they receive a score of $(1-p)^2$. After a while we will be able to know which meteorologist is the best. Assumming one meteorologist knows the scoring system, what is the best way for them to maximise their expected value?</p>

<p>I know they probably should predict with a 50% accuracy everytime because that is where both fucntion intersect but what is the formula i should use to get to the right answer?</p>
",<probability>
"<p>I've been trying to answer this question for the past few days, and I'm absolutely stuck. Without further ado, here's the mystery:</p>

<p>We are given a pair of boxes. There are n red balls in box number 1, and n blue balls in box number 2. Every turn, a ball is randomly taken out of the first box, and not returned. After the ball is taken out, a blue ball from the second box is inserted into the first one. This continues until there are no more blue balls to be transferred from box 2 to box 1. That means that there are n+1 turns in which a ball is randomly taken out of box 1. </p>

<p>I am asked to find the probability that the last ball taken out of box 1 (which means in the (n+1)-th turn) is red. Here's what I've got so far:</p>

<p>Event A - the ball taken out of box 1 in the last turn is red.</p>

<p>Event B(i) - i red balls were left in box 1 after n turns.</p>

<p>I'm trying to find P(A). Bayes wasn't helpful, and the law of total probability was not useful as well, since I can't seem to know what P(B(i)) is for any given i (other than, of course, i=0 and i=n-1). I did find a two-dimensional recursive function for P(B(i)), but that doesn't seem to be the right solution. </p>

<p>Any thoughts about how to properly approach (and hopefully solve) this question would be highly appreciated. </p>
",<probability>
"<p>Suppose we flip 10 times an unfair coin that fall a probability of $p$ on 'heads'. Knowing that we obtained 'heads' 6 times out of the 10 flips, find the conditionnal probability of the first three flips being heads, tails, tails.</p>

<p>My effort: Since we don't know the exact probability of getting heads i would think it would be somthing like:</p>

<p>A: Probability of getting 6 'head' out of 10
B: Probability for the first 3 flip to be 'heads''tails''tails'</p>

<p>$A:\binom{10}{6}(p)^6(1-p)^4$</p>

<p>$B:p(1-p)^2$</p>

<p>$(B|A):???$</p>

<p>Any help to point me in the right direction would be greatly appreciated.</p>

<p>Thank you.</p>
",<probability>
"<p>Suppose we have a set of $b^n$ different numbers. Every time we randomly choose a number from this set and put it in a list of length  $b^\frac  n2$.
So we want to fill this list with unique numbers. However every time we choose a number we place it back to the set, giving it another chance to be selected. of course we would like numbers on our list be unique so I guess the probability to have a list of unique numbers is:</p>

<p>$\prod_{i=0}^{b^\frac n2 -1} \left(\frac{b^n-i}{b^n}\right)$</p>

<p>With sufficiently large $n$ and regardless of base $b$, computer analysis shows that it converges into 0.606 but I cannot fathom the reason. I wonder if someone can show me how this happens.</p>
",<probability>
"<p>I have a circle iwth radius $r$. I want to test the hypothesis that $r \leq 2$ vs. $r &gt;2$ based on the posterior of $r$. $r$ follows the prior distribution: $f(r) = \frac{2}{r^{2}}$, $ r &gt;0.5$. I observed the following points $(x,y): (1,3), (2,1), (0.5,1.4)$. My question is What should be the likelihood?</p>

<p>Edited:
The center of the circle is (0,0)</p>
",<probability>
"<p>I have a large set $S$ of items, but the set is not exactly known. All I know are the cardinal numbers of <em>categories</em> i.e. a number of disjoint subsets, </p>

<p>$ \vert{S_1}\vert \dots \vert S_n\vert$ with $\bigcup S_i = S$. </p>

<p>Thus I also know the cardinal number $\vert S\vert = \sum\vert S_i\vert$.</p>

<p>I then want to apply a predicate $p$ and estimate the cardinal number of the subset of $T \subset S$ which satisfies $p$, i.e. </p>

<p>$\vert T\vert = \vert \lbrace x\in S \mid p(x)\rbrace\vert$. </p>

<p>The predicate $p$ is precisely known and so are the predicates $p_i$ which need to be satisfied to fall into one of the categories $S_i$ (the ""meanings"" of $S_i$ are known).</p>

<p>In other words, I want to describe a large set with a few numbers $ \vert{S_1}\vert \dots \vert S_n\vert$, which still contain enough information, so I can estimate the cardinal number of the result of the filtering with $p$. I am willing to do a costly transformation on $p$ as long as the final computation of $\vert T\vert$ is fast.</p>

<p>I have the feeling that this is a textbook problem, but I don't know where to look for ideas. </p>

<ul>
<li>I don't know how to do this at all, and </li>
<li>I don't know what is the best way to define $p_i$. </li>
<li>What happens when my $S_i$ are not disjoint? Can I still say <em>something</em> about $\vert S\vert$ and $\vert T\vert$?</li>
<li>I suppose the more $p_i$s I have, the better the estimate will be, but I'd like to be be able to say something quantitatively about the accuracy.</li>
</ul>
",<probability>
"<p>Let $n$ be a positive integer. Suppose $a$ and $b$ are randomly (and independently) chosen two $n$-digit positive integers which consist of digits 1, 2, 3, ..., 9. (So in particular neither $a$ nor $b$ contains digit 0; I am adding this condition so that division by $b$ will be possible, and that we don't get numbers of the form $0002$ and so on). Here ""randomly"" means each digit of $a$ and $b$ is equally likely to be one of the 9 digits from $\{1,2,3,..., 9\}$. </p>

<p>My question concerns the divisibility of these integers:</p>

<blockquote>
  <p>1) What is the probability that $b$ divides $a$ ?</p>
</blockquote>

<p>The answer, of course, will depend on $n$. Denote this probability by $p(n)$. I would be happy with rough estimates for $p(n)$ as well :)</p>

<blockquote>
  <p>2) Is it true that $p(n)\to 0$ as $n\to\infty$?</p>
</blockquote>

<p>I think answer to question 2) is yes (just by intuition). </p>
",<probability>
"<p>Let $X_1, \ldots, X_n$ be a collection of random variables. Consider the directed graph with vertex set $\{ 1, 2, \ldots, n \}$ where there is a directed edge $i \to j$ if $\mathbb{P}(X_i &gt; X_j) &gt; \frac{1}{2}$. </p>

<p><strong>Question 1:</strong> What directed graphs can arise in this way? Certainly they must be simple and have no loops. Is that the only restriction? Alternatively, $\mathbb{P}(X_i &gt; X_j) &gt; \frac{1}{2}$ defines an irreflexive antisymmetric relation on $\{ 1, 2, ... n \}$. Which such relations arise in this way? </p>

<p><strong>Question 2:</strong> Does the answer change if we require the $X_i$ to all be defined on a finite sample space? </p>

<p><strong>Question 3:</strong> What if we require the $X_i$ to be independent? </p>

<p>It is known (see <a href=""http://en.wikipedia.org/wiki/Nontransitive_dice"">nontransitive dice</a>) that this graph can have directed cycles even if the $X_i$ are independent; in particular, the corresponding relation need not be transitive. </p>
",<probability>
"<p>Assume that we have a biased coin with probability $p_1$ of getting H and $1−p_1$ of getting T on the first trial, $p_2$ of getting H and $1−p_2$ of getting T on the second trial and so on such that
$2/3&lt;p_1&lt;p_2&lt;p_3...&lt;p_n&lt;1$. The probability $p_i$ of getting H increases as long as we get head in a row. If a tail appears, then we reset to probability $p_1$ of getting H in the next trail and so on.  </p>

<p>What is the expected number of trials to get $n$ H in a row?</p>
",<probability>
"<p>If a particle performs a random walk on the vertices of a cube, what is the mean number of steps before it returns to the starting vertex S? What is the mean number of visits to the opposite vertex T to S before its first return to S and what is the mean number of steps before its first visit to T?</p>

<p>Nobody ever explained random walks to me, so I find it all very odd right now. Maybe someone can explain how to handle these problems or you can link me to a site where these kinds of problems are explained well? Thanks a lot!</p>
",<probability>
"<p>If U has a $\chi^2$ distribution with v df, find E(U) and V(U).</p>

<p>By definition, $E(U)
=\int^{\infty}_{0} u\frac{1}{\gamma(\frac{v}{2})2^\frac{v}{2}}u^{\frac{v}{2}-1} e^\frac{-u}{2}\,du 
=\int^{\infty}_{0} \frac{1}{\gamma(\frac{v}{2})2^\frac{v}{2}}u^\frac{v}{2} e^\frac{-u}{2}\,du$.</p>

<p>How do I integrate this?</p>

<p>Note: This isn't a homework problem.</p>
",<probability>
"<p>Recently I took a Master's-level class on probability. I was very interested in the material, but the class itself was average. It used a textbook which I didn't care for (by Grimmett and Stirzaker). I've been looking for another text on probability to deepen my knowledge. I am familiar with measure theory, so I am fine with a book that employs it.</p>

<p>I am looking at Feller's book <em><a href=""http://rads.stackoverflow.com/amzn/click/0471257087"" rel=""nofollow"">An Introduction to Probability</a></em>, which seems to be well-regarded. I really like the examples and the way the material is covered. One thing I'm concerned about is that the book does not seem to involve measure theory. </p>

<p>How much of the soul of probability will I be missing if I read a book which covers it from non-measure-theoretic perspective? Will I have to consult another resource if I want to really understand the heart of what's going on?</p>

<p>Perhaps I should use Feller to become acquainted with the basics of probability theory, and supplement it by referring to a more theoretical text for the ""real"" proofs of certain theorems?</p>
",<probability>
"<p>Suppose a random walk on an infinite line $[...-3,-2,-1,0,1,2,3,...]$, starting from 0. Probability to go right or left are equal. 
Does such a process stationary?
I think that it is NOT, since the support of each step is different. I.e., $x_1\in{\{-1,1\}},  x_2\in{-2,0,2}, ...$. 
Thanks.</p>
",<probability>
"<p>Let a stochastic process defines as:
$$X(t+1)=A X(t)+B U(t)$$
with: $X(t) \in R^n$, $U(t) \sim N(0,Q_t)$, $Q_t$ semi-positive-definite of size $n \times n$, $X(0) \sim N(0,W_0)$, $A$ of size $n \times n$, $B$ of size $n \times n$.</p>

<p>Is there any condition on $A$ and $B$ to state the existence and the form of the conditional pdf $p_{X_t|X_{t-1}}(x_t|x_{t-1})$?</p>

<p>Thanks in advance.</p>
",<probability>
"<p>Let $X\geqslant 0$ be a random variable. Then, we have</p>

<p>$$\mathcal{E}(X)=\int_0^\infty P(X&gt;t)dt$$</p>

<p>(provided $\mathcal{E}(X)$ exists).</p>

<p>Suppose we have a finite data set $\{(d_1, a_1), \ldots, (d_n, a_n)\}$ consisting of pairs or real numbers where $d_i$ stands for a level (height) of some vessel and $a_i$ is the area of the surface of the vesel at level $d_i$.</p>

<p>How can I apply the above mentioned formula to calculate the (expected) capacity of the vessel?</p>
",<probability>
"<p>Let $X,Y$ random variables with $Y$ real valued. I was wondering if any inequality of the type: 
\begin{equation}
\mathbb{E}[f(X,Y)] \leq g \left[\sup_{\displaystyle s \in \mathbb{R}} \mathbb{E}[f(X,s)] \right]
\end{equation}</p>

<p>exists, where $g$ is some function. It seems very wrong at first sight, but I was wondering if you knew something of this type.</p>
",<probability>
"<p>Assume ""standard"" bingo (75 numbers) with the columns ranging the following inclusive ""semi-random"" values B: 1 to 15, I: 16 to 30, N: 31 to 45, G: 46 to 60, O: 61 to 75. By semi-random I mean restricted to a small range (15 at a time).  There is a free space in the middle of the 5x5 playing board.  Numbers (from 1 to 75 inclusive) are randomly drawn one a time without replacement (without any repeats in each game) and with equal probability of being drawn.  A win (Bingo) is defined as a completed line segment of 5 adjacent squares made only from the drawn numbers but which may include the free space (and must include it if it is beneficial).  A bingo card has 25 of these board squares arranged in a 5x5 matrix.</p>

<p>So my question is if there are 20 players, each with a unique playing card (randomly generated by computer out of I think 552 septillion possible playing cards), what are the chances/probability that 2 or more players will get Bingo on the same drawn number?.  For example, someone could win bingo with as few as 4 drawn numbers but likely it would take much more.  So I am asking if balls are drawn until at least one person wins, what are the chances that at least 2 people will win at the same time?  The game is considered finished / decided when there is at least 1 winner for that game.  You can assume that all players are good enough not to make any mistakes (not true in real bingo but assume here).</p>

<p>I am not sure how to set this up mathematically and because there are so many possible bingo cards, computer simulation of all of them is not a good idea.  Perhaps what can be done with simulation is to first simulate 20 legitimate bingo cards (out of 552 septillion), and then have the computer draw one random number at a time until we have at least 1 winner.  Do this for maybe 1 million trials and count how many have simultaneous multiwinners.  For example, after 11 balls drawn there are no winners yet for that game but on the 12 drawn ball, there are 2 or more winners.  I would like to know how often the multiwinner situation occurs.</p>

<p>I could probably do the simulation with a fair amount of work but wanted to know if this problem can be done mathematically or if is too difficult to set up.</p>

<p>One concern I see is that if one card has for its first column (the B column), from top to bottom, 1, 4, 7, 10, and 15 and some other card has for column B (also from top to bottom), 15, 10, 7, 4, and 1 in that order.  Problem there is even though the cards have different order for the B column numbers, if those 5 numbers are drawn, both players may win at the same time.  So my point is it makes a difference if we say (or not) that multiple bingo cards cannot have the same exact 5 numbers in a row, column, or diagonal but just in a different order.  That might be an interesting problem in itself to figure out how many fewer ""legit"" bingo cards there are with that constraint so someone could comment about it but the actual question is about the multiwinner probability.  I think the answer to the no permutation bingo card restriction is about 111 quadrillion legit bingo cards.</p>
",<probability>
"<p>I am trying to prove that if F is a cdf with finite right extreme ($\tau &lt; \infty $), then $G=F(\tau - 1/x) , x&gt;0$ is a cdf on $(0,\infty)$. For one of the steps:
$$
\lim_{x \to 0} F( \tau - 1/x)
$$
and this should be equal to zero. Intuitively $-1/x$ goes off to $-\infty$ so the limit should be $F(-\infty) =0$ but should I be making another step first? or does the jump make sense?</p>
",<probability>
"<p>John and Mary arrive under the clock tower independently. Let X be John's arrival time and let Y be Mary's arrival time. If John arrives first and Mary is not there then he will leave. If Mary arrives first then she will wait up to one hour before leaving. John's arrival time, X, is exponentially distributed with mean of 1. Mary's arrival time has density $f(y) = {2y\over 9}$, $0≤y≤3$. Calculate the probability they will meet. </p>

<p><strong>What I've tried.</strong> </p>

<p>I figured this problem should be the summation of two probabilities. The probability that Mary arrives before John + The probability that John arrives within an hour of Mary. </p>

<p>the joint distribution is $f(x,y) = {2y\over 9}e^{-x}$, therefore the first probability could be written as $P[X&gt;Y]$ of the joint distribution. </p>

<p>$$\int_0^3 \int_0^y f(x,y) dxdy$$</p>

<p>this gives the answer of .822, which is well above the answer given in the book of .1125, without the second probability even being found. </p>

<p>I'm not even sure how to set up the limits for the second probability.  </p>

<p>Any help would be appreciated. </p>
",<probability>
"<p>If I have that $\limsup_{n}E|X_n|^{r} \leq E|X|^{r}$, is that enough to show that $\{|X_n|^{r}:n\geq 1\}$ is uniformly integrable? I am not sure here if the limsup condition here is as strong as if I had a uniformly bound. Does anyone have any hints as to how I can work with such a definition? Thanks.</p>
",<probability>
"<p>Even before attempting the problem, I immediately defaulted to an answer: $\frac{1}{2}$.</p>

<p>I thought that this was a possible answer since the probability of flipping a head on one flip is definitely $\frac{1}{2}$.</p>

<p>I then worked through the problem:</p>

<p>Let E be the event in the problem statement.</p>

<p>The total number of possible outcomes of $10$ coin flips: $N = 2^{10}$</p>

<p>The total number of ways in which E can result:for $n=10,r=5, nCr= 252$</p>

<p>$P(E) =\frac{n}{N} = .246$</p>

<p>How do I correct my intuition?</p>
",<probability>
"<p>I met an interesting but challenging problem in my homework:</p>

<p>Suppose $n-1$ independent points $x_1$, $x_2$, ..., $x_{n-1}$ are uniformly distributed on unit interval [0,1]. These $n-1$ points seperate the unit interval into $n$ pieces. Suppose the lengths of these $n$ intrvals are $v_1$, $v_2$, ..., $v_n$. What is the probablity that $v_i &lt; a$ for $i = 1,2,...,n$ ?
Here $1/n &lt; a &lt; 1$ is a constant.</p>

<p>The joint distribution of $v_1$, $v_2$, ..., $v_{n-1}$ is a uniform distribution on a $n-2$ dimention simplx with $f(v_1, v_2,...,v_{n-1}) = (n-1)!$. The simplex is
$$v_i \geq 0, \; i=1,2,...,n-1$$
$$v_1 + v_2 + ... + v_{n-1} \leq 1$$
The proof of this density distribution is not easy.</p>

<p>I followed this idea but I found it's still hard. Maybe there are some other starting points to consider this problem.</p>
",<probability>
"<blockquote>
  <blockquote>
    <p>Consider the situation of decoding a 6-digit password that consists of the symbols A to Z and 0 to 9, where all possible combinations are tried randomly and uniformly. </p>
  </blockquote>
</blockquote>

<p>Consider the following decoding method: At first a combination is chosen randomly and uniformly. At the next trial a digit from this combination is chosen uniformly at random and its entry is substituted by a uniformly randomly chosen element from $\left\{A,...,Z,0,...,9\right\}$. This procedure is repeated until the password is found. </p>

<p>(a) What is the probability that the correct password will never be entered?</p>

<p>(b) What is the probability that eventually the same combination will be entered two consecutive times?</p>

<hr>

<p>I already asked how to get the anwers to (a) and (b) without using the here mentioned special decoding method and I got great help, see <a href=""http://math.stackexchange.com/questions/1012075/probability-concerning-a-6-digit-password"">Probability concerning a 6-digit password</a>.</p>

<p>Now I have to answer (a) and (b) using the decoding method and again I have enormous problems! Combinatorical thoughts are not my favourite business. Nevertheless I tried to find the probabilities in an analog way as it was shown to me in the linked thread. </p>

<p>Additionally, <em>I wonder if this task now maybe has something to do with Markov chains</em> because the lecture this task is from is about Markov chains.</p>

<hr>

<p>I think there are (at least) the two following ways to understand the described decoding strategy, which sense is meant?</p>

<p><strong>Sense 1</strong> <em>We choose (randomly and uniformly) one of the $36^6$ possible combinations. If it is the right, we stop. Otherwise we then choose (randomly and uniformly) one of the 6 digits and substitute it (randomly and uniformly) by a symbol out of the alphabet. Then we choose (randomly and uniformly) one of the remaining 5 digits and subtitute it and so on until we substituted all the 6 digits. If this was the right password, we stop. If not, we again start substituting the 6 digits (the digits of the combination that we chosed at the beginning, that is, we stick to this combination).</em></p>

<p><strong>Sense 2</strong> <em>Same as above with the difference that after we substituted all 6 digits and saw that it is the wrong password, we choose (randomly and uniformly) another combination out of the $36^6$ possibilities (it can be the same as before) and then substitute the digits of this new combination.</em></p>

<p><strong>I think that sense 1 is meant and thus I considered the task in this sense.</strong></p>

<p><strong>(a)</strong> Anyway, my result here is $0$, because as far as I see the probability not to have reached the right password after n passages is
$$
\left(1-\frac{1}{36^6}\right)\cdot\left(1-\prod_{k=1}^6\frac{1}{k\cdot 36}\right)^n
$$
and this tends to $0$ as $n\to\infty$.</p>

<p><em>Remark</em>: If we decode without this special method (see the linked thread) then the probability of (a) is 0, too.</p>

<p><strong>(b)</strong> The probability that we have eventually one pair of consecutive equal guesses is - to my results - 
$$
\sum_{n=0}^{\infty}\left(1-\frac{1}{36^6}\right)\cdot\left(\prod_{k=1}^6\frac{35}{k\cdot 36}\right)\cdot\left(\prod_{k=1}^6\frac{34}{k\cdot 36}\right)^n\left(\prod_{k=1}^6\frac{1}{k\cdot 36}\right)
$$</p>

<p><strong>Edit</strong></p>

<p>I think my last result for <strong>(b)</strong> was <em>not</em> correct, I think instead it has to be
$$
\sum_{n=0}^{\infty}\left(1-\frac{1}{36^6}\right)\cdot\left(1-\prod_{k=1}^k\frac{1}{k\cdot 36}\right)\cdot\left(1-\prod_{k=1}^{6}\frac{2}{k\cdot 36}\right)^n\cdot\left(\prod_{k=1}^{6}\frac{1}{k\cdot 36}\right).
$$</p>

<p>If I know compute the geometrical series and use that $1-\prod_{k=1}^{6}\frac{1}{k\cdot 36}\approx 1$, then I get that (with $p:=\frac{1}{36^6}$) the probability of (b) is
$$
\approx \left(\frac{1}{2}\right)^6\cdot (1-p)
$$</p>

<p><em>Remark</em>: When decoding without this method (see the linked thread) then the probability of (b) is $\frac{1}{2}\cdot (1-p)$. That is, using the method, if my result is correct, we have a much smaller probability for (b). So this decoding method is more efficient.</p>

<hr>

<p>Would be great to get a feedback from you to know if I am right. And, as mentioned, I am interested to know if the task has something to do with the context of Markov chains.</p>

<p>Ciao &amp; greetings</p>

<p>Salamo</p>
",<probability>
"<p>I am working through Hamming's <em>The Art of Probability</em> and am having trouble with a problem in the Bernouilli Trials section. The wording is the following</p>

<blockquote>
  <p>Expand the binomials in the probabilities of 0, 1, 2, and 3 occurrences, and show that the expansions cancel out to the next term provided $np \lt 1$. Hence if $np \ll 1$, the first term neglected in the expansion is close to the exact result for 4 or more events.</p>
</blockquote>

<p>I am assuming that the solution should give something like $\sum_{k = 0}^{3}B(k; n, p) = np + \mathcal{O}\left [(np)^4 \right ]$ but I can't actually get anything that cancels to the first order.</p>

<p>Using the recursion relation of Bernouilli trials, $B(k+1; n, p) = \frac{n - k}{k+1}\frac{p}{q}B(k; n,p)$, I get</p>

<p>$$(1 - p)^n \left (1 + \frac{np}{q} + \frac{n(n-1)}{2}\frac{p^2}{q^2} + \frac{n(n-1)(n-2)}{6}\frac{p^3}{q^3} \right)$$</p>

<p>Expanding this and keeping the terms 0th and first order in $np$ yields</p>

<p>$$(1-p)^n \left (1 - \frac{1}{6}\frac{6 p^3 -29 p^2 + 33 p - 12}{(1-p)^3} + \mathcal{O}\left [(np)^2 \right ]\right )$$</p>

<p>Am I misunderstanding the question? I expected the second term to cancel.</p>

<p><strong>Edit:</strong> I guess one could use the expansion for $\exp (-np) \approx \left ((1-p)^{\frac{1}{p}} \right )^{-np}$ and then expand it in a Taylor series, $\exp (-np) = 1 - np + \frac{(np)^2}{2} - \frac{(np)^3}{6} + \mathcal{O}\left [(np)^4\right ]$. This matches the terms of the form $(np)^k$ and they do have the opposite signs, but I don't quite understand why you can get away with ignoring the $q$ in the denominator. That is,</p>

<p>$$\begin{eqnarray}
&amp; \exp (-np) + \frac{np}{q} + \frac{n(n-1)}{2}\frac{p^2}{q^2} + \frac{n(n-1)(n-2)}{6}\frac{p^3}{q^3} \\ 
\approx &amp; 1 - np + \frac{(np)^2}{2} - \frac{(np)^3}{6} + \frac{np}{q} + \frac{n^2p^2}{2q^2} + \frac{n^3p^3}{6q^3} + \mathcal{O}\left [(np)^4\right ]\\
\approx &amp;1 + \mathcal{O}\left [(np)^4\right ] ?
\end{eqnarray}$$</p>
",<probability>
"<p>The hydrocarbon emissions are known to have decreased dramatically during the 1980s.</p>

<p>A study was conducted to compare the hydrocarbon emissions at idling speed, in parts per million (ppm), for automobiles of 1980 and 1990. Ten cars of each year model were randomly selected and their hydrocarbon emission levels were recorded. The data are as follows:</p>

<p>1980 models: 295 545 236 388 290 152 391 291 132 206</p>

<p>1990 models: 281 279 212 157 241 121 275 134 139 217</p>

<p>Assume that the hydrocarbon emission levels are normally distributed.</p>

<p>(a) Conduct a hypothesis test (by specifying the critical region) about the variability in hydrocarbon emission of cars of 1980 versus 1990. Use a 0.10 signiﬁcance level.</p>

<p>(b) Conduct a two-sided hypothesis test (by specifying the critical region) to compare the means of the hydrocarbon emission level of cars of 1980 and 1990 models. Use a 0.10 signiﬁcance level.</p>

<p>(c) Compute (or give bounds to) the p-value for the test in (b).</p>
",<probability>
"<p>A roulette from a casino contain 18 red cases, 18 black cases and 1 green cases. A player shows up with $10$dollars. He decides to bet $1$dollar on red 10 consecutives times. If it's red, he wins a dollar and if not he loses his dollar. Let $S$ be the the amount of money the player has after 10 bets. Find the value $S$ can have, then calculate $P(S&lt;3|S\leq18)$.</p>

<p>So the value S can take is every natural numbers $0\leq S\leq20$</p>

<p>but after that I'm confused on how to proceed. I guess we could do it using a density function but i don't know how.</p>

<p>Any help to point me in the right direction would be greatly appreciated.</p>

<p>Thank you.</p>
",<probability>
"<p>In the book, ""A Practical Guide to Quantitative Finance Interviews"", the following question is posed on page 75:</p>

<blockquote>
  <p>Jason throws two darts at a dartboard, aiming for the center.  The second dart lands farther from the center than the first.  If Jason throws a third dart aiming for the center, what is the probability that the third throw is further from the center than the first?  Assume Jason's skillfulness is constant.</p>
</blockquote>

<p>At first glance, it seems the second throw is irrelevant - since his skillfulness is constant, the second throw in no way influences his third throw.  Furthermore, it seems like this question can only be answered in terms of the distance from the center of the first toss.  Indeed, denote this distance by $D_1$.  Then assuming the area of the dart board is $1$, the probability that the third throw is further than the first is just the area of the annulus, which is given by $1 - \pi D_1^2$.</p>

<p>However, the solution given in this text is $\frac{2}{3}$, which is arrived at by enumerating the possible outcomes.  Let $D_i$ be the distance of the $i$th throw.  Then the possible outcomes assuming the second throw is further than the first are:
$$
D_3 &lt; D_1 &lt; D_2 \\
\boxed{D_1 &lt; D_3 &lt; D_2} \\
\boxed{D_1 &lt; D_2 &lt; D_3}
$$</p>

<p>The two boxed outcomes satisfy our event in question, so the probability must be $\frac{2}{3}$.  But, this seems to be answering a different question, namely,</p>

<blockquote>
  <p>What's the probability the third throw is not the best out of three, given the second throw is worse than the first?</p>
</blockquote>

<p>Are the two block-quoted questions really asking the same thing, and I've misunderstood the first one?</p>
",<probability>
"<p>If $A \sim N (\mu, \frac{1}{a})$ and $B \sim N(0, \frac{1}{b})$, and $S = A + B$, then what is
the distribution of $A$ given $S=s$? Assume $A$ and $B$ are independent.</p>
",<probability>
"<p>Let $X(t)$ be a stationary Gaussian process with mean $\mu$, variance $\sigma^2$ and stationary correlation function $\rho(t_1-t_2)$. If $X(t)$ is a white noise process the correlation function is given by the Dirac delta function $\rho(t_1-t_2) = \delta(t_1-t_2)$.</p>

<p>The integral of this process is given by:</p>

<p>$$I = \int_0^L X(t) \, dt$$</p>

<p>According to this <a href=""http://stats.stackexchange.com/questions/229877/max-and-min-variance-of-the-integral-of-a-stationary-stochastic-process/229896?noredirect=1#comment435077_229896"">CrossValidated post</a> the variance of $I$ is given by:</p>

<p>$$\text{Var}[I] = L\sigma^2$$</p>

<p>However this does not agree with the results I obtained through simulation. The approach is to discretise the white noise Gaussian process into $N$ independent normal variables. The integral can then be approximated through:</p>

<p>$$ I = \int_0^L X(t) \approx \frac{L}{N}\sum_{i=1}^NX_i$$</p>

<p>Where $X_i$ are indepedent random variables $X_i \sim \mathcal{N}(\mu,\sigma^2)$. In simulation I find that as $N$ grows large then $\text{Var}[I] \rightarrow 0$. Why does it not approach $L\sigma^2$? What is the problem with my approximation?</p>
",<probability>
"<p>Let $X, Y$ be conditionally independent given $Z$. Based on this, I am trying to get an intuition of what happens with the associated sigma-algebras and the conditional expectation. In particular, I want to calculate </p>

<p>$$ \mathbb{E}[B \vert X, Y,Z] = \mathbb{E}[B\vert \mathcal{F}]$$</p>

<p>where $B$ is an event and $\mathcal{F}$ is the sigma-algebra generated by $X,Y,Z$. Given the conditional independence stated above, I know that $\sigma(X)$ and $\sigma(Y)$ are conditionally independent given $\sigma(Z)$, but what does that mean for $\mathcal{F}$? Could I somehow partition this sigma-algebra into two independent components depending on $X,Z$ and $Y,Z$? In general, any intuition about the structure of $\mathcal{F}$ given conditional independence would be great.</p>
",<probability>
"<p>Given $$X_1\sim f_{X_1}(x_1)$$ and $$X_2\sim f_{X_2}(x_2)$$ are independent Random Variables, does this mean that $$Z=X_1+X_2$$ has distribution $$f_Z(z)\sim f_{X_1}f_{X_2} $$  or does it mean that the distribution is given by convolution of the two pdfs.</p>
",<probability>
"<p>In a Poker match, asumming 52 cards (13 of each type). This is the state:</p>

<ul>
<li><p>On my hand I have cards [3] and [4] of any type.</p></li>
<li><p>In table, there are these cards: [1] [2] [3] [?] [?], this is, two unknown cards and 1 [3].</p></li>
<li><p>There are other two players, each one with two unknown cards. So in the deck there are remaining 52 - 6 = 46 cards.</p></li>
</ul>

<p>The question is: <strong>what is the probability of other players to get Three of a kind composed of [3]. (Only three of a kind, no four) in ""river""?</strong></p>

<p>I can try to solve this problem partioning probabilities first for table, then for each case probability for next player, then for each case for the last player. But I'll need to solve the same problem for a table for 5 players, so this will be very laborious.</p>
",<probability>
"<p>Here is the problem that I'm solving:<img src=""//i.stack.imgur.com/6NGRt.jpg"" alt=""enter image description here""></p>

<p>So a) was quite easy (if I didn't miss anything :) )
Now for b): my CDF does converge to Exp(1) when x is from 0 to n</p>

<p>But if x more then n my function is constant 1 and Exp(1) is not. </p>

<p>Would it be a good explanation if I say that when n goes to infinity, x less than n with probability 1?
Would appreciate any constructive critique of my solution and advice on explaining part b)</p>
",<probability>
"<p>How do I solve the following?</p>

<p>$$
\lim_{x \rightarrow \infty} \int_0^{x} \left[ 1 + \text{erf} \left( \frac{\epsilon - a}{b} \right) \right] \left[ 1 + \text{erf} \left( \frac{\epsilon - c}{d} \right) \right] d\epsilon
$$</p>

<p>Related (but the problem is slightly different):
<a href=""http://math.stackexchange.com/questions/63026/integral-of-product-of-two-error-functions-erf"">Integral of product of two error functions (erf)</a></p>

<p>Or help me solve this (simpler version of the above)...</p>

<p>$$
\lim_{x \rightarrow \infty} \int_0^{x} \text{erf} \left( \frac{\epsilon - a}{b} \right) \text{erf} \left( \frac{\epsilon - c}{d} \right) d \epsilon
= \lim_{x \rightarrow \infty} \frac{4}{\pi} \int_0^{x} \int_0^{ \frac{\epsilon - a}{b} } \int_0^{ \frac{\epsilon - c}{d} } e^{-t^2 - s^2} ~ ds ~ dt ~ d\epsilon 
$$</p>

<p>Mathematica can provide a closed form solution in terms of erf if we change the order of integration from $ds ~ dt ~ d\epsilon$ to $d\epsilon ~ ds ~ dt$.</p>

<p>We can let,
$$
f(\epsilon, t) = \int_0^{ \frac{\epsilon - c}{d} } e^{-t^2 - s^2} ~ ds
$$</p>

<p>So we can focus on changing the order from $ds ~ dt ~ d\epsilon$ to $ds ~ d\epsilon ~ dt$.
$$
\int_0^{x} \int_0^{ \frac{\epsilon - a}{b} } f(\epsilon, t) ~ dt ~ d\epsilon
$$</p>

<p>To obtain,
$$
\int_0^{\frac{x-a}{b}} \int_{tb+a}^x f(\epsilon, t) ~ d\epsilon ~ dt - \int_{-\frac{a}{b}}^0 \int_0^{tb+a} f(\epsilon, t) ~ d\epsilon ~ dt \\
\int_0^{\frac{x-a}{b}} \int_{tb+a}^x \int_0^{ \frac{\epsilon - c}{d} } e^{-t^2 - s^2} ~ ds ~ d\epsilon ~ dt - \int_{-\frac{a}{b}}^0 \int_0^{tb+a} \int_0^{ \frac{\epsilon - c}{d} } e^{-t^2 - s^2} ~ ds ~ d\epsilon ~ dt
$$</p>

<p>Next, to switch the integral order from $ds ~ d\epsilon ~ dt$ to $d\epsilon ~ ds ~ dt$. Someone please continue the rest...</p>
",<probability>
"<p>I'm trying to prove urns version of Laplace's law of succession my professor suggested. Laplace's law states that the chance that the next trial is a success given $j$ successes out of the first $n$ is $\frac{(j+1)}{(n+2)}$. Here is how the problem states: </p>

<p>""If we have $n+k+1$ urns and urn $i$ has $i$ balls labeled $1$ and $n+k-i$ labeled zero. We pick an urn at random and draw $n$ balls from it without replacement say $j$ of them are ones. Show that the conclusion of Laplace's law holds for this setup. In other word, the chance that the next ball is a one is $\frac{(j+1)}{(n+2)}$.""</p>

<p>I've proved one version of the law, which close to this version <a href=""http://math.stackexchange.com/questions/102417/an-elementary-version-of-laplaces-method-of-succession"">An elementary version of Laplace&#39;s Method of Succession</a>. I tried to use similar approach but somehow my answer always in form of $k$ and I can't get rid of it. What is the intuition behind this $k$? Is it just to increase the complexity of the problem, or it has some meaning behind it?     </p>
",<probability>
"<blockquote>
  <p>The probability that a randomly chosen male has a circulation problem is 0.25.  Males who have a circulation problem are twice as likely to be smokers as those who do not have a circulation problem.  What is the conditional probability that a  male has a circulation problem, given that he is a smoker?</p>
</blockquote>

<p>Let:</p>

<p>C = Circulation Problem</p>

<p>S = smoker.</p>

<p>The problem is asking you to solve for $\Pr(C \mid S)$.  I thought I could solve problem by using a table and I completed it below.  I interpreted the statements as such:</p>

<p>1:  ""a randomly chosen male has a circulation problem is 0.25"":  $\Pr(C)=0.25$</p>

<p>2:  ""Males who have a circulation problem are twice as likely to be smokers as those who do not have a circulation problem."":  $\Pr(C \cap S) = 2x$ and $\Pr(C' \cap S) = x$</p>

<p>\begin{array}
{|c|c|c|c} \hline &amp; C &amp; C' &amp;  \\ \hline S&amp; 2x&amp; x&amp;\\ \hline S'&amp; &amp; &amp;\\ \hline &amp; 0.25 &amp; &amp;1\\ 
\hline . \end{array}</p>

<p>Which becomes:</p>

<p>\begin{array}
{|c|c|c|c} \hline &amp; C &amp; C' &amp;  \\ \hline S&amp; 2x&amp; x &amp; 3x\\ \hline S'&amp; &amp; &amp;\\ \hline &amp; 0.25 &amp; 0.75 &amp;1\\ 
\hline . \end{array}</p>

<p>So I did $\Pr(C\mid S) = \cfrac{\Pr(C \cap S)}{\Pr(S)}=\cfrac{2x}{3x}=2/3$ which is wrong.  The answer is 2/5!  </p>

<p><strong>Am I incorrectly interpreting statement 2 above as an intersection when instead it is a conditional probability?  In other words, should I be writing $\Pr(S\mid C)=2x$ and $\Pr(S\mid C')=x$?</strong></p>

<p>Thank you in advance.</p>
",<probability>
"<p>I've got a general question regarding a certain sticking point I often encounter. When tackling questions where for example an UMVUE (uniformly minimum-variance unbiased estimator) has to found I get stuck at the part where an expectation has to be determined. Maybe I'm overlooking certain theorems. </p>

<p>Example 1.  Consider a random sample of size $n$ with $X_i\tilde{}Poi(\mu)$. </p>

<p>I am able to find the UMVUE of $\mu$ and the MLE (Maximum Likelihood Estimator) of $\theta=e^{-\mu}$. But then for example follows the question: is this MLE unbiased for $\theta$?</p>

<p>If I want to determine the expectation, should I just write out the sum as follows from the definition of the expected value of a discrete distributed variable? This seems to lead to quite an ugly expression...</p>

<p>(<em>For this particular problem I found that Jensen's Inequality could come in handy to show biasedness (or actually non-unbiasedness))</em> </p>

<p><strong>But if I could summarize the problem I've got: how to rewrite expected values of functions of stochast in general?</strong>
Which theorems or properties should I use... for example I do use $E(aX)=aE(X)$ for $a$ a constant and $var(X)=E(X^2)-E(X)^2$ to rewrite certain expectations to a combination of simpler/known ones.</p>

<p>Example 2. Take a random sample $n$ of a distribution with pdf $f(x;\theta)=\theta x^{\theta -1}$ if $0&lt;x&lt;1$ and else $0$ and with $\theta &gt; 0$.</p>

<p>Find the UMVUE of $\theta$</p>

<p>Here I get stuck again at the expectation. I've got a sufficient and complete statistic in 
$\sum ln(X_i)$.</p>

<p>The expected value, i'd figure, is equal to $nE(ln(X_i))=-n/ \theta$  $\,\,\,\,$(where $[-ln(X)]=\frac{1}{\theta}$ was given as a hint).</p>

<p>But what if I want to determine the expected value of $\frac{1}{\sum_{i}ln(X_i)}$ ?</p>

<hr>

<p>In general: how to determine the expectations of functions of stochasts?
For example $E[ln(X)]$or $E[e^{\bar{X}}]$...?</p>
",<probability>
"<p>Let $X$~Binom($n,1/2$) and $Y$~Binom($m,1/2$) be independent. Calculate $P(X=Y)$.</p>

<p>My attempt:</p>

<p>Assume $m\le n$
$$P(X=Y)=\sum_{k=0,\ldots,m} P(X=k)P(Y=k)=(\frac{1}{2})^{n+m} \sum_{k=0,\ldots,m}{n \choose k}{m \choose k}$$</p>

<p>I have no idea how can I move further. Any ideas?</p>
",<probability>
"<p>I have a question regarding conditional probabilities. </p>

<p>Experiment: we toss a coin $10$ times. We count the amount of head and we toss that amount again. Let $X$ be the amount of heads in the first $10$ trials and $Y$ the total amount of heads. </p>

<p>Clearly, $X$~$bin(10, 1/2)$. I suppose $Y$~$bin(10+X, 1/2)$, but not sure about that though.</p>

<p>Question: what is $P[Y=5|X=7]$. But that seems pretty silly to me, because this basically says: 'What is the chance of having $5$ heads in total given that we have $7$ heads in the first $10$ trials'. Isn't that just $0$ since $Y \geq X$? </p>
",<probability>
"<p>Let $A,B,C$ be events. The event ""$A$ and $B$ occur but $C$ does not"" may be expressed as $A \cap B \cap C^c$. </p>

<p>(a)  Find an expression for the event ""at least one of B and C occur, but A does not""</p>

<p>(b)  Show that the probability of event in (a) is equal to</p>

<p>$\mathbb{P}(B)+\mathbb{P}(C)-\mathbb{P}(B\cap C)-\mathbb{P}(A\cap C)+\mathbb{P}(A\cap B \cap C)$</p>

<hr>

<p>I claim that the answer to (a) is $(B \cup C)\cap A^c$. Can someone confirm or deny?</p>

<p>I have no idea how to proceed from here. From previous work, I have proven the following results:</p>

<p>$\mathbb{P}(A\setminus B)=\mathbb{P}(A)-\mathbb{P}(A \cap B)$</p>

<p>$\mathbb{P}(A\cup B)=\mathbb{P}(A)+\mathbb{P}(B)-\mathbb{P}(A\cap B)$</p>

<p>My main concern about (a) is what exactly they mean by ""at least one"" and the use of the operator ""and"".</p>
",<probability>
"<p>Studying for a final:</p>

<blockquote>
  <p>1) Suppose we have four indistinguishable red balls, 6 indistinguishable blue balls, and 2 indistinguishable green balls. How many different color patterns can be obtained by arranging these balls in a straight line?</p>
</blockquote>

<p>I did ${12\choose4} {12\choose6} {12\choose2} = 30,187,080$ but that's definitely not correct.</p>

<blockquote>
  <p>2) A fair, ordinary six-sided die is colored red on one face, blue on two faces, and green on the remaining three faces. Find an explicit expression (but do not simplify it) for the probability that, in the 12 rolls fo this die, red will come up 4 times, blue will come up 6 times, and green will come up 2 times? <em>Hint: What is the probability of observing the sequence RBBRGBBRGBRB?</em></p>
</blockquote>

<p>I'm not really sure what to do and the hint only made me even more confused.</p>
",<probability>
"<blockquote>
  <p>If $X$ is a normally distributed random variable with standard deviation $\sigma=10$, and $P(X&gt;16.34) = .1212$, what is the mean (expected value) of $X$?</p>
</blockquote>

<p><strong>Attempt at solution:</strong>
This problem doesn't make sense... standard deviation is given, by the probability $X&gt;16.34$ has no upper bound, so how can this be computed? The expected value is just the summation of all the values which $=.1212$ here, so I'm not exactly sure what is being asked. please help! </p>
",<probability>
"<p>Let $X_n\xrightarrow[d]{}N(0,\sigma^2_x)$ and $Y_n\xrightarrow[d]{}N(0,\sigma^2_y)$.</p>

<p>$X_n, Y_n$ are not independent.</p>

<p>Can I say that $\left( \begin{array} {}
X_n \\
Y_n \end{array} \right)\xrightarrow[d]{}N(\mathbf{0},\mathbf{C})$, with $\mathbf{C}$ a variance-covariance matrix?</p>

<p>Would $\mathbf{C}=\left( \begin{array}{ccc}
\sigma^2_x &amp; \lim Cov(X_n,Y_n)  \\
\lim Cov(Y_n,X_n) &amp; \sigma^2_y \end{array} \right)$ ?</p>
",<probability>
"<p>I have a system of linear equations as follows.</p>

<blockquote>
  <p>$$M(p) = 1+\frac{n-p-1}{n}M(n-1) + \frac{2}{n} N(p-1) + \frac{p-1}{n}M(p-1)$$
   $$N(p) = 1+\frac{n-p-1}{n}M(n-1) + \frac{p}{n}N(p-1)$$
   $$M(1) = 1+\frac{n-2}{n}M(n-1) + \frac{2}{n}N(0)$$
   $$N(0) = 1+\frac{n-1}{n}M(n-1)$$</p>
</blockquote>

<p>$M(p)$ is defined for $1 \leq p \leq n-1$.  $N(p)$ is defined for $0 \leq p \leq n-2$.  What is $M(n-1)$?</p>
",<probability>
"<p>I have a stochastic process $X_t$, and I have a function $a(x | t)$ that reflects my beliefs about the value of $X_t$ ($a$ is a density function in its first parameter).  I am studying the properties of the stochastic process $Y_t = \int_0^t X_s ds$.  I am thinking of using the following method to find a density function $b(y | t)$ for $Y_t$:</p>

<p>Let $M_n$ be a function that returns the $n^{th}$ moment of a random variable.  By Fubini's Theorem, $\int_0^t M_n(X_s) ds = M_n(\int_0^t X_s ds) = M_n(Y_t)$.  Since this gives me a function that spits out all moments of $Y_t$, and since a random variable is uniquely determined by its moments, this is enough information to find $Y_t$.</p>

<p>My questions:
(1) Have I applied Fubini's Theorem correctly?  I'm having trouble formalizing the proof of that first equality, but I intuitively feel that it's true.
(2) Are there any other obvious flaws with this method?</p>

<p>Thank you.</p>
",<probability>
"<p>We are preparing this for an exam.</p>

<p>Given the division of a plane into a number of regions of different sizes. We would like to find, or guess, which is the biggest region, by doing the following.</p>

<p>We will shoot a number of random points at the plane, and then conclude that the region containing the most points, is also the biggest.</p>

<p>The question is: how do use Chernoff bounds to say how many random points we need to shoot, to know that we have found the biggest region with, say, 75% probability? </p>
",<probability>
"<p>All the workers at a certain company drive to work and park in the company’s lot. The company is interested in estimating the average number of workers in a car. Which of the following methods will enable the company to estimate this quantity? </p>

<ol>
<li>Randomly choose $n$ workers, find out how many were in the cars in
which they were driven, and take the average of the $n$ values.</li>
<li>Randomly choose $n$ cars in the lot, find out how many were driven
in those cars, and take the average of the $n$ values.</li>
</ol>

<p>My intuition goes for number 2, but I'm not able to justify it formally.</p>
",<probability>
"<p>There is a notation used in many sources (e.g. Wikipedia: <a href=""http://en.wikipedia.org/wiki/Exponential_family"" rel=""nofollow"">http://en.wikipedia.org/wiki/Exponential_family</a>) for the natural parameters of exponential family distributions which I do not understand, and I cannot find a description of.</p>

<p>With vector parameters and variables, the exponential family form has the dot product between the vector natural parameter, ${\boldsymbol\eta}({\boldsymbol\theta})$ and the vector sufficient statistic, ${\mathbf{T}}({\mathbf{x}})$, in the exponent. i.e. $e^{{\boldsymbol\eta}({\boldsymbol\theta})^{\top}{\mathbf{T}}({\mathbf{x}})}$. </p>

<p>However, many examples of these parameters for different distributions are vectors composed of matrices &amp; vectors. E.g. the multivariate Normal distribution has parameter $[\Sigma^{-1}\mu\space\space-\frac{1}{2}\Sigma^{-1}]$ and sufficient statistic $[\mathbf{x}\space\space\mathbf{xx^{\top}}]$.</p>

<p>So what are these ""vectors"" and moreover, how is the dot product between them defined? Does this notation have a name?</p>
",<probability>
"<p>Should I use a certain table for this question or should I use a special formula. A random value has a normal distribution with the mean 102.9 and the standard deviation 4.7. What are the probabilities that this random variable will also take on a value</p>

<p>a. Less than 110.1;</p>

<p>b. Greater than 95.6;</p>

<p>c. Between 104.5 and 105.9;</p>

<p>d. Between 98.7 and 150?</p>

<p>I have worked so far by finding the z value and im going on to find it through the table. </p>
",<probability>
"<p>Attempting to understand Exercise 20 (pdf page 44) in the paper: (Warning: large paper; small exercise)</p>

<p><a href=""http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090310.pdf"" rel=""nofollow"">Bayesian Reasoning and Machine Learning</a> </p>

<blockquote>
  <p>The party animal problem corresponds to the network in g(3.14). The boss is angry and the worker has a headache - what is the probability  the worker has been to a party?</p>
  
  <p>When set to 1 the statements are true: P = Been to Party, H = Got a Headache, D = Demotivated at work, U = Underperform at work, A =Boss Angry. Shaded variables (A, H) are observed in the true state.</p>
  
  <p>$\begin{matrix} &amp; &amp; D \\ &amp; &amp; \downarrow \\ P &amp; \rightarrow &amp; U \\ \downarrow &amp; &amp; \downarrow \\ (H) &amp; &amp; (A)\end{matrix} $</p>
</blockquote>

<p>I would like to solve the following:</p>

<ul>
<li>Prove that p(P|H,A ) = a*p(P,H,A) where a is a constant.</li>
<li>Expand p(P, H, A) by marginalizing over the variables U and D.</li>
<li>how to compute 'a'</li>
</ul>

<p>Any help is greatly appreciated.</p>

<p>Thank You.</p>
",<probability>
"<p>would the answer to this question be right by any chance? Thanks.</p>

<p>A stain remover is tried out on various stain patches. It is found that 40% of stain is removed on the first application, but the remaining stains become resistant so that the proportion removed in any subsequent application is only one half that of the preceding application. Find the probability that a stain patch will survive 3 applications:</p>

<p>Answer: 40/( 100)  ×  20/( 100)  ×  10/( 100) =  8000/1000000 = 0.008</p>
",<probability>
"<p>Given  $x_1, x_2,..x_n ; x_i \in R$ that drawn from an unknown distribution $P(x)$ and a constant $ C$  $ 0 \leq C \leq 1$.
Find $x^{*}$ such that
$$P(x^{*}) =C$$.</p>

<p>We want to use the kernel density estimation to estimate $P(x)$ here. So:</p>

<p>$$P(x^{*}) \approx \frac{1}{N} \bigg( \sum_{i=1}^N K_h(x^{*};x_i) \bigg)$$ </p>
",<probability>
"<p>Let $p \in (0,1)$ and $n \in \mathbb{N}$. We consider a sample of $n$ i.i.d. Bernoulli variables $X_1,\dots,X_n$ with parameter p.</p>

<p>Computer $E[e^{\lambda\bar{X_n}}]$ such that $\bar{X_n}= \frac{1}{n} \sum_{i=1}^n X_i$</p>

<p>$E[e^{\lambda\bar{X_n}}]=E[e^{\frac{\lambda}{n} \sum_{i=1}^n X_i}]=e^{\frac{1}{n}}E[e^{\lambda\sum_{i=1}^n X_i}]= e^{\frac{1}{n}}E[e^{\lambda X_1}]\dots E[e^{\lambda X_n}]=e^{\frac{1}{n}}(1-p+pe^{\lambda})^n$</p>

<p>Is it correct ?</p>
",<probability>
"<p>I took mathematical probability last semester and now I am taking financial mathematics, but only probability was a pre requisite for financial math (no finance classes were required). These types of questions re confusing me because I don't quite understand financial terminology and I guess my professor thinks that we had taken finance classes in the past. Can someone explain what a portfolio is and what $V(O)$, $V(T)$, and $K_v$ is referring to in this question?</p>

<blockquote>
  <p>Let $A(0)=90$, $A(T)=100$, $S(0)=25$ dollars and let<br>
    $$S(T) =
\begin{cases}
30,  &amp; \text{with probability } p \\
20, &amp; \text{with probability } 1-p
\end{cases}$$</p>
  
  <p>where $0 &lt; p &lt; 1$. For a portfolio with $x=10$ shares and $y=15$ bonds, calculate $V(0)$, $V(T)$, and $K_V$.</p>
</blockquote>

<p>I know what a random variable is and how to solve for expectation because I learned that in probability, but I just don't know what these finance terms are refering to?</p>
",<probability>
"<p>$\DeclareMathOperator{\var}{var}\DeclareMathOperator{\cov}{cov}$</p>

<blockquote>
  <p>The signal-to-noise ratio (SNR) of a random variable quantifies the accuracy of a measurement of a physical quantity. It is defined as $E^2[X]/\var(X)$ and is seen to increase as the mean, which represents the power of the measurement error, that is, $X - E[X]$, decreases. For example, if $X\sim\mathcal{N}(\mu, \sigma^2)$, then $\text{SNR} = \mu^2/\sigma^2$. Determine the SNR if the measurement is $X = A + U$, where $A$ is the true value and $U$ is the measurement error with $U\sim\mathcal{U}(-1/2, 1/2)$. For an SNR of $1000$, what should be $A$?</p>
</blockquote>

<hr>

<p>The mean and variance of a uniform random variable is $E[U] = 0$ and $\var(U) = \frac{1}{12}$. Since the SNR is $1000$, we have that
  $$
  1000 = \frac{\mu^2}{\sigma^2}\Rightarrow 10\sqrt{10} = \frac{\mu}{\sigma}
  $$
where $\mu = E[X] = E[A + U] = E[A] + E[U] = E[A]$ and $\sigma^2 = \var(X) = \var(A + U) = \var(A) + \var(U) + 2\cov(A, U) = \var(A) + \frac{1}{12} + \cov(A, U)$.</p>

<ol>
<li>Without knowing anything about $A$ how do I find $E[A]$ and $\var(A)$?</li>
<li>What is meant by what should $A$ be? Is it asking what type of distribution?</li>
</ol>
",<probability>
"<p>Define a sequence of r.v.'s {$X_n$}$_{n\ge 1}$ iteratively, such that $X_1\sim\text{Unif}(0,1]$ and $X_{n+1}\sim\text{Unif}(0,X_n]$. </p>

<p>Could someone please explain why this is equivalent to:</p>

<p>Let a sequence of r.v.'s {$U_n$}$ \stackrel{iid}{\sim}\text{Unif}(0,1]$. For a sequence of r.v's {$X_n$}$_{n\ge 1}$, set $X_1=U_1$. Then set $X_{n+1}=U_{n+1}X_{n}$.</p>

<p>In general, what about the construction of $X_n$ in the first formulation tells us that we can construct a sequence of products in the second formulation?</p>

<p>Thank you.</p>
",<probability>
"<p>We say that $X$ is smaller than $Y$ in distribution (which we denote by $X \stackrel{D}{&lt;} Y$) if $\mathbb{E}[h(X)] \leq \mathbb{E}[h(Y)]$ for all positive, increasing and bounded functions $h$.</p>

<p>We say that $X$ and $Y$ are equal in distribution (which we denote by $X \stackrel{D}{\sim} Y$) if $X \stackrel{D}{&lt;} Y$ and $Y \stackrel{D}{&lt;} X$</p>

<p>I've already answered the following question :</p>

<blockquote>
  <ol>
  <li>Prove that $X \stackrel{D}{&lt;} Y$ if and only if the cumulative
  distribution functions $F$ and $G$ of $X$ and $Y$ respectively
  satisfy $F \geq G$</li>
  </ol>
</blockquote>

<p>Indeed, it suffices to take $h(x) = 1\!\!1_{x \geq c}$ for different values of $c$.</p>

<p>But I don't know how to answer the following two questions :</p>

<blockquote>
  <ol start=""2"">
  <li>Prove that $X \stackrel{D}{&lt;} Y$  if and only if we can find two random variables $X'$ and $Y'$ such that $X \stackrel{D}{\sim} X'$, $Y \stackrel{D}{\sim} Y'$ and $X' \leq Y'$ a.s.</li>
  <li>Suppose that $X \leq Y$ a.s. and $X \stackrel{D}{\sim} Y$. Prove that $X = Y$ a.s.</li>
  </ol>
</blockquote>

<p>Can I have some help on these two questions ?</p>
",<probability>
"<p>Two processes $(X_t)_{t \in T}$, $(Y_t)_{t \in T}$ are known to be equal in distribution if and only if they agree on all finite-dimensional distributions, i.e.,
for all $t_1$, $t_2$, $\ldots$, $t_n$, $n \in \mathbb{B}$,
$$
(X_{t_1}, \ldots X_{t_n}) \overset{d}{=}   (Y_{t_1}, Y_{t_2}, \ldots Y_{t_n})
$$</p>

<p>How to give sense of this by using the $\pi-\lambda$ theorem, when the process takes value on a countable state space?</p>
",<probability>
"<p><strong>Q.1)</strong> A family has $n$ children, $n\geq2$. We ask from the father, ""Do you have at least one daughter named Lilia?"" He replies, ""Yes!"". What is the probability that all of their children are girls? </p>

<p>In other words, we want to find the probability that all $n$ children are girls, given that the family has at least one daughter named Lilia. </p>

<p>Here we can assume that if a child is a girl, her name will be Lilia with probability $\alpha\ll1$ independently from other children's names. If the child is a boy, his name will not be Lilia.</p>

<p><strong>Q.2)</strong> In a family of $n$ children. We pick one among them and found that she is a girl. What is the probability that all children are girls?</p>

<hr>

<p>My solution to Q.1)</p>

<p>$$
\begin{equation}
\begin{split}
P(\text{all are girls | at-least one named Lila}) &amp;= \frac{P(\text{at-least one name Lila | all are girls})\ \times\ P(\text{all are girls})}{P(\text{at-leat one named Lila})}\\
&amp;= \frac{{n\choose1}\ \alpha\ (1-\alpha)^{n-1}\ \times\ \frac{1}{2^n}}{{n\choose1}\ \alpha \ \frac{1}{2^{n-1}}}
\end{split}
\end{equation}$$</p>

<p>My solution to Q.2)</p>

<p>$$\begin{equation}
\begin{split}
P(\text{all are girls | at-least one girl}) &amp;= \frac{P(\text{at-least one girl | all are girls})\ \times\ P(\text{all are girls})}{P(\text{at-least one girl})}\\
&amp;= \frac{1\ \times\ \frac{1}{2^n}}{{n\choose1}\ \frac{1}{2} \ \frac{1}{2^{n-1}}}
\end{split}
\end{equation}$$</p>
",<probability>
"<p>I am reading a paper and there is a theorem which says:</p>

<p>Let $(S, A, \mu)$ be a probability space, and let $\theta$ be a $\mu$-measure
preserving transformation on it. Then
there exists a subset $S_0 \subset S$ of full $\mu$-measure such that ....</p>

<p>What does ""there exists a subset $S_0 \subset S$ of full $\mu$-measure"" mean?</p>
",<probability>
"<p>I'm having a bit of trouble with this problem.</p>

<p>Three cards are drawn from a pack of regular playing cards. What's the probability of getting different suits, as well as different denominations (number, face, etc.)?</p>

<p>Bonus question. In the above case, it isn't specified whether the cards are drawn simultaneously, or one by one. Would it make a difference?</p>
",<probability>
"<p>16 players, $S1,S2...S16$ are divided into eight pairs. What's the probability of only one of $S1$ or $S2$ coming out as one of the eight quarterfinalists?</p>

<p>To clarify, only one of them qualify. And all players are of equal strength. And, the pairings are completely random.</p>
",<probability>
"<p>when a biased coin is tossed 5 times the probability of having 2 heads is same as that of having 3 (and not 0) What is the probability of having heads exactly 3 out of 5?</p>
",<probability>
"<p>This problem seems to me a brain teaser, in the form of a probability puzzle. I would be really grateful if someone could help me to solve this simple conditional probability problem.
We have a $n \times n$ $(0,1)$-matrix $M$. Let $R$ be the set of rows and let $C$ be the set of columns. Finally, we indicate the $i$-th row and column with $R_i$ and $C_i$ respectively, and the set $\{1, 2, ..., n\}$ with $[n]$.</p>

<p>We have the following information:</p>

<p>(1) Given any $i \in [n]$ and $y \in \{0,1\}$, we know the number $f_{y}(R_i)$ of indices $k \in [n]$ such that $M_{i, k}=y$. Analogously, we also know the number $f_{y}(C_i)$ of indices $k \in [n]$ such that $M_{k, i}=y$.</p>

<p>(2) Given any pair of integers $(i,j) \in [n] \times [n]$ and any value $y \in \{0,1\}$, we know the number $f_{y, 1}(R_i, R_j)$ of vector component indices $k$ such that $M_{j,k}=1$ when $M_{i,k}=y$.  </p>

<hr>

<p>We select at the same time three integers $i$, $j$ and $k$ uniformly at random in $[n]$. We then read the value of $M_{i, k}$ and observe it is equal to a certain value $y$ (where therefore $y \in \{0,1\}$). </p>

<p>QUESTION: How can we calculate the probability that $M_{j,k}=1$?</p>

<p>Using the second part of information (1) solely, I guess one would answer $f_{1}(C_k)/n$. On the other hand, using all information except the second part of (1), I guess one would answer $f_{y, 1}(R_i, R_j)/f_{y}(R_i)$ (please observe $f_{y}(R_i)$ cannot be null). I do not know how to combine all the information.</p>

<p>Thank you very much for your help!
Cheers,
T.</p>
",<probability>
"<p>I have set of sample which are grouped in different size. If I want to find probability of an event in each group, how can I normalize the probability over all the groups. For example, Let say I have 3 set of samples each sized 3, 8 and 30 respectively. If the probability of an event to occur in set $1$ is $\frac{1}{3}$, in set $2$ it is $\frac{3}{8}$ and in set $3$ it is $\frac{4}{30}$. How can I normalize the probability so that highest probability is assigned for set $3$ then set $2$ and then set $1$.</p>
",<probability>
"<p>Let $X_{n,k}$ be a double sequence of random variables. </p>

<p>Assume for each fixed $k$, $X_{n,k}\xrightarrow[n\rightarrow\infty]{p}\alpha_k$, where $\alpha_k$ is a non-random scalar. Assume further that $\alpha_k\xrightarrow[k\rightarrow\infty]{p.w} \alpha$, where $\alpha$ is again non-random. Is it possible to show the following?</p>

<p>$$X_{n,k}\xrightarrow[n,k\rightarrow\infty]{p} \alpha$$</p>

<p>My attempt: Let $f$ be any arbitrary Lipschitz Cont. function bounded by $M&lt;\infty$ and satisfying the contraction $\vert f(x_1) - f(x_2)\vert \leq K\vert x_1 - x_2\vert$ for some $K$ (by definition). Then consider
\begin{align}
\mathbb{E}\big\vert f(X_{n,k}) - f(\alpha)\big\vert&amp;\leq\mathbb{E}\big[\big\vert f(X_{n,k}) - f(\alpha_k)\big\vert\big] + \big\vert f(\alpha_k) - f(\alpha)\big\vert\\
&amp;=\mathbb{E}\big\vert f(X_{n,k}) - f(\alpha_k)\big\vert\mathbb{1}_{(\vert X_{n,k} - \alpha_k\vert\leq \epsilon)} + \mathbb{E}\big\vert f(X_{n,k}) - f(\alpha_k)\big\vert\mathbb{1}_{(\vert X_{n,k} - \alpha_k\vert&gt; \epsilon)} + \big\vert f(\alpha_k) - f(\alpha)\big\vert\\
&amp;\leq K\epsilon + M\mathbb{P}[\vert X_{n,k} - \alpha_k\vert&gt; \epsilon] + K\vert \alpha_k - \alpha\vert
\end{align}</p>

<p>Now, we know that there exists a $K^*$ large enough such $K\vert \alpha_k - \alpha\vert\leq K\delta$ for all $\delta&gt;0$</p>

<p>Similarly, $\mathbb{P}[\vert X_{n,k} - \alpha_k\vert&gt; \epsilon]&lt;\delta$ for large enough $N^{*}$.</p>

<p>Is it enough to choose $N = \max(N^{*},K^{*})$ to show that $\mathbb{E}\big\vert f(X_{n,k}) - f(\alpha)\big\vert\leq \delta$ for all $n,k\geq N$? Then I could use the Portmanteau lemma to show the rest.</p>
",<probability>
"<p>How do I calculate the pdf for the following case?  In general, if we have 2 r.v. $x,y$ which are normal, then the pdf of the difference of 2 r.v. which are Gaussian will also be Gaussian, I think with mean $\mu_Z = \mu_x - \mu_y$ and variance $\sigma^2_Z = \sigma^2_x + \sigma^2_y$.</p>

<p>Based on this premise, how to find the pdf from a Gaussian Mixture model (GMM). The time series $Z$ has the pdf $f_Z$ which is GMM distribution. The time series contains 2 r.v $x,y$.  So, both the r.v. together constitute a GMM. Considering that there are only 2 mixtures.
I have observations of multivariate time series $Z_i = {[x_i,y_i]}_{i=1}^n$ where $x,y$ are the random variables. The pdf of $Z$ is Gaussian mixture model (GMM). The parameters of the GMM model are learnt through Expectation Maximization.   How to get the functional form for the pdf $f(d_i) = f(x_i-y_i)$ where $d_ i = x_i-y_i$. Thank you for help.</p>
",<probability>
"<p>I am probably missing something obvious but here goes: Lets say we have ten people, and over a period of five days, five of them die. One does each day
The probability of any person dying on Day 1 is 1/10, Day 2 is 1/9, and so on.
I started off with the (obvious) premise that the probability of dying was 1/2. I then calculated that there were 10P5 possible permutations of YES DYING and of NOT DYING. There are 9P4 permutations that just include NOT DYING. 9P5/10P5 = 1/2.</p>

<p>PROBLEM: Some of those 10P5 possibilities include YES DYING more than once, which is not really an valid possibility. So now I am left with the absurd situation where the probability of dying is less than 1/2!
Where is my error?
Thanks</p>
",<probability>
"<p>Fifty marbles numbered 1 to 50 are placed in a barrel and twenty drawn one at a time without replacement. What is the probability that at least one will be drawn in sequence? i.e. 1 is drawn first, two is drawn second etc.</p>
",<probability>
"<p>I <em>understand</em> the concept of standard deviation as the <strong>square root</strong> of the <strong>square</strong> of the <strong>mean</strong> of <strong>each sample value - the <em>mean</em> of the sample values</strong>.<br>
Here is the mathematical representation (I've solved out the proof independently) :</p>

<p>1.) $\sigma = \sqrt{\{x^2\} - \{x\}^2}$<br>
where $\{\,\}$ is the average and $x$ is a sample value.  </p>

<p>2.) There is an alternate mathematical representation using summation sigma (for discrete random variable also) that more people are probably acquainted with. <strong>Or does this one have a slightly different meaning, I'm not sure?</strong> </p>

<p>My question is, can someone explicitly show me the derivation for the standard deviation of a binomial distribution.<br>
Here is  the information I know:</p>

<p>1.) Final formula:  $\sigma = \sqrt{pqN}$</p>

<p>2.) $p =$ probability of event A occurring AKA $p = n(A)/N$</p>

<p>where $A$ is an event OR <strong>the first binomially distributed random variable</strong>, $n(A)$ is the amount of times event $A$ happens, and $N$ is the total number of events</p>

<p>3.) $q =$ probability of event $B$ occurring AKA $p = n(B)/N$ where $B$ is an event OR <strong>the second binomially distributed random variable</strong>, $n(B)$ is the amount of times event $B$ happens, and $N$ is the total number of events. Also, $q = 1-p$ because there are only two events, $A$ and $B$.</p>
",<probability>
"<p>I am currently working on conditional probability and I am somewhat confused about how exactly to complete this problem. I know that to find conditional probability that you utilize:</p>

<p>$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$</p>

<p>I also know that there is a $6/36$ chance to roll a sum of 7, and that if you roll a sum of 7 that there is a $4/6$ chance to get a sum without using the number 2. I do not know what else is necessary however in order to finish this problem and to find $P(A|B)$.</p>
",<probability>
"<p>Let $B=(B_t)_{t\ge 0}$ be a Brownian motion on a probability space $(\Omega,\mathcal{A},\operatorname{P})$. By definition of $B$, for $\operatorname{P}$-almost every $\omega\in\Omega$ $$[0,\infty)\to\mathbb{R}\;,\;\;\;t\mapsto X_t(\omega)\tag{1}$$ is continuous. Generally, a stochastic process $X=(X_t)_{t\in I}$ on $(\Omega,\mathcal{A})$ with $I\subseteq\mathbb{R}$ can be viewed as a mapping $$X:\Omega\mapsto\mathbb{R}^I\;,\;\;\;\omega\mapsto \left(t\mapsto X_t(\omega)\right)\tag{2}$$ I've frequently read that $B$ is considered to be a mapping $\Omega\to C\left([0,\infty)\right)$, where $C(I)$ is the space of continuous functions $I\to\mathbb{R}$.</p>

<hr>

<p>Why can we do that? Clearly, there exists a $\operatorname{P}$-null set $N\subseteq\mathcal{A}$ such that $(1)$ is continuous for all $\omega\in\Omega\setminus N$. Moreover, I know that we can alter measurable functions on null sets without changing their measure related properties. However, is it guaranteed that we can alter $B$ on all null sets on which $(1)$ is not continuous such that $(1)$ is continuous for all $\omega\in\Omega$?</p>

<hr>

<p>Remark: Maybe we can use the <em>Kolmogorov-Chentsov theorem</em> to prove that $(1)$ can indeed be assumed as continuous for all $\omega\in\Omega$. The theorem can be formulated as follows:</p>

<p>Let $X=(X_t,t\ge 0)$ be a real-valued stochastic process such that for all $T&gt;0$, there exists $\alpha,\beta,C&gt;0$ with $$\operatorname{E}\left[\left|X_t-X_s\right|^\alpha\right]\le C|t-s|^{1+\beta}\;\;\;\text{for all }s,t\in [0,T]]$$ Then, there exists a <em>modification</em> of $X$ which is locally Hölder-continuous of order $\gamma\in \left(0,\frac \beta\alpha\right)$.</p>

<p>Stochastic processes $X,Y$ are called <em>modifications</em> of each other, if $X_t=Y_t$ almost surely.</p>
",<probability>
"<p>Here is one I'm stumped on. </p>

<blockquote>
  <p>A ball can be in any one of $n$ boxes. It is the $i^{th}$ box with probability $p_i$. If the ball is in the $i^{th}$ box a search of that box will uncover it with probability $\alpha_i$. Given that a search of box $i$ did not uncover the ball, what is the conditional probability the ball is actually in box $j$ where $i,j=1,\ldots,n$? </p>
</blockquote>

<p>I tried fooling around with the multinomial distribution but kept getting tripped up. Any thoughts?</p>
",<probability>
"<p>Is there any way to simplify the following expression?</p>

<p>$$
\sum_{d = 1}^k \left(\sum_{i=1}^d \frac{1}{i}\right) \frac{{n-t \choose d}{t \choose k-d}}{{n \choose k}}
$$</p>

<p>This formula comes from the expected number of record lows over the first $k$ elements in a permutation of $[1,n]$, given some minimum threshold $t$ below which the elements don't count as a record low. </p>

<p>Let $L$ be the number of record lows, and let $d$ be the number of elements above the threshold.
$$
E[L] = E[E[L|d]] = \sum_{d=1}^k E[L|d]\cdot P(d)
$$</p>

<p>where $P(d)$ is a <a href=""http://en.wikipedia.org/wiki/Hypergeometric_distribution"" rel=""nofollow"">hypergeometric</a> distribution, with n-t success states, population size $n$, and $k$ draws.</p>

<p>This comes up for example in this question: <a href=""http://math.stackexchange.com/q/139523/23846"">Expected number of cards in the stack?</a></p>
",<probability>
"<p>Consider the density $f(x,y)=\large\frac{1}{2\pi}\frac{1}{\sqrt{1-x^2-y^2}}$ on the unit disk centered at the origin. There is a particular characterization of this distribution: it is the unique circularly symmetric distribution whose projections onto any line through the origin are uniformly distributed. </p>

<p>Showing the projections of $f(x,y)$ are uniform is simple calculus. However I can't seem to think of an elegant proof for uniform projections implying $f$ must have the above density. If we let $R$ denote any rotation of coordinates $x,y\rightarrow x',y'$, then by assumption, $f(x,y)=R\circ f(x,y):=f(x',y')$. It would then suffice to show that $f(x,0)=\large\frac{1}{2\pi}\frac{1}{\sqrt{1-x^2}}$. Writing out the projected density as $u(x)$:</p>

<p>$$u(x):=\int_{-\sqrt{1-x^2}}^{\sqrt{1-x^2}}f(x,y)dy$$</p>

<p>and we must have that $u(x)=1/2$ (being uniform on $[-1,1]$). Differentiating in $x$ with Leibnitz's rule seems to get nowhere. I've also tried considering the characteristic function of $f$ but got nowhere. </p>

<p>If it helps, $f(x,y)$ arises from projecting the uniform distribution on the sphere to the $x,y$ plane. So in essence this is Archimedes rule for the sphere inscribed in a cylinder: the surface areas are equal. My gut feeling is that this is not entirely dissimilar from showing that the multivariate Gaussian distribution is the only rotationally invariant distribution with independent components.</p>
",<probability>
"<p>is there a simple way to prove that $X_n \rightarrow_{L^p} X$ implies that $\mathrm{E}(X^p_n) \rightarrow \mathrm{E}(X^p)$? the proof for $p=1$ is easy. but what about the case $p&gt;1$? I would appreciate any comments. many thanks!</p>
",<probability>
"<p>I have encountered two definitions of weak convergence in $L^1$:</p>

<p>1) $X_n\rightarrow X$ weakly in $L_1$ iff $\mathrm{E}(X_n\mathrm{1}_A)\rightarrow \mathrm{E}(X\mathrm{1}_A)$ for every measurable set $A$.</p>

<p>2) $X_n\rightarrow X$ weakly in $L_1$ iff $\mathrm{E}(X_n f)\rightarrow \mathrm{E}(X\mathrm{1}f)$ for every (essentially) bounded measurable function $f$.</p>

<p>my question: are 1) and 2) equivalent?</p>

<p>I see that 2) implies 1) (indicators are bounded), but I have difficulties establishing that 1) implies 2). I tried approximating $f$ by simple functions $f_m$, say, assuming $X_n,X$ are nonnegative for simplicity; the problem: I cannot justify the interchange in the order of taking the limits (first with $n$, and then with $m$). any ideas? I would appreciate any sort of help. many thanks!</p>
",<probability>
"<p>This question is from DeGroot's ""Probability and Statistics"" :</p>

<blockquote>
  <p><strong>Unbounded p.d.f.’s.</strong> Since a value of a p.d.f.(probability density function) is a probability density, rather than a
  probability, such a value can be larger than $1$. In fact, the values of the following
  p.d.f. are unbounded in the neighborhood of $x = 0$:$$f(x) =
\begin{cases}
\frac{2}{3}x^{-\frac{1}{3}}  &amp; \text{for 0&lt;$x$&lt;1,} \\
0 &amp; \text{otherwise.}  \\
\end{cases}$$</p>
</blockquote>

<p>Now, I don't know how the p.d.f. can take value larger than $1$.Please let me know the difference between the probability and probability density.</p>
",<probability>
"<p>I asked <a href=""http://math.stackexchange.com/questions/1402463/2011-aime-problem-12-probability-round-table"">Here</a> This question and I am still confused. I got that, for at least one group together there are:</p>

<p>$$3 \cdot 9 \cdot \binom{6}{3, 3}$$</p>

<p>But why do we subtract: $3 \cdot 9 \cdot 4$. </p>

<p>Lets begin with $AAA$ suppose circularly. (We multiply by $3$ in the end for $BBB, CCC$ so not to worry about that). There are $9$ possible places for $AAA$. I saw that: $BCBCBC, CBCBCB$ are two already. Then :$BBCBCC, BCBBCC$, etcc... that is more than $4$.</p>

<p>What am I missing here? </p>
",<probability>
"<p>Three players A, B and C take turns to roll a fair die; they do this in the order ABCABC... 
(a) Find the probability that, of the three players, A is the ﬁrst to throw a 6, B is the second, and C is the third. 
(b) Find the probability that the ﬁrst 6 to appear is thrown by A, the second 6 to appear is thrown by B, and the third 6 to appear is thrown by C.</p>

<p>I am confused as to the difference between the two questions. 
Also, can anyone give me shorter ways to solve the first part?</p>
",<probability>
"<p><strong>Question</strong>: 
Consider n independent tosses of a $k$-sided fair dice. Let $X_i$ be the number of tosses that result in $i$.</p>

<p>What is the covariance $\mathrm{cov}(X_1,X_2)$ of $X_1$ and $X_2$.</p>

<hr>

<p>\begin{align}
\mathrm{cov}(X_1,X_2) = \mathbf{E}[X_1X_2] - \mathbf{E}[X_1]\mathbf{E}[X_2]
\end{align}</p>

<p>I get a different $\mathbf{E}[X_1X_2]$ than the given solution.</p>

<p><strong>The solution given is</strong></p>

<p>Let $A_t$ (respectively, $B_t$) be a Bernoulli random variabe that is equal to 1 if and only if the $t$th toss resulted in 1 (respectively, 2). We have <strong>E</strong>$[A_tB_t] = 0$ (since $A_t \neq 0$ implies $B_t \neq 0$)</p>

<p>$$ \mathbf{E}[A_tB_s] = \mathbf{E}[A_t]\mathbf{E}[B_t] = \frac{1}{k} \cdot \frac{1}{k}   \mathrm{for}\  s \neq t.$$</p>

<p>Thus,</p>

<p>\begin{align}
\mathbf{E}[X_1X_2] &amp;= \mathbf{E}[(A_1+\cdots + A_n)(B_1+\cdots B_n)]\\
 &amp;=n\mathbf{E}[A_1(B_1+\cdots+B_n)] = n(n-1)\cdot   \frac{1}{k} \cdot \frac{1}{k} \\
&amp;= \frac{n(n-1)}{k^2}
\end{align}</p>

<p>and</p>

<hr>

<p><strong>My solution that gives slightly off answer</strong></p>

<p>My approach uses iterated expectations.
\begin{align}
\mathbf{E}[X_1X_2] = \mathbf{E}[\mathbf{E}[X_1X_2|X2]]
\end{align}
If I had $k$ instead of $k-1$ in the following equation, I would get an answer identical to given solution but if I already know $X_2=x_2$ then dice tosses should be identically distributed among k-1 remaining options, right?
\begin{align}
\mathbf{E}[X_1|X_2=x_2] = \frac{n-x_2}{k-1} 
\end{align}</p>

<p>Then 
\begin{align}
\mathbf{E}[X_1X_2|X_2] = \frac{n-X_2}{k-1} \cdot X_2
\end{align}
\begin{align}
\mathbf{E}[\mathbf{E}[X_1X_2|X_2]] = \mathbf{E}[\frac{nX_2-{X_2}^2}{k-1}]
\end{align}</p>

<p>given $\mathbf{E}[{X_2}^2] = \mathbf{E}[{X_2}] = \frac{n}{k}$</p>

<p>\begin{align}
\mathbf{E}[X_1X_2] = \frac{n(n-1)}{k(k-1)}
\end{align}</p>

<p>So my answer differs to the solution on the matter of $\mathbf{E}[X_1X_2]$</p>

<p>\begin{align}
\mathbf{E}[X_1X_2] = \frac{n(n-1)}{k^2} \neq \frac{n(n-1)}{k(k-1)}
\end{align}
Whats wrong with my logic? Or maybe MIT is wrong.</p>
",<probability>
"<p>Let $X$, $Y,$ and $Z$ be random variables. (There are no restrictions on these variables, but you may assume that these are continuous random variables if you want.) Suppose that $X$ and $Z$ are independent, and also suppose that $Y$ and $Z$ are independent. Does it follow that $\mathrm{cov}(XY,Z)=0$? (I understand that $XY$ and $Z$ may not be independent, but this does not rule out the zero covariance.)</p>

<p>Under the assumption that $X$ and $Z$ are independent and that $Y$ and $Z$ are independent, I am able to show that $$\mathrm{cov}(X,YZ) = \mathrm{cov}(Y,XZ) = \mathrm{cov}(XY,Z) + \mathrm{E}[Z]\cdot \mathrm{cov}(X,Y).$$ Showing this is fairly straightforward: $\mathrm{cov}(XY,Z)=\mathrm{E}[XYZ]-\mathrm{E}[XY]\cdot\mathrm{E}[Z]$; in addition, $\mathrm{cov}(X,YZ)=\mathrm{E}[XYZ]-\mathrm{E}[X]\cdot\mathrm{E}[YZ]=\mathrm{E}[XYZ]-\mathrm{E}[X]\cdot\mathrm{E}[Y]\cdot\mathrm{E}[Z]$, implying that $\mathrm{cov}(X,YZ)=\mathrm{cov}(XY,Z)+\mathrm{E}[XY]\cdot\mathrm{E}[Z]-\mathrm{E}[X]\cdot\mathrm{E}[Y]\cdot\mathrm{E}[Z]$, which is obviously equal to $\mathrm{cov}(XY,Z) + \mathrm{E}[Z]\cdot\mathrm{cov}(X,Y).$ And, of course, $\mathrm{cov}(X,YZ) = \mathrm{E}[XYZ]-\mathrm{E}[X]\cdot\mathrm{E}[Y]\cdot\mathrm{E}[Z]= \mathrm{cov}(Y,XZ)$, proving the above result.</p>

<p>However, to proceed further, my intuition tells me that $\mathrm{cov}(XY,Z) = 0$ and thus that $\mathrm{cov}(X,YZ) = \mathrm{cov}(Y,XZ) = \mathrm{E}[Z]\cdot\mathrm{cov}(X,Y)$. Am I wrong in thinking that $\mathrm{cov}(XY,Z) = 0$? But if it is true that $\mathrm{cov}(XY,Z) = 0$, is there a simple proof that does not possibly involve measure theory? Thanks.</p>
",<probability>
"<p>Let $\mathbf{u} =\begin{bmatrix}u_1 &amp; u_2 &amp; \dots &amp; u_N \end{bmatrix}^T$ and $\mathbf{v} = \begin{bmatrix} v_1 &amp; v_2 &amp; \dots &amp; v_N\end{bmatrix}^T$. All the elements of $\mathbf{u}$ and $\mathbf{v}$ are complex Gaussian random variables with zero mean and variance $\frac{1}{N}$ and $N$ is large. Also let $x_1, x_2 \in \{-1,1\}$ where $x_1$ a can be $-1$ or $1$ with equal probability $(p=0.5)$ and similaraly $x_2$. </p>

<p>Define $d^2$ as the square of the euclidean distance between $\mathbf{u}x_1 \text{and } \mathbf{v}x_2$: $$d^2=\mathbf{|u}x_1-\mathbf{v}x_2|^2 = |u_1x_1-v_1x_2|^2 + \dots +|u_Nx_1-v_Nx_2|^2 \\ \text{let} ~~d_\min = \min(d^2)~~ \text{what is} ~~ E(d_\min) \text{?}$$.</p>

<p>There are $4$ combinations for $(x_1, x_2)$: $(1,1),(1,-1),(-1,1),(-1,-1)$ and therefore $4$ combinations for $d^2$ $$d_1^2=|u_1-v_1|^2 + \dots +|u_N-v_N|^2 \\ d_2^2=|u_1+v_1|^2 + \dots +|u_N+v_N|^2 \\ d_3^2 = |-u_1-v_1|^2 + \dots + |-u_N-v_N|^2 \\ d_4^2 = |-u_1+v_1|^2 + \dots +|-u_N+v_N|^2$$
Since $u_i$ and $v_i$ are complex Gaussian, $|u_i-v_i|$ is Rayleigh and $|u_i-v_i|^2$ is going to be exponential. $N$ is large here and since all terms $|u_i-v_i|^2$ are independent I can use the central limit theorem and say that $d_1^2,d_2^2,d_3^2,d_4^2$ are all Gaussian. I can use the CDF method to calculate the distribution of $d_\min$ and then calculate the average minimum distance but $d_1^2,d_2^2,d_3^2,d_4^2$ are not independent and I don't know how to proceed. Any help/guidance is greatly appreciated.</p>
",<probability>
"<p>I was trying to solve the following question:</p>

<blockquote>
  <p>Out of 2 Boys and 2 Girls, two students are chosen to advance to the next level. What is the probability that two girls advance to the next level</p>
</blockquote>

<p>However, because the question was ambiguous I calculated the probabilities considering all four cases of whether they were distinguishable or indistinguishable, and whether order mattered or didn't matter and got different probabilities for each case.</p>

<p>However, I want to know why this happens. All you are doing is simply picking two students and seeing if they are both girls. You keep doing this and after infinite trials, divide the number of times they were both girls by the number of total trials. How does this outcome depend upon whether you view them as distinguishable, indistinguishable, ordered, or non-ordered? </p>

<p>Cases:</p>

<ol>
<li><p>Indistinguishable, order matters: $\frac{1}{2\cdot 2}=\frac{1}{4}$ (Cases are BB, BG, GB, GG)</p></li>
<li><p>Indistinguishable, order does not matter: $\frac{1}{3}$ (Cases are BB, B+G, GG)</p></li>
<li><p>Distinguishable, order matters: $\frac{2}{\text{Permutation}(4,2)} = \frac{1}{6}$</p></li>
<li><p>Distinguishable, order does not matter: $\frac{1}{\binom{4}{2}}=\frac{1}{6}$</p></li>
</ol>
",<probability>
"<blockquote>
  <p>Six teams play a tournament in which every team plays every other team exactly once. No ties occur, and each team has a $\dfrac{1}{2}$ probability of winning any game it plays. Find the probability that no two teams win the same number of games.</p>
</blockquote>

<p>This is what I have so far:</p>

<p>There are $\binom{6}{2} = 15$ pairs of teams, and $2^{15}$ possible outcomes. The min and max possible # of games won are from $0$ to $5$. If $h$ represents the # of games on by a certain team, than $0 \leq k \leq 5$. Because of this, there are $5!$ outcomes in which no two teams win the same number of games. Therefore, the probability is: $\dfrac{5!}{2^{15}}$. When simplified, we get $\dfrac{15}{4096}$. However, when I imputed this answer into the question, it was wrong. Where was my error, and how can I fix it? </p>

<p>NOTICE: The probability of a team winning in each game is 1/2 , NOT 1/12. </p>
",<probability>
"<p>$8$ students take an exam.</p>

<p>All of them are prepared average, so probability that they will pass or fail is the same.</p>

<p>After checking half of the tests, it's discovered that $3$ of them passed and $1$ failed.</p>

<p>What is probability that in the next $3$ tests, $1$ will pass and $2$ will fail</p>

<p>My reasoning:</p>

<p>If $A$ is event in which $1$ out of checked $4$ has passed and $2$ have failed and $B$ is event in which $3$ out of checked $4$ have passed and $1$ has failed then that two events are independent??</p>
",<probability>
"<p>I have a 60-40 weighted distribution, of uniform(0,7.5) and uniform(7.5,10) respectively, i.e. 
$$f_X(x)=(0.6/7.5)1_{x∈[0,7.5)}+(0.4/2.5)1_{x∈[7.5,1]}$$</p>

<p>I have worked out that 
$$E(X) = 0.6(7.5/2) + 0.4((10+7.5)/2) = 5.75$$ 
$$Var(X) = 9.0208$$</p>

<p>Right now this distribution is continuous, and I would like to make it discrete via the method of moment matching, with number of moments p = 2 and span h = 1.25.</p>

<p>How do I go about this?</p>

<p>I sort of understand how the equations work for p=1 (matching $m_0^k$ and $m_1^k$), but I'm not sure how to work it out for p=2.</p>

<p>*Notes: </p>

<p>Method of moment matching for arithmetizing a continuous distribution</p>

<p>We construct an arithmetic distribution that matches p moments of the arithmetic and the true severity distributions. Consider an arbitrary interval of length $ph$, denoted by $[x_k ; x_{k+ph})$. We locate point masses $m^k_0$, $m^k_1$, $m^k_2$, ... , $m^k_p$ at points $x_k$, $x_k + h$, ... , $x_k + ph$ so that the first p moments are preserved.</p>

<p>The system of p + 1 equations reflecting these conditions is
$$\sum_{j=0}^p (x_k + jh)^rm^k_j = \int_{x_k􀀀􀀀 - 0}^{x_k􀀀􀀀 + ph - 0} x^r dF_X(x) - (*)$$</p>

<p>where r = 0,1,2,...,p and the notation “􀀀- 0” at the limits of the integral indicates that discrete probability at $x_k$ is to be included but discrete probability at $x_k + ph$ is to be excluded.</p>

<p>Arrange the intervals so that $x_k+1 = x_k + ph$ and so the endpoints coincide. Then the point masses at the endpoints are added together.</p>

<p>With $x_0 = 0$, the resulting discrete distribution has successive probabilities:</p>

<p>$f_0 = m_0^0$, $f_1 = m_1^0$, $f_2 = m_2^0$, ...</p>

<p>$f_p = m_p^0 + m_0^1$, $f_{p+1} = m_1^1$, $f_{p+2} = m_2^1$, ...</p>

<p>We need to solve the system of equations defined by $(*)$.</p>

<p>The solution of $(*)$ is $$m_j^k = \int_{x_k􀀀􀀀 - 0}^{x_k􀀀􀀀 + ph - 0} \prod_{i \neq j}\frac{x - x_k - ih}{(j-i)h}dF_X(x)$$
where j = 0,1,...,p</p>
",<probability>
"<p>When you pick three cards, without replacement, from a standard 52 card deck, what are the probabilities of:</p>

<ul>
<li>only one suit in your three cards</li>
<li>two different suits in your three cards</li>
<li>three different suits in your three cards</li>
</ul>

<p>For the first I have the probability of $4 \cdot \frac{13}{52} \cdot \frac{12}{51} \cdot \frac{11}{50} = \frac{22}{425} $</p>

<p>But I cannot think of a way to determine the possibilities you have two or three different suits in the three chosen cards. Any help is appreciated.</p>
",<probability>
"<p>Let $(X_{n,m})_{n\geq 1,m\geq1}$ be a double sequence of random variables such that $X_{n,m}\Rightarrow X_m$ (weak convergence) as $n\rightarrow\infty$, $X_{n,m}\rightarrow X_n$ (almost sure convergence) as $m\rightarrow\infty$, and $X_m\rightarrow X$ (almost sure convergence) as $m\rightarrow\infty$. I am trying to determine whether these conditions are sufficient to establish that $X_n \Rightarrow X$ as $n\rightarrow\infty$, or if not, what additional assumptions would have to be made in order to ensure the result. This is what I have done so far. Let $f$ be continuous and bounded. Using continuity of $f$ and then its boundedness (to apply dominated convergence theorem) we have:
$$\mathbb{E}(f(X))=\lim_{m\rightarrow\infty}\mathbb{E}(f(X_m))=\lim_{m\rightarrow\infty}\lim_{n\rightarrow\infty}\mathbb{E}(f(X_{n,m})).$$
If I could interchange the two above limits, I would be able to conclude, since:
$$\mathbb{E}(f(X))=\lim_{n\rightarrow\infty}\lim_{m\rightarrow\infty}\mathbb{E}(f(X_{n,m}))=\lim_{n\rightarrow\infty}\mathbb{E}(f(X_{n})),$$
also by continuity of $f$ and dominated convergence. However, I am having difficulties proving that we can effectively interchange these two limits (to do so, one of the two limits needs to be uniform), and wondering if the assumptions I currently have are sufficient? Any comments or ideas would be greatly appreciated.</p>

<p>update: I think it's probably better to consider $f$ to be continuous with compact support instead, as we can therefore use the uniform continuity of $f$.</p>
",<probability>
"<p>A box contains two coins: a regular coin and one fake two-headed coin (P(H)=1). One coin is choose at random and tossed $n$ times. </p>

<p>If the first n coin tosses result in heads, What is the probability that the $(n+1)^{th}$ coin toss will also result in heads?</p>

<hr>

<p>My solution:</p>

<p>$$\text{Required Probability} = \frac{1}{2}\times \left(\frac{1}{2}\right)^n \times \frac{1}{2}+\frac{1}{2} \times 1^n \times 1$$</p>
",<probability>
"<p>Suppose that you are playing blackjack against the dealer. In a freshly shuffled deck (standard $52$ cards), what is the probability that neither of you are dealt a blackjack. Blackjack being $2$ cards adding to $21$ i.e. $Ace + 10,J,Q,or K$ (or vice versa as order does not matter).</p>

<p>The farthest I've really come is that the odds of the first player getting dealt a blackjack is $128\over 2652$. </p>

<p>First case: Odds of getting an Ace are $4\over52$, odds of the next being 10,J,Q,or K are $16\over51$.</p>

<p>Other case: Odds of getting 10,J,Q,or K are $16\over52$ and Ace $4\over 51$ so ${((4*16)*2)\over (52*51)} == {128\over 2652}$</p>

<p>Not sure where to go from here...</p>
",<probability>
"<p>Having two binary numbers of length 6, what is the probability that they match exactly? What is the probability that they have hamming distance of exactly 1?  or of 2?  </p>

<p>For the first part, the number of possible variants of the binary number is 2^6 I believe. What is the probability of the second binary number matching the first?  It seems to me that this probability would be 1/2 ^ 6?  </p>

<p>If this probability is found is it a simple matter to then find the probability of when they mismatch by exactly one number?  </p>
",<probability>
"<p>We note that given a probability distribution function $P$ over a space $U$ the expected value of a function of the elements in U:</p>

<p>$$ E(f(x)) = \int_{U} f(x)P(x) $$ </p>

<p>We thus consider the mean as the expected value of the numbers that is:</p>

<p>$$ E(x) = \int_{U} x P(x) $$</p>

<p>Now we consider ""standard deviation"" to be the expected difference between a variable from the mean that is</p>

<p>$$ Std(x) = E(|x - E(x)|)  = E\left(\sqrt{(x - E(x))^2}\right) $$</p>

<p>Yet Standard deviation is always measured as:</p>

<p>$$ \sqrt{E((x - E(x)^2)} $$</p>

<p>The latter formula doesn't make sense to me. Can someone explain why mine is wrong and hte latter is corret?</p>
",<probability>
"<p>Consider the following game of chance. A fair coin is tossed until the first tails appears.
You place an initial bet of k. If the 1st tails appears on the nth toss, you receive a total of $2^n$ (2 to the power of n) in return for your initial bet. How large should k be in order for your expected winnings to be zero (note, expected winnings of zero is sometimes called a “fair” game)?</p>

<p>I did the question and the answer comes to infinite. Is that correct? If not, what did I do wrong?</p>

<p>In other words, we can rephrase the question as: what is the expectation value of $2^n$ given $p(n)=1/2^{(n+1)}$?</p>
",<probability>
"<p>The material I'm reading derives Jeffrey's prior (or rather, the Fisher information for the Jeffrey's) for single-parameter binomial distribution in a manner quite similar to <a href=""https://en.wikipedia.org/wiki/Fisher_information#Single-parameter_Bernoulli_experiment"" rel=""nofollow"">this Wikipedia article</a>.</p>

<p>I could work out the steps until (following Wikipedia's notation, $A$ is number of successes, $B$ failures, $A+B$ total number of trials)</p>

<p>$$E [\frac{A}{\theta^2} + \frac{B}{(1-\theta)^2}] \\
= \frac{E[A]}{\theta^2} + \frac{E[B]}{(1-\theta)^2}
$$</p>

<p>Maybe my background in probability calculus is just lacking, but I'm not exactly sure about the justification for this step. $E[\frac{A}{\theta^2}] + E[\frac{B}{(1-\theta)^2}]$ follows from the linearity properties of expected value, but the next step? Are we treating $\frac{1}{\theta^2}, \frac{1}{(1-\theta)^2}$ as constants?</p>
",<probability>
"<p>I have this scenario:</p>

<blockquote>
  <p>1 animal with 30% probability of be moved to Japan. <br> 1 animal with
  30% probability of be moved to Japan. <br> 1 animal with 30%
  probability of be moved to Japan. <br> 1 animal with 30% probability
  of be moved to Japan. <br> 1 animal with 30% probability of be moved
  to Japan. <br> 1 animal with 30% probability of be moved to Japan.
  <br> 1 animal with 30% probability of be moved to China. <br> 1 animal
  with 30% probability of be moved to Japan. <br> 1 animal with 80%
  probability of be moved to Brazil. <br> 1 animal with 30% probability
  of be moved to Japan. <br> 1 animal with 20% probability of be moved
  to Brazil. <br> 1 animal with 30% probability of be moved to Japan.
  <br> 1 animal with 50% probability of be moved to Mexico. <br> 1
  animal with 30% probability of be moved to Japan. <br> (...)</p>
</blockquote>

<p>Resuming, 10 animals with 30% of probability of being moved to Japan.</p>

<p>Is that ""right"" to expect that 3 animals gonna be moved to Japan?</p>

<p>The formula is:
30/100 * 10 = 3</p>

<p>Can I use <strong>Binomial Distribution</strong> for this scenario?
If yes, how to elaborate the formula?</p>

<p>Thanks a lot!</p>
",<probability>
"<p>I understand the question but I am not sure how to solve it. For example, if we flip HHHTTTTT then the next three must be heads because of the question. This however seems counterintuitive. I believe that there are $2^{10}$ possible strings, but I am unsure of how to count all possible strings that begin with HHH.</p>
",<probability>
"<p>Given that time interval $T^*$ in seconds between certain events has a negative exponential distribution. </p>

<p>The instrument cannot detect intervals which are less than $\delta$ seconds.</p>

<p>Let $T_1, ..., T_n$ be a sample of independent intervals measyred by the instrument. The distribution of one of those observation $T_i$ is the conditional distribution of $T^*$ given that $T^*&gt;\delta$</p>

<p>In this question, if I want to find the probability density function of $T_i$, should I consider the <strong>shifted exponential distribution</strong> such that:
$$f_T(t) = \begin{cases} \lambda e^{-\lambda (t- \delta)} &amp; t&gt;\delta, \\ 0 &amp; otherwise \end{cases}$$</p>

<p>with $E(T)=\delta + \frac{1}{\lambda}$ and $Var(T) = \frac{1}{\lambda ^2}$ </p>

<p>Thank you</p>
",<probability>
"<p>Let $X,Y$ be two i.i.d. r.v.'s with zero mean and unit variance. If $X+Y$ and $X-Y$ are independent, then $X$ and $Y$ are both standard normal distributed.</p>

<p>Is there any short proof for this problem?</p>
",<probability>
"<p>There are two independent variables X and Y. Y is an input for non deterministic algorithm f, and the output of f(Y) is Z. How to prove that X and Z are independent?</p>
",<probability>
"<p>I'm trying to calculate the probabilities of different lengths of repetitions of X length number however I know I'm doing it incorrectly since when I add all the probabilities together they don't total to 1</p>

<p>e.g. 
Here is my reasoning to calculate the probabilities of the different lengths repetitions for length 4</p>

<p>Probability that there are 0 repeating sequences: 
e.g. WXYZ
10/10 * (9/10)^3 = 729</p>

<p>Probability that there is 1 repeating sequence of length 2: 
e.g XXYZ or YXXZ or YZXX
10/10 * (9/10)^2 * 1/10 * 3 = 243</p>

<p>Probability that there is 2 repeating sequence of length 2: 
e.g XXYY or YYXX
10/10 * 9/10 * (1/10)^2 * 2 = 18</p>

<p>Probability that there is a repeating sequence of length 3: 
e.g XXXY or YXXX
10/10 * 9/10 * (1/10)^2 * 2 = 18</p>

<p>Probability that there is a repeating sequence of length 4:
e.g XXXX
10/10 * (1/10)^2 * 3 = 1</p>

<p>When I add the number of outcomes I get 1009, when I should be getting a 1000.
Anyone know what I'm doing wrong?</p>

<p>Thanks in advance!</p>
",<probability>
"<p>I have recently seen a probability question which says<br>
""i am asking randomly the persons I met if they are having two chidren and one of them is a boy who was born on tuesday. At last I met one whose answer is yes. What is the probability that the other child is also a boy. Assume equal probability to either gender and equal probability to be born on each day of the week""<br></p>

<p>I could actually solve it to 2/21. Did I do it right or can some one help me solve it?</p>
",<probability>
"<p>I'm a little confused in some simple question in probability theory,</p>

<p>Say that the probability for rain in London in some random day is $P_{rain}$; and the probabilities of rain in one day and another are independent.</p>

<p>We know that the probability for rain in exactly 3 days of 7 is $C(7,3) \cdot P_{rain}^3 \cdot(1-P_{rain})^4$.</p>

<p><em>Question:</em> Say that I arrived to London in a rainy day, what's the probability of rain in 3 days of the current week?</p>

<p>Does it equal $C(6,2) \cdot P_{rain}^2 \cdot(1-P_{rain})^4$ ? (I think so because of the fact that the probabilities are independent...)</p>
",<probability>
"<p>Assuming I can play forever, what are my chances of coming out ahead in a coin flipping series?</p>

<p>Let's say I want ""heads""...then if I flip once, and get heads, then I win, because I've reached a point where I have more heads than tails (1-0).  If it was tails, I can flip again.  If I'm lucky, and I get two heads in a row after this, this is another way for me to win (2-1).</p>

<p>Obviously, if I can play forever, my chances are probably pretty decent.  They are at least greater than 50%, since I can get that from the first flip.  After that, though, it starts getting sticky.</p>

<p>I've drawn a tree graph to try to get to the point where I could start see the formula hopefully dropping out, but so far it's eluding me.</p>

<p>Your chances of coming out ahead after 1 flip are 50%.  Fine.  Assuming you don't win, you have to flip at least twice more.  This step gives you 1 chance out of 4.  The next level would be after 5 flips, where you have an addtional 2 chances out of 12, followed by 7 flips, giving you 4 out of 40.</p>

<p>I suspect I may be able to work through this given some time, but I'd like to see what other people think...is there an easy way to approach this?  Is this a known problem?</p>
",<probability>
"<blockquote>
  <p>Let $X$, $Y$ be independent random variables with the common pdf
   \begin{eqnarray*} f(u) &amp;=&amp; \left\{\begin{array}{ll} u\over2 &amp;
 \mbox{for } 0 &lt; u &lt; 2\\ 0 &amp;\mbox{elsewhere} \end{array}\right.\\
\end{eqnarray*}
  Set up an explicit double integral for $P(X Y &gt; 1)$</p>
  
  <p>Let $Z$ be the maximum of $X,Y$ (That is, $Z = X$ if $X \geqslant Y$, and $Z= Y$ 
   if $Y &gt; X$). Find $P(Z\leqslant 1)$</p>
  
  <p>Find the pdf $g(z)$ of $Z$, being sure to define $g(z)$ for all
   numbers $z$.</p>
</blockquote>

<p>This is a problem on a practice exam I'm studying, but I really have no idea how to approach the problem.</p>
",<probability>
"<p>Let $\pi$ be a random permutation of $n$ objects and let $ T := \text{the number of transpositions in } \pi $. Use Chebychev's Inequality to find an upper bound for $T\geqslant k$.</p>

<p>Okay the problem I'm having here is with $\mathbb{Var}(T)$, I'm not sure how to find it. I know the expectation is $\frac{1}{2}$, so my formula so far is $$\mathbb{P}\left(T-\frac{1}{2}\geqslant k\right) \leqslant \frac{\mathbb{Var}(T)}{k^2}$$ </p>
",<probability>
"<p>Let $F$ be the number of fixed points of a random permutation on $n$ items. Show that as $n$ approaches infinity, the distribution of $F$ approaches a Poisson distribution with a mean $(\lambda)=1$.</p>
",<probability>
"<p>In his book <a href=""http://rads.stackoverflow.com/amzn/click/159420411X"">The Signal and the Noise</a>, Nate Silver presents this example application of Bayes's Theorem on pp. 247-248:</p>

<blockquote>
  <p>Consider a somber example: the September 11 attacks. Most of us would
  have assigned almost no probability to terrorists crashing planes into
  buildings in Manhattan when we woke up that morning. But we recognized
  that a terror attack was an obvious possibility once the first plane hit
  the World Trade Center. And we had no doubt we were being attacked
  once the second tower was hit. Bayes's theorem can replicate this result.</p>
</blockquote>

<p>You can view the complete example in Amazon.com's previw, and I've made the two pages available <a href=""http://imgur.com/YI2rv,nIakZ,rwbYH,MB8U6,07VIA#0"">here</a>.</p>

<p>Silver assumes the prior probability of a terrorist plane attack to be 1 in 20,000. After the first plane crash, using Bayes's Theorem he updates that to 38%. And after the second plane crash, he comes up with a 99.99% probability. However, I think he may be mistaken. I'll provide the details below.</p>

<p>To be precise, let us define the following three events:</p>

<ul>
<li>$PC$ = Plane Crash: At least one plane crashes into a Manhattan skyscraper on a given day. </li>
<li>$TPA$ = Terrorist Plane Attack: At least one plane is intentionally crashed into a Manhattan skyscraper on a given day.</li>
<li>$APC$ = Accidental Plane Crash: At least one plane is accidentally crashed into a Manhattan skyscraper on a given day.</li>
</ul>

<p>We assume all plane crashes into buildings are either terrorist plane attacks or accidental (i.e. $PC = TPA \cup APC$). Using historical data, Silver estimates the prior probability of an accidental plane crash to be 1 in 12,500. In summary: $$P(TPA) = \frac{1}{20000},$$$$P(APC) = \frac{1}{12500}.$$</p>

<p>Furthermore, Silver assumes $P(APC) = P(PC|\overline{TPA})$ (which is true if $APC$ and $TPA$ are independent events).</p>

<p>Applying Bayes's Theorem, he comes up with 
$$\begin{align}P(TPA|PC) &amp;= \frac{P(PC|TPA) \times P(TPA)}{P(PC|TPA) \times P(TPA) + P(PC|\overline{TPA})(1-P(TPA))} \\
&amp;= \frac{1 \times \frac{1}{20000}}{1 \times \frac{1}{20000} + 
\frac{1}{12500} \times (1 - \frac{1}{20000})} = 0.385\end{align}$$</p>

<p>Silver continues:</p>

<blockquote>
  <p>The idea behind Bayes's theorem, however, is not that we update our 
  probability estimates just once. Instead, we do so continuously as new
  evidence presents itself to us. Thus our posterior probability of a
  terror attack after the first plane hit, 38 percent, becomes our
  <em>prior</em> probability before the second one did. And if you go through the calculation again, to  reflect the second plane hitting the World
  Trade Center, the probability that we were under attack becomes a
  near-certainty -- 99.99 percent.</p>
</blockquote>

<p>That is (this is Silver's calculation): $$P(TPA|PC) = \frac{1 \times 0.385}{1 \times 0.385 + 
\frac{1}{12500}(1-0.385)} = 99.99 \%$$</p>

<p>""Cool!"" I thought, until I thought a bit more. The problem is that you can apply the same logic to calculate the conditional probability of an <em>accidental</em> crash, too. I'll spare you the math, but I come up with $P(APC|PC) = 0.615$ after the first crash, and $P(APC|PC) = 99.997\%$ after the second.</p>

<p>So we can be almost certain the second plane crash is a terrorist attack, and we can be even more certain that it's accidental?  </p>

<p>I think the problem is that when Silver applies Bayes's Theorem after the second crash, he uses the updated probability of a terrorist plane attack as his prior, but fails to update the prior probability of an accidental plane crash (which should become 0.615). After the second crash, then, the correct formula is
$$P(TPA|PC) = \frac{1 \times 0.385}{1 \times 0.385 + 
0.615(1-0.385)} = 0.504$$</p>

<p>Similarly, the probability that we're observing an accidental crash given that there have been two crashes is 
$$P(APC|PC) = \frac{1 \times 0.615}{1 \times 0.615 + 
0.385(1-0.615)} = 0.806$$</p>

<p><strong>Question 1</strong>: Am I correct that Nate Silver is doing it wrong?</p>

<p><strong>Question 2</strong>: Am I doing it right?</p>
",<probability>
"<p>In my book:
$\mathbf{X}=(X_1,\ldots,X_n)$
$f(\mathbf{x})$ is the joint density, where $f$ is either $f_0 \text{ or } f_1$.</p>

<p>Suppose we want to test $H_0: f=f_0$ or $H_1: f=f_1$. The test, whose test function is</p>

<p>$$\phi(\mathbf{X})=1\text{ if }\frac{f_1}{f_0}\geq k;$$</p>

<p>$$\phi(\mathbf{X})=0 \text{ otherwise,}$$</p>

<p>(for some $0&lt;k&lt;\infty$) is a most powerful test of $H_0$ versus $H_1$ at level $E_0(\phi(\mathbf{X}))$.</p>

<p>My question is how is $k$ defined? Can I interpret the lemma as if $\forall k \in (0,\infty)$ there will be a test function $\phi(\mathbf{X})$ such that it will determine the size for which the test with test function $\phi(\mathbf{X})$ is most powerful?</p>

<p>I'm just trying to understand which kind of relationship $k$ and the size of the test have between each other.</p>

<p>EDIT: </p>

<p>Here is a citation from the book I'm using: «the Neyman-Pearson lemma as stated here does not guarantee the existence of an MP $\alpha$ level test but merely states that the test that rejects $H_0$ for $T(X)\geq k$ will be an MP for some level $\alpha$» This makes me want to interpret as $\forall k \exists \alpha$. May I? </p>
",<probability>
"<p>Suppose I have $n$ random number generators.  Once an hour, on the hour, each one generates a random real number $x_k$ such that $0 \le x_k \lt \infty$. Each generator produces its values according to its own independent probability distribution function $f_k()$, which is a known function.  For example, one generator might follow an exponential distribution, another might follow a normal distribution, etc.</p>

<p>Let $X = \sum\limits_{k=1}^n x_k$ for all of the number generators in any one hour.</p>

<p>Given $y$ such that $0 \le y \lt 1$ (a probability), I need to find a value $z$ such that $P(X \le z) = y$.</p>

<p>Basically, I need to be able to do something like find the value that $X$ will be less than or equal to 50% of the time.</p>

<p>I apologize if I've gotten any of the notation wrong, I'm actually a software engineer so I know some things about math but not others.  I know enough about probability to express the problem above, but I don't even know where to begin in terms of solving it. Any help, or even suggested readings would be much appreciated.</p>
",<probability>
"<p>Under an insurance policy, a maximum of five claims may be filed per year by a policy holder. Let $p_n$ be the probability that a policy holder files $n$ claims during a given year, where $n = 0, 1, 2, 3, 4, 5.$</p>

<p>An actuary makes the following observations:</p>

<blockquote>
  <p>(i) $p_n\geq p_{n+1}$ for $0\leq n \leq 4$</p>
  
  <p>(ii) The difference between $p_n$ and $p_{n+1}$ is the same for $0 \leq n \leq 4$</p>
  
  <p>(iii) Exactly $40\%$ of policyholders file fewer than two claims during a given year.</p>
</blockquote>

<p>Calculate the probability that a random policyholder will file more than three
claims during a given year.</p>

<p><strong>Source:</strong> Marcel B. Finan's <em>A Probability Course for the Actuaries</em></p>

<p><strong>My thoughts:</strong> The goal is to find $P(n &gt; 3)$. We are given that $P(n&lt;2)=.4$, which means that $P(n\geq2)=.6$. $P(n&gt;3) = p_4 + p_5$. From this, we know that $p_0 + p_1 = .4$, and $p_2 + p_3 + p_4 + p_5 = .6$; so, $p_2 + p_3 + P(n &gt; 3) = .6$. But I don't know how to solve for $p_2$ or $p_3$ to find $P(n &gt; 3)$. Any help would be greatly appreciated.</p>
",<probability>
"<p>We'll start off with an example of a question of finding expected value.</p>

<p>What is the expected number of tries to get ’6′ when rolling dice?
E(x)=1/6*1 + 5/6 (1+E(x))
E(x)=6</p>

<p>I understand the intuition, there is a 1/6 probability of getting 6 in one roll and 5/6 of not getting 6, so we roll again and add 1 to the counter since we have done one roll. I wonder what is the formal proof of such method?</p>
",<probability>
"<p>Given a square $S$ with size $1 \times 1$. Two randomly selected points $A$ and $B$ are inside the square. Let $U$ be a square with diagonal $AB$. How to find out the probability $P$($U$ is inside $S$).
<img src=""http://i.stack.imgur.com/KO82j.png"" alt=""example""></p>
",<probability>
"<p>My task is: assess the probability that for some number $n_0$, $A_n$ will happen for every $n&gt;n_0$, where $A_n = \{|\frac{S_n}{n} -p| \le \epsilon\}$ ($S_n $ is the number of successes in Bernoulli scheme with probability of the success equal to $p$).
I don't really understand what exactly should I do. My first attempt was that I found $P(|\frac{S_n}{n} -p| \ge \epsilon) = e^{\frac{-n\epsilon^2}{4}}$ (which was quite easy to do), but I don't know how to use it here? May somebody show me? I would be grateful.</p>
",<probability>
"<p>Given $A$, $B$ and $C$ where $A$ and $B$ are mutually exclusive. </p>

<p>If $P(A\cap C)=0.2$, $P(B\cap C)= 0.1$, $P(C)=0.6$, $P(A\cup B)= 0.6$, $P(A\cup C)= 0.8$, and the relation $P(A) = 2P(B)$, find the probabilities of $A$ and $B$.</p>
",<probability>
"<p>Let us have a walk on $\mathbb Z$ of size $2^n$. To compute the final height of the walk, the trivial way is to sum $1$ for an ascending step and $-1$ for a descending step all along the walk. I would like to have some method to infer the final height in a more efficient way.</p>

<p>More formally, let us denote $\bar h = \frac{h}{2^n}$, where $h$ is the final height. Given an error $\epsilon$ and a probability $p$, I'd like to compute $g$ such that $\mathbb P(\lvert g- \bar h \rvert &gt; \epsilon) &lt; 1-p$, in some ""efficient"" way (i.e in polynomial time in $n$, and the smaller $\epsilon$ and $p$ are, the more I'll give myself time).</p>

<p>What I have is that if I take $i\in \{1,\cdots,2^n\}$, and set $X_i$ to be $1$ if the $i$-th step is ascending and $-1$ if the step is descending, then the expectation of $X_i$ is exactly $\bar h$. I repeat this $k$ times and set $g = \frac{1}{k}\sum_{i=1}^k X_i$. What should be $k$ for $g$ to be close to $\bar h$ with good probability? I guess this should have the flavour of a Chernoff bound, but all I found on this talks about boolean random variables, and I don't know if I can finish we this. How should I conclude?</p>
",<probability>
"<p>I would like to calculate conditional expectation $E[X|A]$, where $A$ is a set, only from the characteristic function $\phi(\omega)$ of a random variable $X$. How can I do this?</p>

<p>Since the characteristic function describes the density function completely, I should be able to do everything at the frequency domain but I dont know how it can be done. If there is no conditioning then, the result is simply the derivative of the characteristic function.</p>

<p>I also wonder how to calculate 
$$\int_{-\infty}^A f(t)\mathrm{d}t$$
from the chracteristic function $\phi(\omega)$ without going back to the density domain.</p>

<p>Thanks alot...</p>

<p>NOTES:</p>

<p>I found a solution to the second part of my question from</p>

<p>$$F_X(x)=\frac{1}{2}+\frac{1}{2\pi}\int_0^\infty \frac{e^{iwx}\phi_X(-w)-e^{-iwx}\phi_X(w)}{iw} \mathrm{d}w$$
with $F_X(A)$</p>
",<probability>
"<p>how would I be able to answer this question?</p>

<p>The first box contains 3 white and 7 black balls, and the second box contains 6 white and 3 black balls, A ball is chosen at random from the first box, and, without looking at its colour, put into the second box. Then a ball is chosen at random from the second box, and it is white. Is it more likely that the ball moved from the first box to the second was black?</p>
",<probability>
"<p>I am currently reading a book ""measure, integral and probability"" by Capinski and Kopp. The correlation between random variables $X$ and $Y$ is defined as the cosine of the angle between $X_c$ and $Y_c$, that is:
$$
\operatorname{corr(X,Y)} = \frac{(X_c,Y_c)}{(\|X\|\cdot\|Y\|)},
$$
where $X_c$, $Y_c$ are centered random variables defined by $X_c=X-\mathbb{E}(X)$, $Y_c=Y-\mathbb{E}(Y)$.
The question that immediately arises is:
Why do we divide by $\|X\|\cdot\|Y\|$, and not $\|X_c\|\cdot\|Y_c\|$? </p>

<p>I want to understand the concept of correlation, independence and other concepts of probability from the point of view of functional analysis. I would be very happy if you could recommend some literature that has a treatment of this topic, and desirably, detailed discussion of the following questions: Are uncorrelated variables just orthogonal? or only if $\mathbb{E}(X)=0=\mathbb{E}(Y)$? How is the ""centred"" vector different from original, ""geometrically""? I know in infinite dimenstions it is hard to visualise geometry, but still... Thank you very much.</p>
",<probability>
"<p><strong>There are $n$ seats in a room. If $n$ people come to the room, what is the probability that $j$ specified people occupy $j$ specified seats? ($j$ names were tagged on the $j$ seats)</strong></p>

<p>$n$ people can occupy $n$ seats in $n!$ ways.
I can't get my head around how to go forward from here.
Any help would be appreciated.</p>
",<probability>
"<p>Suppose I have three random variables, $X,Y,Z$ with $X$ independent of $Z$, $Y$ independent of $Z$.</p>

<p>Which transformation can I apply to $X,Y$ to that the result is again a random variable independent of $Z$? Or better, for which $f(x,y)$ is $f(X,Y)$ independent of $Z$?</p>

<p>For example $f(x,y) = xy$ does not work because in general $XY$ is not independent of $Z$. </p>

<p>Is there a general result?</p>
",<probability>
"<blockquote>
  <p>Suppose we have a box containing $n$ balls numbered $1, 2,\dotsc,n$. A random sample of size $k$ is drawn without replacement and the numbers on the balls noted. These balls are than returned to the box and a second random sample of size $r$ is then drawn without replacement. $(r + k &lt; n)$ Find the probability that two samples contain all different balls.</p>
</blockquote>

<hr>

<p>This was my approach.<br>
Take the $k$ balls first.<br>
Then the probability that of not getting the same $k$ balls again is $$1-\frac{1}{(n)(n-1)(n-2)\dotsm(n-k)}$$
I hope that makes sense.<br>
In this I ignored the second sample size, I don't think that matters. 
Is it right?</p>
",<probability>
"<p>Suppose a fair coin is tossed $900$ times. Find the probability of getting more than $475$ heads. Use the continuity correction.</p>

<p>My answer:</p>

<p>$n=900, p=1/2, q=1/2$</p>

<p>$\mu=900(1/2)=450, npq=\sigma^{2}=225,\sigma=15$</p>

<p>$Z=(X-\mu)/\sigma$</p>

<p>$P_B(X\geq475)=P_B(475 \leq x \leq 900)$</p>

<p>$=P_N(474.5 \leq 900.5)$</p>

<p>$Z=(X-\mu)/\sigma$</p>

<p>$=(474.5-450)/15=1.63$</p>

<p>$Z=(X-\mu)/\sigma$</p>

<p>$(900.5-450)/15=30.03$</p>

<p>$=P_N(0&lt;z&lt;1.63)+P_N(0,z,30.03)$</p>

<p>$.4484+.5000$</p>

<p>$.9484$</p>

<p>Then we get:</p>

<p>$1-.9484$</p>

<p>$.0516$</p>

<p>I was just trying this problem to see what it would be like after reading about the topic. I wanted to know what I did wrong. The answer says $.0446$. Can someone help me with this?</p>
",<probability>
"<p>I want to show</p>

<p>Px$(B(s)\ge0 $ for all 0 $\le s \le t$ and B(t) $\in$ M) = Px(B(t) \in M)$-$P-x$(B(t) \in M)$</p>

<p>where,x>0,M is measurable set in [0,$\infty$).</p>

<p>The difficulty for me is how to handle the left side of the equation.I know the distribution of hitting time.But here it is not only about hitting time.</p>
",<probability>
"<p>Question: </p>

<p>I have $5$ yellow bulbs and $4$ red bulbs. These bulbs will be placed in a straight line such that $2$ on the left side are the same colour as each other, and $2$ on the right side are also the same colour (but not the same as the left side). How many ways are there of planting the bulbs?</p>

<p>I'm really not sure about how to answer this question, I assumed you would go about it by doing $5C2 + 5C3$. But I'm really not too sure. </p>

<p>Thanks!</p>
",<probability>
"<p>So the question is really hard I think. I tried using a simple way by calculating the probability of each combination that makes a sum divisible by six, but it would take forever. Does anyone have any ideas?</p>

<p>Suppose that we roll a six-sided die ten times. What is the probability
that the total of all ten rolls is divisible by six?</p>
",<probability>
"<p>I was wondering how the ordinary expectation value $E(X)$ is related to $E(X|\mathcal{F})$ where $\mathcal{F} \subset \mathcal{E}$ where the latter is supposed to be the sigma algebra on our probability space.</p>

<p>My first thought was that $E(X|\mathcal{E}) = E(X),$ but this is clearly wrong, as $X$ is $\mathcal{E}$ measurable and thus $E(X|\mathcal{E})= X.$</p>

<p>Then I noticed that by the total law of expectation $E(E(X|\mathcal{F}))=E(X)$ we have something like a tower property for the standard expectation value, but in the sense that $E(.)$ wins over any $E(.|\mathcal{F}).$ Spoken in terms of tower properties, this would mean that if $E(X)$ can be represented as a conditional expectation, it must be a maximally small sigma algebra. So my guess is $E(X|\{\emptyset, \Omega\})=E(X),$ is this true?</p>

<p>At first glance, it seems to fulfill all the properties of the conditional expectation, so my guess is yes, but I would like to have your confirmation.</p>
",<probability>
"<p>Determine the probability that in a group of $7$ randomly drawn cards from well
mixed deck of $52$ cards will be exactly $2$ cards with a picture and exactly $4$ red cards (hearts or diamonds)</p>

<p>I don't know how to start I have trouble with reasoning in this case</p>
",<probability>
"<p>Convergence in Probability talks about two RVs, $X_n$ and $X$ , associated with an experiment  - </p>

<p>$\lim_{n \rightarrow \infty} P\big(|X_n-X| \geq \epsilon \big)=0, \qquad \textrm{ for all }\epsilon&gt;0.$</p>

<p>I'm quite not able to understand what $P( \vert X_n - X \vert &gt; \epsilon)$ means in terms of elementary outcomes of the experiment.</p>

<p>I can understand that $P(X_n &gt; a)$ means the sum of probabilities of all outcomes, $o_i$,  such that $X_n(o_i)$ = $x_i$ and $x_i &gt; a$</p>

<p>Similarly, an expression like $P(X &gt; b)$.</p>

<p>But an expression like $P( \vert X_n - X \vert &gt; \epsilon)$ involves 2 random variables. How do we explain this expression in terms of elementary outcomes of the experiment ?</p>
",<probability>
"<p>You have $n = n_A + n_B$ $k$-sided dice. The $n_A$ dice are thrown and a <em>set</em> of the resulting values, call it $S_A$, is built; likewise for the $n_B$ dice, calling the resulting set $S_B$.</p>

<p>What is the probability of $S_A \cap S_B = \varnothing$?</p>
",<probability>
"<p>Past Exam Paper Question -</p>

<p>Prof. Smith is crossing the Pacific Ocean on a plane, on her way to a conference.
The Captain has just announced that an unusual engine fault has been
signalled by the plane’s computer; this indicates a fault that only occurs once in
10,000 flights. If the fault report is true, then there’s a 70% chance the plane
will have to crash-land in the Ocean, which means certain death for the passengers.
However, the sensors are not completely reliable: there’s a 2% chance of
a false positive; and there’s a 1% chance of the same fault occurring without the
computer flagging the error report.</p>

<p><strong>Question</strong> </p>

<p>Formulate this problem in terms of conditional probabilities of outcomes, existence of
a fault and whether or not it is reported and use Bayes’ rule to compute Prof. Smith’s chances of survival. </p>

<p><strong>My Attempt</strong></p>

<p>P(Fault) - 0.0001</p>

<p>P(Crash | Fault) - 0.7</p>

<p>P(FalsePositive | Fault) - 0.02</p>

<p>P(NoReport | Fault) - 0.01</p>

<p>I have no idea what to do next, every example I look at seems  a lot easier than this. Could someone help me out? </p>
",<probability>
"<p>Consider the random variables $W_i,W_j, X_i, X_j$ with $X_i\sim X_j$, $X_i\perp X_j$ and $W_i\sim W_j, W_i\perp W_j$, where $\sim$ denotes equal probability distribution and $\perp$ denotes independence. </p>

<p>Suppose $W_i, W_j$ are continuously distributed with support $\mathcal{W}$ with everywhere strictly positive density. Suppose $X_i, X_j$ have support $\mathcal{X}$. </p>

<p>Consider the function $p(X_i,X_j): \mathcal{X}\times \mathcal{W}\rightarrow [0,1]$ continuous in $W_i$ $(W_j)$ for any realisation of $X_i$ $(X_j)$. </p>

<p>Consider the function $\lambda(p): [0,1]\rightarrow \mathbb{R}$ continuous in p.  </p>

<p>Suppose that the support of $(X,W)$ is $\mathcal{X}\times \mathcal{W}$. </p>

<p>I think that under these conditions
$$
\mathbb{P}(\lambda(p(X_i,W_i))-\lambda(p(X_j,W_j))&lt;\epsilon| X_i=x, X_j=\tilde{x})&gt;0 \hspace{2cm}(\star)
$$
$\forall x, \tilde{x}\in \mathcal{X}^2, x\neq\tilde{x} $, $\forall \epsilon&gt;0$. </p>

<p>Could you help me to formalise a proof? Do I need $\mathcal{W}$ compact?</p>

<hr>

<p><strong>Attempt:</strong> </p>

<p>(1) Since $\lambda$ is the composition of continuous functions, it is continuous in $W_i$ $W_i$ $(W_j)$ for any realisation of $X_i$ $(X_j)$. </p>

<p>(2) Given $X_i=X_j=x$ (and if $\mathcal{W}$ is compact?) $\exists$ $w,\tilde{w}\in \mathcal{W}^2, w\neq \tilde{w}$ such that $\lambda(p(x,w))-\lambda(p(x,\tilde{w}))&lt;\epsilon$ $\forall \epsilon&gt;0$</p>

<p>(3) Does (2) imply $\lambda(p(x,w))-\lambda(p(\tilde{x},\tilde{w}))&lt;\epsilon$ $\forall \epsilon&gt;0$ $\forall x, \tilde{x}\in \mathcal{X}^2, x\neq\tilde{x} $?</p>

<p>(4) As the support of $(X,W)$ is $\mathcal{X}\times \mathcal{W}$ and $W$ has everywhere positive density on $\mathcal{W}$, $(\star)$
follows. </p>

<p>(5) Where do I use the fact that $X_i\sim X_j$, $X_i\perp X_j$ and $W_i\sim W_j, W_i\perp W_j$?</p>
",<probability>
"<p>In a book about machine learning, it reads,</p>

<blockquote>
  <p>Generally, the probability that $x$ generated independently by a continuous probability distribution $p(x)$ have the same value is zero. Otherwise, $\int p(x)dx = \infty$. </p>
</blockquote>

<p>Why is it impossible that the same value is sampled more than once?</p>
",<probability>
"<p>Assume we toss a coin 10 times, independent of each other. 
Each time we can get Heads (H) or Tails (T) , regardless of whether it is fair or not.</p>

<p>So for example this is one possible outcome: HHHHHHHHHH ie 10 heads. Let's call this a 10-toss sequence. </p>

<p>The total number of possible 10-toss sequences is 2^10 because each toss has 2 possible outcomes: Heads or Tails.</p>

<p>--The first question is What is the meaning of 10!/(3!7!) ? The answer is that It is the total number of ways, by which we can place the three heads inside the 10-toss sequence. The order by which we place the three heads does not matter. </p>

<p>--The second question is : What is the total number of possible 3-head sequences?
Is it 10!/(3!7!)? Some say YES. Others, say NO.<br>
A 10-toss sequence with three heads, can be this one: HHH THTHTHT. So three positions are fixed to 'heads'. The remaining seven positions can be H or T. So we have 2^7 possible 7-toss sequences. And the three heads can be anywhere in this 10-toss sequence. </p>

<p>So an answer can be that the total number of 10-toss sequences with 3 heads is 10!/(3!7!) multiplied by (2^7) . That is in the 10-toss sequence the number of ways by which we can place 3 heads is 10!/(3!7!). Then, we have to say something about the remaining 7-toss sequence. Each position can take Heads or Tails . So 2^7 is the possible number of this 7-toss sequence. </p>

<p>However, I know that the correct answer is 10!/(3!7!) and that we don't need to multiply by 2^7. But I cannot understand why.  </p>
",<probability>
"<p>In an example, where a test has a maximum score of $200$, and a minimum score of $0$, can one eliminate the infinite boundaries?</p>

<p>Let's say my $\mu$ is $100$, and my $\sigma$ is $50$. Integrating the normal function for $200 \leq X$, I get:</p>

<p>$$\int^\infty_{200} \frac{e^{-\frac{(x-100)^2}{2\times50^2}}}{50\sqrt{2\pi}}dx \approx 0.022$$</p>

<p>There is a 2.2% chance that one will score over the limit of 200. Is there any way to make scoring over 200 impossible, and changing the maximum bound from $\infty$ to 200, and $-\infty$ to 0 in a way, that the integral below is true:</p>

<p>$$\int^{200}_{0} \frac{e^{-\frac{(x-100)^2}{2\times50^2}}}{50\sqrt{2\pi}}dx = 1$$</p>
",<probability>
"<p>If $Y_1,\ldots,Y_n$ independent each having pdf:
$$ f(y\mid \beta,\theta, x)=\theta e^{-\theta(y-\beta x)},~~ y&gt;\beta x$$
where $x_1,\ldots,x_n$ are given, the parameters $\beta$ and $\theta$ are unknown. I know the joint sufficent statistics for $\beta$ and $\theta$ are $\overline{Y}$ and $\min\{Y_i\}$. But can I say that the sufficient statistic for $\beta$ is $\min\{Y_i/X_i\}$?</p>

<p>I don't know why, but I feel strange calculating sufficient statistics for only part of the parameters.</p>
",<probability>
"<p>I have been working on this problem from a previous exam in Probability theory but I can't understand the next step I am supposed to take. Here is the problem:</p>

<p>Suppose that $Z_1$ and $Z_2$ are independent exponential random variables with
parameter 1 and let $X=\frac{Z_1}{Z_1 + Z_2}$.
Find the cumulative distribution function of X and identify the corresponding distribution.</p>

<p>I tried this:
$$
\begin{align}
P(X \le x) &amp;= P(\frac{Z_1}{Z_1+Z_2} \le x)\\
&amp;= P(Z_1 \le xZ_1+xZ_2)\\
&amp;= P(Z_1 \le Z_2(\frac{x}{1-x}))\\
&amp;= P(Z_1 \le g(Z_2))\\
\end{align}
$$</p>

<p>At this point I would like to use CDF for $Z_1$ like this $1-e^{-x*g(Z_2)}$ but the key uses
$\int_{0}^{\infty}e^{-x}P(Z_1 \le\frac{x}{1-x}z)dz$.  How do I make this jump because it doesn't seem to follow from the work I have done.</p>
",<probability>
"<p>I am using Ross' A First Course In Probability (4th). On page 113, Example 1d states the following:</p>

<blockquote>
  <p>Independent tirals consisting of the flipping of a coin having
  probability $p$ of coming up heads are continually performed until
  either a head occurs or a total of $n$ flips is made. If we let $X$
  denote the number of times the coin is flipped, then $X$ is a random
  variable taking on one of the values 1, 2, 3, ..., n with respective
  probabilities</p>
  
  <p>$P\{X=1\} = P\{H\} = p$</p>
  
  <p>$P\{X=2\} = P\{(T,H)\} = (1-p)p$</p>
  
  <p>$P\{X=3\}=P\{(T,T,H)\} = (1-p)^2p$</p>
  
  <p>.</p>
  
  <p>.</p>
  
  <p>.</p>
  
  <p>$P\{X=n-1\}=P\{(T,T,...,T,H)\}=(1-p)^{n-2}p$</p>
  
  <p>$P\{X=n\}=\{(T,T,...,T,T),(T,T,...,T,H)\}=(1-p)^{n-1}$</p>
</blockquote>

<p>There are a couple of things that are confusing to me here. To my understanding, they are defining $X$ to be ""the number of times the coin is flipped until a head occurs or a total of $n$ flips is made."" Then, We would have $P\{X\}$ to be</p>

<p>$P\{X = n\} = \sum\limits_{i=1}^n (1-p)^{i-1}p^i + (1-p)^i$    , isn't it?</p>

<p>Also, wouldn't $P\{X=n\} =(1-p)^{n-1}p$? How does it turn out to be $(1-p)^{n-1}$?</p>

<p>Any help would be appreciated.</p>
",<probability>
"<p>I am new in this forum and I am happy to find it, because it seems a very precious place for asking questions.
My question is  about some probability inequality. I formulate this as following.
Let $(X_k)_{1\leq k \leq K}$, with $K$ a nonzero integer, be a discrete-random variables live in   $\{2, ..., n\}$ where $n \geq 2$ . Then, if  $(n_k)_{0\leq k \leq K}$, with $n_0 =1$, is a sequence of integers in  $\{1, ..., n\}$, one can prove that
$$ \mathbb{P}\big( \exists k\in \{1, ...., K\};\, X_k \leq n_{k-1}\big) \leq \sum_{k=1}^K 2^{k-1}\mathbb{P}\big(\max\{ 1\leq \ell \leq K; X_\ell \leq n_{\ell-1}\} = k\big).
$$ </p>

<p>Thank you very much for your sugggestions, </p>

<p>Emera</p>
",<probability>
"<p>This is what I got. $\dfrac{1}{6} \cdot \dfrac{1}{6} = 2.78\% \cdot 24 = 66.72\%$</p>

<p>I believe that since it is a six sided dice, since you roll both of them simultaneously it would be $\dfrac{1}{6} \cdot \dfrac{1}{6}$. </p>

<p>So since they are rolling them $24$ times, I would just multiply it by $24$, so $2.78\% * 24$ would be $66.72\%$, which would mean I have a  $67.7\%$ chance of rolling a double six.</p>

<blockquote>
  <p>Do you think this is correct? Am i doing this correctly? </p>
</blockquote>
",<probability>
"<p>I'm facing the following problem.</p>

<p>Let's say I have N dices in a hand. I need to calculate how much time I should roll my dices to make all of them equal selected (pre-defined) number. Each time when I roll dice with selected number I'm removing this dice(s) from my hand and roll the rest.</p>

<p>Example:
I have 2 dices and I want to all six. I'm rolling two dices until I will get 6 (or possible two) six. When I will get one I will remove this dice and will roll 1 dice instead of two. How much times I need roll dices in my hand to get all six (to make my hand empty)?</p>

<p>I suppose that correct answer is (for two dices): 1/6 + 1/6 + 1/6*1/6 but it seems to be wrong because I tried to implement algorythm to calculate probability running 1M continuous rolls to calculate average amount of required rolls.</p>

<p>Any help appreciated</p>
",<probability>
"<p>I've had a crack at this question however I don't seem to be getting the correct answer and I can't figure out why. I've been given a table of the 'Normal Distribution Function' where the left tail is tabulated for $0\leq x\leq 4$.</p>

<blockquote>
  <p>Given that $Z\sim N(0,1)$, what is the value of $P(-1.1 &lt; Z &lt; 0.35)$
  to 4 decimal places?</p>
</blockquote>

<p>My working is as follows:
Rearranging the equation first then looking up the values in the given tables.
$$P(Z &lt; 0.35) - P(X &gt; -1.1)$$
$$P(Z &lt; 0.35) - P(X &lt; 1.1)$$
$$0.6368 - 0.8643 = -0.2275$$</p>

<p>The given answer for this question is $0.5011$.</p>

<p>Could someone please explain where I'm falling short and how to correctly solve this question?</p>
",<probability>
"<p>A family is considering buying a dog. The probability that they will buy a small dog is $0.1$, that they will buy a medium-size dog is $0.3$, that they will buy a large dog is $0.2$, and that they will buy a very large dog is $0.1$. What is the probability that the family will buy a dog? </p>
",<probability>
"<blockquote>
  <p>Let $\phi_X(t)$ be the characteristic function of $X$. Let $N$ be a Poisson random varivale with mean $1$ and $(X_i)_{\in\mathbb{N}}$ be i.i.d. copies of $X$. Then how to derive the charactersitics function of $S=\sum_{i=1}^NX_i$.</p>
</blockquote>

<p>My attempts: $\phi_S(t)=E(e^{it\sum_{i=1}^NX_i}).$ How to continue?</p>
",<probability>
"<p>A restaurant has 3 fish dishes, 6 meat dishes and 5 vegetarian dishes on the menu. Suppose that customers select their dish at random. Five customers enter the restaurant.
Note that more than one customer can have the same dish.</p>

<p>a) What is the probability that the first customer chooses a vegetarian and two fish dishes are chosen?</p>

<p>b)What is the probability that two customers choose a fish dish given that only one customer chooses a vegetarian dish?</p>

<p>c)What is the probability that the first customer orders a fish and the second, a vegetarian?</p>

<p>Very interested in how probability distribution can be used in solving these, my current approach was the conventional use of combinations and permutations but not getting the correct answers.</p>
",<probability>
"<p><strong>Problem.</strong> Suppose we have $n + 1$ random variable $\xi_0, \xi_1, \dots, \xi_n$ and they are independent and all standard normal distributed. Find probability that $\xi_0$ greater than $\max\{\xi_1, \dots, \xi_n\}$ at least in $\alpha &gt; 1$ times.</p>

<p><strong>Solution.</strong></p>

<ol>
<li><p>Let's find distribution of $\max\{\xi_1, \dots, \xi_n\}$. For me it's pretty obvious and intuitive that if $\xi_0$ has a distribution $F(x)$ and density $f(x)$ then $\max\{\xi_1, \dots, \xi_n\}$ distributed like $(F(x))^n$ and its density will be $f(x)n(F(x))^{n-1}$.</p></li>
<li><p>Let's integrate it over suitable domain.
$$
\int_{-\infty}^{+\infty} \int_{\alpha y}^{+\infty} f(x) n(F(y))^{n-1}f(y) dx dy = \int_{-\infty}^{+\infty} (1 - F(\alpha y)) n(F(y))^{n-1}f(y) dy
$$</p></li>
</ol>

<p>and I don't know how to deal with this integral because $F(x)$ has no good explicit form.</p>
",<probability>
"<p>I got this problem:</p>

<p>Given a $7\times 7$ grid, if we distribute $29$ disks on the grid such that each square cannot hold more than $1$ disk, what is the probability that there will be at least one row full of disks on the grid?</p>

<p>My first try:<br/>
$P(\{\text{there is at least one row full of disks}\}= \frac{7\times{42\choose 22}}{49\choose 29}$</p>

<p>Since we have $7$ ways to choose the row that we will fill by disks, and then we have remaining $22$ disks which we will distribute over the remaining $42$ squares. But this is obviously wrong since we count some combinations multiple times.</p>

<p>My second try:<br/>
$P(\{ \text{there is at least one row full of disks}\}= P(\{\text{there is exactly 1 row full of disks}\}\cup\{\text{there is exactly 2 rows full of disks}\}\cup\{\text{there is exactly 3 rows full of disks}\}\cup\{\text{there is exactly 4 rows full of disks}\})=P(\{\text{there is exactly 1 row full of disks}\}+P(\{\text{there is exactly 2 rows full of disks}\}+P(\{\text{there is exactly 3 rows full of disks}\}+P(\{\text{there is exactly 4 rows full of disks}\}$</p>

<p>But this probably makes things harder and does not simplifies things.</p>

<p>My third try:<br/>
$P(\{\text{there is at least one row full of disks}\}= 1-P(\{\text{there are no rows full of disks}\})$</p>

<p>But I got stuck, I tried to count the number of combinations in which each row got an empty square but here too I counted some combinations multiple times.</p>

<p>Any hint/help will be appreciated.</p>
",<probability>
"<p>How do we use the Chernoff bound to prove that </p>

<p>$$ Q(x)\leq e^{-\frac{x^{2}}{2}} $$</p>

<p>where Q(x) is the probability that a standard normal random variable X takes a value greater than x</p>
",<probability>
"<p>I am having a very hard time understanding Strict Sense Stationary Random Processes(SSSRP). One of the examples I am given has $X[n]$ being a SSSRP. We then have $Y[n] = X[n]^2$. Does this make $Y[n]$ SSS? </p>

<p>I originally thought yes, because $Y[n+t] = X[n+t]^2$ which would make it SSS, correct? </p>

<p>(Homework)</p>
",<probability>
"<p>I just came back from a class on Probability in Game Theory, and was musing over something in my head.</p>

<p>Assuming, for the sake of the question:</p>

<ul>
<li>Playing cards in their current state have been around for approximately eight centuries</li>
<li>A deck of playing cards is shuffled to a random configuration one billion times per day</li>
<li>Every shuffle ever is completely (theoretically) random and unaffected by biases caused by human shuffling and the games the cards are used for</li>
<li>By ""deck of cards"", I refer to a stack of unordered $52$ unique cards, with a composition that is identical from deck to deck.</li>
</ul>

<p>This would, approximately, be on the order of $3 \cdot 10^{14}$ random shuffles in the history of playing cards.</p>

<p>If I were to shuffle a new deck today, completely randomly, what are the probabilistic odds (out of $1$) that you create a new unique permutation of the playing cards that has never before been achieved in the history of $3 \cdot 10^{14}$ similarly random shuffles?</p>

<p>My first thought was to think that it was a simple matter of $\frac{1}{52!} \cdot 3 \cdot 10^{14}$, but then I ran into things like <a href=""http://en.wikipedia.org/wiki/Birthday_Paradox"">Birthday Paradox</a>.  While it is not analogous (I would have to be asking about the odds that any two shuffled decks in the history of shuffled decks ever matched), it has caused me to question my intuitive notions of Probability.</p>

<p>What is wrong in my initial approach, if it is wrong?</p>

<p>What is the true probability?</p>

<p>And, if the probability is less than $0.5$, if we how many more years (centuries?) must we wait, assuming the current rate of one billion shuffles per day, until we reach a state where the probability is $0.5$+?   $0.9$+?</p>

<p>(Out of curiosity, it would be neat to know the analogous birthday paradox answer, as well)</p>
",<probability>
"<p>Randomly break a stick (or a piece of dry spaghetti, etc.) in two places, forming three pieces.  The probability that these three pieces can form a triangle is $\frac14$ (coordinatize the stick form $0$ to $1$, call the breaking points $x$ and $y$, consider the unit square of the coordinate plane, shade the areas that satisfy the triangle inequality <em>edit</em>: see comments on the question, below, for a better explanation of this).</p>

<p>The other day in class<sup>*</sup>, my professor was demonstrating how to do a Monte Carlo simulation of this problem on a calculator and wrote a program that, for each trial did the following:</p>

<ol>
<li>Pick a random number $x$ between $0$ and $1$.  This is the first side length.</li>
<li>Pick a random number $y$ between $0$ and $1 - x$ (the remaning part of the stick).  This is the second side length.</li>
<li>The third side length is $1 - x - y$.</li>
<li>Test if the three side lengths satisfy the triangle inequality (in all three permutations).</li>
</ol>

<p>He ran around $1000$ trials and was getting $0.19$, which he said was probably just random-chance error off $0.25$, but every time the program was run, no matter who's calculator we used, the result was around $0.19$.</p>

<p>What's wrong with the simulation method?  What is the theoretical answer to the problem actually being simulated?</p>

<p>(<sup>*</sup> the other day was more than $10$ years ago)</p>
",<probability>
"<p>Two mathematicians each come into a coffee shop at a random time between 8:00 a.m. and 9:00 a.m. each day. Each orders a cup of coffee then sits at a table, reading a newspaper for 20 minutes before leaving to go to work.</p>

<p>On any day, what is the probability that both mathematicians are at the coffee shop at the same time (that is, their arrival times are within 20 minutes of each other)?</p>
",<probability>
"<p>Recently I discussed an experiment with a friend. Assume we start a random experiment. At first there is an array with size $100,000$, all set to $0$. We calculate at each round a random number modulo $2$ and select one random position in that array. If the number in the array is $1$, nothing is changed and otherwise the pre-computed value is set. The question is: how many distinct hash values would we have added in $1$%, $5$%, $50$%, $95$%, $99$% of all cases?</p>

<p>Example: $4$ rounds with array of size $10$:</p>

<pre><code>Array                     Position   random number
[0,...,0]                    5              0
[0,...,0]                    7              1
[0,...0,1,0,0,0]             6              1
[0,..0,.1,1,0,0,0]           6              0
[0,..0,.1,1,0,0,0]           2              0
</code></pre>

<p>First we considered this a somehow simple problem, but after thinking for some hours, searching the web, and asking some math students, we couldn't find a solution. Do you know a probability distribution for this problem? </p>

<p>Remark: Was also posted on <a href=""http://mathoverflow.net/questions/33335/looking-for-a-probability-distribution"">Math Overflow</a> and got its answer there.</p>
",<probability>
"<p>Say there are three jars, $j_1, j_2, j_3$ filled with different binary sequences of length two.  </p>

<p>The distribution of the binary sequences in each of the jars is given by the $p_i^k(1-p_i)^{n-k}$, where 
$p_i = \frac{i}{m + 1}$ where $m$ is the number of jars, $i$ is the jar index, $k $is number of 1$$'s and $n$ is the length of the string.  </p>

<p>So for three jars we have $p_1 = 0.25, p_2 = 0.5$, and $p_3 = 0.75$ for $j_1, j_2, j_3$ respectively.  </p>

<p>Here are the sequences and their probabilities for $j_1$ with $p_1 = 0.25$:</p>

<p>\begin{align*}
P(00) = 9 / 16 \\
P(10) = 3 / 16 \\  
P(01) = 3 / 16 \\  
P(11) = 1 / 16.
\end{align*}</p>

<p>If I tell you that I have selected a binary sequence and the first element is $1$ what is the E($p_i$)?</p>

<p>Well, this can be calculated by looking at each of the jars and adding up the probability of candidate sequences times the value of $p_i$.</p>

<p><strong>Edit:</strong> I wasn't normalizing this conditionally space properly. I'm skipping a step which I'll explain, someone wants.</p>

<p>\begin{equation*}
E(p_i) = (4/24 * 1/4) + (8/24 * 1/2) + (12/24 * 3/4) = 14 / 24 = 0.58.
\end{equation*}</p>

<p>So the question is ... what is $E(p_i)$ when the numbers of jars goes to infinity (or alternatively, when $p$ can take on values between $0$ and $1$)? Also what happens when the size of the binary strings goes to infinity? Does it have an effect on the outcome? If it does, does the order we take the limits change the answer?</p>

<p>And most importantly what is the general case for when I have $s$ 1's and $r$ $0$'s?, with a continuous $p$ from $0$ to $1$ and infinite sequences?</p>
",<probability>
"<p>A rather fundamental concept which I somewhat failed to grasp and now is jeopardising my further understanding/solving of probability problems..</p>

<p>In the case of this question, where we are to find the probability, that the minimum of two throws of a fair die equals $k$, $k \leq 6, k \in \mathbb{N}$, do we have to account for the ordering of the dice? </p>

<p>I.e., assuming $k = 3$, is the probability $P(\{3\}) = \frac{1}{6}\times\frac{4}{6}\times 2$ in order to account for the fact that the first throw could be $3$ and the second throw anything from $3$ onwards OR vice versa (the first throw anything from $3$ onwards and the second throw $= 3$)? Or should it just be $P(\{3\}) = \frac{1}{6}\times\frac{4}{6}$ since the dice are similar and there is no mention that the two dice are unique (e.g. different in colour, size etc.).</p>

<p>The general question, hence, is, for cases where coins/dice are involved and are not uniquely labelled, should the order be regarded, if there is no additional mention of a first/second throw? </p>

<p>Hope you all get my drift..</p>
",<probability>
"<blockquote>
  <p>Let $A$ and $B$ be events, $P(A) = \frac{1}{4} $, $P(A\cup B) = \frac{1}{3} $ and $ P (B) = p $. </p>
  
  <ol>
  <li>Find $p$, if $A$ and $B$ are mutually exclusive.</li>
  <li>Find $p$, if $A$ and $B$ are independent.</li>
  <li>Find $p$, if $A$ is a subset $B$.</li>
  </ol>
</blockquote>

<p>Can someone help me to solve it?</p>
",<probability>
"<p>$n$ balls, each with a weight $p_i$, are thrown into $m$ bins. Each bin is chosen with uniform probability.</p>

<p>Prove or disprove that the expected value of the maximum load among the loads of bins is $\frac1m\sum_{j=1}^n p_j$, where with ""load"" means the sum of the weights of the balls in that bin.</p>

<p>Now, I was able to model the problem on the expected value of each bin and this is:
$E[X_i]=\frac1m\sum_{j=1}^n p_j$, where $X_i$ is the load of the bin $i$.</p>

<p>Should I calculate something like this:
$$E[\max_{1 \leq i \leq n} {X_i}]$$</p>

<p>Do you have any idea? Or is the equation to disprove? But, I have no idea how to have to find a counterexample to disprove with expected values.</p>
",<probability>
"<p>Suppose we pick a random real number between 0 and 1 and call it $x$. There are $2^{\aleph_0}$ possible values, so the chance of picking any specific number (such as $x$) in that range is 0. But in the end, we did manage to pick $x$, despite its probability of 0.</p>

<p>Does this mean that a 0% chance is actually possible, or is there some flaw in this logic?</p>
",<probability>
"<p>Given the probability density function of the random variable $X$ is $f_X(x)$ and the probability of set $A=\{x:a&lt;X&lt;b\}.$ How can we find the conditional probability density function $f_{X\mid A}(x)$?</p>

<p>My attempt:</p>

<p>When $x\notin A$, $f_{X\mid A}(x)=0.$</p>

<p><strong>Correction according to comment</strong>
When $x\in A$, $f_{X\mid A}(x)=\cfrac{f_X(y)}{P(A)}$ where $P(A)$ is the probability of even $A$.</p>

<p>This seems to give a valid probability distribution that sums to 1. But I am not sure if it is correct. Also is it a definition that I just wrote? Or can we derive it from some fundamentals?</p>

<p>Thanks a lot in advance. </p>
",<probability>
"<p>Suppose I have $20$ red balls in one box and $20$ blue balls in another box. There $12$ red balls and $7$ blue balls have stars on them. </p>

<p>I randomly take out one red ball and one blue ball at each time, don't put them back, and repeat this $10$ times. </p>

<p>What is the probability that I get one red ball with stars and one blue ball with stars for at least $5$ times?   </p>
",<probability>
"<p>I have four random variables <strong>A,B,C</strong> and <strong>S</strong>. A,B and C are conditionally independent given S. So, I need to obtain <strong>P(A,B,C,S)</strong></p>

<p>By the chain rule:
$$P(A,B,C,S)=P(S)P(A|S)P(B|A,S)P(C|A,B,S)$$
By the conditional independence
$$P(B|A,S)=P(B|S)$$
Is this correct? $$P(C|A,B,S)=P(C|S)$$
So $$P(A,B,C,S)=P(S)P(A|S)P(B|S)P(C|S)$$</p>

<p>Also I have doubts in the calculation of the joint probability between A,B and C.
$$P(A,B,C)=P(A)P(B|A)P(C|A,B)$$
I am not sure how to compute $P(C|A,B)$ I know the diverse marginal probabilities and conditional between two variables.
Thanks.</p>
",<probability>
"<p>Suppose $\sum_{n=1}^\infty X_n = \infty$ almost surely for nonnegative $X_n$. Let $\mathcal F_n = \sigma(\{X_0, X_1, \ldots, X_n \})$. </p>

<p>Can we show that $\sum_{n=1}^\infty \mathbf{E} (X_n | \mathcal F_{n-1}) = \infty$? </p>
",<probability>
"<p>Suppose $X_1, X_2, \ldots, X_n$ are a Bernoulli($\theta$) with pmf:</p>

<p>$$P(X|\theta)=\theta^X(1-\theta)^{1-X}, \; X \in \{0,1\}$$</p>

<p>Prove or disprove that $\bar{X}(1-\bar{X})$ is an unbiased estimator of $\theta(1-\theta)$</p>

<p>My attempt:</p>

<p>After taking the expectation of $\bar{X}(1-\bar{X})$, I'm getting $E(\bar{X})-E(\bar{X}^2)$. I know that $E(\bar{X}^2)=Var(\bar{X}^2)+[E(\bar{X})]^2$.</p>

<p>If I'm on the right course, how do I calculate $Var(\bar{X}^2)$?
Or Is there an alternative method for this?</p>
",<probability>
"<p>First of all, I'm asking this because I'm writing a game, so this is probably not a typical question in probability. However I'm new to game design so I don't even know what this would be called.</p>

<p>Suppose I have a certain event that should occur and the probability of that event happening (over a certain amount of time). For example, let's suppose the probability is 15% per day.</p>

<p>Is there a <em>single calculation</em> I can perform that will, along with an evenly distributed random number, predict <em>when</em> that event will occur?</p>

<p>Predict is probably not the best word to use here, since I'm not going for accuracy in a single prediction, only a realistic distribution, were it to be predicted over and over again with different random numbers.</p>

<p>Ideally it should not be limited to a certain temporal resolution. I.e. it should not be an even number of days, but should be a continuous function giving me a result right down to the second.</p>
",<probability>
"<p>Suppose you roll a fair 6-sided dice three times. There are $6^3$ possible outcomes and each is equally likely.</p>

<p>Let $A_1$, $A_2$, $A_3$, $A_4$, $A_5$, and $A_6$ be the events that the last value is a $1$,$2$,$3$,$4$,$5$, and $6$ respectively. 
Let $B$ be the event that the first value is less or equal to the second value and the second value is less or equal to the
third value.</p>

<p>What is $P(B)$?</p>

<p>The total probability law states that </p>

<p>$$P(B) = P(A_i)P(B|A_i)+....+P(A_n)P(B|A_n)$$</p>

<p>Let's assume that $x_i$ are values of $A_i\cap B$, then 
$$P(B|A_n) = \frac{x_i}{P(A_i)}$$</p>

<p>What I am getting confused at is that, if I substitute $P(B|A_n)$ in I'd get $$P(A_i)\frac{x_i}{P(A_i)} = x_i+...+x_n$$ which is just the addition of $x_i$'s.</p>

<p>I don't feel this is the correct way to simplify the total probability law. I am looking for some clarification. </p>
",<probability>
"<p>How to find a Probability Density/Mass Function for a random variable without assuming it follows a predetermined distribution, say, Normal or Poisson etc,. Lets say i have two hours of data of vehicle arrives at a specific point and i want to know what kind of distribution it has? Please help me understand this. Thank You, Guys.</p>
",<probability>
"<p>Good morning,</p>

<p>I want to calculate the probability density function of a random variate $Z=cos(Y)$, where $Y=Φ_1−Φ_2$ and $Φ_{1,2}∼U(0,2π)$, that is both variables are uniformly distributed in $(0,2π)$ and also independent. This last hypothesis makes $Y$ a triangular distribution in $(−2π,2π)$, so with this pdf:
\begin{equation}
f_Y(y)=\frac{1}{2π}\biggl( 1−\frac{|y|}{2π} \biggr )
\end{equation}</p>

<p>I calculate Z using the funtamental theorem. When $−2π&lt;y&lt;2π$ we have 4 solution to the equation $z=cos(y)$:
\begin{align}
f_Z(z) &amp; =∑f_Y(\cos^{−1}(z)) \biggl |\frac{1}{−\sin(\cos^{−1}(z))} \biggr |  =4⋅\frac{\frac{1}{2π}(1−\frac{|\cos^{−1}(z)|}{2π})}{\sqrt{1-z^2}} \\
&amp;=\frac{2\biggl (1−\frac{\cos^{−1}(z)}{2π}\biggr)}{π \sqrt{1 - z^2}},  \ \ \ \ \ \ −2π&lt;\cos^{−1}(z)&lt;2π
\end{align}</p>

<p>I have now two problems:</p>

<ol>
<li><p>it's not verified the property of normalisation of $f_Z(z)$;</p></li>
<li><p>if I calculate the cumulative distribution function by integrating the pdf from $-1$ to $\infty$
$$
    F_Z(z)=\frac{\cos^{−1}(z)(\cos^{−1}(z)−4π)}{2π^2}+3/2
$$
and compare this CDF with the ECDF estimated with Matlab, the <a href=""http://i.stack.imgur.com/CQIWD.jpg"" rel=""nofollow"">comparison</a> is not good, it's like something's missing.</p></li>
</ol>

<p>Somebody can tell me where's the mistake?</p>

<p>Thanks for your help, </p>

<p>Stephen</p>
",<probability>
"<p>Recall that a forward contract on $S_T$ contracted at time $t$, with time of delivery $T$, and with forward price $f(t; T, S_T)$ can be seen as a contingent T-claim $X$ with payoff:
$$
X = S_T - f(t; T, S_T)
$$
The forward price is determined at time t in such a way that the price of X is zero at time t, i.e. $\pi(t;X) = 0$.</p>

<p>How can I compute the forward price $f(t; T, S_T )$ in the Black-Scholes model?</p>
",<probability>
"<p>Assume that $X,X_1,X_2,...$ are iid with characteristic function $\phi(t)=\mathbb E[e^{itx}]$, and let $S_n = X_1 + X_2 + X_3 + ...$.</p>

<p>(a) For a random variable $X$, $X$ and $-X$ have the same distribution iff $\phi$ is real.</p>

<p>Comment: I can show this by the inversion formula. I just plug in the formula; however, is there a simple way to see this rather than writing all the steps out?</p>

<p>(b) Express the characteristic function of the sample average, $\phi_{\frac{S_n}{n}}(t)$, in terms of $\phi$.</p>

<p>Comment: This one is easy. $\phi_{\frac{S_n}{n}}(t) = [\phi(\frac{t}{n})]^n$.</p>

<p>(c) Assume $\phi'(0)=0$. Show that $\frac{S_n}{n}$ converges to zero in probability.</p>

<p>I have no idea about this one.</p>

<p>For (d) and (e), we assume $X$ has density: $f(x)=c\frac{1}{x^2 ln|x|} \chi_{\{|x|&gt;4\}}$, where $c$ is the appropriate normalizing constant.</p>

<p>(d) Show $\mathbb E{|X|}=\infty$</p>

<p>Commnet: Just plug in the definition formula of the expectation.</p>

<p>(e) Show that the ch.f for $X$ has $\phi'(0)=0$.</p>

<p>Like (c), I don't know how to show this one either.</p>

<p>Thanks.</p>
",<probability>
"<p>If I have a $6,7$ spades in my hand, and the flop shows $4$ clubs, $5$ spades and $Q$ spades. What is the probability I will hit my straight or flush with $2$ cards still to come?  I see $15$ cards that help me with $32$ that don't help me. So about $32\%$ with one card. But with $2$ yet to come?</p>
",<probability>
"<p>I'm given a circle with point $A$ defined by $(x,y)$. Then $T=1-d[O,A]$, so $T=1-\sqrt{(x^2+y^2)}$.</p>

<p>Asked to find:</p>

<ol>
<li>$P[T&lt;=u]$</li>
<li>$E[T]$</li>
<li>$Var(T)$</li>
</ol>

<hr>

<p>Alright, so $d[O,A]$ has the CDF $u^2$. So, for the first piece, $P[T&lt;=u]=1-u^2$.</p>

<p>However, our professor has given us also the hint that $u^2$ is beta-distributed. For $Y$ that follows a beta distribution,</p>

<p>$$E[Y]=\frac{a}{a+b}$$
  $$Var(Y)=\frac{ab}{(a+b)^2(a+b+1)}$$</p>

<p>So I kind of know what the answers are supposed to be, aside from the fact that I don't see how $u^2$ follows a beta distribution, so I don't know what the values $a$ and $b$ would be. </p>

<p>Ultimately, I think the generalized answers are:</p>

<ol start=""2"">
<li>$$E[T]=E[1-Y]=E[1]-E[Y]=1-\frac{a}{a+b}$$</li>
<li><p>$$Var(T)=Var(1-Y)=Var(T)=\frac{ab}{(a+b)^2(a+b+a)}$$</p>

<p>Help uncovering the beta distribution parameters would be greatly appreciated. Thank you!</p></li>
</ol>
",<probability>
"<p>I am developing an app that will create a chart and show trends based on a couple factors, but cannot find the <em>correct</em> way of calculating deviations over a period of time where multiple different scenarios are taken into account and averaged. <strong>Here is an example scenario to help:</strong></p>

<p>Lets say we have two people playing games in a casino. Bob and Tom.</p>

<p>Bob plays <code>3000 hands</code> of Mississippi Stud at an average bet of <code>$25/hand</code>.
Mississippi stud has a <code>variance of 121</code>. House edge of <code>5% (-0.05 EV)</code>.
Here is what Bob's end of session results would look like:</p>

<pre><code>Expected Return (ER): -$3,750 (EV * Avg Bet * Hands)
Most Expected (2 Devs): +$26,374.74 (ER + (sqrt(Hands)*((Avg Bet * sqrt(Variance))*2)
Least Expected (2 Devs): -$33,874.70 (ER - (sqrt(Hands)*((Avg Bet * sqrt(Variance))*2)
</code></pre>

<p>Tom plays <code>5 hands</code> of Blackjack at an average bet of <code>$200/hand</code>. Blackjack has a variance of <code>1.44</code>. House edge of <code>0.5% (-0.005 EV)</code>. Here is what Tom's end of session results would look like:</p>

<pre><code>Expected Return (ER): -$26
Most Expected: +$506
Least Expected: -$454
</code></pre>

<p>Calculating these things is very easy. The problem comes when I try to get the average. If I were to combine both into one line chart, and try to find the Most/Least Expected after BOTH players had finished, I get a very unrealistic number. Since Tom only played 5 hands, his play should barely affect Bob's play and possible variance. After both I get:</p>

<pre><code>ER: -$3,776
Most Expected: $13,440.37
Least Expected: -$17,164.40
</code></pre>

<p>This is definitely incorrect because you cannot simply average them and must take into account the weight of duration. Any help would be greatly appreciated.</p>
",<probability>
"<p>Total number of trials = N. The trials are independent. Probability of success = p. Probability of failure = 1-p. What would be probability of getting m or more consecutive successes?</p>

<p>Is there some online/downloadable efficient software where I can input N, m, p and it gives me the answer? Can scientific calculators can do this job?</p>
",<probability>
"<p>I have the following problem where I have difficulties grasping the intuition: </p>

<blockquote>
  <p>Lets say we have three boxes, with two of them empty and one
  containing a gold price. Lets say we randomly select one of the boxes.
  After our selection, we are given which one of the remaining two boxes <strong>does not</strong> contain the price. Now the question is: Should I
  stick with my original selection or select another box from the two
  possible alternatives left. What are the probabilities?</p>
</blockquote>

<p>I empirically tried this problem by making a computer program to repeat this experiment 1,000,000 times with first staying with the original choice and then always changing the selection. I got the probabilities to be: </p>

<p>$$P(golden\; price\;with\;original\;selection)\approx33\%$$
$$P(golden\; price\;with\;changing\;selection)\approx 66\%$$</p>

<p>Intuitively the probabilities seem at first to be 50% for both of these choices, but it seems it's not the case. I can't grasp on why?...</p>

<p>P.S. please let me know if my question is unclear</p>
",<probability>
"<p>We have: $X_n \rightarrow X$ in $L^p$ and $Y_n \rightarrow Y$ in $L^q$. Moreover $p,q&gt;1$ are such that $\frac{1}{p} + \frac{1}{q} =1$. Prove that $X_nY_n \rightarrow XY$ in $L^1$. Please, can you help?</p>
",<probability>
"<blockquote>
  <p>A fair die and two unbiased coins are tossed. What are possible outcomes of each object and the probability of each outcome?</p>
</blockquote>

<p><strong>My solution:</strong></p>

<ul>
<li>Probability for a fair Die $D$: $\frac{1}{6}$</li>
<li>Probability for unbiased Coin $C_1$:  $\frac{1}{2}$</li>
<li>Probability for unbiased Coin $C_2$: $\frac{1}{2}$</li>
</ul>

<p>So we can calculate it this way:</p>

<p>$$\Pr(D) * \Pr(C_1) * \Pr(C_2) = \frac{1}{6} * \frac{1}{2} * \frac{1}{2} = \frac{1}{6 * 2 * 2} = \frac{1}{24}$$</p>

<p><em>or</em></p>

<p>$$\frac{1}{\binom{6}{1} \binom{2}{1} \binom{2}{1}} = \frac{1}{6 * 2 * 2} = \frac{1}{24}$$</p>

<p><strong>Is this correct?</strong></p>
",<probability>
"<p>I know that the pdf $X$ conditional on $Y=y$ is
$$f_{X|Y}(x|y)=\frac{f_{(X,Y)}(x,y)}{f_Y(y)},$$
and this can be used to calculate conditional probabilities such as $P(X&gt;\alpha | Y&gt;\beta)$ (for example). My question is know do we find conditional probabilities such as $P(X^2&gt;a|Y=b)$ or things of this nature, wherein there is a transformation of the random variable $X$. </p>
",<probability>
"<p>What is the probability that in a randomly chosen group of r people at least three people have the same birthday?</p>

<p>I have tried doing this question as fallows- 1-(probability of all people having different birthday+ any 2 people have same birthday+2 people in each of 2 sets have same birthdays but distinct for particular set+probability of 2 people in each of 3 sets have same birthdays but distinct for particular set+.......) </p>

<p>but i can't generalize it....</p>
",<probability>
"<blockquote>
  <p>Let $50$ balls numerated on a box. Let the event be ""to draw five balls such that the order doesn't matter and there's no replacement"".
  Let $\alpha \in\{1,2,...,50\}$. The probability of $\alpha$ being drawn is given by 
  $\frac{^{49}C_{4}}{^{50}C_{5}}$.</p>
</blockquote>

<p>But if the event is repeated $n$ times, how can I compute the probability of the $\alpha$ number to be drawn again, $p$ times $\left( p \leq n \right)$?</p>

<p>I thought about the binomial distribution where the probability of success is $\frac{^{49}C_{4}}{^{50}C_{5}} \approx 0,1$.</p>
",<probability>
"<p>Let $X_t,t\geq 0$ be a Poisson process with rate parameter $\lambda$. Compute the Karhunen-Loève expansion of $X$ in interval $[0, T]$. How about the KL expansion of the centered process $X_t−\lambda t$?</p>

<p>The auto-correlation function of Poisson process is $R(s,t)=\lambda^2st+\lambda \min(s,t)$. By definition, KL expansion should satisfy $\int^T_0 R(s,t)\phi_n(t)dt=\lambda_n \phi_n(s)$.</p>

<p>I've problems figuring out how to solve the integrated equation.</p>

<p>For Wiener process, <a href=""http://mathoverflow.net/questions/59337/karhunenloeve-approximation-of-brownian-motion-and-diffusions"" rel=""nofollow"">this link</a> and Wikipedia article on KL expansion was useful.</p>

<p>This is a mirror question of <a href=""http://mathoverflow.net/questions/95941/karhunen-loeve-expansion-of-poisson-process"" rel=""nofollow"">this MO question</a>.</p>
",<probability>
"<p>A fair die is thrown until a score of less than 5 is obtained. How to find the probability of less than 3 in the last throw?</p>

<p>I am not too sure how to approach this one, any ideas?</p>
",<probability>
"<p>On <a href=""http://en.wikipedia.org/wiki/Benford%27s_law"" rel=""nofollow"">wikipedia</a> i have find this statement:</p>

<blockquote>
  <p>...it is scale invariant, and the only continuous distribution that fits this (scale invariance) is one whose logarithm is uniformly distributed.</p>
</blockquote>

<p>how can be proven?</p>
",<probability>
"<p>I am trying to show the following: </p>

<p>\begin{equation*}
E[e^{-\gamma W}]=e^{-\gamma(E[W]-\frac{\gamma}{2}Var [W])}
\end{equation*}</p>

<p>but I really can't remember what I am supposed to do to get from the LHS to the RHS. I have tried using integration this way</p>

<p>\begin{equation*}
\int We^{-\gamma W}dW
\end{equation*}</p>

<p>and then use integration by parts, but even though what I get resembles it, it can't be correct (because $e^{-\gamma W}$ is not the distribution of W).</p>

<p>I have also tried using Taylor series expansion, but I think I am way off, and I don't think an approximation here is what I need, because the equality above is exact.</p>

<p>FYI, this is not homework, I am working through a <a href=""http://www.princeton.edu/~markus/research/papers/liquidity.pdf"" rel=""nofollow"">paper</a> (page 10) and I would really like to know how every step was derived.</p>

<p>Can anyone at least point me to the right direction?</p>

<p><strong>EDIT</strong>: This expectation on the RHS is very similar to the moment generating function formula (with a negative exponent). If you check <a href=""http://en.wikipedia.org/wiki/Moment-generating_function#Examples"" rel=""nofollow"">here</a>, you will see that the moment generating function for the normal distribution is like the LHS (but with a positive sign). So in a way I have my answer, but I still would like to know how to derive it, if there is a way. I know little if anything at all about moment generating functions, so maybe I shouldn't try and derive it but rather just use the result? Does it even make sense to try and derive it?</p>
",<probability>
"<p>When I have watched Deal or No Deal (I try not to make a habit of it) I always do little sums in my head to work out if the banker is offering a good deal. Where odds drop below ""evens"" it's easy to see it's a bad deal, but what would be the correct mathematical way to decide if you're getting a good deal?</p>
",<probability>
"<p>Let $P$ be a probability function. It satisfied <a href=""http://en.wikipedia.org/wiki/Probability_axioms"" rel=""nofollow"">probability axioms</a>. Can we deduce from it that if $P(A)=0$ then $A=\emptyset $ ?</p>
",<probability>
"<p>I'm failing to understand how to come to the answer to this question.</p>

<p>If you roll a fair die six times, what is the probability that the numbers recorded are $1$, $2$, $3$, $4$, $5$, and $6$ in any order?</p>

<p>The answer given is $6!(1/6)^6 = 3/324$</p>

<p>Can anyone explain to me how to get to that answer? I would really appreciate the help! :)</p>
",<probability>
"<p>There is a classic problem:</p>

<blockquote>
  <p>Suppose that $X_1,\ldots,X_n$ form an i.i.d. sample from a distribution with the following pdf:</p>
  
  <p>$$f(x\mid\theta) = 
\begin{cases}
e^{\theta-x}\quad&amp;\text{for }\, x&gt; \theta \\
0 &amp;\text{otherwise}.
\end{cases}$$</p>
</blockquote>

<p>I would like to show that the MLE of $\theta$ does not exist. </p>

<p>The argument I have is that the likelihood function will be a maximum when $\theta$ is made as large as possible subject to the strict inequality $\theta &lt; \min\{X_1, \ldots, X_n\}$. Therefore, the value $\theta = \min\{X_1,\ldots , X_n\}$ cannot be used and there is no MLE.</p>

<p>However, I do not understand WHY we want $\theta$ to the equal to the maximum of the values. </p>

<p>Also, is there a way to show mathematically why this MLE doesn't exist?</p>

<p>I get that the log-likelihood function is:</p>

<p>$$L(\theta) = n\theta - (X_1+\ldots+X_n)$$</p>

<p>but when you differentiate via $\theta$ and set to $0$, we get:</p>

<p>$n=0$. How does the fact $n=0$ fit into the fact the MLE doesn't exist for $\theta$? Thanks!</p>
",<probability>
"<p>Let $X$ be random variable such that $\begin{align} F_X(x) = 1- e^{-x} \end{align}$ if $x \ge 0$ and $F_X(x)=0$ in other case. Find distribution function $Y= \min(1,X)$, $Z=\max(1,X)$. </p>

<p>If I have to find $\max(X,Y)$ or $\min(X,Y)$ ($X,Y$ - random variable) I don't have any problem. But in this case I have number - what should I do?</p>
",<probability>
"<p>A wheel of fortune is divided into 40 sectors, numbered from 1 to 40. Tickets are sold representing each sector. Tickets are \$1 each. All 40 tickets must be sold before the wheel can be spun. Only the winning ticket receives a \$10 prize.  Calculate the probability of winning the \$10 prize in one game and again in the next game.</p>
",<probability>
"<p>Given $\bar{X} \sim N(\bar{\mu}, \sigma)$ is a vector of independent continuous random variables (with identical variance) and $Y_j = ( \bar w_{j} \cdot \bar X + b_j &gt; 0)$ is a set of dependent discrete random variables is there a nice expression for the joint probability $P(Y_0 = 1, Y_1 = 1)$?</p>

<p>Also, I think the marginal distributions are given by: </p>

<p>$P(Y_j=1) = \int^{\infty}_{0} N(\bar w_j \cdot \bar \mu + b_j, \sigma) dx = 0.5 - 0.5 erf \left ( -{\bar w_j \cdot \bar \mu + b_j \over \sigma \sqrt {2}} \right )$</p>

<p>Is this correct?</p>
",<probability>
"<p>I have a midterm I am studying for and I don't have the solutions to this homework problem. Can anyone please explain how to do it? I would really appreciate it.
Here is the problem:</p>

<p><a href=""http://i.stack.imgur.com/oTIYm.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/oTIYm.png"" alt=""two tables of MATHEMATICS letters""></a></p>

<p>I googled the answer for this question, but I did not understand the solution. Can anyone please explain or give their own interpretation of the answer? Thanks!!</p>

<p><a href=""http://users.wpi.edu/~hservat/cs2022d12finalsolutions.pdf"" rel=""nofollow"">http://users.wpi.edu/~hservat/cs2022d12finalsolutions.pdf</a> </p>

<p>Page 2 has the solution I am confused about.</p>
",<probability>
"<p>Suppose you pick a number between $1$ and $30$ uniformly at random. Let $A$ be the event that
the number is even. Let $B$ be the event that the number is divisible by $3$. Let $C$ be the event that the number
is divisible by $5$. Using the above formula, what is the probability that the number is divisible be at least one
of the values $2, 3,$ or $5$?</p>

<p>$$|A|=15$$
 $$ |B| = 10$$
 $$ |C| = 6$$</p>

<p>From what I have worked out</p>

<p>$$P(A) = \frac{1}{2}$$
$$P(B) = \frac{1}{3}$$
$$P(C) = \frac{1}{5}$$
$$P(A \cap B) = \frac{1}{6}$$
$$P(A \cap C) = \frac{1}{10}$$
$$P(B \cap C) = \frac{1}{15}$$</p>

<p>I am using the formula of inclusion=exclusion</p>

<p>$$ P(A ∪ B ∪ C) = P(A) + P(B) + P(C) − P(A ∩ B) − P(A ∩ C) − P(B ∩ C) + P(A ∩ B ∩ C)$$</p>

<p>When I add it all up I end up with $P(A ∪ B ∪ C) = 1.03$ </p>

<p>How is that possible? Logically, there are 8 numbers that would not divide by $2,3,5$ Thus, if you take the opposite, there should be $\frac{22}{30}$ probability that you would get a number divisible by $2,3,5$. So what am I doing wrong?!</p>
",<probability>
"<p>Let $X\to Y\to Z$ be three random variables.</p>

<p>The data processing inequality states $I(X;Y)\geq I(X;Z)$.</p>

<p>Further assume $Y=f(X)$ where $f:\mathcal{X}\to\mathcal{Y}$ is an arbitrary function.</p>

<p>What more can we say about how $I(Y;Z)=I(f(X);Z)$ relates to $I(X;Y)=I(X;f(X))$ and $I(X;Z)$?</p>

<p>I.e. can one somehow add the mutual informations along the path or obtain an inequality relating the three pairwise mutual informations? Somehow the choice of $f$ establishes an upper bound on what information can potentially be shared between $X$ and $Z$, but how does it affect the mutual information $I(Y;Z)=I(f(X);Z)$?</p>
",<probability>
"<p>There are four Envelopes with letters. Two are chosen Randomly and opened and found that they are wrongly addressed. Find the Probability that there are exactly two wrongly addressed envelopes.</p>

<p>My Try: Let the Envelopes be $E_1$,$E_2$,$E_3$ and $E_4$ and Corresponding Letters be $L_1$,$L_2$,$L_3$ and $L_4$ Since two opened are found wrongly addressed,implies there are minimum of two wrongly addressed envelopes.So Favorable cases are :</p>

<p>$1$.There are Exactly two wrongly addressed Envelopes i.e.,Remaining two are correctly addressed and this can happen in $\binom{4}{2}=6$ ways</p>

<p>$2$.There are exactly three wrongly addressed envelopes i.e., remaining one has correctly addressed and this an happen in $\binom{4}{3}\left(3!-1\right)=20 $ways</p>

<p>$3.$There are exactly four wrongly addressed envelopes and this an happen in $\left(4!-1\right)=23 $ways, so Required Probability is</p>

<p>$\frac{6}{6+20+23}=\frac{6}{49}$. I am not sure whether i have done in a right way, please help me if i am wrong.</p>
",<probability>
"<p>Boxes 1 and 2 contain 4 white, 3 red and 3 blue balls; and 5 white, 4 red and 3 blue balls respectively. If one ball is drawn at random from each box, what is the probability that both the balls are of the same colour?</p>
",<probability>
"<p>Say we have custom-marked 6-sided die: 1-2-3 is marked as a toad, 4-5 - as a bird, 6 - as a monkey.</p>

<p>So, what is probability of rolling both toad and monkey rolling 4 dice?</p>

<p>I'm totally confused with this example.</p>
",<probability>
"<p>I'm studying Markov's chain and a very important condition is the detailed balance condition.I'm a bit curious and my question is:</p>

<p>Why is detailed balance condition called like that?</p>

<p>Anyone explains this name in any book.</p>
",<probability>
"<p>Assume we have a server, the time between two packets received by our server is defined as exponential distributed with intensity   $\lambda=2 \text{ packets}/10 \text{ mins}$</p>

<p>Now calculate the probability that the time from $X$(random time) until the next packet-received, is $5$ minutes or longer?</p>

<p>We that we can model waiting time as exponential function, therefore we get </p>

<p>$f(t)= (1/10)*e^{-t/10}$</p>

<p>Now we need to find out the probability that the time from $X$(random time) until the next packet-received, is 5 minutes or longer?</p>

<p>We integrate $f(t)$ and get $[e^{-t/10}]$ but i am not sure how to choose my limits ?</p>

<p>Am I thinking right here?</p>
",<probability>
"<blockquote>
  <p>Two envelopes are given. Envelope 1 contains $x$ dollars and envelope 2 contains $2x$ dollars. We opened one of them and found in it $100$$. Now we have the option  to change envelopes or not.</p>
  
  <ol>
  <li><p>Formulate the problem as a Baisian estimation problem.</p></li>
  <li><p>Is it worthwhile to exchange the envelopes? </p></li>
  </ol>
</blockquote>

<p>This is the original question. In order to simplify it, I added the condition that the envelopes can contain only $5{$},10{$},20{$}$ and $100{$}$ bills. </p>

<p>As I understood, a Baisian estimation problem formulation contains the following terms:</p>

<p>$\Omega$-the set of possible states, $X$-the set of observations,$P$-a probablistic model, $A$-possible actions, $\Gamma$-cost for any action.</p>

<p>I also understood that there is a risk function: $R(\alpha_k|x_j)=\Sigma_{\omega_i\in \Omega}\lambda(\alpha_k|\omega_i)P(\omega_i|x_j)$, a probability to be in a state $\omega_i$: $P(\omega_i|x_j)=\frac{P(x_j|\omega_i)}{P(x_j)}P_0(\omega_i)$, and a total risk function: $R[\alpha(x)]=\Sigma_jR(\alpha(x_j)|x_j)P(x_j)$.</p>

<p>But, I am not sure how to build this risk function for the above exercise.
It seems like the set of optional actions is $a_0$-not exchanging envelopes,$a_1$-exchanging envelopes.
What are the possible states set $\Omega$ for example? What is a possible state in this example? If I choose to exchange envelopes and there are $5+5+10$ bills in envelope 1 and $5+5+10+20$ in envelope 2, is this considred one possible state? Also, what is the cost of being wrong?</p>

<p>Any ideas?
Thanks!</p>
",<probability>
"<p>My question is based on the beginning of Chapter 8.3.2 in the book ""Modelling Extremal Events"" by Embrechts,Klüppelberg and Mikosch.
We consider a Cramer-Lundberg-Model and assume that the conditions of the Cramer-Lundberg-Theorem are satiesfied.</p>

<p>More specifically:
Let $X$ be a positive random variable with distribution function $F$ and mean $E[X] = \mu$, $Y$ a random variable, which follows an $Exp(\lambda)$-distribution independent of $X$, $c &gt; 0$ a constant.
We set $Z:=X-cY$.</p>

<p>We assume that the Lundberg-Exponent exists, i.e. a  $\nu &gt; 0$, that satisfies 
$
\int_{0}^{\infty} xe^{\nu x}(1-F(x))dx = \frac{c}{\lambda}
$
Further assume that $X$ has a moment generating function, which is finite in some neighbourhood of $0$.</p>

<p>Now consider $\kappa(s):= E\left[e^{sZ}\right]$. My goal is to show that $\kappa(\nu) = 1$, which is stated in the book without further explaination.</p>

<p>I get that:
$
\kappa(\nu) = E\left[e^{\nu Z}\right] = E\left[e^{\nu X}e^{-\nu cY}\right] = E\left[e^{\nu X}\right] E\left[e^{-\nu cY}\right] = E\left[e^{\nu X}\right] \frac{\lambda}{\lambda + \nu c}
$
but I am stuck at this point and don't see how to use the equation above to show that this is equal to $1$.</p>

<p>Any help is appreciated :)</p>
",<probability>
"<p>I am trying to prove theorem 7.3.23 in Casella and Burger. </p>

<p>Theorem: 
Let T be a complete sufficient statistic for a parameter $\theta$, and let $\phi(T)$ be any estimator based only on T. Then $\phi(T)$ is the best unbiased estimator of its expected value. </p>

<p>Here is my attempt to prove this:</p>

<p>To show that $\phi(T)$ is best unbiased estimator for E[$\phi(T)$], I must show that $\phi(T)$ is uncorrelated with any unbiased estimator of $0$. So I must show that $Cov({\phi(T)}, W)=0$ for any W such that W is an unbiased estimator of $0$. For any W such that E[W]=0, $E[E[W|T]]=0$=>$E[W|T]=0$ by completeness of T. So $E[W|T]=0=E[W]$. So W is independent of T. So W and $\phi(T)$ are uncorrelated? So $\phi(T)$ is best unbiased for  its expected value. Is this correct?</p>
",<probability>
"<p>I have a Markov chain as follows:</p>

<ul>
<li>$G+1$ finite states, it begins from $s=G$ and completes at $s=0$</li>
<li>A transition ($s\to s-1$) occurs in case if event $A$ happens. No other form of transition is possible. Denote the transition probabilities by $P_{ij}^{A}$</li>
</ul>

<p>We want to improve this system to complete faster. So I devised a set of operations denoted by $B$. If $B$ is successful with probability $\beta$, it is just like $A$ is repeated $k$ times, where $k$ is a random variable with known probability. Otherwise, another [real] $A$ should happen to change the state (with probability 1-$\alpha$). Any advise on how to model the Markov chain of the improved system using on $P_{ij}^A$ is appreciated.</p>
",<probability>
"<p>I tried to look for a similar question but didn't find the exact thing (the closest I found was <a href=""http://math.stackexchange.com/questions/102673/what-is-the-expected-number-of-trials-until-x-successes"">this</a> but AFAIK it is not the same)</p>

<p>I have $r$ coins that land heads w.p. $p$. I want at least $m$ heads. On the first step I toss all of $r$ coins and from the second step and on I toss only the coins that didn't land heads already.</p>

<p>What is the expected number of steps until I have at least $m$ heads?</p>

<p>Thank you in advance!</p>
",<probability>
"<p>A and B play a game of chess. They play 20 games of which A wins 12 and B wins 4.The remaining 4 games are drawn.If 3 games are played between them, find the probability that 
i)B wins at least 1 game?ii) the probability that 2 games are drawn?</p>
",<probability>
"<p>I have a problem about intuition:
substracting the mean of iid RVs seems to increase the mutual information.</p>

<p>Say $X,Y$ are real iid RVs, then $\frac{X-Y}{2}$ and $\frac{Y-X}{2}$ are not independent because one is just the negative of the other?</p>

<p>If this is right, it seems contraintuitive to me, as subtracting the mean is such a usual preprocessing step for iid samples.
What am I missing or where did I go completely wrong? It's probably rather stupid but my intuition fails or tricks me here. Guess I am mixing up things. Sorry.</p>
",<probability>
"<p>A box contains $3$ coins . Among these three , each of  two coins have the probability of giving head $\dfrac 23$ and the remaining one have the probability of turning head $\dfrac 12$ . One coin is chosen randomly from the box and tossed three times and each time it turns out to be head . What is the probability that the coin chosen from the box was the unbiased one i.e. the one with head probability $\dfrac 12$ ?   </p>
",<probability>
"<blockquote>
  <p>Let $p,q \in (0,1)$. Let $Y$ be the R.V denotes the number of days of the storm in the ocean. $Y\sim \text{Bin}(n,p)$. Let $X$ be the number of ships drowned during the storm and we know that the number of ships drowned in the $k$-day is $\text{Bin}(k,q)$. Find $E(x)$.</p>
</blockquote>

<p>Now, I look at the solution which starts like this:</p>

<p>$$E(X) = E(E(X|Y)) = E(q+2q+\ldots + Yq) = \ldots $$</p>

<p>Why is it true?  </p>

<p>I thought about splitting $X$ to $\sum_{k=1}^Y X_k$ where $X_k$ is the number of ships drowned in the $k$ day. Hence,</p>

<p>$$E(X|Y) = E(\sum_{n=1}^Y X_k| Y) = \sum_{n=1}^Y E(X_k |Y) = \sum_{n=1}^Y E(X_k) = Y\cdot kq$$</p>

<p>But it's not the right answer, apparently.</p>
",<probability>
"<p>I looking for confirmation, or not, that I am on the correct track with the following proof.</p>

<p>Show that if $P(A\mid B) = P(A\mid B^c), 0 &lt; P(B) &lt; 1$, then $A$ and $B$ are independent? </p>

<p><strong>Attempt</strong></p>

<p>By way of contradiction assume that $A$ and $B$ are not independent. Then,
\begin{align*}
P(A\mid B)=P(A\mid B^c)&amp;\Rightarrow\dfrac{P(B\cap A)}{P(A)}=\frac{P(B^c\cap A)}{P(A)}\quad\text{by definition}\\
&amp;\Rightarrow P(B\cap A)= P(B^{c}\cap A)
\end{align*}
But this contradicts $0 &lt; P(B) &lt; 1$. Thus, $A$ and $B$ must be independent.</p>

<p>Thank you in advance for any helpful feedback. Cheers.</p>
",<probability>
"<p>A container contains 4 Red marbles and 2 Green Marbles. I pull out each marble one at a time, without putting an marbles back in. X is a random variable that is the number of red marbles before pulling out a Green. Y is a random variable that is the number of green marbles that I draw out in the first three tries.</p>

<p>What is the Range of X and Y?</p>

<p>Calculate the probability mass function of P(x) for X? (Make a table of the  probability distribution for X)</p>

<p>Is {X ≤ 1} or {Y ≤ 1} likely?</p>

<hr>

<p>I figured the range of X is {0, 1, 2, 3, 4} since you cant have more than 4 marbles coming before Green and for Y is {0, 1, 2} since you're only limited to two green marbles. </p>

<p>For the second question would P(X = 0) = Pr(GGRRRR) since no Reds come before Green and P(X = 1) = Pr(RGGRRRR), P(X = 2) = Pr(RRGGRRR) and so on. I'm not sure how to find these values, since these events aren't independent I would have to make a tree diagram with 6 sets of ""generations"" until I have an outcome space of 6 marbles. </p>
",<probability>
"<p>Is there a difference between:</p>

<p>$$p(y|x,z) = \frac{p(y,x|z)}{p(x|z)}$$
and
$$p(y|x,z) = \frac{p(y,x,z)}{p(x,z)}$$</p>

<p>I was working on a problem that asks one to prove $p(x, y|z) = p(x|z)p(y|x, z)$ and the second one just came to my mind.</p>
",<probability>
"<p>You flip a coin with the goal of maximizing the ratio of heads to total flips; you can decide to stop whenever. What is the expected value of this ratio?</p>

<p>My thoughts: The ratio is at least $3/4$. With $1/2$ probability we flip heads first and stop.</p>

<p>Otherwise, we flip until the ratio is $1/2$ (random walks return to the origin with probability $1$) giving an expected ratio of $3/4$. Can we do better?</p>
",<probability>
"<p>Suppose you want to show $sup_{x\in D}|f_n(x)|\to_p 0$, for $n\to \infty$, where $D\subset \mathbb R$ is a compact interval, $f$ is continuous depending on one or more random variables, and $\to_p$ means convergence in probability. For example, $f_n(x)=\sum_{i=1}^n(X_i-x)$ (this, however, is not the problem).</p>

<p>Because showing statements as the one above directly is rather difficult I was wondering if it is sufficient to show $sup_{x\in D}| Ef_n(x)|\to 0$ and $sup_{x\in D}| Var(f_n(x))|\to 0$. Where $E$ and $Var$ are the expectation and variance operator, respectively. If some of you know a good read on this I appreciate your suggestions. Thanks in advance. Cheers.</p>
",<probability>
"<p>I am wondering if there is a closed form for finding the expected value or variance for a conditional exponential distribution.</p>

<p>For example:
$$ E(X|x &gt; a) $$ where X is exponential with mean $\lambda$.</p>

<p>Same question for variance.</p>

<p>What about for a joint distribution of independent exponentials?</p>

<p>$$ E(X|y &gt; a) $$ where X is exponential with mean $\lambda$, Y is exponential with mean $\theta$ and X &amp; Y are independent.</p>

<p>A sample problem for the actuarial P/1 exam (#124 for those also studying) asks:</p>

<blockquote>
  <p>The joint probability for $f(x,y) = 2e^{-x-2y}, ~ x &gt; 0, ~ y &gt; 0$. Calculate the variance of Y given $x &gt; 3, ~ y &gt; 3$.</p>
</blockquote>

<p>The solution goes like this: (Math on the right, reasoning on the left)</p>

<blockquote>
  <ol>
  <li>$Var (Y|x&gt;3, y&gt;3) =$</li>
  <li>$Var (Y|x&gt;3) = ~~~~~$Independence</li>
  <li>$Var (Y + 3) = ~~~~~$Memoryless</li>
  <li>$Var (Y) + Var (3) =~~~~~$Independence of Y and 3.</li>
  <li>$Var (Y) = ~~~~~ $ Since $Var (3) = 0$.</li>
  <li>$0.25 ~~~~~ $Exponential Variance, $\lambda = 2$.</li>
  </ol>
</blockquote>

<p><strong>So this says to me that  $Var (Y|x&gt;3) = Var (Y)$.</strong>  Is that true?  If so, is it always true?  If not, then how does this solution work?</p>

<p>Could one also replace E(Y) for Steps 1 - 4, Use $E(a) = a$ and get $E(Y| y&gt;a) = E(y) + a$?</p>

<p>Shortcuts like this are immensely valuable for a timed test.  (Not just faster, but less error prone).</p>
",<probability>
"<blockquote>
  <p>A fair coin is tossed:</p>
  
  <ul>
  <li>If <em>heads</em>: an unbiased die is thrown <strong>three</strong> times. The sum of the outcomes of the three rolls is recorded.</li>
  <li>If <em>tails</em>: an unbiased dice is thrown <strong>once</strong>. The result is recorded.</li>
  </ul>
  
  <p>What are the possible outcomes of each action and the probability of
  each outcome?</p>
</blockquote>

<p><strong>My solution:</strong></p>

<ul>
<li><p>if <em>heads</em>:</p>

<ul>
<li>Dice 1: $\Pr(\frac{1}{6})$</li>
<li>Dice 2: $\Pr(\frac{1}{6})$  </li>
<li>Dice 3: $\Pr(\frac{1}{6})$</li>
</ul>

<p>This outcome will yield  $\Pr(\frac{1}{6}) * \Pr(\frac{1}{6}) * \Pr(\frac{1}{6}) = \frac{1}{36}$</p></li>
<li><p>if <em>tails</em>:</p>

<ul>
<li>Dice: $\Pr(\frac{1}{6})$</li>
</ul>

<p>This outcome will yield  $\Pr(\frac{1}{6}) = \frac{1}{6}$</p></li>
</ul>

<p><em>Am I on the right track?</em></p>
",<probability>
"<p>If 20 persons were invited for a party, in how many ways will two particular persons be seated on either side of the host in a circular arrangement?</p>

<p>According to me the answer should be $17!.2!$. But the given answer is $18!.2!$.</p>

<p>If we consider the guest and the host as one unit and let them take the first three chairs the other 17 can be occupied by 17! ways and the two particular persons can then rearrange them by 2! ways.</p>

<p>What am i doing wrong?</p>
",<probability>
"<blockquote>
  <p>A fair die is thrown three times:  </p>
  
  <ul>
  <li>What is the probability of getting: three sixes?  </li>
  <li>What is the probability of getting: six, one, six?</li>
  </ul>
</blockquote>

<p><strong>My solution:</strong></p>

<p>Probability of getting three sixes:</p>

<p>$$\Pr(\text{1st dice six}) + \Pr(\text{2nd six}) + \Pr(\text{3rd six})$$
$$ = \frac{1}{6} \frac{5}{6} \frac{5}{6} + \frac{5}{6} \frac{1}{6} \frac{5}{6} + \frac{5}{6} \frac{5}{6} \frac{1}{6} = 3 \cdot \frac{25}{216} = \frac{75}{216} = \frac{25}{72}$$</p>

<p>Probability of getting six, one, six:</p>

<p>$$\Pr(\text{1st dice six}) + \Pr(\text{2nd one}) + \Pr(\text{3rd six})$$
$$ = \frac{1}{6} \frac{5}{6} \frac{5}{6} + \frac{5}{6} \frac{1}{6} \frac{5}{6} + \frac{5}{6} \frac{5}{6} \frac{1}{6} = 3 \cdot \frac{25}{216} = \frac{75}{216} = \frac{25}{72}$$</p>

<p><em>Am I on the right track?</em></p>

<hr>

<h2><em>Update:</em></h2>

<p>The problem am trying to solve does not specify order. Can we assume that order matters?</p>

<p>How about the following question, would the result be different from the initial question?</p>

<blockquote>
  <p>A fair die is thrown three times:  </p>
  
  <ul>
  <li>What is the probability of getting: three sixes, where the first throw MUST be six?</li>
  <li>What is the probability of getting: six, one, six, where the first throw MUST also be six?</li>
  </ul>
</blockquote>
",<probability>
"<p>I know this question is asked over and over, but I still can't understand anything.</p>

<p>Say I'm introduced to a random father of two and I want to know what's the probability that both his children are boys. Currently:</p>

<ul>
<li><strong>BB</strong> BG GB GG ⇢ 1/4</li>
</ul>

<p>Where the first letter represents the younger sibling and the second letter represents the older sibling. So far so good.</p>

<p>(1) Now the father tells me that his youngest child is boy:</p>

<ul>
<li><strong>BB</strong> BG <strike><em>GB GG</em></strike> ⇢ 1/2</li>
</ul>

<p>(2) If, instead, he told me that at least one of his children is a boy:</p>

<ul>
<li><strong>BB</strong> BG GB <strike><em>GG</em></strike> ⇢ 1/3</li>
</ul>

<p>Makes sense, kind of.</p>

<p>(3) But if the father brought one of his children with him without telling whether he's the younger child or the older child and that child happened to be a boy, I think I could have still honestly arrived to the 50/50 probability:</p>

<ul>
<li><strong>BB</strong> BG <strike><em>GB GG</em></strike> ⇢ 1/2</li>
</ul>

<p>Where the first letter represents the boy I've just seen and the second letter represents his sibling.</p>

<p>Now, say, the father first told me that he has at least 1 boy. That's the case (2).</p>

<p>Then the father called (one of) the boy(s) here, and somehow the situation turned into the case (3)!</p>

<p>What exactly has changed? What kind of new information did I just get? OK, I've seen (one of) the boy(s), but the only thing it tells me is that one of the children is a boy, which I already knew from the father's own words.</p>

<p>It seems to me that anything he could bring that has some kind of relationship to (one of) the boy(s) so as to allow me to uniquely identify him would work: a photo, a footprint on a beach, etc. Even if he simply told me that he has just thought about one of his children who is a boy, I think I could still have done this:</p>

<ul>
<li><strong>BB</strong> BG <strike><em>GB GG</em></strike> ⇢ 1/2</li>
</ul>

<p>Where the first letter represents the boy the father has thought about at XX/XX/XXXX XX:XX:XX UTC, and the second letter represents his other child.</p>

<p>Is this magic? Or am I just stupid?</p>

<p>Can't I simply construct such a way of identification myself? For example, let the first letter represent the youngest boy (the only boy if there's just one), and let the other letter represent the other child. Since the father is not an abstract entity, this would uniquely identify some child.</p>

<hr>

<p>I don't see how changing the representation changes things.</p>

<p>Say I saw one of the father's on a photo behind a thick blurry glass that doesn't let me see whether it's a girl or a boy. Therefore:</p>

<ul>
<li><strong>BB</strong> BG GB GG ⇢ 1/4</li>
</ul>

<p>Where the first letter represents the child on the photo and the second letter represents the other child.</p>

<p>Now the glass is removed and I can see the photo clearly and it's indeed a boy:</p>

<ul>
<li><strong>BB</strong> BG <strike><em>GB GG</em></strike> ⇢ 1/2</li>
</ul>
",<probability>
"<p>(There are 4 districts in the land of Oz. At home, the inhabitants of
each region wear ties of a special colour, Munchkins (M) wear blue, Scarecrows
(S) wear purple, Tin Men (T) wear red and Wizards (W) wear yellow. When visiting the Emerald city however, some inhabitants wear green ties, 25% of Munchkins, 35% of Scarecrows, 45% of Tin Men and 55% of Wizards. As a visitor approaches, the gatekeeper of the Emerald city who knows the tourism rate for the last few years assigns the probabilities as follows:</p>

<p>P(M)=1/3, P(S)=1/4; P(T)=1/6; P(W)=1/4</p>

<p>(i) What is the probability that the visitor will wear a green tie?</p>

<p>(ii) Given that a visitor is wearing a green tie, calculate the
probability that the visitor is a Munchkin.</p>
",<probability>
"<p>Given a box which contains $3$ red balls and $7$ blue balls. A ball is drawn from the box and a ball of the other color is then put into the box. A second ball is drawn from the box, What is the probability that the second ball is blue? </p>

<p>could anyone provide me any hint? </p>

<p>Please, don't offer a complete sketch of the solution, a hint is enough for me as this is a homework problem. </p>
",<probability>
"<p>From medical investigations it is known that the symptoms $S_1$ and $S_2$ can appear with three different diseases $K_1, K_2, K_3$. The conditional probabilities $a_{i,j}=P(S_j|K_i), i \in \{1,2,3\}, j \in \{1,2\}$ are given by the following matrix.</p>

<p>$$ A= (a_{i,j}) = \left(
\begin{array}{cc}
 0.8 &amp; 0.3 \\
 0.2 &amp; 0.9 \\
 0.4 &amp; 0.6 \\
\end{array}
\right)$$</p>

<p>In the first part of the question I already calculated $P(S_j)$ and $P(K_i|S_j)$. For the second part of the question, I'm given the conditional probabilities $P(S_1 \cap S_2 |K_i)$ by the following vector $(0.2, 0.1, 0.3)$. Assuming that a patient shows symptoms $S_1$, but not $S_2$, what is the probability that he suffers from $K_1, K_2$ and $K_3$?</p>

<p>So I'm looking for $P(K_i|S_1 \cap S_2^C)$. I can get $P(S_1 \cap S_2^C)$ using $P(S_1 \cap S_2^C) = P(S_1) - P(S_1 \cap S_2)$, but I have problems getting the joint distribution $P(S_1 \cap S_2, K_i)$.</p>

<p>First I was trying to show that $S_1$ and $S_2$ are independent and use that to derive the joint probability, but they are not. Alternatively, is it true that the formula $P(S_1 \cap S_2^C) = P(S_1) - P(S_1 \cap S_2)$ remains true when conditioning on K, so that I have $P(S_1 \cap S_2^C | K_i) = P(S_1 | K_i) - P(S_1 \cap S_2 | K_i)$? Or can anybody help me by providing an alternative way to solve this?</p>
",<probability>
"<blockquote>
  <p>Let $(\mu_n)_{n\in\mathbb{N}}$ be a sequence of probability measures on $\mathbb{N}$, such that the Laplace transform $\phi_n(\lambda)=\int e^{-\lambda x}\mu_n(dx)$ converges pointwise to a limit $\phi(\lambda)=\int e^{-\lambda x}\mu(dx)$ for some probability measure $\mu$ and $\lambda$ in some non-empty interval $(a,b)$. Prove that $\mu_n$ converges weakly to $\mu$.</p>
</blockquote>

<p>Hint: Fix $\lambda_0\in(a,b)$ and rewrite $\phi_n$ as Laplace transform of the new probability measure $\eta_n(dx)=e^{-\lambda_0x}\frac{\mu_n(dx)}{\phi_n(\lambda_0)}$, with $\eta$ defined similarly. Show that $\eta_n$ is tight and converges weakly to $\eta$ and then deduce that $\mu_n$ converges weakly to $\mu$.</p>

<p>This is the continuity theorem with respect to Laplace transform. I have found it in ""An introduction to Probability"" by Feller. But it doesn't provide proofs clearly. Does anyone see the proof of the the continuity theorem with respect to Laplace transform? Can you recommend me the source about this?</p>
",<probability>
"<p>Okay so <a href=""http://math.stackexchange.com/questions/835/optimal-strategy-for-deal-or-no-deal"">this question</a> reminded me of one my brother asked me a while back about the hit day-time novelty-worn-off-now snoozathon <a href=""http://en.wikipedia.org/wiki/Deal_or_no_deal"">Deal or no deal</a>.</p>

<h3>For the uninitiated:</h3>

<p>In playing deal or no deal, the player is presented with one of 22 boxes (randomly selected) each containing different sums of money, he then asks in turn for each of the 21 remaining boxes to be opened, occasionally receiving an offer (from a wholly unconvincing 'banker' figure) for the mystery amount in his box.</p>

<p>If he rejects all of the offers along the way, the player is allowed to work his way through several (for some unfathomable reason, emotionally charged) box openings until there remain only two unopened boxes: one of which is his own, the other not. He is then given a choice to stick or switch (take the contents of his own box or the other), something he then agonises pointlessly over for the next 10 minutes.</p>

<h3>Monty hall</h3>

<p>[If you have not seen the monty hall 'paradox' check out this <a href=""http://en.wikipedia.org/wiki/Monty_Hall_problem"">wikipedia link</a> and prepare to be baffled, then enlightened, then disappointed that the whole thing is so trivial. After which feel free to read on.]</p>

<p>There is a certain similarity, you will agree, between the situation a deal or no deal player finds himself in having rejected all offers and the dilemma of Monty's contestant in the classic problem: several 'bad choices' have been eliminated and he is left with a choice between a better and worse choice with no way of knowing between them.</p>

<h3>So???</h3>

<blockquote>
  <p><strong>Question:</strong> The solution to the monty hall problem is that it is, in fact, better to switch- does the same apply here? Does this depend upon the money in the boxes? Should every player opt for 'switch', cutting the 10 minutes of agonising away???</p>
</blockquote>
",<probability>
"<p>Say a gossip magazine editor is paying 2 sources to gather information about a particular celebrity. From his past experience the editor knows source# 1 is right 80% of time, and source# 2 is right 65% of the time. </p>

<p>Now, for a particular case, the editor wants to be sure at least 95%. Say the source #1, gives the editor some information piece x. What is the end probability in the following cases:</p>

<p>a. source# 2 also gives same information
b. source# 2 gives a different information about same thing than what source#1 gave. </p>

<p>In both above cases, what is the probability of the info from source #1 being correct?</p>
",<probability>
"<blockquote>
  <p><strong>Possible Duplicate:</strong><br>
  <a href=""http://math.stackexchange.com/questions/140836/a-probability-problem"">A probability problem</a>  </p>
</blockquote>



<blockquote>
  <p>Let $A$ and $B$ be events, $P(A) = \frac{1}{4} $, $P(A\cup B) = \frac{1}{3} $ and $ P (B) = p $. </p>
  
  <ol>
  <li>Find $p$, if $A$ and $B$ are mutually exclusive.</li>
  <li>Find $p$, if $A$ and $B$ are independent.</li>
  <li>Find $p$, if $A$ is a subset $B$.</li>
  </ol>
</blockquote>

<p>I know for 1) mutually exclusive: $P(A) + P(B) = P(A \cup B)$, but  how I can find p ? 
I don't know how to solve it. Please help me.</p>

<p>Obs: Sorry for duplicate post.</p>
",<probability>
"<p>I am trying to prove Boole’s inequality</p>

<p>$$P\left(\ \bigcup_{i=1}^\infty A_i\right) \leq \sum_{i=1}^\infty P(A_i).$$</p>

<p>I can show it of any finite $n$ using induction. What to do for $\infty$ ?</p>
",<probability>
"<p>Show that if $P(A_{i}) = 1$ for all $i \geq 1$ then $P(\ \bigcap_{i=1}^{\infty}A_i)=1$</p>
",<probability>
"<p>When playing many board games, the first step is to have everyone roll a die to see who goes first, with a roll off in the case of a tie.  While doing that over the Christmas break, my husband suggested that we roll two dice instead of one, with the assertion that this would make ties less likely.  My brother disagreed, claiming it wouldn't make any difference.  I'm interested in investigating this question.</p>

<p>I've been able to calculate the probability of ties for the case of rolling one die for any number of players, and the case for rolling two dice with two players.  However, I haven't actually found a general solution in either case (I mostly used a brute force approach in the one die case).  Is anyone here aware of any sources that have investigated this issue?</p>

<p>(For the record, I'm pretty sure both my husband and brother have forgotten the conversation, so you don't need to worry about hurting anyone's feelings.  :) )</p>
",<probability>
"<p>I have what seems like a simple question, but it's been a while since I've done any P/S. So i come to SE for help!</p>

<blockquote>
  <p>Two player pool/billiards: P1 has probability p of sinking a ball on any shot and has N balls remaining, while P2 has prob q and M balls remaining.</p>
  
  <p>Question: What is the probability of the first player winning?</p>
</blockquote>

<p>I can type out my reasoning (and will in an edit - will post before i reason it out though), but my answer has come down to:</p>

<p>$$\sum_{j=0}^{M-1} [p^N q^j \sum_{i=0}^\infty [(1-p)^i (1-q)^i]]$$</p>

<p>Is this correct or close?</p>

<p>Reasoning:</p>

<p>not really theoretical reasoning, but extrapolating the simple cases outwards:</p>

<p>(hit = h, miss = m, with probability p = w/p)</p>

<p>1 ball each: possible victory paths - </p>

<p>P1 wins w/p, </p>

<p>P1 m w/ (1-p), P2 m w/ (1-q), P1 wins w/ p</p>

<p>... etc - $p \sum_{i=0}^\infty (1-p)^i (1-q)^i$</p>

<p>2 and above balls each:</p>

<p>At some point, all the P1 hits must occur - $p^N$</p>

<p>All possible amounts of P2 hits must be accounted for - $\sum_{j=0}^{M-1} q^j$</p>

<p>Every miss variation is accounted for - <em>*</em> &lt;-- This is where i think I am wrong. Is it actually a double sum in and of itself? IE $\sum_{i=0}^\infty \sum_{k=0}^\infty (1-p)^i (1-q)^k$ ?</p>

<p>EDIT: Some wolfram alpha shows me that $\sum_{i=0}^\infty (1-p)^i = \frac 1 p$, so I guess my final final equation can be simplified to</p>

<p>$$\sum_{j=0}^{M-1} \frac{p^N q^j}{pq} $$ ??</p>

<p>etc.</p>
",<probability>
"<p>what is the probability of a person with a last name “Doe” replacing a house vacated by another “Doe”. Assuming 1% of the population has last name “Doe”? I have a excel sheet where I have 50 most popular name in the US. Let's say ""Doe"" is the most popular name and 1% of the toal population have last name ""Doe""? Let's assume ""John"" is the second most popular name and .8% of the population have last name ""John"". I want to know what is the probability of ""Doe"" replacing a house vacated by another ""Doe""? Or ""John"" replacing a house vacated by another ""John""? Thanks</p>
",<probability>
"<p>Let there be a family consisting of 2 children such that :</p>

<p>B : Event in which both children in a family are girls.</p>

<p>L : Event in which at least one child is a girl<br>
\begin{align}
P(B\mid L)&amp;=?\\
\text{I found it by enumeration as:}&amp;
\{(g,g),(g,b),(b,g),(b,b)\}\\
P(B|L)=1/3
\end{align}
Is there a way of arriving at this answer <em>without</em> enumerating?
\begin{align}
P(B|L)&amp;=P(BL)/P(L)\\
P(L)&amp;=1-P(\text{No Girls})\\&amp;=1-1/4=3/4\\P(BL)&amp;=?
\end{align}
Obviously, the answer has to be $1/4$ and I can see that from the enumeration but I can't deduce why.</p>
",<probability>
"<p>I want to calculate a conditional expectation and I do not see where my mistake is. I'm solving the following exercise (namely, 1a):</p>

<blockquote>
  <p><img src=""http://i.stack.imgur.com/IotJc.png"" alt=""enter image description here""></p>
</blockquote>

<p>(<a href=""http://www.math.ethz.ch/education/bachelor/lectures/hs2012/math/mff/MFF_2012_ex05beta.pdf"" rel=""nofollow"">source</a>)</p>

<p>Here $S^1,S^2$ are stochastic processes, which model for example a stock price. At time (day) $0$, $S^1_0=100$, which means that the price of stock one is $100$ unit of money. Then the stock price can go up, with probability $p_u=\frac{2}{3}$ and go down with prob. $p_d=\frac{1}{3}$. Hence at day $1$ it can take the prices $S^1_1=104$ or $S^1_1=98$ and so on.</p>

<p>The exercise want to construct an equivalent martingale measure for $S^1$.
To have an equivalent martingale for $S^1$ I need to find transition probabilities $q_u,q_d=1-q_u$ (u=up, d=down) and $q_{u,u},q_{u,d}=1-q_{u,u},q_{d,u},q_{d,d}=1-q_{d,u}$ such that $S^1=(S^1_k), k=0,1,2$ is a $Q^1$-martingale. The equivalence of the measure follows immediately if $q_j,q_{j,i}&gt;0$, $i,j\in\{u,d\}$. The filtration is generated by $S^1$, i.e. $\mathcal{F}_0=\sigma(S^1_0)$ which is trivial and $\mathcal{F}_1=\sigma(S^1_1)$. To be a martingale under the measure $Q^1$ (which is characterized by transition probabilities $q_j,q_{j,i}$ $i,j\in\{u,d\}$ we have to solve the equations:</p>

<p>$$E_{Q^1}[S^1_1]=100$$
$$E_{Q^1}[S^1_2|\mathcal{F}_1]=S^1_1$$</p>

<p>the first one is easy to solve and gives $q_u=\frac{1}{3}$ and $q_d=\frac{2}{3}$. Now for the second equation I want to use that $\mathcal{F}_1$ is generated by $\sigma(A_1,A_2)$, where $A_1=\{S^1_1=104\},A_2=\{S^1_1=98\}$. Then I know that </p>

<p>$$E_{Q^1}[S^1_2|\mathcal{F}_1]=\sum_{j=1}^2\frac{E[S^1_2\mathbf1_{A_j}]}{Q^1[A_j]}\mathbf1_{A_j}$$</p>

<p>hence for writting this out gives two equations:</p>

<p>$$\frac{1}{Q^1[A_1]}(116.48\cdot q_{u,u}+(1-q_{u,u})\cdot 99.84)=104$$
$$\frac{1}{Q^1[A_2]}(101.92\cdot q_{d,u}+(1-q_{d,u})\cdot 96.04)=98$$</p>

<p>where clearly $\frac{1}{Q^1[A_1]}=\frac{1}{q_u}$ and $\frac{1}{Q^1[A_2]}=\frac{1}{q_d}$. I would get the right result without the $\frac{1}{Q^1[A_j]}$ in the front of the equations. But I do not see why they do not have to be there. Right result: $q_{u,u}=\frac{1}{4}$, $q_{d,u}=\frac{1}{3}$. It would be very helpful, if someone could point out, where I my mistake is exactly.</p>

<p>As mentioned in the comment, I used the following theorem for calculating the conditional expectation:</p>

<blockquote>
  <p>Let $(\Omega,\mathcal{F},P)$ be a prob. space, $A_i\in \mathcal{F}$, for $1\le i\le N\le\infty$ pairwise disjoint measurable sets with $P(A_i)&gt;0$ and $\bigcup_{i=1}^N A_i=\Omega$. Let $\mathcal{A}=\sigma(A_i;1\le i\le N)$. Let $X\in L^1(\Omega,\mathcal{F},P)$, then
  $$E[X|\mathcal{A}]=\sum_{i=1}^N\frac{E[X\mathbf1_{A_i}]}{P(A_i)}\mathbf1_{A_i}$$</p>
</blockquote>
",<probability>
"<p>I'm reading <a href=""http://machinelearning.wustl.edu/mlpapers/paper_files/BleiNJ03.pdf"" rel=""nofollow"">http://machinelearning.wustl.edu/mlpapers/paper_files/BleiNJ03.pdf</a> and trying to understand the notation and concepts behind LDA, in order to implement it myself. I've followed some tutorials about the Poisson and Dirichlet distribution but I'm not super comfortable with them as topics yet.</p>

<p>Can someone explain what is meant on page 4 of the PDF:</p>

<blockquote>
  <p>LDA assumes the following generative process for each document w in a
  corpus D:</p>
  
  <ol>
  <li>Choose N ~ Poisson(ξ).</li>
  <li>Choose θ ~ Dir(α).</li>
  </ol>
</blockquote>

<p>What are these symbols referring to? Extracting words from the Poisson Distribution? How is that even possible? And extracting parameters from a Dirichlet distribution is equally confusing.</p>
",<probability>
"<p>I understand that when you apply a transformation $\sigma Y+\mu$ to $Y$~$N(0,1)$ ,we get a new random variable that is distributed $N(\mu,\sigma^2)$. However, I dont know through which mechanism this was obtained. Is it by multiplying the pdf by $\sigma^2$ and adding $\mu$? In general, I usually think about it as the following for normal distribution manipulations:</p>

<p>If you multiply, leave the mean alone but square the variance</p>

<p>If you add, leave the variance alone but add to the mean. </p>

<p>Is there another way of thinking about normal manipulations? Thanks!</p>
",<probability>
"<p>Let $\{X_i\}_{i\in\mathbb{N}}$ 
be a sequence of random variables taking values in 
$\{\pm e_1,\pm e_2\}$, 
where $\{e_1,e_2\}$ is the standard basis of $\mathbb{R}^2$.
If $\{X_i\}$ are i.i.d. uniformly distributed over $\{\pm e_1,\pm e_2\}$, 
then the simple random walk $S_N$ on $\mathbb{Z}^2$ have the mean square displacement given by $\mathbb{E}[\|S_N\|^2] =N$. We also know that the random 
walk $S_n$ is recurrent, i.e., 
$$
\sum_{n=1}^{\infty}\mathbb{P}(S_{2n}=0)=+\infty.
$$
<b>Question</b> Suppose now that the random variables $X_i$ $(i=1,2,\ldots)$ have some kind of dependence between its coordinates (but the steps $X_{i}'s$ are independent) so that the mean 
square displacement now obeys the following inequality for any $N\in\mathbb{N}$
$$
C_1N^2 \leq \mathbb{E}[\|S_N\|^2] \leq C_2N^2,
$$ 
where $0&lt;C_1&lt;1/2$ and $1/2&lt;C_2&lt;1$ are positive constants.
Is this inequality enough to assures that this random walk is transient ?</p>
",<probability>
"<p>I had a doubt on one ""diagonlization argument"" used to show Prohorov's Theorem. I am simplifying the discussion, so lets say that we consider the space $\mathbb{R}^{\infty}$
, and a set Π of probability measures on $\mathbb{R}^{\infty}$. Let us assume that $\Pi$ is tight. Now we want to show that it is relatively compact, i.e. every sequence $P_n \in \Pi$ has a subsequence $P_{n^{\prime}}$ which has a weak limit.</p>

<p>This statement below is easily proved:</p>

<p>Statement: Assume that we know that if the replace in the above statement $\mathbb{R}^{\infty}$ by $\mathbb{R}^{k}$
, then it is true, i.e. for a tight set in the set of measures on $\mathbb{R}^{k}$
, every sub-sequence has a weak limit.
Now we note that the projections $π_k$ of elements of $\mathbb{R}^{\infty}$ to $\mathbb{R}^{k}$
 are continuous, which implies that the sets ${Pπ^{−1}_k:P∈\Pi}$ are tight. Hence for every sequence $P_n$, by passing onto subsequence, $π^{−1}_k P_{n^{\prime}}$ has a weak limit. Now there is a ""diagonalization"" argument, which I had a problem absorbing. It essentially says that ""prune"" the above $P_{n^{\prime}}$ inductively to obtain a sequence $P_{\tilde{n}}$ which is such that the projection sequence of measures $π^{−1}_l P_{\tilde{n}}$ have a weak limit for all $l=1,2,\ldots$.</p>

<p>My problem is that if the ""pruning"" of the sequence $P_{n^{\prime}}$ was to be done only finitely many times, I know that after ""pruning"" the left over sequence is non-empty. But the problem here is that I have to prune the sequence infinitely many times, so how come in the end I get a non-empty set? I have seen this diagonalization argument used at many places but no one talks of the non-empty limit set.</p>

<p>I posted this question at some other site and was told this is connected to Tychonoff's Theorem. I get it that if at each step of pruning if I take the closure of the the subsequence, then they are compact, and hence their intersection is compact. But then who guarantees me that the intersection is non-empty??</p>
",<probability>
"<p>In a problem in the book, there is a batter never swings, and the pitcher has a .5 probability to strike and a .5 to throw a ball.</p>

<p>In this situation, we found:
P(batter strikes out) = 21/32
P(batter walks) = 11/32</p>

<p>(Keep in mind that it takes 3 strikes to strike out and 4 balls to walk)</p>

<p>We found this by adding the probability of the batter striking out on the 3rd, 4th, 5th, and 6th pitch. </p>

<p>To do this:</p>

<pre><code>P(Strike out on 3rd pitch) = (.5)^3 = 4/32  (3 strikes)

P(Strike out on 4th pitch) = C(3, 2)(.5)^2(.5)(.5) = 6/32
</code></pre>

<p>(2 strikes, 1 ball, then another strike)</p>

<pre><code>P(Strike out on 5th pitch) = C(4, 2)(.5)^2(.5)^2(.5) = 6/32
</code></pre>

<p>(2 strikes, 2 balls, then another strike)</p>

<pre><code>P(Strike out on 6th pitch) = C(5, 2)(.5)^2(.5)^3(.5) = 5/32
</code></pre>

<p>(2 strikes, 3 balls, then another strike)</p>

<p>So, add those up to get <code>P(Strike out) = 21/32</code></p>

<p>Find the probability for P(pitcher throws a strike) for which P(batter walks) = P(batter strikes out) = 1/2</p>

<p>My thinking was to turn this into an equation. Let x represent P(pitcher throws a strike). So then we have P(strike out) is</p>

<p>= <code>x^3 + C(3,2)x^3(1-x) + C(4,2)x^3(1-x)^2 + C(5,2)x^3(1-x)^3</code></p>

<p>= <code>x^3( 1 + C(3,2)(1-x) + C(4,2)(1-x)^2 + C(5,2)(1-x)^3</code></p>

<p>This makes it a little easy to try out... but to solve I was thinking that if we need 21/32 to be 1/2 then.... (21/32)x = .5 => x = 16/21. But I am not sure what to do with this</p>
",<probability>
"<p>From <em>A First Course in Probability (9th Edition)</em>:</p>

<blockquote>
  <p>3.5 An urn contains 6 white and 9 black balls. If 4 balls are to be
  randomly selected without replacement, what is the probability that
  the first 2 selected are white and the last 2 black?</p>
</blockquote>

<p>This method is straightforward and results in the correct answer (according to the book):
$$\frac{6}{15} \cdot \frac{5}{14} \cdot \frac{9}{13} \cdot \frac{8}{12} = \frac{6}{91} $$</p>

<p><em>(This is just the multiplication principle and probability of drawing the color of that ball at that time)</em> </p>

<p>However, I want to understand this in terms of conditional probability. I don't understand why <em>this</em> doesn't work:</p>

<p>$$P(E \mid F) = \frac{P(E \cap F)}{P(F)} ={\frac{{6 \choose{2}}{9 \choose 2}}{{15 \choose{2}}{13 \choose 2}}}÷{\frac{{6 \choose{2}}}{{15 \choose{2}}}} = {\frac{{9 \choose 2}}{{13 \choose 2}}} = \frac{6}{13} \ne \frac{6}{91}$$</p>

<p>$\frac{6}{13}$ is exactly 7 times more than the previous answer. Why does this method fail to work? What mistake have I made? I tried to use the exact same method used in question 3.3, where this resulted in the correct answer. </p>

<hr>

<p>Optional – About 3.3</p>

<pre><code>3.3 Use Equation (2.1) to compute in a hand of bridge the conditional 
probability that East has 3 spades given taht North and South have a 
combined total of 8 spades.
</code></pre>

<p>Here, we see that:
$$P(E \mid F) = \frac{P(E \cap F)}{P(F)} ={\frac{{13 \choose{8}}{39 \choose 18}{5 \choose 3}{21 \choose 10}}    {{52 \choose{26}}{26 \choose 13}}}÷{\frac{{13 \choose{8}}{39 \choose 18}}{{52 \choose{26}}}} = {\frac{{5 \choose 3}{21 \choose 10}}{{26 \choose 13}}} = \frac{29}{115} \approx 0.339$$</p>

<p>Which is the answer in the back of the book.</p>
",<probability>
"<p>I have N balls and M boxes. The balls are thrown at random onto the boxes. What is the probability that some box contains at least 3 balls? </p>

<p>Based on the Birthday problem, I know how to find the solution to the problem if we are finding the probability that some box contains at least 2 balls.</p>

<p>I am struggling to find the solution to this problem. Any help is much appreciated.</p>

<p>Here is what I have tried:
If we have M boxes and N balls, then the probability that some box contains at least two balls is approximately equal to $$ 1-e ^{ (-n^2 / 2m)} $$</p>

<p>Let us suppose that we have 4 balls and 6 boxes, then the probability that some box has at least 2 balls is approximately equal to $$ 1 - e ^ {(-16/12)} .$$ This is approximately equal to 0.7364.</p>

<p>Now I would like to find the probability that some box will contain at least 3 balls, when 4 balls are thrown at random onto 6 boxes.</p>

<p>Thanks
Sekhar </p>
",<probability>
"<p>(Quant job Interviews - Questions and Answers - Joshi et al, Question 3.5)</p>

<blockquote>
Suppose you have a fair coin. You start with 1 dollar, and if you toss a H your position doubles, if you toss a T your position halves. What is the expected value of the money you have if you toss the coin to infinity ?
</blockquote>

<p>Now the answer is stated as follows:</p>

<blockquote>
We work out what happens with one toss, then $n$ tosses and then let $n$ tend to infinity.

Let X denote a toss then:
$$\mathbb E (X) = \frac{1}{2} * 2 + \frac{1}{2} * 0.5= {5\over4} $$

Provided the tosses are independent, the product of expectations is the expectation of the product. Let $X_j$ be the effect of toss $j$. This means that
$$ \mathbb E (\prod_{j=1}^{n} X_j) = \prod_{j=1}^{n} \mathbb E (X_j) = ({5\over4})^n$$
this clearly tends to infinity as n tends to infinity
</blockquote>

<p>Now, I don't understand this answer :(</p>

<p>First, the way the answer is written out, surely the ${5\over4}$ is the expectation of the outcome of the first toss $X_1$ , not that of a toss $X_j , j \ge 1$ ?</p>

<p>Secondly, whilst I do understand that the tosses are independent, it would seem that the $X_{j+1}$ is actually quite heavily dependent on the $X_{j}$ before it ?</p>

<p>So then why is it so obvious that $\mathbb E ( X_{j+1} ) = \mathbb E ( X_{j})$ ?</p>
",<probability>
"<p>My aim is to become sharp in the necessary knowledge of basic probability and counting to follow my studies of Statistics for Computer Science.</p>

<p>Right now I found the following book:</p>

<p><a href=""http://aops-cdn.artofproblemsolving.com/products/intro-counting/toc.pdf"" rel=""nofollow"">http://aops-cdn.artofproblemsolving.com/products/intro-counting/toc.pdf</a></p>

<p>But the price is too high.</p>

<p>Are there any other cheaper alternatives that cover the same ground?</p>
",<probability>
"<p>Suppose a sample of <strong>120</strong> items is drawn from a population of manufactured products and the number of defective items is recorded. Prior experience has shown that the proportion of defectives is <strong>0.05</strong>.</p>

<p>a) Describe the sampling distribution of <strong>p̂</strong>, the proportion of defectives.</p>

<p>b) What is the probability that the sample proportion is less than <strong>0.10</strong>?</p>

<p>My Work:</p>

<p>a) $$np ≥ 5$$</p>

<p>because</p>

<p>$$ 120*0.05 =6$$</p>

<p>and</p>

<p>$$nq = 120*0.95 = 114 ≥ 5$$</p>

<p>therefore the sampling distribution is approximately normal distributed. </p>

<p>b) $$P(p̂&lt;0.10) = P(z &lt; (0.10-0.05)/[\sqrt{(0.05x0.95)/120)})$$</p>

<p>$$= P(z&lt;0.05/0.0199) = P(z&lt;2.5126)$$</p>

<p>$$ =0.9940$$</p>

<p>resulting in <strong>99.4%</strong>.</p>

<p>I'm not sure if I'm solving this problem properly, any help is greatly appreciated!</p>
",<probability>
"<p>Let $X_{1}, X_{2},\ldots, X_{n},\ldots$ be independent and identically distributed random variables with distribution</p>

<p>$P(X_{n}=0)=P(X_{n}=1)=P(X_{n}=2)=P(X_{n}=3)=\frac{1}{4}$</p>

<p>Assume that the sequence $\{Y_n\}$ is defined as: $Y_{0}=0$ and for all $n\in\mathbb{N}$ we have</p>

<p>$Y_n=\begin{cases} 3 &amp;\text{if } X_n=3,\\\min{\{Y_{n-1},X_n\}} &amp;\text{if } X_n&lt;3. \end{cases}
$</p>

<p>Compute $\displaystyle \lim_{n\to +\infty}E[Y_{n}Y_{n-1}]$?</p>

<p>I don't know how to start.</p>
",<probability>
"<p>If I get to draw $3$ cards from a standard deck. </p>

<p>With replacement - a) What is the probability of drawing a $10,J,Q$ in that order?</p>

<p>Without replacement - b) What is the probability of drawing a $10,J,Q$ in that order?</p>

<p>With replacement - c) Drawing at least one ace (I got $= \left(\frac{4}{52}\right)^3$).</p>

<p>Without replacement - d) Drawing at least one ace (I got $= \left(\frac{4}{52}\right)\left(\frac{3}{51}\right)\left(\frac{2}{50}\right)$)</p>

<p>Having trouble with a) and b) I know that order matters so I would have to use combinations.</p>

<p>Any thoughts? Thanks.</p>
",<probability>
"<p>We flip a fair coin. Then, if the result is tails, we stop.  If it is heads, 
we flip a second time and then stop. Let $X$ be the number of heads from the flip(s). If $X = 0$, let $Y = 0$. If $X = 1$ or $X = 2$, choose $Y$ uniformly at random from the interval $[0,X]$.</p>

<p>How can I find the c.d.f. of $Y$? I know that $P(X = 0) = 1/2$, $P(X = 1) = 1/4$, and $P(X = 2) = 1/4$. But, how can I extend this information to get the c.d.f. of $Y$?</p>
",<probability>
"<p>You have $n$ urns, each containing $p_i$ white balls and $q_i$ black ones. One of the urns is selected uniformly at random, and balls are extracted with replacement from it until a black ball is found.</p>

<p>What is the distribution of the number of draws needed?</p>

<p>This is simply a random selection out of $n$ independent geometric distributions, but I'm unable to find this particular ensemble anywhere. Does this distribution have a proper name? Is it a geometric distribution itself?</p>
",<probability>
"<p>In this <a href=""http://math.stackexchange.com/questions/331280/probability-distribution-of-tossing-a-coin-until-obtaining-k-heads"">Question</a> the correct answer is the negative binomial distribution. My problem is: What is the distribution if I want the k heads in a row?</p>

<p>Any help is apreciated.</p>
",<probability>
"<blockquote>
  <p>The finite population correction in sample survey is :</p>
  
  <p>(A) $n/N$</p>
  
  <p>(B) $N/n$</p>
  
  <p>(C) $1-\frac{n}{N}$</p>
  
  <p>(D) $1-\frac{N}{n}$</p>
</blockquote>

<p>I know that , finite population correction factor is 
$$fpc=\sqrt{\frac{N-n}{N-1}}$$where , $n=$sample size and $N=$population size.</p>

<p>How I can deduce from it to get one option ? Please help..</p>
",<probability>
"<p>In a dancing party, there are 3 pairs of married couple. Each husband will pick his female dancing partner randomly. So, the probability of every wives will dance with her non-husband is...?</p>
",<probability>
"<p>I am trying to set up a probability table for the events of drawing two cards from a $52$ card deck. What counts is either an exact match or a match in flush with two already drawn cards from another deck. I am not sure the table is correct, so I would be grateful for a judgement. </p>

<ol>
<li><p>Two exact matches:                        $\frac{1}{1326}$</p></li>
<li><p>One exact match + one flush match: $\frac{13 \times 2-1}{1326}$</p></li>
<li><p>One exact match only:                  $\frac{26 \times2}{1326}$</p></li>
<li><p>Two flush matches:               $\frac{13 \times13\times2-39}{1326}$</p></li>
<li><p>One flush match only:            $\frac{26 \times 26-52}{1326}$</p></li>
<li><p>No match at all:                        $\frac{325}{1326}$</p></li>
</ol>
",<probability>
"<p>May I please borrow your expertise or could anyone check if I'm on the right track please?</p>

<p>Consider customers arriving at a bank. The bank has $2$ types of customers - business and personal. On average, $10$ business customers arrive per hour, and $20$ personal customers. The times between arrivals of the business customers are independent of each other, and exponentially distributed, and is the same for personal customers. The two streams are independent of each other. Suppose no new customers have entered the bank in the last $5$ minutes.</p>

<p>1) What is the probability that no business customers arrive in the next $3$ minutes?</p>

<p>I'm assuming the arrival time for business customers is 60/10 = 6 customers
Thus, by using the exponential equation and also that this is a memoryless property,</p>

<p>$1 - e^{-3/6}$ (am I on the right track here anyone)?</p>

<p>2) What is the probability that no personal customers arrive in the next $3$ minutes?</p>

<p>Assuming the arrival time for personal customers to be 60/20 = 3 customers
Using the equation from question 1)</p>

<p>$1 - e^{3/3}$ (is this right)?</p>

<p>3) What is the probability that no customers at all arrive in the next $3$ minutes?</p>

<p>Assuming that this is something like $1 - ((1-e^{-3/6}) + (1-e^{-3/3}))$
by adding the no personal customer and business customer together and minus it by 1</p>

<p>4) What is the distribution of the time until the arrival of the next customer?</p>

<p>(Any hints on how I might be able to use to get the answer for this question?)</p>

<p>Your helps are much appreciated. Thanks,</p>
",<probability>
"<p>For a discrete RV $X$, is it true that the conditional distribution $P_{X \mid Y} (B \mid y)$ is discrete as well for all $y$?</p>

<p>I only managed to prove that this is true almost surely. Let $\Pr(X\in C) = 1$ for countable $C$, then by definition $\Pr (X \in C, Y \in \mathbb{R}) =\mathbf{E} [P_{X \mid Y} (C \mid Y) ; Y \in \mathbb{R}] = 1$.</p>
",<probability>
"<p>Say I have an image, with pixels that can be either $0$ or $1$. For simplicity, assume it's a $2D$ image (though I'd be interested in a $3D$ solution as well). </p>

<p>A pixel has $8$ neighbors (if that's too complicated, we can drop to $4$-connectedness). Two neighboring pixels with value $1$ are considered to be connected. </p>

<p>If I know the probability $p$ that an individual pixel is $1$, and if I can assume that all pixels are independent, how many groups of at least $k$ connected pixels should I expect to find in an image of size $n\times n$?</p>

<p>What I really need is a good way of calculating the probability of $k$ pixels being connected given the individual pixel probabilities. I have started to write down a tree to cover all the possibilities up to $k=3$, but even then, it becomes really ugly really fast. Is there a more clever way to go about this?</p>
",<probability>
"<p>I want to solve the following exercise:</p>

<p>Suppose that two sets $X$ and $Y$ are chosen independently and uniformly at random from all the $2^n$ subsets of $\{1, \dotsc, n\}$. </p>

<p>Determine $P[X \subseteq Y]$ and $P[X \cup Y = \{1, \dotsc, n\}]$.</p>

<p><strong>MY IDEA:</strong> </p>

<p>My idea is that a random subset is the same as deciding for each element independently with probability $\frac{1}{2}$ whether it is in the subset or not. </p>

<p>Then $$P[X \subseteq Y] = P[ \forall x \in X \colon x \in Y]$$</p>

<p>Now I have the first problem, because I am not sure how to express the for all $x$ that are in $X$. I mean, is this the same as computing $$\sum_{x=1}^n P[x \in X] P[x \in Y | x \in X] = \sum_{x=1}^n P[x \in X] P[x \in Y] = \frac{n}{4},$$
since this computation does not feel right. Can someone intuitively tell me what the above calculation calculates and how to solve the actual problem? My problem is here that we have somehow to condition on the event that this $x$ is already in $X$, otherwise we need not do anything. </p>

<p>The second one sounds a bit easier to me. There we have
$$P[X \cup Y = \{1, \dotsc, n\}] = P[\forall x \in \{1, \dotsc, n\} \colon x \in X \vee x \in Y] =\prod_{x=1}^n P[x \in X \vee x \in Y] = \prod_{x=1}^n (P[x \in X] + P[x \in Y] - P[x \in X \wedge x \in Y] ) =\prod_{x=1}^n (P[x \in X] + P[x \in Y] - P[x \in X] P[ x \in Y] =  \prod_{x=1}^n (\frac{1}{2}+\frac{1}{2}-\frac{1}{4})= \left(\frac{3}{4}\right)^n.$$
Is this correct? Is there an easier way?</p>
",<probability>
"<p>I have a somewhat open-ended question. Let's say I have a sequence of random variables $(X_n: n \geq 1)$ which are <strong>neither independent, ergodic, nor identically distributed</strong>. Normally I would say that I am completely dead in the water, <strong>but let's say that $X_n \overset{d}{\to} X$</strong>. Are there any additional assumptions under which I can say that:</p>

<p>$$ \frac{1}{n} \sum_{i=1}^n X_n \;\overset{P}{\to}\; \mathbb{E}X $$</p>

<p>Even if I assume that expectation of the left-hand side converges to $\mathbb{E}X$, I'm stuck thinking about this more generally. Any tips?</p>

<p>EDIT: Thinking about this some more, I feel like making a martingale out of the LHS and then checking under what conditions we have the desired martingale convergence would be a reasonable route to follow. Any thoughts on this?</p>

<p>EDIT 2: per Nate Eldredge's comment below, I need to assume that the expectation of the LHS of the partial-sum object converges to $EX$... it doesn't follow from $X_n \overset{d}{\to} X$. </p>
",<probability>
"<p>Two piece of gold are contained in two same-looking black boxes respectively. It is known that one piece weights twice as the other, but do not know which is which.  </p>

<p>Two persons, say A and B, randomly choose a box. One person, say A, opened his box, but he does not known whether it is lighter or weightier. </p>

<p>Question: Is A willing to exchange his box with B?</p>

<p>This might be a well-known problem, but I do not know the proper name to search it online. </p>

<p>Intuitive, it make no difference to exchange the boxes.</p>

<p>On the other hand, if we compute the expectation for A, it seems that A should change the box (the expectation is 1.25 of the current holding).</p>

<p>I would like to hear your answer to the question and preferably fuller story about this paradox.</p>
",<probability>
"<p>Here is lottery machine, if the current no. on the screen is ""1"" then probability of getting ""2"" is ""a"" and probability of getting ""3"" is ""b"" and probability of getting ""1"" is ""c"", after each step.<br>
where ""a""+""b""+""c""=1.<br>
similarly, <br>
if the current no. on the screen is ""2"" then probability of getting ""1"" is ""d"" and probability of getting ""3"" is ""e"" and probability of getting ""2"" is ""f"".<br>
where ""d""+""e""+""f""=1.<br>
and,<br>
if the current no. on the screen is ""3"" then probability of getting ""1"" is ""g"" and probability of getting ""2"" is ""h"" and probability of getting ""3"" is ""i"".<br>
where ""g""+""h""+""i""=1.<br>
<br>
then what is probability of getting ""1"" after exactly k steps, if current no. on screen is:<br>
a)""1"",<br>
b)""2"",<br>
c)""3"".<br></p>
",<probability>
"<p>Trying to work out the probability of a fault occuring over a 12.5 year time period. The probability of a fault occuring per year is 1/15. </p>

<p>Does this mean the probability of no fault occuring over the period is (14/15)^12.5 = 0.422</p>

<p>And of one fault occuring 1 - 0.422 = 0.578?</p>

<p>How could I go about calculating the probabilty of two faults occuring?</p>
",<probability>
"<p>My book states that $E[X^2]$ is the average power. It then says for $\mathcal{N}(0, 1)$, the average power is $\frac{1}{2}$ and for $\mathcal{N}(0, \sigma^2)$ is $\frac{\sigma^2}{2}$.</p>

<p>How can this be?</p>

<p>The second moment of the Gaussian is $\mu^2 + \sigma^2$ so the average power for $\mathcal{N}(0, 1)$ should be one and $\sigma^2$ for $\mathcal{N}(0, \sigma^2)$.</p>

<p>It makes since to multiple by $1/2$ since we average by $1/N$ but the book just states the average power is $E[X^2]$ not $\frac{1}{2}E[X^2]$.</p>

<p>Can someone shed some light on this?</p>
",<probability>
"<p>In Bayesian probability, does the prior distribution $\pi(\theta)$ only depend on $\theta$? For example, suppose the prior distribution of the unknown parameter $\theta$ is binomial. Then does $$ \pi(\theta) = \binom{n}{\theta} p^{\theta} (1-p)^{n-\theta}$$ </p>

<p>Whereas if $f(\theta|x_1)$ is binomial then  $$f(\theta|x_1) = \binom{n}{x_1} p^{x_1}(1-p)^{n-x_1}$$</p>

<p>Is this correct?</p>
",<probability>
"<p>Is there any way to calculate the restricted Laplace transform of the random variable $X$, i.e., $$ 
\int_{0}^{u}e^{-sx}dF(x)\ 
$$
$(u&lt;\infty)$, based on its Laplace transform?</p>
",<probability>
"<p>Given the definition of conditional expectation as E$[X|B] = \frac{E[1(B) \cdot X]}{P(B)}$, and understanding $1(B)$ as an indicator function that returns $1 (0)$ when $B$ is true (false), it would seem $E[1(B)\cdot X]$ takes the expected value of members of $X$ where $B$ is true. Why the further division by $P(B)$? Intuitive as well as formal explanation requested.</p>

<p>E.g., suppose we have:</p>

<blockquote>
  <p>$(S,P)$:
  $(1,3)$
  $(1,4)$
  $(0,3)$
  $(0,2)$
  $(0,1)$
  $(0,0)$
  $(1,1)$
  $(1,2)$
  $(1,3)$
  $(0,2)$</p>
</blockquote>

<p>$P(S = 1) = 0.5$, $E[1(S=1) \cdot P] = \frac{13}{5} \approx 2.6 $ (where $1[x]$ is an indicator function that flips to $1$ if $S = 1$ and $0$ if $S != 1$). </p>

<p>Thus $E[P|S = 1] = \frac{E[1(S=1) \cdot P]}{P(S = 1)} \approx 5.2$? What am I doing wrong here?</p>
",<probability>
"<p>A book has 10 short and 10 long chapters. Short chapters span 10 pages, and long chapters span 20 pages.</p>

<p>Why does the probability that you will pick a long or a short chapter differ between these  strategies?</p>

<p>Strategy #1: Flip to a random page, back up to the start of that chapter, and start reading.<br />
Strategy #2: Flip to a random page, go forward to the start of the next chapter, and start reading (and pick the first chapter if the page you pick lies within the last chapter).</p>
",<probability>
"<p>Math and probability wasn't ever my strong side, so I need to ask for a help in calculating simple (as I assume) value. Here is situation description.</p>

<blockquote>
  <p>Player <strong>A</strong> throws seven times with ten-side dice (numbers in range 1-10). Writes down results and shows them to player <strong>B</strong>. Player <strong>B</strong> throws three times with the same dice.</p>
  
  <p><em>What is the probabillity that among all ten numbers (both players) there will be no repeats - i.e. all three player <strong>B</strong>'s numbers will be exactly different than all player <strong>A</strong>'s numbers?</em></p>
</blockquote>

<p>This comes from a simple game, that I've been playing. Developer of this game suggests that above mentioned situation can appear fairly often (sometimes 2-3 times per particular game) completely random. While I'm pretty sure, that probability of such situation is so extremely low, that it can't happen that often, without any changes to game random generator, to make game itself much harder to complete. In other words -- I claim that game's random generator is not that random.</p>

<p>Thank you in advance for any help.</p>
",<probability>
"<p>How would I calculate the probability of randomly selecting a house and getting the current owner’s last naming same as previous owner’s last name? For example, let’s say 1% of the population has the last name “Doe” and picking a house randomly getting the current owner’s name is Doe and the previous owner’s name was Doe too. Thanks </p>
",<probability>
"<p>Jar $A$ contains $3$ red and $3$ black marbles, and Jar $B$ contains $4$ red and $6$ black marbles. If a marble is randomly selected from each Jar, what is the probability that the marbles will be the same color?</p>

<p>I'm having difficulty approaching this problem.  It seems like I should use Baye's Theorem to solve this problem, that is, let $A = \{\text{select a marble randomly from Jar A and Jar B}\}$ and $B = \{\text{the color of the marbles is the same}\}$.  I think I'm suppose to find $P(A|B)$ by Baye's Theorem.  However, I get stuck here as I need $P(A)$ and $P(B|A)$.  Perhaps my approach is incorrect.   </p>
",<probability>
"<p>I've heard somewhere from someone about a theorem that roughly says ""the probability of an event decreases as time increases""</p>

<p>I couldn't find the exact theorem (assign it exists at all.)</p>

<p>So figure I should ask all of you great mathematicans here if anyone has ever heard of this.</p>
",<probability>
"<p>Forgive me if this is really basic:</p>

<blockquote>
  <p>Tammy is a general contractor and has submitted two bids for two projects (A and B). The probability of getting project A is 0.65. The probability of getting project B is 0.77. The probability of getting at least one of the projects is 0.90. What is the probability that she will get both projects?</p>
</blockquote>

<p>Is this a simple question using the addition law or am I missing something? I calculated that her probability of getting both would be 0.52.</p>

<p>(0.65 + 0.77 - 0.90) = .52</p>
",<probability>
"<blockquote>
  <p>Given the random vector $(X,Y)$ with joint probability $P(0,1)=\frac{1}{18}$, $P(1,2)=\frac{3}{18}$, $P(1,4)=\frac{5}{18}$, $P(2,0)=\frac{2}{18}$, $P(2,1)=\frac{4}{18}$, $P(2,3)=\frac{3}{18}$ and $0$ otherwise, I need to find:</p>
</blockquote>

<ol>
<li>Domain, probability and distribution for $(Y|X=2)$</li>
<li>Domain and probability for $Z=X+Y$</li>
</ol>

<p>1) I need to find the marginal $p_X$ for $X$ as:</p>

<p>$p_X(2)=\sum_yp(2,y)=\frac{2}{18}+\frac{4}{18}+\frac{3}{18}=\frac{9}{18}$</p>

<p>Now I can calcolate che conditional $(Y|X=2)$</p>

<p>$p_{Y|X}(2,0)=\frac{p(2,0)}{p_X(2)}=\frac{2}{9}$</p>

<p>$p_{Y|X}(2,1)=\frac{p(2,1)}{p_X(2)}=\frac{4}{9}$</p>

<p>$p_{Y|X}(2,3)=\frac{p(2,3)}{p_X(2)}=\frac{3}{9}$</p>

<p>2) I read that $p_Z(z)=\sum_{x}p(x,z-x)$, but I don't know how to proceed. And suggestion?</p>
",<probability>
"<p>I need help with solving one of the questions the teacher gave us to prepare for an upcoming exam. I tried solving it but with no luck. Here is the question:</p>

<blockquote>
  <p>On one shelf there are 5 hardcover books and 6 paperbacks and on the other shelf there are 7 hardcover and 4 paperback. From the first shelf we pick two books randomly and put them on a table and from the second shelf we pick one book randomly and also put it on the table. And at last we randomly pick one book from the table. What is the possibility for that book to be a hardcover?</p>
</blockquote>

<p><strong>EDIT:</strong> I gave it a shot with something like this: I calculated $P1$ like so with fractions $P(5 hard, 6 paper) = 5/11 + 4/10$ - so I can see the probability of taking 2 hardcover books from the first shelf then I gave $P2 7/11$ from the second shelf and the final P(A) I tried adding then together. This was of no use as you can see.. </p>
",<probability>
"<p>Let $X^1$ and $X^{-1}$ be two simple random walk in $\mathbb{Z}$ starting respectively from $1$ and $-1$. Let $\tau$ be the first time one of them reaches the origin,</p>

<p>$$\tau = \inf \{ j \geq 0 \, : \, X^{-1}(j) = 0 \, \, \mbox{or}\, \,  X^{1}(j) = 0 \}.$$</p>

<p>How much is $E[\tau]$, the expectation of $\tau$?</p>

<p><strong>Comment:</strong> If there was only a random walk, then the expectation of $\tau$ would be infinite, as $P(\tau &gt;k ) \sim 1/k$. However, for two random walks I think it should not be infinite, as $P(\tau &gt;k ) \sim 1/k^2$. How to make it rigorous and compute the exact number? </p>
",<probability>
"<p>Say we role $n$ identical, fair dice, each with $d$ sides (every side comes up with the same probability $\frac{1}{d}$). On each die, the sides are numbered from $1$ to $d$ with no repeating number, as you would expect. So an ordinary $d$ sided die pool. </p>

<p>Every dice in the outcome that shows a number equal or higher than the threshold number $t$ is said to show a hit. Every die that shows the maximum result of $d$ is rolled again, which we call ""exploding"". If the re-rolled dice show hits, the number of hits is added to the hit count. Dice that show the maximum after re-rolling are rolled again and their hits counted until none show a maximum result. <strong>Given the values of</strong></p>

<p>$$ d\ ...\ \text{Number of sides on each die}\ \ d&gt;0 $$
$$ n\ ...\ \text{Number of dies rolled}\ \ n\ge 0$$
$$ h\ ...\ \text{Number of hits, we want the probability for}$$
$$ t\ ...\ \text{Threshold value for a die to roll a hit}\ \ 0 &lt; t \le d$$</p>

<p><strong>what is the probability to get exactly exactly $h$ hits?</strong> Lets call it: $$p^\text{exploding}(d,n,t,h) = p_{d,n,t,h}$$ 
Can you derive a formula for this probability?</p>

<p><strong>Example roll:</strong></p>

<p><em>We roll 7 six-sided dice and count those as hits that show a <code>5</code> or a <code>6</code>. In this example, $d=6$, $n=7$, $t=5$. The outcome of such a roll may be <code>6</code>,<code>5</code>,<code>1</code>,<code>2</code>,<code>3</code>,<code>6</code>,<code>1</code>. That's three hits so far, but we have to roll the two sixes again (they explode). This time it's <code>6</code>, <code>2</code>. One more hit, and one more die to roll. We are at four hits at this point. The last die to be re-rolled shows <code>6</code> again, we re-roll it yet another time. On the last re-roll it shows a <code>4</code> - no more hits. That gives five hits in total and the roll is complete. So, for this roll $h=5$.</em></p>

<p><strong>Simple case for just one die $n=1$</strong>:</p>

<p>If we roll only one die with the same threshold as above, so ($d=6$, $n=1$,  $t=5$), the probabilities can be easily calculated:</p>

<p>$$ p_{6,1,5,0} = \frac{4}{6} \quad \text{(Probability for exactly 0 hits - roll 1-4 on the first roll, no explosion here)} $$
$$ p_{6,1,5,1} = \frac{1}{6} + \frac{1}{6} \cdot \frac{4}{6} \quad \text{(Probability for exactly 1 hit - roll either a 5 or a result of 1-4 after a 6)} $$
$$ p_{6,1,5,2} = \frac{1}{6} \cdot \frac{1}{6} + \frac{1}{6} \cdot \frac{1}{6} \cdot \frac{4}{6} \quad \text{(Probability for exactly 2 hits - either a 6 and 5 or two sixes and 1-4)} $$
$$ p_{d,1,t,h\ge 1} = \left(\frac{1}{d}\right)^{h-1}\frac{d-t}{d}  + \left( \frac{1}{d} \right)^h \cdot \frac{t-1}{d} \quad \text{(Probability for exactly $h\ge 1$ hits - either $h-1$ maximum rolls and non-maximal success or $h$ maximum rolls and a non-success )} $$</p>

<p><strong>Without Explosion:</strong></p>

<p>For none-exploding dice the probability would just be <a href=""https://en.wikipedia.org/wiki/Binomial_distribution"" rel=""nofollow"">binomially distributed</a>:</p>

<p>$$ p^\text{non-exploding}_{d,n,t,h} = \binom{n}{h} \left( \frac{d-t+1}{d} \right)^h \left( 1 - \frac{d-t+1}{d} \right)^{n-h} $$</p>

<p>$$ E^\text{non-exploding}_{d,n,t} = n \frac{d-t+1}{d}; \qquad V^\text{non-exploding}_{d,n,t} = n \frac{(d-1)(d-t+1))}{d^2} $$</p>

<p>Where $E_{d,n,t}$ is the expected number of hits, and $V_{d,n,t}$ its variance.</p>

<hr>

<p><strong>Edit1:</strong> In the mean time I found <a href=""http://math.stackexchange.com/q/391792/11949"">Probability of rolling $n$ successes on an open-ended/exploding dice roll</a>. However I'm afraid, I don't fully get the answer there. E.g. the author says $s = n^k + r$, which does not hold for his examples. Also I'm not sure how to get $s$, $k$ and $r$ from my input values stated above (which are $d$, $n$, $h$ and $s$).</p>

<p><strong>Edit2:</strong> If one had the probability for $b$ successes via explosions, given that the initial role had $l$ successes prior to the explosions, one could just subtract all those probabilities for all values of $b$ from the value for the pure binomial distributions with $l$ successes and add the respective value to the pure binomial probability of $b+l$ successes. Just an idea. I suppose this should be something like a combination of geometric and binomial distribution.</p>

<p><strong>Edit3:</strong> I accepted <a href=""http://math.stackexchange.com/users/224454/brian-tung"">Brian Thug</a>'s excellent answer, giving the formula:
$$ p^\text{exploding}_{d,n,t,h} = \frac{(t-1)^n}{d^{n+h}}
             \sum_{k=0}^{\max\{h, n\}} \binom{n}{k} \binom{n+h-k-1}{h-k}
             \left[ \frac{d(d-t)}{t-1} \right]^k $$</p>

<p>$$ E^\text{exploding}_{d,n,t} = n\frac{d+1-t}{d-1}; \qquad V^\text{exploding}_{d,n,t} = E_{d,n,t} - n\frac{(d-t)^2-1}{(d-1)^2} $$</p>

<p>Here is a graph from a <a href=""https://github.com/con-f-use/Exploding-Diepool"" rel=""nofollow"">simulation</a> (<a href=""http://con-f-use.github.io/Exploding-Diepool/"" rel=""nofollow"">html</a>) that illustrates the whole thing:</p>

<p><img src=""https://github.com/con-f-use/Exploding-Diepool/raw/master/img/hist_d6_n15_t5.png"" alt=""Comparison between exploding and non-exploding dice pools""></p>
",<probability>
"<p>My text book uses the linearity of the expected value to compute it. It defines a random variable $X_i$ that indicates whether the urn $i$ contains $k$ balls or not. So the asked value is $E[X_1 + X_2 + ... + X_n]$.</p>

<p>What is not entirely clear for me is the following. It states that $P\{X_i=1\}={r \choose k}({{1}\over{n}})^k({1-{{1}\over{n}}})^{r-k}$. However, if the balls are indistinguishable, I thought we should compute that probability with the ""stars and bars"" approach?</p>
",<probability>
"<p>Is there an explicit formula for the probability that a simple symmetric random walk on $\mathbb{Z}$ starting from $1$ will not hit $0$ before time $t$?</p>
",<probability>
"<p>For $X_i \sim$ i.i.d with cdf $F_x$, and $\forall c \in \mathbb R$, then, letting $M_n$ denote the maximum observation</p>

<p>$$
M_n \le c+  \sum_i^n (X_i - c) \mathbb I(X_i &gt; c)
$$
I proved this by taking $n=1$ and $X_i = c$ and showing that this was a contradiction. Now, I am trying to show that, for $\bar{F}(x)= 1- F(x)$ :</p>

<p>$$
E(M_n) \le c + n \int_c^\infty \bar{F}(x)dx
$$
My attempt so far, using the initial result, then then the same inequality must hold for the expectations of both sides, that is:
\begin{align*}
E(M_n) &amp;\le c+  \sum_i^n E[(X_i - c) \mathbb I(X_i &gt; c)] \\
&amp;\overset{\text{iid}}{=}c+ n E[(X - c) \mathbb I(X &gt; c)] \\
&amp;= c+ n \int_{-\infty}^ \infty (x-c)\mathbb I(X &gt; c) f_X(x) dx \\
&amp;= c+ n \int_{c}^ \infty (x-c)  f_X(x) dx \\
&amp;= c+ n \int_{c}^ \infty x  f_X(x) dx - nc \int_c^\infty f_X(x)dx \\
&amp;=c+ n \int_{c}^ \infty x  f_X(x) dx -nc \bar{F}(c)\\
\end{align*}</p>

<p>I think i take a wrong turn somewhere, maybe I shouldn't expand the term in the integral, I still don't quite get where the second integral would come from, to define the CDF in the required inequality... any hints?</p>
",<probability>
"<p>The probability that it will rain on Saturday is 25% and the probability that it will rain on Sunday is also 25%. Is it true that the probability that it will rain on the weekends is 50%. Explain why or why not.</p>

<p>What i tried</p>

<p>I know that it is not true.
Let $P(A)$ represent the probability that it will rain on Saturday, while $P(B)$ represent the probability that it will rain on Sunday. Hence $P(A \cap B)$ will represent the probability that it would rain on both days (weekends) and since $$P(A \cap B)=P(A)+P(B)-P(A\cup B)$$</p>

<p>From the formula above we can see that the sum of  $P(A)$ and  $P(B)$ alone would not add up to $P(A \cap B)$ which means that we simply could not just add the two probabilities together to get $50$%. Since i find this rather counterintutive could anyone provide a simpler and more clearer explanation to this problem.Thanks</p>
",<probability>
"<p>If I have the probability distribution $f(x) = \begin{cases}x &amp; 0 &lt; x &lt; 1 \\ 2-x &amp; 1 &lt; x &lt; 2 \\ 0 &amp; \text{otherwise} \end{cases}$,</p>

<p>how do I calculate $E(X)$, and $E(X^2)$, and therefore $VAR(X)$?</p>

<p>I understand that $E(X)=\int_{-\infty}^\infty xf(x)~dx$, and $E(X^2)=\int_{-\infty}^\infty x^2f(x)~dx$, but don't know how to apply this given that the function takes different forms over $(0,1)$ and $(1,2)$.</p>
",<probability>
"<blockquote>
  <p>In a coorporation there are 3% absence due to sickness each day. 60% of those are men. 30% of the cooporations employees are women. So, </p>
  
  <p>M = the employee are  male<br>
  F = the employee are female<br>
  S = employee is home due to sickness.  </p>
  
  <p>My task is to find P(M) and P(S|M).</p>
</blockquote>

<p>I've tried to look around for some examples to apply to this problem but I can't manage to find one.</p>

<p>I need a hint or something to get started. I'm very rusty in probability-calculation.</p>
",<probability>
"<blockquote>
  <p>Suppose there are three students wanting to meet a Professor in his office. The time $i$-th student takes with the Professor is $Exponential(\lambda_i)$ distributed, and they are independent. Assume all students start talking to the Professor simultaneously. Find the distribution of the time until only one student is left.</p>
</blockquote>

<p>If I denote $T$ to be the desired time and $T_i$ to be the discussion time for $i$-th student, how exactly can I write $\{T\leq x\}$ in terms of $T_i$?</p>

<p>It seems that $\{T\leq x\}=\{T_1\leq x,T_2\leq x\}\cup\{T_2\leq x,T_3\leq x\}\cup\{T_1\leq x,T_3\leq x\}$.</p>

<p>Am I right?</p>
",<probability>
"<p>A player with unlimited money decides to play roulette. He bets $1$ on red, if he loses, he bets $2$, if he loses again he bets $4$ and so on till he wins. Prove that he is guaranteed to make a profit.</p>

<p>I know this is the theory of martingale and I looked for proof online but everything I found was way more complex than what's needed here. Isn't there a formula to prove that after unlimited tries the probability of never winning is $0$?</p>

<p>I understand why it works but I'm having a hard time explaining it using mathematics. Any help would be greatly appreciated.</p>

<p>Thank you</p>
",<probability>
"<blockquote>
  <p>Given $x_1, \ldots, x_N$, independent and all distributed as a
  Gaussian with mean $\mu$ and variance $\sigma^2$. Then, the average
  $$\bar{x} = \frac{1}{N}\sum_{i=1}^Nx_i$$ is distributed as a Gaussian
  with mean $\mu$ and variance $\frac{\sigma^2}{N}.$</p>
</blockquote>

<p>This is a very well-known result. Anyway, I'm looking around to find a proof for this and I'm not having luck.</p>
",<probability>
"<p>I have a problem proving an inequality regarding probabilities.
You may prefer to skip to the definitions and the inequality right away 
without reading the paragraph below.</p>

<p>Suppose there are $n$ light bulbs. Independently, each light bulb is ON with probability $x$ and OFF with probability $(1-x)$. After the random variables realized, i.e., after it is observed which bulbs are on or off, one is smashed and this one is selected randomly among those that are OFF.</p>

<p>Let 
$$\alpha = x^{n-2}$$
be the probability that, given two lights $i$ and $j$ are OFF, all other lights are ON.</p>

<p>Let 
$$\beta_i = \sum\limits_{i=0}^{n-2} {n-2 \choose i}\frac{1}{i+1} x^{n-2-i}(1-x)^i = \frac{1-x^{n-1}}{(n-1)(1-x)}$$
be the probability that, given light $i$ is off and light $j$ is on, light bulb $i$ is smashed.
Similarly, define $\beta_j = \beta_i$.</p>

<p>Let 
$$\beta_{i,j} = \sum\limits_{i=0}^{n-2} {n-2 \choose i}\frac{2}{i+2} x^{n-2-i}(1-x)^i$$
be the probability that either $i$ or $j$ are smashed, given both of them are OFF.</p>

<p>It should hold that
$$ (1-\alpha) (1-\beta_{i,j}) \geq (1-\beta_i) (1- \beta_j) \quad  \forall n &gt; 2, x\in [0,1]  $$
Unfortunately, I am not able to show that this generally holds. It would be great to point me to the basic math that I am missing.</p>
",<probability>
"<p>Let $\{X_t\}_{t\ge 0}$ be a Poisson Process with parameter $\lambda$. Suppose that each event is type 1 with probability $\alpha$ and type 2 with probability $1-\alpha$. Let $\{X^{(1)}_t\}_{t\ge 0}$ the number of type 1 events up until time $t$ and $\{X^{(2)}_t\}_{t\ge 0}$ the number of type 2 events up until time $t$ </p>

<p>Prove that $\{X^{(1)}_t\}_{t\ge 0}$ and $\{X^{(2)}_t\}_{t\ge 0}$ are Poisson Processes with parameter $\lambda \alpha$ and $\lambda(1-\alpha)$ respectively </p>

<p>Furrthermore prove that for each $t\ge 0$ the random variables $\{X^{(1)}_t\}_{t\ge 0}$ and $\{X^{(2)}_t\}_{t\ge 0}$ are independent</p>

<p>My attempt: In order to prove that they are poisson process I will use the next definition:</p>

<p>An stochastic process $\{Y_t\}_{t\ge 0}$ is a poisson process iff:</p>

<p>a) $Y_0=0$</p>

<p>b) It has independent increments</p>

<p>c) $Y_{t+s}-Y_{s}$~$Poisson(\lambda t)$ for any values $s\ge 0$ and $t&gt;0$</p>

<p>a) For any $t\ge 0$ we have: $X_t=X^{(1)}_t+X^{(2)}_t$; we know that $\{X_t\}_{t\ge 0}$ is a poisson process hence $X_0=0$ $\Rightarrow X^{(1)}_0+X^{(2)}_0=0 \Rightarrow X^{(1)}_0=0$ and $X^{(2)}_0=0$</p>

<p>b)Let $n\in \mathbb N$ In this part I need to prove that for any $n$ arbitrary times $0&lt;t_1\le t_2\le...\le t_n$ and states $x_1,...,x_n$
 $$P[X^{(1)}_{t_1}=x_1,X^{(1)}_{t_2}-X^{(1)}_{t_1}=x_2,...,X^{(1)}_{t_n}-X^{(1)}_{t_{n-1}}=x_n]=P[X^{(1)}_{t_1}=x_1]P[X^{(1)}_{t_2}-X^{(1)}_{t_1}=x_2]...P[X^{(1)}_{t_n}-X^{(1)}_{t_{n-1}}=x_n]$$</p>

<p>I don´t know how to Formally prove this part, and I don´t think this is trivial. Any help would be highly appreciated </p>

<p>c) $$P[X^{(1)}_t=k]=\sum_{i=k}^\infty P[X^{(1)}_t=k|X_t=i]P[X_t=i]=\sum_{i=k}^\infty \binom{i}{k}\alpha^i(1-\alpha)^{i-k}{e^{-\lambda t}(\lambda t)^i \over i!}={e^{-\lambda \alpha t}(\lambda \alpha t)^k\over k!}$$</p>

<p>Know I need to compute $P[X^{(1)}_{t+s}-X^{(1)}_t=n]=\sum_{j=0}^\infty P[X^{(1)}_{t+s}-X^{(1)}_t=n|X^{(1)}_s=j]P[X^{(1)}_s=j]$</p>

<p>This part is also giving me trouble because I dont´know what to do from here </p>

<p>I would really apreciate if you can help me with this problem. Also I hope that this won´t be marked as a duplicate because I haven´t seen a formal proof about the splitting poisson process.</p>
",<probability>
"<p>Let $\{ X(t), t \ge 0\}$ be standard Brownian motion.</p>

<p>How do I find Cov$[X(3) - 2X(2), X(4)]$? </p>

<p>The answer is $-1$, but I can't seem to get there no matter what I hit it with. I know that $X(3) \sim N(0,3)$, $2X(2) \sim N(0, 4)$, and $X(4) \sim N(0,4)$. Furthermore, I know that $\text{Cov}[X(s),X(t)] = \min (s,t)$; but this is only for Brownian motion, which isn't preserved (I assume) by the new normal random variable created by $X(3) - 2X(2)$. I imagine that I need to somehow transform $X(3) - 2X(2)$ in a way that preserves Brownian motion so that I can use the above property, but I'm at a loss here.</p>
",<probability>
"<p>A fair die having two faces coloured blue, two red and two green, is thrown repeatedly. Find
the probability that not all colours occur in the first $k$ throws.</p>

<p>My attempt:
Denote by $G_k$ the number of results=green in $k-$tosses, and similarly $R_k$ and $B_k$. Then we have $\mathbb{P}(G_k=0)=(2/3)^k=\mathbb{P}(R_k=0)=\mathbb{P}(B_k=0)$. In this way the answer is $\mathbb{P}(G_kR_kB_k=0)=(2/3)^{3k}$.</p>
",<probability>
"<p>A crabs' life expectancy can be modeled exponentially, and a crab lives 3 months on average.</p>

<p>I am absolutely not sure about this, because there is nothing concerning this in our book, so I guess it was meant to be solved in some obvious/easy fashion, here's what I tried:</p>

<blockquote>
  <p>If it were only one crab, I could simply plug 9 into $\lambda e^{-\lambda x}$, where $\lambda=1/3$. </p>
  
  <p>$60$ is $10\%$ of $600$, so maybe I need to look after what time 90% died, intuitively I would resort to</p>
  
  <p>$$1-e^{-x/3}=0.9$$
  $$0.1=e^{-x/3}$$
  and so on, which would give me $\approx 6.9$ months, and then do <em>something</em> about the remaining $2.1$ months.</p>
</blockquote>

<p>The last thing I was thinking of is to calculate the probability for 540 crabs dying at some point before the 9 month mark, and then taking the converse probability, but that I'd only know how to do with the help of a computer. </p>
",<probability>
"<p>Assume the expected number of transitions (events) it takes until a Markov chain with $G+1$ states ranging from $s=0$ to $s=G$ is completed is $M$. Suppose we have $K$ independent instances of this Markov chain synchronized such that a single event can lead to a transition to all of them. We are interested in the average number of transitions required to complete all of them.</p>

<p>One approach is to model this using a new Markov chain with a state space of size $(G+1)^K$ that corresponds to vectors (0,...0) to (G,...,G), calculate the transition probabilities and then derive the expected number of required transitions. We are already aware of how to do it. However, it is very interesting and insightful to approximate this using $M$ and $K$. Any suggestions is appreciated.</p>
",<probability>
"<p>I have a very basic propability question:</p>

<p>There are 200 socks in my basket. 25 are white, the rest are black. If I pick 10 socks, what's the probability of picking 3 white ones?</p>

<p>I tried to calculate as follows:</p>

<p>P_3 = [25/200 * 24/199 * 23/198] * [175/197 * 174/196 * 173/195 * 172/194 * 171/193 * 170/192 * 169/191]</p>

<p>However, when I create all P_{0-10} and sum them up I don't get a overall probability of 1. So I must be doing something wrong?</p>
",<probability>
"<p>Let $C(\mathbb{R})$ be the set of continuous and bounded functions $\mathbb{R}\to\mathbb{R}$.</p>

<p>Is there a probability measure $p$ on $C(\mathbb{R})$ such $\forall g\in C(\mathbb{R}),\ \forall \varepsilon&gt;0,\ p(\{f\in C(\mathbb{R}), |f-g|&lt;\varepsilon\})&gt;0$ ?</p>
",<probability>
"<p>I want to know if the following distribution has a name or a PDF in the literature. I have:
$$ Z = \left(\sum_{i=1}^{N}{X_i^2}\right)^{\alpha}$$
where $\{X_i\}_{i=1}^{N}$ are independent Gaussian random variables with mean $\mu_i$ and variance $\sigma_i^2$ and $\alpha \in \mathbb{R}$.</p>

<p>Thanks.</p>
",<probability>
"<p>I am looking to solve the following integration. Given $Z_1,Z_2,\ldots,$ are all independent and standard gaussian variable, $Z_i = N(0,1)$.</p>

<p>For the first case, I am interested in the probability $P(Z_1+Z_2&lt; \sqrt{2}\alpha , Z_2+Z_3&lt;\sqrt{2}\alpha)$, or simply 
$$\frac{Z_1+Z_2}{\sqrt{2}} &lt; \alpha$$
$$\frac{Z_2+Z_3}{\sqrt{2}} &lt; \alpha$$</p>

<p>where $\alpha$ is a constant. I can solve this numerically. First question is there analytical solution to this?</p>

<p>Secondly, for higher dimensional case, what is the probability that
$$\frac{Z_1+Z_2+Z_3}{\sqrt{3}} &lt; \alpha$$
$$\frac{Z_2+Z_3+Z_4}{\sqrt{3}} &lt; \alpha$$
$$\frac{Z_3+Z_4+Z_5}{\sqrt{3}} &lt; \alpha$$</p>

<p>Thank you for your help.</p>
",<probability>
"<p>In fair gambler's ruin problem, we already knew that the expected time of winning is $E(\tau_n|\tau_n&lt;\tau_0)=\frac{n^2-k^2}{3}$, where $k$ is how much money we have in the beginning and $\tau_i$ is the first hitting time at $i$. I know how to solve it by using first step analysis in markov chain, but my teacher wants me to use martingale instead, he said it is much easier. I think I have to use optional stopping time theorem, but I don't know how to use it in this problem. Any idea? Thank you so much</p>
",<probability>
"<p>I'm trying to solve a certain problem but I'm stuck at a point.</p>

<p>The problem is:
I have a uniform r.v. $U$ on [$-\pi,+\pi$] and I have a sequence of r.vs $X_1,X_2...$ where $X_k=cos(kU)$.
Then I have $S_n=X_1+X_2+...+X_n$.</p>

<p>The problem is to find what is $lim_{n\rightarrow \infty} P{(|\frac{S_n}n|&lt;\epsilon)}$</p>

<p>My attempt:</p>

<p>I know that $E[\frac{S_n}n]=0$ and its variance is $2\pi^2/n$.</p>

<p>I know that $P{(|\frac{S_n}n|&lt;\epsilon)} =P_{\frac{S_n}n}([-\epsilon,+\epsilon])= \begin{matrix} \int_{-\epsilon}^{\epsilon} f_{\frac{S_n}n}(x) dx \end{matrix}$</p>

<p>Where $f_{\frac{S_n}n}(x)$ is the probability density of $\frac{S_n}n$.</p>

<p>I don't know how to find this probability density: I tried with the characteristic function of the sum but the result is not very ""writable"".</p>

<p>Maybe there is another way to find that probability with $n\rightarrow \infty$?</p>

<p>Thank you</p>
",<probability>
"<p>I asked similar question in <a href=""http://math.stackexchange.com/questions/395666/a-special-random-subset-of-uniformly-distributed-numbers-is-still-uniformly-dist"">A special random subset of uniformly distributed numbers is still uniformly distributed?</a></p>

<p>Here, I slightly change my random number generation method, and want to see whether the sampled numbers are still uniformly distributed.</p>

<p>Assume that I have a value range [1,1000].</p>

<p>Goal: I want to have 10 numbers randomly sampled from [1,1000].</p>

<p>case1:</p>

<pre><code>I sample 20 numbers, a1, ..., a20 from [1,1000].

Then I sample b1, ..., b10 from [a1, a2, ..., a20].

b1, ..., b10 are what I want.
</code></pre>

<p>case2:</p>

<pre><code>I sample 20 numbers, a1, ..., a20 from [1,1000].

Then I sort a1, ..., a20.

For notation simplicity, I assume that after sorting, a1&lt; ...&lt; a20.

Then, for each consecutive pair of numbers, 

I uniformly at random select one number.

Eventually, I can get 10 numbers as well.
</code></pre>

<p>I know the numbers in case1 are uniformly distributed.</p>

<p>Does anyone have idea whether the numbers in case2 are uniformly distributed?</p>
",<probability>
"<p>The probability generating function of $X$ is $G_x(s)=\frac{1}{2}(s^9(1+s^2))$. Find $EX$ and probability distribution function.</p>

<p>$EX=G_x^{'}(s)=\frac{1}{2}(9s^8+11s^{10})$</p>

<p>How about pdf? Do I need to expand $G_x(s)$ function with Taylor series?</p>
",<probability>
"<p>Two of my facebook friends had their birthdays on the same day</p>

<p>The first guy's name was ""Wael Toujeni""</p>

<p>The second guy's name was ""Wael Jeni""</p>

<p>How do I calculate the probability of this event happening?</p>

<p>The event: two of your facebook friends have the same birthday, and have similar names except an n characters difference in their last names.</p>
",<probability>
"<p>I have a two week holiday after finishing this semester and I was hoping to study something. Ideally, I would like to study something that I can benefit from in any upcoming courses I take. I'm thinking of either fourier series or probablity theory. Currently I'm leaning towards Fourier because it's very relevant to electrical engineering, however, Probablity theory seems intriguing to me (also opens up interesting areas such machine learning).</p>

<p>So how relevant is probablity theory to undergraduate electrical engineering ?!</p>
",<probability>
"<p>If we know that the two random sequences $\{X_n\}$ and $\{Y_n\}$
$$
X_n \stackrel{d}{\longrightarrow} N(0,\sigma_1^2)\\
Y_n \stackrel{d}{\longrightarrow} N(0,\sigma_2^2)
$$
and $\mathrm{cov}(X_n,Y_n) \to 0$, can we conclude that the joint random vectors
$$
\pmatrix{X_n \\ Y_n} \stackrel{d}{\longrightarrow} N(0,\mathrm{diag}\{\sigma_1^2, \sigma_2^2\})
$$</p>

<p>Is there any counterexample or proof? I have considered using the characteristic function with expansion but did not work it out....</p>

<p>Many thanks to any help!</p>
",<probability>
"<p>Consider the normal distribution. We know that $$p(x| \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} $$</p>

<p>The kernel is $$ p(x| \mu, \sigma^{2}) \propto e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} $$ omitting the part that isn't a function of $x$. Why write $p(x|\mu, \sigma^{2})$ like this? </p>
",<probability>
"<p>A cube is painted on all its faces. It is then cut into 64 smaller identical cubes, which are then thoroughly. What is the probability that 2 randomly chosen smaller cubes have exactly 2 coloured faces each?</p>

<p>I have a doubt , there will be 24 <strong>identical</strong> cubes whose 2 faces will be painted, 8 <strong>identical</strong> cubes with 3 faces painted,  24 <strong>identical</strong> cubes with one face painted and 8 <strong>identical</strong> cubes with no face painted. 
Now total cases to pick 2 smaller cubes should be 4C2 not 64C2 .
correct me if I am wrong</p>
",<probability>
"<p>Is there a way to determine the maximum percentage of values that fall below the average in a given sample?  How would someone go about this?  How does this relate to what Markov's inequality and Chebyshev's inequality are saying?</p>
",<probability>
"<p>Let $P_{XY}$ be the joint probability distribution of discrete random variables $X$, $Y$. Then I would like to prove the following inequality:</p>

<p>$$
\sum_{y}\max_xP_{XY}(x,y)\leq |Y|\max_xP_X(x)
$$</p>

<p>where $P_X$ is the marginal distribution of random variable $X$ and $|Y|$ is the cardinality of random variable $Y$. Hints are also welcome.</p>
",<probability>
"<blockquote>
  <p>$X_n$ is a sequence of random variables, and $\{\varphi_n(t)\}$ is the corresponding sequence of characteristic functions. Show that $X_n \stackrel{d}{\longrightarrow} 0$ iff $\{\varphi_n(t)\}$ converges to 1 in some neighbourhood of $t=0$.</p>
</blockquote>

<p>The necessity follows immediately from Levy's continuity theorem. I am struggling with showing the sufficiency: it remains to show that $\varphi_n(t) \rightarrow 1$ for all $t \in \mathbb{R}$, but how to do that? Appreciate for any help!</p>
",<probability>
"<p>Assuming I start with $n$ dice that have been rolled once, is it beneficial to choose the higher dice when I roll less than $n$ dice again (assuming I want a high roll)?</p>

<hr>

<p>In some board games, dice play a big part. And depending on the circumstance, a different amount of dice is used. In this particular case, I'm assuming that the higher the resulting number, the better. Also, I'm assuming not perfectly fair dice, since those rarely happen in everyday play.</p>

<p>Here's my logic: If I roll 7 dice, and 4 are high (5/6) while 3 are low (1/2), should I pick the high dice if I only need to roll 4? (Assuming that this is the first time I see the dice rolled).</p>

<p>This seems as if it might make sense because if the dice aren't fair and the only result we've seen so far is a higher number, it might make it seem as if that number is more likely to occur.</p>

<p>On the other hand, this almost seems like the gambler's fallacy. Except that fallacy assumes perfect dice.</p>

<p>So should I pick the higher dice? Or does it really not matter? If it does matter, by how much?</p>
",<probability>
"<p>$N(t)$ is a Poisson process with parameter  $\lambda&gt; 0$, and $X_1,X_2,...$ are independent and identically distributed random variables with a common mean and positive variance. Let
$$L(t)=\sum_{i=1}^{N(t)}X_i.$$
Find $E[L(t)|N(t) = n]$.</p>

<p>Any help with this is appreciated.</p>
",<probability>
"<p>I know that the events $x_1 &gt;0$ and $x_2 &gt;0$ are not independent, but I can't think of a way to find a conditional probability so I can solve this.</p>

<p>Thanks!</p>
",<probability>
"<p>I try to prove that according to binomial distribution $P(X=k)={n \choose k}p^k(1-p)^{n-k}$ the maximum probability $P(X=k)$ is achieved at maximum likelihood, i.e. $p=\frac{k}{n}$.</p>

<p>Let's apply $\log$ and take the first derivative.</p>

<p>$$\log P(X=k) = \log {n \choose k}+k \log p+ (n-k)\log (1-p)$$</p>

<p>$$\frac{d \log P(X=k)}{dp} = \frac{k}{p}-\frac{n-k}{1-p}=0 \quad \iff \quad  p=\frac{k}{n}$$</p>

<p>The problem is to show that what I found is indeed the global maximum, i.e. I need to show that the second derivative is negative everywhere.</p>

<p>I would appreciate if someone could help me with the second derivative.</p>

<p>The second derivative</p>

<p>$$\frac{d(\frac{k}{p}-\frac{n-k}{1-p}) }{dp} = -\frac{k}{p^2}-\frac{n-k}{(1-p)^2}$$</p>

<p>it's negative because $n&gt;k$</p>
",<probability>
"<p>I have some difficulties with the following exercise in combinations:</p>

<blockquote>
  <p>There are $8$ beans in the box: $6$ white beans, $2$ green beans. Two players one by one pick $2$ beans; first player one picks $2$ beans, after that player two picks $2$ beans. For every green bean that player picks he gets $5$ points.</p>
  
  <p>What's the expected number of points for player one? What's the probability that player two picks only one green bean?</p>
</blockquote>

<p>Solution:</p>

<p>$$E(\text{points of player one}) = 4 \cdot 5 \cdot \frac{2}{8} \cdot \frac{6}{7} + 10 \cdot 2 \cdot \frac{2}{8} \cdot \frac{1}{7}$$</p>

<p>Unfortunately I didn't find any good way to fir the binomial distribution here. </p>

<p>I don't get any idea how to answer the second question.</p>
",<probability>
"<p>Probability was never included in my high school classes, so I'm trying to learn it now from the internet. The downside of this is that you don't get anyone grading your work and catching flaws. This is something I'm really uncertain about, so if someone could just point out my mistake or verify that I've understood it, I'd be appreciative.</p>

<p>Just for fun, the topic I chose to practice with was working out the chance that any of the children on <em>19 Kids and Counting</em> (a TV series about a very conservative family raising children with strict traditional roles) would be LGBT. The probability of being gay is roughly $3\%$ ($0.03$) and the probability of being bisexual is roughly $4\%$ ($0.04$).</p>

<p>So combined, this makes $7\%$ ($0.07$). The probability of being heterosexual is $93\%$ ($0.93$). Would I be correct in saying that the probability of at least one being gay or bisexual would be $1-A^{B}$, where $A$ is the probability of being born heterosexual ($0.93$) and $B$ is the number of family members ($25$) -- or $83.7\%$?</p>

<hr>

<p>As a followup question: there's a controversial finding by some studies, <a href=""https://en.wikipedia.org/wiki/Fraternal_birth_order_and_male_sexual_orientation"" rel=""nofollow"">the fraternal birth order effect</a>, that suggests that for each older brother a man has, the probability that he is gay rises by ~$38\%$, hypothesised to be due to hormonal influences during fetal development. There are $10$ boys in the family. How would I include this detail when calculating the probability? </p>
",<probability>
"<p>So when dealing with the Bayes' Rule and Binomial distributions,  the value $p^k(1-p)^{n-k}$ loses precision and becomes 0 when $n$ and $k$ are large(noting that the binomial coefficient can be safely ignored since it cancels out). For example if $n = 500\,,\,k=250\,\,$for  $p= 0.05$  , then my TI-83 calculator returns 0.</p>

<p>I found out about a ""fix""  for this using the following expression that replaces $p^k(1-p)^{n-k}$.</p>

<p>$e^{k\,ln \,p + (n-k)ln(1-p)-max_i(kln(p_i) + (n-k)ln(1-p_i)}$</p>

<p>I think the $p_i$ in the expression is suppose to represent any one of prior probabilities. Can anyone explain why this works ?</p>
",<probability>
"<p>""I'm really sorry that I'm asking a basic question, I tried to understand but couldn't understand completely""</p>

<p>These are the 3 parameter results of rolling a single die.
Expectation  E(x)=3.5</p>

<p>Variance Var(x)=2.92</p>

<p>Standard deviation SD=1.709</p>

<p>Expectation is nothing but the average of outcomes of die when we perform infinite trials, that means if we roll a die infinite times and take out the mean of outcome , its value is close to 3.5. that's what I understood.
I searched in Google and read about variance ,standard deviation.
I didn't understand anything about those two, but left with this sentence ""The variance measures how far each number in the set is from the mean""
Can anyone please explain what do those two values indicate regarding to my die problem??
Thanks in advance</p>
",<probability>
"<p>EDIT:</p>

<p>Let $X,Y$ be random variables over some probability space with  joint distribution  $P$. Then the mutual information between two random variables is defined as</p>

<p>$I(X;Y):=\sum\limits_{(x,y)\in\text{supp}(P)}P(x,y)\log\frac{P(x,y)}{\sum\limits_rP(r,y)\sum\limits_sP(x,s)}$. </p>

<p>It is clear that mutual information $I(X;Y)$ can now be viewed as a function of joint distribution $P$. Hence we denote hereafter the mutual information associated with a joint distribution $P$ between two random variables as $I(P)$.</p>

<p>Now to the problem definition:</p>

<p>Assume $P_1,P_2$ be two joint distributions over $\{1,2,3...,N \}\times \{1,2,3,...N\}$ and hence $P_1,P_2 \in \mathbb{R}^{N \times N}$. Assuming $Q= \frac{P_1+P_2}{2}$, which again defines a new joint distribution over the same set over which $P_1,P_2$ were defined, I am trying to bound the mutual information $I(Q)$ using $I(P_1)$ and/or $I(P_2)$.</p>

<p>I have a bound on $I(Q)\ge\sum \limits_{i,j}\frac{P_1(i,j)}{2}\log{\frac{P_1(i,j)}{\gamma_{ij}}}+\sum \limits_{i,j}\frac{P_2(i,j)}{2}\log{\frac{P_2(i,j)}{\gamma_{ij}}}$ </p>

<p>with</p>

<p>$\gamma_{ij}:= \left( \sum \limits_s \frac{P_1(i,s)+P_2(i,s)}{2} \right )$$\left(  \sum \limits_r \frac{P_1(r,j)+P_2(r,j)}{2} \right)$. With this I am interested in lower bounding $\frac{1}{\gamma_{ij}}$. With some calculations I landed a lower bound of the form, </p>

<p>$\frac{1}{\gamma_{ij}}\ge \frac{1}{\sum \limits_rP_1(i,s)\sum \limits_rP_2(r,j)+\frac{1}{4}(\text{sum of three positive terms all $\le$ 1})}$</p>

<p>However, I am interested in knowing is there anyway I can further get a  lower bound of the RHS of the  above expression such that I have $\frac{1}{\sum \limits_rP_1(i,s)\sum \limits_rP_2(r,j)}$ term left.</p>

<p>The motivation is to get a neat lower bound on $I(Q)$ involving mutual informations of $P_1$ and $P_2$.</p>

<p>Any help/idea will be highly appreciated.</p>

<p>Thanks</p>
",<probability>
"<p>Give the density $ f_{X}(x)=\frac {x^2} {9} $ with $0 &lt; x &lt; 3$</p>

<p>I should calculate with transformation the density of $Y=X^3$.</p>

<p>Is my calculation correct?</p>

<p>$f_{Y}(y)=f_{X}(g^{-1}(y))\mid\frac{d}{dy}g^{-1}(y)\mid = \frac {(y^{\frac{2}{3}})^2} {9}*\frac{1}{3y^2} =\frac{1}{27}$</p>
",<probability>
"<p>Does $Z_n=\sum_{k=1}^{n}\sqrt{k}X_k$ satisfy the strong law of large numbers if $ X_n: \begin{matrix}-\frac{1}{n} &amp; \frac{1}{n} \\ \frac{1}{2} &amp; \frac{1}{2} \end{matrix}, n=1,2,...$ are independent random variables. </p>

<p>I have the following theorems, but I cannot prove this, I have tried all which I understand, the theorems I have are:</p>

<p>1.) Strong Law of large numbers states that the sequence $X_1,X_2,...$ must satisfy:</p>

<p>$$\frac{1}{n}\sum_{k=1}^{n}(X_k-EX_k)\to^{a.s.}0, n\to \infty$$</p>

<p>2.) Kolmogorov Law: If $(X_n)$ independent random variables, such that $\sum_{n=1}^{\infty} \frac{\text{Var}(X_n)}{n^2}&lt;\infty$, then the strong law of large numbers is satisfied.</p>

<p>3.)Borels:  If $ S_n:\mathcal B(n,p)$ (binomial distribution), then $$\frac{S_n}{n}\to^{a.s.}p, n \to \infty$$</p>

<p>or the consequence: </p>

<p>Let $X_n$, sequence of independent random variables, equally distributed, such that $EX_k=a$ and $\text{Var}X_k= \omega^2, k=1,2,3... \implies$</p>

<p>$$\frac{1}{n}\sum_{k=1}^{n}X_k\to^{a.s.}a, n\to \infty$$</p>
",<probability>
"<p>The strong law of large numbers states that the sample average converges almost surely to the expected value $\overline{X}_n\ \xrightarrow{\text{a.s.}}\ \mu \qquad\mathrm{when}\ n \to \infty$ .</p>

<p>That is,$$\Pr\left( \lim_{n\to\infty}\overline{X}_n = \mu \right) = 1.$$</p>

<p>I want to ask whats the domain of the random variable $\overline{X}_n$, given that all $X_n$ have same domain $\Omega$?</p>

<p>For the coin tossing problem $\Omega =\{H,T\} $ and $X_n(H)=1 \quad and \quad X_n(T)=0 \quad\forall n $ so, if the domain of $\overline{X}_n$ was $\Omega$ the $\overline{X}_n(H)=1 \quad \overline{X}_n(T)=0 \quad \forall n$, so $\Pr\left({\omega \in \Omega : \overline{X}_n(\omega)=1/2}\right)=0$ , which is not what strong law says.</p>

<p>I want to know whats the domain of this random variable $\overline{X}_n$?</p>
",<probability>
"<p>Suppose that you spin a dial that freely moves  marked out in 360 degrees in steps of 5 degrees. Whats the proability that the dial will land somewhere between 5 degrees and 300 degrees. Al so how do you calculate the answer </p>
",<probability>
"<p>Suppose we have discrete random variable given by
\begin{align*}
P(X=x_i)=\frac{1}{N},  i=1...N
\end{align*}
and Gaussian r.v. $Z \sim \mathcal{N}(0,1)$. Assume $Z$ and $X$ are independent.
Suppose $X$ and $Z$ form an new r.v. $Y$ give by
\begin{align*}
Y=X+Z
\end{align*}</p>

<p>I am interested in computing $E[X|Y]$?</p>

<p>Here are some of the distributions that I have computed:
\begin{align*}
f_Y(y)&amp;=\frac{1}{N} \sum_{i=1}^N \frac{1}{\sqrt{2\pi}} e^{-(y-x_i)^2/2}\\
f_{Y|X}(y|x_i)&amp;=\frac{1}{\sqrt{2\pi}} e^{-(y-x_i)^2/2}=f_Z(z)\\
f_{X|Y}(x_i|y)&amp;=???
\end{align*}</p>

<p>But I am not sure how to proceed next. For example does density $f_{X|Y}(x_i|y)$ even exists?
Thank you for any help or suggestions.</p>
",<probability>
"<p>I'm really new to statistics and probability so sorry if this is a really basic question. I just wasn't sure how to do it. I tried looking it up but can't find much information. </p>

<p>If I'm given a cdf for a random variable X, how do I find it for a new random variable Y which was in terms of X? Do I just plug in Y? My example is following:</p>

<pre><code>FX(x) = 1 −1/x
for 1 &lt; x &lt; ∞
Find the cdf for the new random variable Y = -X + 2
</code></pre>
",<probability>
"<p>Let $III$ be a root node of a tree. It may have between $0, \cdots , B_1$ children type $X$, and $1 \cdots B_2$ children type $Y$ with probability $\beta_{i,X}$ and $\beta_{i,Y}$, respectivelly. Let $p_X$ be probabilty a node chidren type $X$ evaluate $1$. And $p_Y$ is probabilty a node chidren type $Y$ evaluate $1$.</p>

<p>Find the probability the root node $III$ evaluates $0$? </p>

<p><a href=""http://i.stack.imgur.com/dQXo9.png"" rel=""nofollow""><img src=""http://i.stack.imgur.com/dQXo9.png"" alt=""enter image description here""></a></p>

<p>My solution is</p>

<p>The prob. the root node $III$ evaluates $0$ is</p>

<p>$$P(III=0)=P(X=0)P(Y=0)$$ </p>

<p>We have, $p(.)$ is prob. of a node (.) , $P(.)$ is prob. of a group node</p>

<p>$$p(X=0)=1-p(X=1); P(X=0)=\sum_{j=0}^{B_1} \beta_{j,X} (1- p_X)^j$$
$$p(Y=0)=1-p(X=1); P(Y=0)=\sum_{i=1}^{B_2} \beta_{i,X} (1- p_Y)^i$$</p>

<p>Finaly, we got, </p>

<p>$$P(III=0)=P(X=0)P(Y=0)=\sum_{j=0}^{B_1} \beta_{j,X} (1- p_X)^j \sum_{i=1}^{B_2} \beta_{i,X} (1- p_Y)^i$$</p>

<p>However, I found that someone provides another solution as</p>

<p>$$P_{1}(III=0)=\sum_{i=1}^{B_1+B_2-1} \sum_{j=0}^{i-1} \bigg( \beta_{j,X} \big(1-y_X \big)^j \beta_{i-j,Y} \big(1-y_Y \big)^{(i-j)} \bigg) $$</p>

<p>My question is does $P(III=0)$ same with $P_{1}(III=0)$. If not, which one is correct? Thank all</p>
",<probability>
"<p>Ill give some background first before asking questions.(the text below is straight out of the book)</p>

<p>Each individual in the population is assumed give birth at an exponential rate of $\lambda$ in addition ,there is a an  exponential rate of increase  $\theta$ due to external source of immigration. Hence the total birth rate where there are $n$ persons in the system is $n\lambda + \theta$ . Deaths are assume to occur at an exponential rate $\mu$  for each member of the population, so $\mu_n = n\mu$.</p>

<p>Let $X(t)$ denote the population size at time $t$. Suppose $X(0)= i$ and let $M(t) = E[X(t)]$ . So they will determine $M(t)$ by deriving and then solving a differential equation that is satisfies.</p>

<p>we start by deriving an equation for $M(t+h)$ by conditioning on $X(t)$ this yields:</p>

<p>$M(t+ h) = E[X(t+h)] = E[E[X(t+h)\vert X(t)]]$ </p>

<p>Now,given the size of the population at time $t$ then, ignoring  events whose probability is $o(h)$, the population at time $t+h$ will either increase in size by 1 if a birth or immigration occurs in $ (t,t+h)$ , or, decrease by 1 if a death occurs in this interval, or remain the same if neither of these two possibilities occurs that is given $X(t)$</p>

<p>$X(t+h)$=
\begin{cases}
 X(t) + 1 , with- probability  &amp; [\theta + X(t)\lambda]h + o(h) \\
 X(t) - 1, with-probability &amp; X(t)\mu h + o(h)\\
 X(t), with-probability &amp; 1-[\theta + X(t)\lambda + X(t)\mu]h +o(h)
\end{cases}</p>

<p>therefore,
$E[X(t+h) \vert X(t)] =  X(t) + [\theta + X(t)\lambda - X(t)\mu]h + o(h)$</p>

<p>.....
.....
.....(text continues)</p>

<p>$\textbf{questions:}$</p>

<p>$\textbf{(1)}$,  i understand the first two cases , but the last case i don' t quiet get: $X(t)$, with-probability  $1-[\theta + X(t)\lambda + X(t)\mu]h +o(h)$. can someone explain this?</p>

<p>$\textbf{(2)}$How do i interpret this statement: $E[X(t+h) \vert X(t)] =  X(t) + [\theta + X(t)\lambda - X(t)\mu]h + o(h)$</p>

<p>these are my two questions.</p>
",<probability>
"<p>Assume that $H$ is a separable Hilbert space and $\{e_k\}$ is an orthonormal and complete basis of $H$.</p>

<p>$\{\xi_k\}$ is a sequence of normal Gaussian random variables that are independent.</p>

<p>It seems that $\sum_{k=1}^{\infty}\xi_k \langle e_k,h\rangle,\ h\in H$ is square-integrable, but I am not sure how to prove. Who can give me some hints?</p>

<p>Thanks a lot.</p>
",<probability>
"<p>I'm having a tough time finding the next example's sample space's size:</p>

<p>We have at our disposal the following: the three letters $\;a,b,c\;$ , and the five digits $\;1,2,3,4,5\;$ . We have to form with them all the possible passwords of six ($\;6\;$) characters, under the condition that there <em>must be</em> at least one letter and at least one number in each password. Other than this there are no more restrictions (and, thus, one can repeat at will numbers, letters and etc.)</p>

<p><strong>What I tried:</strong> I fixed one letter and one digit, thus getting $\;3\cdot 5=15\;$ possibilities fir each fixation (?), and for the other $\;4\;$ characters needed in the password I have $\;8\;$ possibilities for each (as repetitions are allowed), rendering $\;8^4\;$ possibilities. </p>

<p>I further thought of multiplying the above by two to indicate that the first two characters I chose can be interchanged, and finally I multiplied by $\;\binom 62=15\;$ possibilities to place them within the six places of the password.</p>

<p>All in all, the above says there are $\;15\cdot8^4\cdot2\cdot\binom65\;(**)$ possibilities</p>

<p>All fine...but there are repetitions! For example, suppose for simplicity I have fixed the characters $\;a,1\;$ , and say I place them in positions $\;2-3\;$ . But then one possible password is $\;1-a-1-1-1-1\;$ , yet if I swap my fixed characters in these positions I get the password $\;1-1-a-1-1-1\;$ , which already appears when the positions $\;2-3\;,\;2-4\;,\;2-5\;,$ etc. are chosen.</p>

<p>Thus, the number (**) abo0ve is way over the actual number I'm seaching, and I'm about to become a friar in Malta out of desperation, so any help will be greatly appreciated.</p>
",<probability>
"<blockquote>
  <p>Suppose $X\in N(0,1)$. Show that $X$ and $|X|$ are not jointly continuous.</p>
</blockquote>

<p>I am not sure how I can approach this problem. But the following method seems plausible to me:</p>

<blockquote>
  <p>$$P(X\leq x||X|=u)=\lim_{a\to0}\dfrac{P(X\leq x,|X|\in (u-a,u+a))}{P(|X|\in (u-a,u+a))}$$</p>
</blockquote>

<p>Of course we must have $u&gt;0$. Now let $-u&lt;x&lt;u$ then after some stage, I will get $a$ so small that $x$ will not belong to $(u-a,u+a)$ or $(-u-a,-u+a)$. In that case, the intersection of $(-\infty,x)$ and $(-u-a,-u+a)\cup(-u-a,-u+a)$ will be $(-u-a,-u+a)$ due to which the result becomes:</p>

<blockquote>
  <p>$$\lim_{a\to0}\dfrac{P(X\in(-u-a,-u+a))}{P(X\in(-u-a,-u+a))+P(X\in(u-a,u+a))}=\lim_{a\to0}\dfrac{\Phi(-u+a)-\Phi(-u-a)}{\Phi(-u+a)-\Phi(-u-a)+\Phi(u+a)-\Phi(u-a)}=0.5$$</p>
</blockquote>

<p>Now for $x&lt;-u$, the intersection $\{X\leq x\}$ with $\{|X|\in(u-a,u+a)\}$ is null, hence the numerator is $0$, so the probability is $0$. If $x&gt;u$ then the probability is $1$.</p>

<p>Hence $P(X\leq x||X|=u)=0$ if $x&lt;-u$, is $0.5$ if $-u&lt;x&lt;u$ and is $1$ if $x&gt;u$. So it has two jumps and cannot be a continuous distribution function, and hence cannot have a density. But this density is precisely the conditional density of $X$ on $|X|$, which therefore does not exist. This can happen only when $f_{X,|X|}(x,u)$ does not exist, which shows there is no joint density of $X$ and $|X|$.</p>
",<probability>
"<p>If we define absolutely continuous random variables by Lebesgue integrals &amp; Lebuesgue measures, i.e.</p>

<p>$$F(t) = \int_{-\infty}^{t} f(x) d x$$</p>

<p>for some Lebesgue integrable $f(x)\ge 0$, is it always the case that $F$ is continuous? I know this is true for Riemann integrals due to the fundamental theorem of calculus. Forgive me if this is a stupid question, I'm doing a probability course with very little measure theory in it.</p>
",<probability>
"<p>What is the likelihood of a fair coin given that it has landed heads up 10 times?</p>

<p>You have a fair coin or a double-headed coin...</p>

<p>$\mathsf P($Fair$\mid 10$ heads$) = \dfrac{(1/2)(1/2^{10})}{(1+2^{10})/(2\cdot 2^{10})}= \dfrac{(1/2)(1/1024)}{1025/2048}$</p>

<p>Is this the correct procedure for setting up this problems?</p>

<p><strong>You have a fair coin or $70\%$ weighted coin favoring heads...</strong></p>

<p><strong>What is the likelihood of a fair coin given that it has landed heads up 10 times?</strong></p>
",<probability>
"<p>So there are 12 files in total and 3 of them contain viruses. If a file with a virus is selected, it is removed and a new file is then selected. What is the expected number of files that need to be selected to get a virus free computer? </p>

<h3>Progress</h3>

<p>I thought it was an expected value problem. So summation $\sum X(\xi)p(\xi)$ but that doesn't make sense.   </p>

<p>Is this correct?  Let n be the number of viruses, then the result is $n(1/1 + .. +1/n)$ $$3(1/1 + 1/2 +... + 1/12) = 9.3096$$  </p>
",<probability>
"<p>According to <a href=""http://en.wikipedia.org/wiki/Characteristic_function_%28probability_theory%29"" rel=""nofollow"">Wikipedia</a>, a characteristic function completely determines the properties of a probability distribution. This means it must be unique. However, the definition given is:</p>

<p>$$
\text{Char of }X (t)=E[e^itX]$$</p>

<p>Now $e^{iz}$ repeats for every $2 \pi$ increase in $z$. So how can it be unique?</p>
",<probability>
"<p>$A, B, C$ are independently sampled from an uniform distribution in $[0, 1]$.</p>

<p>We know $P(A &gt; B) = 0.7, P(B &gt; C) = 0.6$, what is $P(A &gt; C)$?</p>

<p>Is this a well defined problem? Does it have a sensible answer?</p>

<p>EDIT: Suppose we have two careless observers.
An observer observes $A &gt; B$ and there are 70% probability that she is right.
Another observer observes $B &gt; C$ and there are 60% probability that she is right. So what is the probability of $A &gt; C$ in the underlying event?</p>
",<probability>
"<p>Here $G_{n,p}$ represents the Erdős-Rényi random graph model, where the graph has order $n$ and each edge is added independently with probability $p$. I am faced with proving the following claim:</p>

<blockquote>
  <p>Show that there is a constant $c&gt;0$ such that, for every $p$ we have: </p>
  
  <p>$\mathbb{P}(G_{n,p}$ is disconnected) $\leq c \mathbb{P}(G_{n,p}$ has an isolated vertex).   $\,\,\,(*)$</p>
</blockquote>

<p>From the appearance of the question I think it is meant to be interpreted as asking ''in the limit $n \to \infty$''. It is clear that $\mathbb{P}($a fixed vertex of $G_{n,p}$ is isolated$)=(1-p)^{n-1}$. It is easy to calculate the expected number of isolated vertices using this, but I'm not convinced that helps.</p>

<p>As a last thought, a followup to the question asks ""What value of $c$ would be acceptable""? It is therefore probably not the case that a valid choice of $c$ will actually be obtained in the proof, although it may be reasonably clear how to calculate one; perhaps that clarifies the nature of the solution a little. Many thanks for your help.</p>

<p><strong>Edit:</strong> Update - I have thought a little more about it, and I have the following theorem we can hopefully make use of (if anyone is willing to help me!): suppose $p = \frac{\log{n}+\gamma(n)}{n}$ and $\gamma(n)$ grows at most slowly (say $o(\log \log n)$); then </p>

<blockquote>
  <p>if $\gamma(n) \to +\infty$, $\mathbb{P}(G_{n,p}$ disconnected)$\to 0$, </p>
  
  <p>if $\gamma(n) \to -\infty$, $\mathbb{P}(G_{n,p}$ disconnected)$\to 1$, </p>
  
  <p>if $\gamma(n) \to k$, $\mathbb{P}(G_{n,p}$ disconnected)$\to 1-e^{-e^{-k}}$.</p>
</blockquote>

<p>Now in the first case, being connected implies no isolated vertex, so $(*)$ holds with any constant $c$ since both probabilities are 0. Likewise, in the second case, the graph is almost surely disconnected: while this doesn't immediately imply that an isolated vertex exists, we can hopefully say for $X:=\#$ of isolated vertices,</p>

<p>$\mathbb{E}(X)=n(1-p)^{n-1} = n(1-\frac{\log{n}+\gamma(n)}{n})^{n-1} \sim ne^{-(\log{n}+\gamma(n))} = e^{-\gamma(n)} \to \infty$.</p>

<p>I <strong>think</strong> this last step holds but it may depend on $\gamma$: in general I'm not sure for which functions $(1+\frac{f(n)}{n})^n \to e^{f(n)}$, I know this is true for the log term but maybe not if $\gamma$ grows very fast (though obviously it can't grow any faster than $1-\frac{\log{n}}{n}$ otherwise we would have $p&gt;1$).</p>

<p>We can also calculate the second moment and get $\mathbb{E}(X^2)-\mathbb{E}(X)^2 \sim e^{-\gamma(n)}$ and deduce that with high probability there is an isolated vertex. Thus again, both probabilities are equal and we can take (e.g.) $c=1$. </p>

<p>The <em>hard</em> case is where $\gamma(n) \to k$: in this case we can reapply the same method to get $\mathbb{E}(X) \sim e^{-k}$, a constant. We can calculate again $\mathbb{E}(X^2) -\mathbb{E}(X)^2 \sim e^{-k}$, and use Chebyshev's inequality to calculate $\mathbb{P}(X=0) \leq e^{-k}/e^{-2k} = e^k$. If $k&lt;0$, then this gives us an actual bound on the probability; otherwise we just get $\mathbb{P}(X=0) \leq 1$. </p>

<p>Supposing $k&lt;0$ then; we can rewrite as $s:=\mathbb{P}(X&gt;0)=\mathbb{P}$(isolated vertex)$\geq 1-e^k$, $d:=\mathbb{P}($disconnected) and using the fact $d =1- e^{-e^{-k}}$ and a little rearranging I think we get out the inequality $d \leq 1-\exp\left(\frac{1}{s-1}\right)$. We can then find a $c$ which works by applying the lower bound to $s$ in terms of $k$ then looking at the values of $c$ such that $cx \geq 1-\exp\left(\frac{1}{s-1}\right),\,x \in [1-e^k,1]$. However, this is only for fixed $k$! If we try and do this for <em>every</em> $k&lt;0$ (i.e. every probability of this type) simultaneously, then we find that $c$ must be arbitrarily large. What's worse, this method doesn't work at all for $k \geq 0$ where we don't have a lower bound for $s$: in this case $s$ can be arbitrarily small and we can't pick a $c$ big enough to always work. So close and yet so far. </p>

<p>I am aware this question's length has spiralled out of control, so apologies for that - I know there's a good chance Math.SE is not going to provide me with an answer to this one. Nevertheless, it says add your working and this is what I managed to do! A proof which works for all <em>slowly</em> decreasing $\gamma$ or $\gamma \to k \in (-\infty,-\epsilon],$ any $\epsilon &gt; 0$. </p>

<p>I have a strong suspicion this is not how I was meant to try and tackle the question, but tragically this is the best I could do so far. Thank you in advance to anyone who actually reads through all this!</p>
",<probability>
"<p>I've found this formula in a blog, it is in the answer to one question. But I don't know how to prove this:</p>

<blockquote>
  <p>Let $x_n$ be a sum of $n$ i.i.d. Bernoulli random variables with parameter $1/2$. Let $q\geq 2$.
  Show that 
  $$
E((2x_n-n)^q)=\sum_{k=0}^n{n \choose k}2^{-n}(2k-n)^q
$$</p>
</blockquote>

<p>Thank you for your help.</p>
",<probability>
"<p>The formula for the expected value in a binomial distribution is:</p>

<p>$$E(X) = nP(s)$$
where $n$ is the number of trials and $P(s)$ is the probability of success.</p>

<p>The formula for the expected value in a hypergeometric distribution is:</p>

<p>$$E(X) = \frac{ns}{N}$$
where $N$ is the population size, $s$ is the number of successes available in the population and $n$ is the number of trials.</p>

<p>$$E(x) = \left( \frac{s}{N} \right)n $$
$$P(s) = \frac{s}{N}$$
$$\implies E(x) = nP(s)$$</p>

<p>Why do both the distributions have the same expected value? Why doesn't the independence of the events have any effect on expected value?</p>
",<probability>
"<p>There are $m$ normally distributed, independent random variables $N_1, \ldots, N_m$ with distinct means $\mu_1, \ldots \mu_m$ and standard deviations $\sigma_1, \ldots, \sigma_m$. Then, we get a permutation of the numbers $\{1, \ldots, m\}$. How can we efficiently compute, numerically, the (log) probability of observing the random variables in same ordering as this permutation?</p>

<p>An example:</p>

<ol>
<li>we have four independent random variables $N_1, N_2, N_3, N_4$, all with different means and variances. </li>
<li>We are given the permutation (3, 1, 2, 4).</li>
<li>What's $\Pr(N_3 &gt; N_1 &gt; N_2 &gt; N_4)$?</li>
</ol>

<p>A closed-form solution is not necessary, but computing the solution using an efficient algorithm with good accuracy is. Also, it's probably necessary to compute a log probability due to the fact that when the number of variables becomes large, computing the actual probability will result in a floating-point underflow. </p>

<h3>Some starting points, perhaps...</h3>

<p>The most direct way to compute this value, using the example above, is evaluating one of the following integrals, which I believe are equivalent:</p>

<p>$$ \int_{-\infty}^\infty \int_{n_4}^\infty \int_{n_2}^\infty \int_{n_1}^\infty p(n_1)p(n_2)p(n_3)p(n_4)\ dn_3 dn_1 dn_2 dn_4 $$</p>

<p>$$ \int_{-\infty}^\infty \int_{-\infty}^{n_3} \int_{-\infty}^{n_1} \int_{-\infty}^{n_2} p(n_1)p(n_2)p(n_3)p(n_4)\ dn_4 dn_2 dn_1 dn_3 $$</p>

<p>Where $p(n_i)$ is the density function of the variable $N_i$. However, when I tried to implement this numerically, it is inefficient, prone to inaccuracy, and runs into underflow errors when the number of variables gets large. <strong>If you think you can compute this integral in an acceptable way, please do post your answer!</strong></p>

<p>From one of the answers below, we observe that it's possible to compute $\Pr(N_3 &gt; N_1 &gt; N_2 &gt; N_4)$ directly by evaluating a multivariate normal CDF of dimension $(m-1)$, or 3 in this case. However, this is still nontrivial (though there may be libraries for it), and will underflow for many variables.</p>

<p>Perhaps we can divide the probability up as follows:</p>

<p>$$\Pr(N_3 &gt; N_1 &gt; N_2 &gt; N_4) = $$
$$\Pr(N_3 &gt; N_1 \mid N_1 &gt; N_2, N_2 &gt; N_4 )\Pr(N_1 &gt; N_2 \mid N_2 &gt; N_4 )\Pr(N_2 &gt; N_4)$$</p>

<p>Being able to compute the probabilities of each part directly would make it very easy to compute the log probability simply by adding. We can compute the conditional probabilities separately using the MVN CDF method, which could help if the product might underflow.</p>

<p>Another observation: the $m!$ possible probabilities corresponding to the different permutations must sum to 1. Perhaps there is a way to compute the probabilities iteratively or using dynamic programming: i.e.: $(N_2 &gt;  N_3)$, an ordering over a pair, has some fixed probability, which is further divided into three values by the three possible places to insert $N_1$ into the ordering, further divided into the four values by the possible places to insert $N_3$. This is semantically equivalent to the conditional probabilities above but it might be easier to think of it this way.</p>

<p>Any math wizards have suggestions on how to solve this problem? I would greatly appreciate any ideas!</p>
",<probability>
"<p>I am not a mathematics guy, and the question which I am gonna ask would be pretty simple for mathematicians. In fact I am doing research and was reading some blogs. I wanted to derive the density function for $n$ number of independent variables. following relation I find on Internet for calculating CDF of $n$ number of independent variables
$$
F(T) = 1 – (1 - F_1(T)) (1 - F_2(T)) \dots (1- F_n(T)) = 1 – \prod_{i=1}^n F_i (T)
$$
 but for PDF they says that take the derivative of it . In fact I learned mathematics decades ago, so I don't know how to do that. Can some one just help me in steps how to take derivate of above equation and finally what would be the Density function?</p>
",<probability>
"<p>I need to show that if $E_1,E_2,\ldots, E_n$ are independent then $E_1^c ,E_2^c,\ldots, E_n^c$ are independent too. Please provide a hint.</p>
",<probability>
"<p>First, let's recall the definitions of 4 different types of convergence:almost surely, in $r$th mean, in probability and in distribution:</p>

<ol>
<li>$X_n\xrightarrow{a.s.}X$ if $\{\omega \in \Omega:X_n(\omega)\rightarrow X(\omega),$ as $n\rightarrow\infty\}$ is an event with probability 1.</li>
<li>$X_n\xrightarrow{L^r} X$ if $\forall r\geq1,\mathbb{E}[X_n^r]&lt;\infty$, $\mathbb{E}[|X_n-X|^r]\rightarrow 0$, as $n\rightarrow \infty$</li>
<li>$X_n\xrightarrow{p}X$ if $\forall \epsilon &gt;0,\mathbb{P}(|X_n-X|&gt;\epsilon)\rightarrow 0,$ as $n\rightarrow \infty$</li>
<li>$X_n\xrightarrow{D}X$ if $\mathbb{P}(X_n\leq x)\rightarrow \mathbb{P}(X\leq x)$ as $n\rightarrow \infty$</li>
</ol>

<p>In Wikipedia I got the following implication relations:</p>

<ol>
<li>If $s&gt;r\geq1,(X_n\xrightarrow{L^s} X) \Rightarrow (X_n\xrightarrow{L^r}X)$</li>
<li>Convergence almost surely and convergence in mean both imply convergence in probabilty.</li>
<li>$(X_n\xrightarrow{p}X)\Rightarrow (X_n\xrightarrow{D}X)$ (Convergence in probability implies convergence in distribution)</li>
</ol>

<p>So, what I want to ask here is that if somebody can give me some simple examples to briefly explain why the implication works and some counter examples why it doesn't work conversely(the other direction of the implication arrow), because all those definitions look so similar to me, especially, for example, why convergence in probability doesn't imply convergence almost surely? For the definitions of them are really the same thing.</p>

<p>I'll really appreciate if you can help me out. Thanks!</p>
",<probability>
"<blockquote>
  <p>An insurance company examines its pool of auto insurance customers and gathers the following information:</p>
  
  <p>(i)  All customers insure at least 1 car</p>
  
  <p>(ii)  64% of all customers insure more than one car</p>
  
  <p>(iii)  20% of the customers insure a sports car</p>
  
  <p>(iv)  Of those customers who insure more than one car, 15% insure a sports car.  </p>
  
  <p>What is the probability that a randomly selected customer insures exactly one car, and that car is not a sports car?</p>
</blockquote>

<p>Let's use the following variable definitions:</p>

<p>O= owns 1 car, O' = owns more than 1 car</p>

<p>S= sports car, S' = Not sports car.  </p>

<p>N() = Cardinal Number of a set</p>

<hr>

<p>From statements (i)-(iii), we get the following:  $N(O') = 64, N(O) = 36, N(S) = 20$</p>

<p>From statement (iv):  $\Pr(S \mid O')=15$</p>

<p>We are asked to find $\Pr(S' \mid O)$</p>

<hr>

<p>By definition:<br>
$Pr(S' \mid O) = \cfrac{\Pr(S' \cap O)}{\Pr(O)}=\cfrac{\Pr(S' \cap O)}{N(O)}\tag{1}$</p>

<p>Pr()=N() since this is a uniform distribution--I interpret this when it says ""randomly selected""</p>

<p>Next, I did: </p>

<p>$N(S)=N(S \cap O') + N(S \cap O)\tag{2}$</p>

<p>$0.2 = .64*.15 + .36 * x$</p>

<p>$x=0.28$, but we want 1-x because we want S' in $\Pr(S' \cap O)$ which equals 0.72.</p>

<p>See diagram below:</p>

<p><img src=""http://i.stack.imgur.com/n5UVW.png"" alt=""Tree diagram""></p>

<p>So plugging back into (1):  </p>

<p>$Pr(S' \mid O) = \cfrac{0.71*.36}{.36}=.71$</p>

<p>but the answer is .26.  </p>

<p><strong>I mainly wanted to know why equation (2) is wrong.  I know that is the crux of the problem.  Why can't I use that equation in this case?</strong></p>

<p>Any help is appreciated.  Thank you.</p>
",<probability>
"<p>$\newcommand{\E}{\operatorname{\mathbb E}}$
$\newcommand{\Var}{\operatorname{\mathbb Var}}$
If $\E[X] = {^1\!/\!_3}(\E[X\mid Y=1] + \E[X\mid Y=2] + \E[X\mid Y=3]) = 10$</p>

<p>Where $\E[X|Y=1] = 2,\; \E[X|Y=2] = 3+\E[X],\; \E[X|Y=3] = 5+\E[X]$</p>

<p>is $\E[X^2|Y=1] = 4,\; \E[X^2|Y=2] = 9 + 6\E[X] + 6\E[X^2],$ and so on?</p>

<p>This is to find the $\Var(X)$.</p>

<p>where $\Var(X) = \E[X^2] - (\E[X])^2$</p>

<p>Question: How do you find the Variance of this given that $\E[X] = 10$?</p>
",<probability>
"<p>The characteristic function of a random variable $X$ is given as:</p>

<p>$$E(e^{jvX}) = \int _{-\infty} ^{\infty} e^{jvx} p(x) dx $$</p>

<p>This is interpreted as either mean of function $e^{jvx}$ or fourier transform of pdf $p(x)$. I know that $x$ represents random variable, but what does $v$ represents? What is its physical significance?</p>
",<probability>
"<p>Let $m$ be a finite measure on $X \subseteq \mathbb{R}^n$, so that $m(\mathbb{R}^n) &lt; \infty$.</p>

<p>Define the hyperplanes on $\mathbb{R}^n$, parametrized by $A \in \mathbb{R}^{n \times n}$ and $b \in \mathbb{R}^n$, as
$$ H(A,b) := \{ x \in \mathbb{R}^n \mid A x = b \}. $$</p>

<p>Take many ""extractions"" $y_1, y_2, ...$ from $m$. </p>

<p>Say under what conditions on $m$ no more than $n$ i.i.d. extractions $y_i$'s belong to the same hyperplane almost surely, i.e.</p>

<p>$$ m^{n+1}\left( \left\{ (y_1, y_2, ..., y_{n+1}) \in X^{n+1} \mid \\
\exists (A,b) \in (\mathbb{R}^{n \times n} \times \mathbb{R}^n) \text{ such that } y_1, y_2, ..., y_{n+1} \in H(A,b)  \right\} \right) = 0,$$</p>

<p>where $m^{n+1} := m \times m \times \cdots \times m$ ($n+1$ times) is the product measure.</p>

<p>I was thinking about merely atomless $m$, but I then thought it may not be enough.</p>
",<probability>
"<p>I've been struggling for a while to understand the meaning of liminf of a sequence of sets. </p>

<p>I know that the definition is </p>

<p>$\liminf_{n\to\infty}A_n:=\bigcup_{n\in\mathbb{N}}\bigcap_{m\geq n} A_n$.</p>

<p>When I break it up into pieces, I get $(A_1 \cap A_2 \cap A_3 \cap A_4 \cap \ldots) \cup (A_2 \cap A_3 \cap A_4 \ldots) \cup (A_3 \cap A_4 \cap \dots) \cup (A_4 \cap \ldots) \cup \ldots$.</p>

<p>Here, $A_n$ occurs infinitely many times, doesn't it? Because it is in each set of parentheses?</p>

<p>Or do the unions mean <strong>OR</strong>, so that $A_n$ might or might not be in any of the sets? </p>

<p>However, this is wrong! Why? I'm confused.</p>
",<probability>
"<p>I am studying stochastic processes and have stumbled on a result that is puzzling me. I have searched elsewhere for an answer without luck so hoping some proper mathematicians here can explain the result for me.</p>

<p>Given a two-state Markov process with probability transition matrix
$$
\begin{array}{c|c}
&amp;\begin{matrix}0&amp;1\end{matrix}\\
\hline
\begin{matrix}0\\ 1\end{matrix}
&amp;\pmatrix{a&amp;b\\ c&amp;d}
\end{array}
$$</p>

<p>I have found that the simplest way to calculate its steady-state probability distribution is :</p>

<p>state 0: $c \over {b + c}$</p>

<p>state 1: $b \over {b + c}$</p>

<p>This result holds for all examples I have tried, but I have been unable to explain it from theory, so cannot prove it. My questions are:</p>

<ol>
<li>what is the theoretical explanation for this result?</li>
<li>does it extend to any $n\times n$ transition matrix?</li>
</ol>
",<probability>
"<p>So the exercise is this:</p>

<p>We have and infinite chessboard and we have a coin. Every grid is of length and width $a$, whereas the coin has diameter $2 \cdot r&lt;a$. We throw a coin into a chessboard and we want to know with what probability the coin will falll into the grid.</p>

<p><img src=""http://i.stack.imgur.com/y9SI1.jpg"" alt=""enter image description here""> </p>

<p>So let $S_{1}$=area of green rectangle, $S_{2}=S_{1}+$ area of the red border.
So the probability in my opinion is 
$ P(a)= \frac{S_{1}}{S_{2}} $.</p>

<p>Is this in any shape or form correct?</p>
",<probability>
"<p>I'm reading a book with chances and probability.
The book has the following problem:
""The draw for the fifth round of the FA Cup is about to be made.  There are 16 teams, leading to eight matches.  Your task is to pair the teams off, in an attempt to guess as many as possible off the actual matches in the real Cup draw.  You are not asked which teams will be drawn at home, just which pairs will be selected.  I am prepared to pay you $1.50 for each correct guess; how much would you be prepared to offer for the right to make the eight guesses?""
(What is the minimum fair entrance fee?)</p>

<p>I would like pointers to solve this by using ordinary combinatorics, if possible.  (Author uses the idea: sum of the averages is the averages of the sum, which it might be faster, but I would be jumping to an easy way of solving this without first analyzing/understand/adopt it).</p>

<p>I have tried the following:<br>
First Match arrangements (16x15)=240<br>
Second Match arrangements (14x13)=182<br>
Third Match arrangements (12x11)=132<br>
Fourth Match arrangements (10x9)=90<br>
Fifth Match arrangements (8x7)=56<br>
Sixth Match arrangements (6x5)=30<br>
Seventh Match arrangemets (4x3)=12<br>
Eighth Match arrangements (2x1)=2<br>
So I get 744 possible arrangements.  </p>

<p>I'm starting to question my above approach because on the Eighth Match arrangements, I have 2 possible ways, when intuitively I know there is only one way (it is the last matching pair), so I know I'm missing something but cannot point it down.</p>

<p>Thanks in advance!</p>
",<probability>
"<p>$\newcommand{\Var}{\operatorname{Var}}$
$\newcommand{\Cov}{\operatorname{Cov}}$</p>

<p>I am supposed to show the following:</p>

<p>$$
\Var(Y-E(Y\mid X)) = E(\Var(Y\mid X))
$$</p>

<p>My attempt involved using simple property of conditional expectations and variances, I get a close result, but not quite:</p>

<p>$$
\Var(Y-E(Y\mid X)) = \Var(Y) + \Var(E(Y\mid X)) - 2\Cov(Y,E(Y\mid X))
$$</p>

<p>Since $\Cov(Y,E(Y\mid X)) = \Cov(Y,Y) = \Var(Y)$, then:</p>

<p>$$
\Var(Y) + \Var(E(Y\mid X)) - 2\Cov(Y,E(Y\mid X)) = \Var(E(Y\mid X)) - \Var(Y)
$$</p>

<p>But $\Var(Y) = \Var(E(Y\mid X)) - E(\Var(Y\mid X))$ and then:</p>

<p>$$
\Var(E(Y\mid X)) - \Var(Y) = \Var(E(Y\mid X)) - \Var(E(Y\mid X)) - E(\Var(Y\mid X)) = -E(\Var(Y\mid X))
$$</p>

<p>Thanks!</p>
",<probability>
"<p>Two n bit binary strings S1 and S2 are chosen randomly with uniform probability.The probability that the Hamming distance in between these strings (the number of bit positions where the two strings differ) is equal to d is</p>

<pre><code>1)nCd/2^n 
2)nCd/2^d
3)d/2^n 
4)1/2^d
</code></pre>

<p>? Choose the right answer.(Sorry for ambiguity).
I tried to solve the problem ,but indeed didn't find any suitable way to tackle this problem.</p>
",<probability>
"<p>I have the following problem. I'm struggling a little bit with the expression $P(X \in A)$. My problem is that $A$ is a set, whereas $X$ is a function. I can not really related this two items. </p>

<p>Here are some related definitions from the textbook I use. </p>

<ul>
<li>Sample space: $\Omega$ </li>
<li>Outcome: $\omega$</li>
<li>Event: $A, (A \subset \Omega) $</li>
<li>Random variable:  $X: \Omega \rightarrow R $ $\big($X assigns a real number to each outcome ($X(\omega)$ $\big)$.</li>
</ul>

<p>I tried to work out a simple example: Toss a coin two times and let X be the number of heads. Then: $\Omega = \{HH,HT,TH,TT\} $. For example if we have a look on the prob. that we get two heads, $P(X=2)=\frac{|\{HH\}|}{|\Omega|}=\frac{1}{4}$, it makes kind of sense to me, because we compare the cardinality of of two sets. However, I can not imagine a Example of $P(X \in A)$.</p>

<p>Can maybe someone give some intuition for that or give a small example in the case of a coin toss (or dice or ...). </p>

<p>cheers!</p>
",<probability>
"<blockquote>
  <p>Suppose that $X$ and $Y$ are independent and identically distributed:
  $$P (X = k) = P (Y = k) = ρ (1 − ρ)^k$$
  for $k = 0, 1, \dots$ and let $Z := X + Y$. Find the joint distribution of $(X, Z)$ and find the conditional distribution of $X$, given $Z = n$.</p>
</blockquote>

<p>I need a little help just setting this up.</p>

<p>If I understand the question, I'm looking for $P (X = k, Z = n)$ and $P (X = k \mid Z = n)$. I'm a little confused, though, because I've never done this when I have a variable defined as a linear function of the other random variables. I also don't really understand what $n$ is.</p>

<p>Any hints or help getting started is appreciated.</p>
",<probability>
"<p>Suppose that $Y$ has the binomial distribution, $Bin(20, 0.25)$ and conditioned on $Y$, a random variable $X$ that has the binomial distribution, $Bin(Y, 0.5)$. </p>

<p>How can I derive the $k$th moment of $X$?</p>

<p>I know the general case:</p>

<p>$E(X)=∑_{x∈Ω_X}x^kP[X=x]$, and I know that just for one binomial random variable, $E[X]$ is $np$, but I'm not sure how to deal with $X$, which is conditioned on a second variable.</p>
",<probability>
"<p>So the question is: given that you roll $10$ dice, what is the probability of the sum of the total dice rolls adding up to $57$? </p>

<p>I know that there are three ways to do this:</p>

<ol>
<li>Seven die rolls must be $6$ with three $5$s</li>
<li>Eight die rolls must be $6$, one die roll must be $5$ and one must be $4$</li>
<li>Nine die rolls must be $6$, and one roll must be $3$</li>
</ol>

<p>The solution states that the probability of the events are:</p>

<ol>
<li>$ \binom{10}{3} \cdot \frac{1}{6^{10}}$</li>
<li>$ \binom{10}{1} \cdot \binom{9}{1} \cdot \frac{1}{6^{10}} $</li>
<li>$ 10 \cdot 9 \cdot \frac{1}{6^{10}} $</li>
</ol>

<p>I really don't understand why the probabilities work this way. I would really, really appreciate it if someone could perhaps explain this in a more intuitive way for me. </p>

<p>Edit: I am really sorry for the mistake. Edited so that the question reads sum up to 57. </p>

<p>Edit 2: Also, I think my solution sheet is missing the fact that you should sum all of these probabilities and set them over $6^{10}$. I apologize for the mess and I appreciate all the comments that pointed this out.</p>
",<probability>
"<p>We have that $N$ and $X_1, X_2, \dots$ are all independent. We also have $\operatorname{E} [X_j] = \mu$ and $\operatorname{Var}[X_j] = σ^2$.</p>

<p>Then, we introduce an integer–valued random variable, $N$, which is the random sum such that:
$$Z = \sum_{j=1}^{N+1}X_j.$$
Assuming that $N$ is distributed $\sim\mathrm{Poisson}(\lambda)$, what is the first moment and what is the variance of $Z$?</p>

<p>For a normal Poisson distribution, I know the variance is just $\lambda$, as is the mean. I'm having trouble understanding the implication of having the bounds be poisson distributed. Normally, I would just say ""variance of the sum is the sum of the variance,"" but I don't think that's how it works with random sums. Any hints/guidance appreciated.</p>
",<probability>
"<p>I know how to calculate expected value for a single roll, and I read several other answers about expected value with rerolls, but how does the calculation change if you can make your reroll before choosing which die to keep?</p>

<p>For instance, what is the expected value of rolling $2$ fair $6$-sided dice and keeping the higher value?  And can you please generalize to $n$ $x$-sided dice?</p>
",<probability>
"<p>I've been exercising probability and came across the following problem:</p>

<p>A club has $N$ members, where $N$ is a random variable with probability-mass-function: $p_N(n)=p^{n-1}(1-p)$.</p>

<p>Each member has a probability $q$ to show up and all the ""showing-up""s and $N$ are pairwise independent.</p>

<p>If $A$ is a r.v. telling how many members will show up, I need to compute the variance of $A$.</p>

<p><strong>Could you please tell me why the following part of the official solution is correct:</strong></p>

<blockquote>
  <p>$\text{var}(A) = E\ [N]\cdot \text{var}(B) + (E \ [B])^2\cdot\text{var}(N)$</p>
  
  <p>where $B$ is a Bernoulli random variable describing whether a member turns up or not.</p>
</blockquote>

<p>I am familiar with the law of total variance: $\text{var}(X) = E\ [\text{var}(X|Y)]+\text{var}(E\ [X|Y])$</p>

<p>and with the law of iterated expectations: $E\ [X] = E\ [\ E\ [X|Y]\ ]$,</p>

<p>but I can't see any connection between any of them and the statement I want to know the justification of. Thanks!</p>

<p>PS It probably has something to do with expressing $A$ as a sum of $A_1+\ldots+A_n$ where $A_i$ is an indicator function of whether the $i$th member shows up, but still, what do I do with $\text{var}(A) = \text{var}(A_1+\ldots+A_n | N=n)$?</p>
",<probability>
"<p>The chance that a team wins a game when loosing 3+ turnovers is 13.3% if their opponent is not the Lakers. If
their opponent is the Lakers, the percentage of winning is 24%.</p>

<p>There are 30 teams in total. What is the likelihood of a team winning a game
if their opponent is <strong>not</strong> the Lakers.</p>

<p>I have done the following: <br>
P(Playing not the Lakers) = 28/29 or .9655 <br>P(Playing the Lakers) = 1/29 or .0345<br> P(Win | Playing the Lakers) = .133<br> P(Win | Not the Lakers) = .24<br><br>
However, now that I calculated these values, I am unsure of where to go with them. Any help or elaboration would be much appreciated. Thank you.</p>
",<probability>
"<p>I have $S=R+\epsilon$ where $R \sim Cauchy(r, 1/\alpha)$ and $\epsilon \sim Cauchy(0, 1/\beta)$. I want to calculate the distribution of $R$ given $S=s$. </p>

<p>I've tried the following:</p>

<p>By the definition of conditional probability, we have $f_{R | s}(x | s) = \frac{f_R(x) f_\epsilon(s-x)}{f_{R+\epsilon}(s)}$. </p>

<p>We know that for $X \sim Cauchy (x_0, \gamma)$ we have the density function
$f(x) = \frac{1}{\pi \gamma} \frac{\gamma^2}{(x-x_0)^2 + \gamma^2}$</p>

<p>Then plugging this in (and simplifying a bit), we get</p>

<p>$$f_{R | s}(x | s) = \frac{1}{\pi (\frac{1}{\alpha + \beta})}
\frac{\frac{1}{(\alpha+\beta)^2}}{\frac{((x-r)^2+\frac{1}{\alpha^2})((s-x)^2+\frac{1}{\beta^2})}{(x-r)^2+(\frac{\alpha+\beta}{\alpha \beta})^2
}
}
$$</p>

<p>I can't simplify the part on the bottom to get what would be desired if it were a Cauchy distribution. Any hints/tips? Thank you!</p>
",<probability>
"<p>Given a process $X_n \xrightarrow{d} X$ on some probability space $(\Omega,\mathcal{A},P)$. If for every $B \in \mathcal{A}$ it holds, that
$$
\lim_{n\rightarrow \infty} P(X_n\in A,B)=P(X\in A)P(B)
$$
and any set $A$ of continuity of the distribution function of $X$, we say the convergence is renyi-mixing in the classical discrete time sense. 
This is shown to be equivalent to: Fix $m$ and for $B\in \sigma(X_{1},\ldots,X_{m})$ with $P(B)&gt;0$ then
$$
\lim_{n\rightarrow \infty} P(X_n\in A|B)=P(X\in A)
$$</p>

<p>Now lets say we have a continuous time proces $X_t$ with $X_t\xrightarrow{d}X$. If we want the convergence to be renyi-mixing, is it sufficient to show, that for any countable grid $(t_i)_{i\in\mathbb{N}}$ with $t_{i}\rightarrow \infty$ as $i\rightarrow \infty$
$$
\lim_{i\rightarrow \infty}P(X_{t_i}\in A,B)=P(X\in A)P(B)
$$
holds?</p>

<p>Best regards</p>
",<probability>
"<p>Let $X$ be Brownian motion on a Riemannian manifold $M$ starting at $x\in M$, D a domain and $f$ a bounded continuous function on $D$. Define $\tau_D$ to be the first exit time of $X$ from $D$. $u_f\left(t,x,y\right)=\int_Dp_D\left(t,x,y\right)f\left(y\right)dy$ solves</p>

<p>$
\begin{cases} 
L_Mu_f\left(t,x\right)=0, &amp; t&gt;0,x\in\overline{D}, \\
u_f\left(t,x\right)=0, &amp; t&gt;0,x\in\partial D, \\
\lim_{t\downarrow0}u_f\left(t,x\right)=f\left(x\right), &amp; x\in D.
\end{cases}
$</p>

<p>I have seen the following two formulae written:</p>

<p>$$\mathbb{P}_x\left(X_t\in C, t&lt;\tau_D\right)=\int_Cp_D\left(t,x,y\right)dy$$
and
$$E_x\left(f\left(X_t\right),t&lt;\tau_D\right)=\int_Dp_D\left(t,x,y\right)f\left(y\right)dy.$$</p>

<p>Is $\mathbb{P}_x$ the joint probability of the events $\{X_t\in C\}$ and $\{t&lt;\tau_D\}$? I also do not know what the interpretation for $E_x\left(f\left(X_t\right),t&lt;\tau_D\right)$ is. Any help is appreciated.</p>
",<probability>
"<p>Let $x$ and $y$ be two random variables with support of $\left[1\hspace{5pt}10\right]$  and $\left[50\hspace{5pt}90\right]$ respectively. The distribution of each of these variables is $p_X(x)$ and $p_Y(y)$ respectively. and their joint distribution is $p_{X,Y}(x,y)$. Now let us define two random variables as follows</p>

<p>\begin{equation}
z_1 = \frac{y}{x} \\
z_2 = x
\end{equation}</p>

<p>The distribution, $p_{Z_1}(z_1)$, of $z_1$ can be found as follows
\begin{equation}
p_{Z_1}(z_1) = \int_{-\infty}^{\infty} z_2p_{X,Y}(z_1z_2,z_2)dz_2
\end{equation}</p>

<p>My question is: </p>

<ol>
<li>Does the distribution $p_{Z_1}(z_1)$ depend upon the values of $x$, $y$ and $\frac{y}{x}$?</li>
<li>Does the distribution $p_{Z_1}(z_1)$ depend only upon $\frac{y}{x}$?</li>
</ol>

<p>The reason for asking this question is as follows:</p>

<p>Assume $y = 70$ and $x = 7$. Then $Z_1 = 10$. Since these values of $x$ and $y$ are within their support, the joint distribution $p_{X,Y}(x,y)$ will be zon-zero and so will be $p_{Z_1}(z_1)$. On the other hand, assume $y = 150$ and $x = 15$. Both these values are outside their support. But $Z_1$ is still $10$. $p_{X,Y}(x,y)$ will be close to zero. What can we say about $p_{Z_1}(z_1)$? In fact, for given $Z_1 = 10$, we can choose several values of $x$ and $y$ within their support and with different $p_{X,Y}(x,y)$. Does the choice of the values of $x$ and $y$ affect $p_{Z_1}(z_1)$?</p>
",<probability>
"<p>An urn contains 5 red and 6 blue and 8 green balls. 3 balls are randomly selected from the urn, find the probability that they are all of the different colors if the balls are drawn without replacement</p>
",<probability>
"<p>I'm trying to endow a set of probability measures $\triangle\left(X\right)
 $  with the weak * topology, where $X=\left\{ x_{1},\, x_{2},\,...,\, x_{N}\right\} \subseteq\mathbb{R}$ is a finite set of elements. Of course, this seems very straightforward because the set of probability measures can be identified with the n-dimensional simplex</p>

<p>$\hspace{5cm} S^{N}=\left\{ p\in\mathbb{R}^{N}\,|\, p\geq0,\,\underset{i}{\sum}p_{i}=1\right\}$ </p>

<p>My question is this identification. Is it ""natural"" to identify $\triangle\left(X\right)
 $ with $S^{N}$ (that is, treating each $p \in S^{N}$ as a probability measure defined as, for each $A \subset X$,  $p(A)=\underset{x_{i}\in A}{\sum}p_{i} $ ) and then give $S^{N}$ the topology with subbasis elements of the form </p>

<p>$\hspace{3cm}\left\{ p\in S^{N}\,|\,\left|\underset{i}{\sum}f\left(x_{i}\right)p_{i}-a\right|&lt;\varepsilon\right\},$ $\hspace{1cm} f:X \rightarrow \mathbb{R}$, $a \in \mathbb{R}$, and $\epsilon&gt;0$ </p>

<p>Thanks for your help. I know it's kind of a weird question I just want to make sure I'm staying consistent with definitions. </p>
",<probability>
"<p>Playing ""Rim World"", I noticed a geometric variable, but whose individual event probability increases with each attempt (or with time).  So I first model it with a simple probability, having a single failure parameter $q$.  The $n$th event has probability $1-q^n$ to succeed.  The probability to suceed at event $n$ is thus</p>

<p>\begin{equation}
P(n) = \left( \prod_{k=1}^{n-1} q^k \right) \cdot (1-q^n)
\end{equation}</p>

<p>My question is, what is the expected value of that random variable ?
Here is what I got so far :</p>

<p>\begin{align}
E[n] &amp;= \sum_{n\ge1} n\cdot \left( \prod_{k=1}^{n-1} q^k \right) \cdot (1-q^n)\\
     &amp;= \sum_{n\ge1} n\cdot \left( q^{\sum_{k=1}^{n-1} k} \right) \cdot (1-q^n)\\
     &amp;= \sum_{n\ge1} n\cdot q^{(n-1)\cdot n} \cdot (1-q^n)\\
     &amp;= \left( \sum_{n\ge1} n\cdot q^{(n-1)\cdot n} \right)
     -  \left( \sum_{n\ge1} n\cdot q^{(n-1)\cdot n^2} \right) \\
\end{align}
Now I'd like to do, $\zeta = q^n$ and $\xi = q^{n^2}$,
\begin{align}
E[n] &amp;= \left( \sum_{n\ge1} n\cdot \zeta^{(n-1)} \right)
     -  \left( \sum_{n\ge1} n\cdot \xi^{(n-1)} \right) \\
     &amp;= \frac{1}{(1-\zeta)^2} - \frac{1}{(1-\xi)^2}\\
\end{align}
But $\zeta$ and $\xi$ depends on $n$, so that's not a valid way of computing that.  How would you do it ?  Thank you for suggestions !</p>
",<probability>
"<p>I have read some introductory probability theory textbooks and found that for a continuous random variable, $P(x=a) =0\;\forall a$ , that means, whatever what possible outcome I choose, the possibility of it happens is zero. I found it strange, because it stated that no outcome is possible. (But I have no problem in understanding that it does not implies $P(whole sample space) = 0$)</p>

<p>Maybe the above interpretation is flawed, if yes, please correct me, thank you.</p>
",<probability>
"<p>Let's say I have a signal $y(t) = Acos(2\pi f_c t)$, where $f_c$ is the carrier frequency and $t$ is the independent variable. Since I work with discrete signals i sample this signal with a sampling rate $f_s = 100f_c$, so I obtain $y[n] = \big\{ t \rightarrow \frac{n}{100f_c}\big\} = Acos(\frac{\pi}{50} n)$. Now if we consider $x= \frac{\pi}{50}n$, we have $y = Acos(x)$ where $x \sim unif[-\pi,\pi]$ and $A$ is deterministic. Which is the probability density funciton of variable $y$, i.e. $f_y(y)$ ?</p>

<p>Thanks a lot.</p>
",<probability>
"<p>This is my second question following this <a href=""http://math.stackexchange.com/questions/771551/a-3-minute-algebra-problem"">post</a>.</p>

<blockquote>
  <p>Three players are playing a game. They all have small amounts of
  money, let say: player 1 has $\$a$, player 2 has $\$b$, and player 3
  has $\$c$, where $a&lt;b&lt;c$. The probability of each player wins each
  turn of the game is $p$ for player 1, $q$ for player 2, $r$ for player
  3, and $s$ for having draw, where $p+q+r+s=1$. The losers will transfer a dollar ($\$1$) to the winner for each turn. The game ends until one
  player has all the money. What is the probability of each player going
  bankrupt? What is the expected number of turns so that only one player
  left as the winner?</p>
</blockquote>

<p>Suppose that they play blackjack, if player 1 gets 20 points, player 2 gets 19 points, and player 3 gets 18 points, then the winner of that turn is player 1, so the other two players must pay a dollar to the player 1. If there are two players get, for example, 19 points and the another player gets 18 points, then that turn is considered draw. If they all get 19 points, this is also considered draw. <strong>If one player loses all the money, then he will stop playing and only two player will continue the game with probability of winning for each player is $x$ and $y$, also the probability of draw is $z$</strong>. Each turn will be repeated until one player has all the money.</p>

<p>To be honest, I can't answer this question and I really don't get it. I left my answer sheet totally empty for this one. (─‿‿─)</p>

<p>Please help me to answer this question and provide a simple explanation about the answer you submit. Every answer would be greatly appreciated.</p>
",<probability>
"<p>I am interested in probabilistic or explicit ways to construct an $\epsilon$-net of the $l_2$ unit ball in $\mathbb{R}^{d}$.</p>

<p>I know that, for every $\epsilon &gt; 0$, there exists an $\epsilon$-net $\mathcal{N}_{\epsilon}$ for the unit sphere in $d$ dimensions such that
$$
M\triangleq\left|\mathcal{N}_{\epsilon}\right|
\le \left( 1+\frac{2}{\epsilon}\right)^{d}.
$$
(Lemma 5.2 in <a href=""http://www-personal.umich.edu/~romanv/papers/non-asymptotic-rmt-plain.pdf"" rel=""nofollow"">http://www-personal.umich.edu/~romanv/papers/non-asymptotic-rmt-plain.pdf</a>)
To my understanding, the aforementioned bound holds for an $\epsilon$-net of the entire ball, not only the sphere.</p>

<p>In the case of the sphere, we can construct an $\epsilon$-net with high probability, 
by drawing a sufficient number ($O(M\log{M})$) of independent random vectors according to a Gaussian distribution $N(\mathbf{0}, \mathbf{I})$, and normalizing the length to $1$.
I believe that one way to get an $\epsilon$-net for the ball,
would be to repeat the above procedure $O(1/\epsilon)$ times, for all spheres of radii $\epsilon, 2\epsilon,3\epsilon, \dots, 1$.
The union of the $\epsilon$-nets, should be able to cover the ball.
However, it would require $\tilde{O}\left((1+2/{\epsilon})^{d+1}\right)$ points (ignoring the logarithmic factor).</p>

<ul>
<li>Is there a simple way to construct an $\epsilon$-net for the unit ball directly, $\textit{i.e.}$, without constructing nets for multiple spheres?</li>
<li>Is there way to achieve the bound on $\left|\mathcal{N}_{\epsilon}\right|$ (possibly up to logarithmic factors)?</li>
</ul>

<p>I would appreciate any pointers to either probabilistic or explicit methods.</p>
",<probability>
"<p><strong>First of 2 part query</strong>: I have observed a 100 doctors and recorded the number of times they touch surfaces while in a room:</p>

<p>\begin{array}{l|c|c|c|c}
\text{Dr number}&amp;\text{Bed}&amp;\text{Table}&amp;\text{Chair}&amp;\text{Door}\\
\text{Dr } 1&amp;3&amp;2&amp;2&amp;1\\
\text{Dr } 2&amp;4&amp;1&amp;0&amp;1\\
\vdots&amp;\vdots &amp;\vdots &amp;\vdots &amp;\vdots\\
\text{Dr } 100&amp; 2&amp;3&amp;1&amp;1
\end{array}</p>

<p>I would like to calculate the probability of him touching the Table for example. Would it be:</p>

<p>\begin{array}{l|c|c|c|c|c}
&amp;\text{Bed}&amp;\text{Table}&amp;\text{Chair}&amp;\text{Door}&amp;\text{Total}\\
\text{Average contacts}&amp;3&amp;2&amp;1&amp;1&amp;\text{7}\\
P(\text{surface})&amp;3/7&amp;2/7&amp;1/7&amp;1/7&amp;1
\end{array}</p>

<p>This is confusing me but I think a fresh pair of eyes will spot it instantly.</p>

<p><strong>2nd part</strong>: I gave questionnaires out to some doctors who ticked boxes instead of giving contact numbers, i.e.:</p>

<p>\begin{array}{l|c|c|c|c}
\text{Dr number}&amp;\text{Bed}&amp;\text{Table}&amp;\text{Chair}&amp;\text{Door}\\
\text{Dr}  &amp;\checkmark&amp;\checkmark&amp;\checkmark&amp;\checkmark\\
\text{Dr } \checkmark&amp;\checkmark&amp;\checkmark&amp;\times&amp;\checkmark\\
\vdots&amp;\vdots &amp;\vdots &amp;\vdots &amp;\vdots\\
\text{Dr } 100&amp; \checkmark&amp;\checkmark&amp;\checkmark&amp;\checkmark
\end{array}</p>

<p>What can I deduce from this table of ticks to compare with the table of exact values? Or is it not worth anything? </p>

<p>Please ask for any clarification if needed,
Regards</p>

<p><strong>EDIT: Revised Surface contact Probability</strong>  Should this replace table 2?</p>

<p>\begin{array}{l|c|c|c|c|c}
&amp;\text{Bed}&amp;\text{Table}&amp;\text{Chair}&amp;\text{Door}&amp;\text{Total}\\
\text{Total contacts}&amp;9&amp;6&amp;3&amp;3&amp;\text{21}\\
P(\text{surface})&amp;9/21&amp;6/21&amp;3/21&amp;3/21&amp;1
\end{array}</p>
",<probability>
